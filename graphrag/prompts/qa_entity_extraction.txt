
-Goal-
Given a text that is potentially relevant to this activity, first identify all entities needed from the text in order to capture the information and ideas in the text.

-Steps-
1. Identify all entities. For each identified entity, extract the following information:
- entity_name: Name of the entity, capitalized
- entity_type: [{entity_types}]
Format each entity as ("entity"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>)

2. Return output in The primary language of the provided text is "English." as a single list of all the entities identified in step 1. Use **{record_delimiter}** as the list delimiter.

4. If you have to translate into The primary language of the provided text is "English.", just translate the descriptions, nothing else!

5. When finished, output {completion_delimiter}.

######################
-Examples-
######################
Example 1:
Entity_types: CONCEPT
Text:
each other with equal probability 1/3 (it was trained on a training set with equal counts for the 3 colors), and a test set T = “red red red red blue”.
Let’s ﬁrst convince ourselves that if we compute the perplexity of this artiﬁcial digit language on this test set (or any such test set) we indeed get 3. By Eq. 3.15, the perplexity
For example if we use both begin and end tokens, we would include the end-of-sentence marker </s> but not the beginning-of-sentence marker <s> in our count of N; This is because the end-sentence token is followed directly by the begin-sentence token with probability almost 1, so we don’t want the probability of that fake transition to inﬂuence our perplexity.

--- Page 42 ---
42 CHAPTER 3 • N- GRAM LANGUAGE MODELS
######################
Output:
("entity"{tuple_delimiter}TRAINING SET{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}TEST SET{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}PERPLEXITY{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}N-GRAM LANGUAGE MODELS{tuple_delimiter}CONCEPT)
{completion_delimiter}

######################
Example 2:
######################
Entity_types: CONCEPT
Text:
Decoder
Encoder Block
Decoder Block Unembedding Matrix
Multi-Head Attention Layer Normalize Layer Normalize Feedforward
Causal Multi-Head Attention Layer Normalize Layer Normalize
Feedforward Layer Normalize Cross-Attention Language Modeling Head Henc

Figure 13.6 The transformer block for the encoder and the decoder. The ﬁnal output of the encoder Henc = h1, ... hn is the context used in the decoder. The decoder is a standard transformer except with one extra layer, the cross-attention layer, which takes that encoder output and uses it to form its K and V inputs.
That is, where in standard multi-head attention the input to each attention layer is X, in cross-attention the input is the the ﬁnal output of the encoder Henc = h1, ..., hn.
######################
Output:
("entity"{tuple_delimiter}ENCODER{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}DECODER{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}TRANSFORMER BLOCK{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}MULTI-HEAD ATTENTION{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}CROSS-ATTENTION{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}LAYER NORMALIZE{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}FEEDFORWARD{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}UNEMBEDDING MATRIX{tuple_delimiter}CONCEPT)
{record_delimiter}
("entity"{tuple_delimiter}LANGUAGE MODELING HEAD{tuple_delimiter}CONCEPT)
{completion_delimiter}

######################
-Real Data-
######################
Entity_types: {entity_types}
Text: {input_text}
######################
Output: