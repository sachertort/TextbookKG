--- Page -7 ---
Speech and Language Processing
An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition
with Language Models
Third Edition draft
Daniel Jurafsky
Stanford University
James H. Martin
University of Colorado at Boulder
Copyright ©2024. All rights reserved.
Draft of August 20, 2024. Comments and typos welcome!

--- Page -6 ---
Summary of Contents
I Fundamental Algorithms for NLP 1
1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Regular Expressions, Tokenization, Edit Distance . . . . . . . . . . . . . . . 4
3 N-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4 Naive Bayes, Text Classiﬁcation, and Sentiment . . . . . . . . . . . . . . . . . 56
5 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
6 Vector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
8 RNNs and LSTMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
9 The Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
10 Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
11 Masked Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
12 Model Alignment, Prompting, and In-Context Learning . . . . . . . . . 242
II NLP Applications 261
13 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
14 Question Answering, Information Retrieval, and RAG . . . . . . . . . . 289
15 Chatbots & Dialogue Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
16 Automatic Speech Recognition and Text-to-Speech . . . . . . . . . . . . . . 331
III Annotating Linguistic Structure 359
17 Sequence Labeling for Parts of Speech and Named Entities . . . . . . 362
18 Context-Free Grammars and Constituency Parsing . . . . . . . . . . . . . 387
19 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
20 Information Extraction: Relations, Events, and Time. . . . . . . . . . . . 435
21 Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
22 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 481
23 Coreference Resolution and Entity Linking . . . . . . . . . . . . . . . . . . . . . 501
24 Discourse Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531
Bibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
Subject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
2

--- Page -5 ---
Contents
I Fundamental Algorithms for NLP 1
1 Introduction 3
2 Regular Expressions, Tokenization, Edit Distance 4
2.1 Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4 Simple Unix Tools for Word Tokenization . . . . . . . . . . . . . 17
2.5 Word and Subword Tokenization . . . . . . . . . . . . . . . . . . 18
2.6 Word Normalization, Lemmatization and Stemming . . . . . . . . 23
2.7 Sentence Segmentation . . . . . . . . . . . . . . . . . . . . . . . 25
2.8 Minimum Edit Distance . . . . . . . . . . . . . . . . . . . . . . . 25
2.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 30
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3 N-gram Language Models 32
3.1 N-Grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.2 Evaluating Language Models: Training and Test Sets . . . . . . . 38
3.3 Evaluating Language Models: Perplexity . . . . . . . . . . . . . . 39
3.4 Sampling sentences from a language model . . . . . . . . . . . . . 42
3.5 Generalizing vs. overﬁtting the training set . . . . . . . . . . . . . 43
3.6 Smoothing, Interpolation, and Backoff . . . . . . . . . . . . . . . 45
3.7 Advanced: Perplexity’s Relation to Entropy . . . . . . . . . . . . 49
3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 52
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4 Naive Bayes, Text Classiﬁcation, and Sentiment 56
4.1 Naive Bayes Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . 57
4.2 Training the Naive Bayes Classiﬁer . . . . . . . . . . . . . . . . . 60
4.3 Worked example . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.4 Optimizing for Sentiment Analysis . . . . . . . . . . . . . . . . . 62
4.5 Naive Bayes for other text classiﬁcation tasks . . . . . . . . . . . 64
4.6 Naive Bayes as a Language Model . . . . . . . . . . . . . . . . . 65
4.7 Evaluation: Precision, Recall, F-measure . . . . . . . . . . . . . . 66
4.8 Test sets and Cross-validation . . . . . . . . . . . . . . . . . . . . 69
4.9 Statistical Signiﬁcance Testing . . . . . . . . . . . . . . . . . . . 70
4.10 Avoiding Harms in Classiﬁcation . . . . . . . . . . . . . . . . . . 73
4.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 75
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5 Logistic Regression 77
5.1 The sigmoid function . . . . . . . . . . . . . . . . . . . . . . . . 78
5.2 Classiﬁcation with Logistic Regression . . . . . . . . . . . . . . . 80
5.3 Multinomial logistic regression . . . . . . . . . . . . . . . . . . . 84
5.4 Learning in Logistic Regression . . . . . . . . . . . . . . . . . . . 87
3

--- Page -4 ---
4CONTENTS
5.5 The cross-entropy loss function . . . . . . . . . . . . . . . . . . . 88
5.6 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.7 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.8 Learning in Multinomial Logistic Regression . . . . . . . . . . . . 96
5.9 Interpreting models . . . . . . . . . . . . . . . . . . . . . . . . . 98
5.10 Advanced: Deriving the Gradient Equation . . . . . . . . . . . . . 98
5.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 100
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6 Vector Semantics and Embeddings 101
6.1 Lexical Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.2 Vector Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.3 Words and Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6.4 Cosine for measuring similarity . . . . . . . . . . . . . . . . . . . 110
6.5 TF-IDF: Weighing terms in the vector . . . . . . . . . . . . . . . 111
6.6 Pointwise Mutual Information (PMI) . . . . . . . . . . . . . . . . 114
6.7 Applications of the tf-idf or PPMI vector models . . . . . . . . . . 116
6.8 Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6.9 Visualizing Embeddings . . . . . . . . . . . . . . . . . . . . . . . 123
6.10 Semantic properties of embeddings . . . . . . . . . . . . . . . . . 124
6.11 Bias and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . 126
6.12 Evaluating Vector Models . . . . . . . . . . . . . . . . . . . . . . 127
6.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 129
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
7 Neural Networks 132
7.1 Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
7.2 The XOR problem . . . . . . . . . . . . . . . . . . . . . . . . . . 135
7.3 Feedforward Neural Networks . . . . . . . . . . . . . . . . . . . . 138
7.4 Feedforward networks for NLP: Classiﬁcation . . . . . . . . . . . 142
7.5 Training Neural Nets . . . . . . . . . . . . . . . . . . . . . . . . 145
7.6 Feedforward Neural Language Modeling . . . . . . . . . . . . . . 152
7.7 Training the neural language model . . . . . . . . . . . . . . . . . 155
7.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 157
8 RNNs and LSTMs 158
8.1 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . 158
8.2 RNNs as Language Models . . . . . . . . . . . . . . . . . . . . . 162
8.3 RNNs for other NLP tasks . . . . . . . . . . . . . . . . . . . . . . 165
8.4 Stacked and Bidirectional RNN architectures . . . . . . . . . . . . 168
8.5 The LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.6 Summary: Common RNN NLP Architectures . . . . . . . . . . . 174
8.7 The Encoder-Decoder Model with RNNs . . . . . . . . . . . . . . 174
8.8 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 182
9 The Transformer 184
9.1 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185

--- Page -3 ---
CONTENTS 5
9.2 Transformer Blocks . . . . . . . . . . . . . . . . . . . . . . . . . 190
9.3 Parallelizing computation using a single matrix X. . . . . . . . . 193
9.4 The input: embeddings for token and position . . . . . . . . . . . 196
9.5 The Language Modeling Head . . . . . . . . . . . . . . . . . . . 198
9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 201
10 Large Language Models 203
10.1 Large Language Models with Transformers . . . . . . . . . . . . . 204
10.2 Sampling for LLM Generation . . . . . . . . . . . . . . . . . . . 207
10.3 Pretraining Large Language Models . . . . . . . . . . . . . . . . 210
10.4 Evaluating Large Language Models . . . . . . . . . . . . . . . . . 214
10.5 Dealing with Scale . . . . . . . . . . . . . . . . . . . . . . . . . . 216
10.6 Potential Harms from Language Models . . . . . . . . . . . . . . 219
10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 220
11 Masked Language Models 223
11.1 Bidirectional Transformer Encoders . . . . . . . . . . . . . . . . . 223
11.2 Training Bidirectional Encoders . . . . . . . . . . . . . . . . . . . 226
11.3 Contextual Embeddings . . . . . . . . . . . . . . . . . . . . . . . 230
11.4 Fine-Tuning for Classiﬁcation . . . . . . . . . . . . . . . . . . . . 234
11.5 Fine-Tuning for Sequence Labelling: Named Entity Recognition . 237
11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 241
12 Model Alignment, Prompting, and In-Context Learning 242
12.1 Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
12.2 Post-training and Model Alignment . . . . . . . . . . . . . . . . . 248
12.3 Model Alignment: Instruction Tuning . . . . . . . . . . . . . . . . 249
12.4 Chain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . 254
12.5 Automatic Prompt Optimization . . . . . . . . . . . . . . . . . . . 255
12.6 Evaluating Prompted Language Models . . . . . . . . . . . . . . . 258
12.7 Model Alignment with Human Preferences: RLHF and DPO . . . 259
12.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 259
II NLP Applications 261
13 Machine Translation 263
13.1 Language Divergences and Typology . . . . . . . . . . . . . . . . 264
13.2 Machine Translation using Encoder-Decoder . . . . . . . . . . . . 268
13.3 Details of the Encoder-Decoder Model . . . . . . . . . . . . . . . 272
13.4 Decoding in MT: Beam Search . . . . . . . . . . . . . . . . . . . 274
13.5 Translating in low-resource situations . . . . . . . . . . . . . . . . 278
13.6 MT Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
13.7 Bias and Ethical Issues . . . . . . . . . . . . . . . . . . . . . . . 284
13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 286
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
14 Question Answering, Information Retrieval, and RAG 289

--- Page -2 ---
6CONTENTS
14.1 Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . 290
14.2 Information Retrieval with Dense Vectors . . . . . . . . . . . . . . 298
14.3 Answering Questions with RAG . . . . . . . . . . . . . . . . . . 301
14.4 Evaluating Question Answering . . . . . . . . . . . . . . . . . . . 304
14.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 306
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
15 Chatbots & Dialogue Systems 309
15.1 Properties of Human Conversation . . . . . . . . . . . . . . . . . 311
15.2 Frame-Based Dialogue Systems . . . . . . . . . . . . . . . . . . . 314
15.3 Dialogue Acts and Dialogue State . . . . . . . . . . . . . . . . . . 317
15.4 Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
15.5 Dialogue System Design . . . . . . . . . . . . . . . . . . . . . . . 325
15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 328
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
16 Automatic Speech Recognition and Text-to-Speech 331
16.1 The Automatic Speech Recognition Task . . . . . . . . . . . . . . 332
16.2 Feature Extraction for ASR: Log Mel Spectrum . . . . . . . . . . 334
16.3 Speech Recognition Architecture . . . . . . . . . . . . . . . . . . 339
16.4 CTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
16.5 ASR Evaluation: Word Error Rate . . . . . . . . . . . . . . . . . 346
16.6 TTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
16.7 Other Speech Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 353
16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 354
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
III Annotating Linguistic Structure 359
17 Sequence Labeling for Parts of Speech and Named Entities 362
17.1 (Mostly) English Word Classes . . . . . . . . . . . . . . . . . . . 363
17.2 Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . 365
17.3 Named Entities and Named Entity Tagging . . . . . . . . . . . . . 367
17.4 HMM Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . 369
17.5 Conditional Random Fields (CRFs) . . . . . . . . . . . . . . . . . 376
17.6 Evaluation of Named Entity Recognition . . . . . . . . . . . . . . 381
17.7 Further Details . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 384
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
18 Context-Free Grammars and Constituency Parsing 387
18.1 Constituency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
18.2 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 388
18.3 Treebanks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
18.4 Grammar Equivalence and Normal Form . . . . . . . . . . . . . . 394
18.5 Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
18.6 CKY Parsing: A Dynamic Programming Approach . . . . . . . . 397
18.7 Span-Based Neural Constituency Parsing . . . . . . . . . . . . . . 403

--- Page -1 ---
CONTENTS 7
18.8 Evaluating Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . 405
18.9 Heads and Head-Finding . . . . . . . . . . . . . . . . . . . . . . 406
18.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 408
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
19 Dependency Parsing 411
19.1 Dependency Relations . . . . . . . . . . . . . . . . . . . . . . . . 412
19.2 Transition-Based Dependency Parsing . . . . . . . . . . . . . . . 416
19.3 Graph-Based Dependency Parsing . . . . . . . . . . . . . . . . . 425
19.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
19.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 433
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
20 Information Extraction: Relations, Events, and Time 435
20.1 Relation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . 436
20.2 Relation Extraction Algorithms . . . . . . . . . . . . . . . . . . . 438
20.3 Extracting Events . . . . . . . . . . . . . . . . . . . . . . . . . . 446
20.4 Representing Time . . . . . . . . . . . . . . . . . . . . . . . . . . 447
20.5 Representing Aspect . . . . . . . . . . . . . . . . . . . . . . . . . 450
20.6 Temporally Annotated Datasets: TimeBank . . . . . . . . . . . . . 451
20.7 Automatic Temporal Analysis . . . . . . . . . . . . . . . . . . . . 452
20.8 Template Filling . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
20.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 459
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
21 Semantic Role Labeling 461
21.1 Semantic Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
21.2 Diathesis Alternations . . . . . . . . . . . . . . . . . . . . . . . . 462
21.3 Semantic Roles: Problems with Thematic Roles . . . . . . . . . . 464
21.4 The Proposition Bank . . . . . . . . . . . . . . . . . . . . . . . . 465
21.5 FrameNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
21.6 Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . 468
21.7 Selectional Restrictions . . . . . . . . . . . . . . . . . . . . . . . 472
21.8 Primitive Decomposition of Predicates . . . . . . . . . . . . . . . 476
21.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 478
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
22 Lexicons for Sentiment, Affect, and Connotation 481
22.1 Deﬁning Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . 482
22.2 Available Sentiment and Affect Lexicons . . . . . . . . . . . . . . 484
22.3 Creating Affect Lexicons by Human Labeling . . . . . . . . . . . 485
22.4 Semi-supervised Induction of Affect Lexicons . . . . . . . . . . . 487
22.5 Supervised Learning of Word Sentiment . . . . . . . . . . . . . . 490
22.6 Using Lexicons for Sentiment Recognition . . . . . . . . . . . . . 495
22.7 Using Lexicons for Affect Recognition . . . . . . . . . . . . . . . 496
22.8 Lexicon-based methods for Entity-Centric Affect . . . . . . . . . . 497
22.9 Connotation Frames . . . . . . . . . . . . . . . . . . . . . . . . . 497
22.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499

--- Page 0 ---
8CONTENTS
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 500
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
23 Coreference Resolution and Entity Linking 501
23.1 Coreference Phenomena: Linguistic Background . . . . . . . . . . 504
23.2 Coreference Tasks and Datasets . . . . . . . . . . . . . . . . . . . 509
23.3 Mention Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 510
23.4 Architectures for Coreference Algorithms . . . . . . . . . . . . . 513
23.5 Classiﬁers using hand-built features . . . . . . . . . . . . . . . . . 515
23.6 A neural mention-ranking algorithm . . . . . . . . . . . . . . . . 517
23.7 Entity Linking . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
23.8 Evaluation of Coreference Resolution . . . . . . . . . . . . . . . . 524
23.9 Winograd Schema problems . . . . . . . . . . . . . . . . . . . . . 525
23.10 Gender Bias in Coreference . . . . . . . . . . . . . . . . . . . . . 526
23.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 528
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
24 Discourse Coherence 531
24.1 Coherence Relations . . . . . . . . . . . . . . . . . . . . . . . . . 533
24.2 Discourse Structure Parsing . . . . . . . . . . . . . . . . . . . . . 536
24.3 Centering and Entity-Based Coherence . . . . . . . . . . . . . . . 540
24.4 Representation learning models for local coherence . . . . . . . . 544
24.5 Global Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . 546
24.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
Bibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 550
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552
Bibliography 553
Subject Index 585

--- Page 1 ---
Part I
FUNDAMENTAL ALGORITHMS FOR
NLP
In the ﬁrst part of the book we introduce the fundamental suite of algorithmic
tools that make up the modern neural language model that is the heart of end-to-end
NLP systems. We begin with tokenization and preprocessing, as well as useful algo-
rithms like computing edit distance, and then proceed to the tasks of classiﬁcation,
logistic regression, neural networks, proceeding through feedforward networks, re-
current networks, and then transformers. We’ll also see the role of embeddings as a
model of word meaning.

--- Page 3 ---
CHAPTER
1Introduction
La derni `ere chose qu’on trouve en faisant un ouvrage est de savoir celle qu’il faut
mettre la premi `ere.
[The last thing you ﬁgure out in writing a book is what to put ﬁrst.]
Pascal
3

--- Page 4 ---
4CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
CHAPTER
2Regular Expressions, Tok-
enization, Edit Distance
User: I am unhappy.
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY
User: I need some help, that much seems certain.
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
User: Perhaps I could learn to get along with my mother.
ELIZA: TELL ME MORE ABOUT YOUR FAMILY
User: My mother takes care of me.
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU
User: My father.
ELIZA: YOUR FATHER
User: You are like my father in some ways.
Weizenbaum (1966)
The dialogue above is from ELIZA , an early natural language processing system ELIZA
that could carry on a limited conversation with a user by imitating the responses of
a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple
program that uses pattern matching to recognize phrases like “I need X” and translate
them into suitable outputs like “What would it mean to you if you got X?”. This
simple technique succeeds in this domain because ELIZA doesn’t actually need to
know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is
one of the few dialogue genres where listeners can act as if they know nothing of the
world. ELIZA’s mimicry of human conversation was remarkably successful: many
people who interacted with ELIZA came to believe that it really understood them
and their problems, many continued to believe in ELIZA’s abilities even after the
program’s operation was explained to them (Weizenbaum, 1976), and even today
such chatbots are a fun diversion. chatbots
Of course modern conversational agents are much more than a diversion; they
can answer questions, book ﬂights, or ﬁnd restaurants, functions for which they rely
on a much more sophisticated understanding of the user’s intent, as we will see in
Chapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA
and other chatbots play a crucial role in natural language processing.
We’ll begin with the most important tool for describing text patterns: the regular
expression . Regular expressions can be used to specify strings we might want to
extract from a document, from transforming “I need X” in ELIZA above, to deﬁning
strings like $199 or$24.99 for extracting tables of prices from a document.
We’ll then turn to a set of tasks collectively called text normalization , in whichtext
normalization
regular expressions play an important part. Normalizing text means converting it
to a more convenient, standard form. For example, most of what we are going to
do with language relies on ﬁrst separating out or tokenizing words from running
text, the task of tokenization . English words are often separated from each other tokenization
by whitespace, but whitespace is not always sufﬁcient. New York androck ’n’ roll
are sometimes treated as large words despite the fact that they contain spaces, while
sometimes we’ll need to separate I’minto the two words Iandam. For processing
tweets or texts we’ll need to tokenize emoticons like:)orhashtags like#nlproc .

--- Page 5 ---
2.1 • R EGULAR EXPRESSIONS 5
Some languages, like Japanese, don’t have spaces between words, so word tokeniza-
tion becomes more difﬁcult.
Another part of text normalization is lemmatization , the task of determining lemmatization
that two words have the same root, despite their surface differences. For example,
the words sang ,sung , and sings are forms of the verb sing. The word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
Lemmatization is essential for processing morphologically complex languages like
Arabic. Stemming refers to a simpler version of lemmatization in which we mainly stemming
just strip sufﬁxes from the end of the word. Text normalization also includes sen-
tence segmentation : breaking up a text into individual sentences, using cues likesentence
segmentation
periods or exclamation points.
Finally, we’ll need to compare words and other strings. We’ll introduce a metric
called edit distance that measures how similar two strings are based on the number
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. Edit distance is an algorithm with applications throughout language process-
ing, from spelling correction to speech recognition to coreference resolution.
2.1 Regular Expressions
One of the most useful tools for text processing in computer science has been the
regular expression (often shortened to regex ), a language for specifying text searchregular
expression
strings. This practical language is used in every computer language, in text process-
ing tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a
regular expression is an algebraic notation for characterizing a set of strings. Reg-
ular expressions are particularly useful for searching in texts, when we have a pat-
tern to search for and a corpus of texts to search through. A regular expression corpus
search function will search through the corpus, returning all texts that match the
pattern. The corpus can be a single document or a collection. For example, the
Unix command-line tool grep takes a regular expression and returns every line of
the input document that matches the expression.
A search can be designed to return every match on a line, if there are more than
one, or just the ﬁrst match. In the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the ﬁrst
match. We’ll show regular expressions delimited by slashes but note that slashes are
notpart of the regular expressions.
Regular expressions come in many variants. We’ll be describing extended regu-
lar expressions ; different regular expression parsers may only recognize subsets of
these, or treat some expressions slightly differently. Using an online regular expres-
sion tester is a handy way to test out your expressions and explore these variations.
2.1.1 Basic Regular Expression Patterns
The simplest kind of regular expression is a sequence of simple characters; putting
characters in sequence is called concatenation . To search for woodchuck , we type concatenation
/woodchuck/ . The expression /Buttercup/ matches any string containing the
substring Buttercup ;grep with that expression would return the line I’m called lit-
tle Buttercup . The search string can consist of a single character (like /!/) or a
sequence of characters (like /urgl/ ) (see Fig. 2.1).
Regular expressions are case sensitive ; lower case /s/ is distinct from upper

--- Page 6 ---
6CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
Regex Example Patterns Matched
/woodchucks/ “interesting links to woodchucks and lemurs”
/a/ “Mary Ann stopped by Mona’s”
/!/ “You’ve left the burglar behind again! ” said Nori
Figure 2.1 Some simple regex searches.
case/S/ (/s/ matches a lower case sbut not an upper case S). This means that
the pattern /woodchucks/ will not match the string Woodchucks . We can solve this
problem with the use of the square braces [and]. The string of characters inside the
braces speciﬁes a disjunction of characters to match. For example, Fig. 2.2 shows
that the pattern /[wW]/ matches patterns containing either worW.
Regex Match Example Patterns
/[wW]oodchuck/ Woodchuck or woodchuck “Woodchuck ”
/[abc]/ ‘a’, ‘b’, or‘c’ “In uomini, in solda ti”
/[1234567890]/ any digit “plenty of 7 to 5”
Figure 2.2 The use of the brackets []to specify a disjunction of characters.
The regular expression /[1234567890]/ speciﬁes any single digit. While such
classes of characters as digits or letters are important building blocks in expressions,
they can get awkward (e.g., it’s inconvenient to specify
/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/ (2.1)
to mean “any capital letter”). In cases where there is a well-deﬁned sequence asso-
ciated with a set of characters, the brackets can be used with the dash ( -) to specify
any one character in a range . The pattern /[2-5]/ speciﬁes any one of the charac- range
ters2,3,4, or5. The pattern /[b-g]/ speciﬁes one of the characters b,c,d,e,f, or
g. Some other examples are shown in Fig. 2.3.
Regex Match Example Patterns Matched
/[A-Z]/ an upper case letter “we should call it ‘D renched Blossoms’ ”
/[a-z]/ a lower case letter “my beans were impatient to be hoed!”
/[0-9]/ a single digit “Chapter 1 : Down the Rabbit Hole”
Figure 2.3 The use of the brackets []plus the dash -to specify a range.
The square braces can also be used to specify what a single character cannot be,
by use of the caret ^. If the caret ^is the ﬁrst symbol after the open square brace [,
the resulting pattern is negated. For example, the pattern /[^a]/ matches any single
character (including special characters) except a. This is only true when the caret
is the ﬁrst symbol after the open square brace. If it occurs anywhere else, it usually
stands for a caret; Fig. 2.4 shows some examples.
Regex Match (single characters) Example Patterns Matched
/[^A-Z]/ not an upper case letter “Oyfn pripetchik”
/[^Ss]/ neither ‘S’ nor ‘s’ “Ihave no exquisite reason for’t”
/[^.]/ not a period “our resident Djinn”
/[e^]/ either ‘e’ or ‘ ^’ “look up ˆ now”
/a^b/ the pattern ‘ a^b’ “look up aˆ b now”
Figure 2.4 The caret ^for negation or just to mean ^. See below re: the backslash for escaping the period.

--- Page 7 ---
2.1 • R EGULAR EXPRESSIONS 7
How can we talk about optional elements, like an optional sinwoodchuck and
woodchucks ? We can’t use the square brackets, because while they allow us to say
“s or S”, they don’t allow us to say “s or nothing”. For this we use the question mark
/?/, which means “the preceding character or nothing”, as shown in Fig. 2.5.
Regex Match Example Patterns Matched
/woodchucks?/ woodchuck or woodchucks “woodchuck ”
/colou?r/ color or colour “color ”
Figure 2.5 The question mark ?marks optionality of the previous expression.
We can think of the question mark as meaning “zero or one instances of the
previous character”. That is, it’s a way of specifying how many of something that
we want, something that is very important in regular expressions. For example,
consider the language of certain sheep, which consists of strings that look like the
following:
baa!
baaa!
baaaa!
. . .
This language consists of strings with a b, followed by at least two a’s, followed
by an exclamation point. The set of operators that allows us to say things like “some
number of as” are based on the asterisk or *, commonly called the Kleene * (gen- Kleene *
erally pronounced “cleany star”). The Kleene star means “zero or more occurrences
of the immediately previous character or regular expression”. So /a*/ means “any
string of zero or more as”. This will match aoraaaaaa , but it will also match the
empty string at the start of Off Minor since the string Off Minor starts with zero a’s.
So the regular expression for matching one or more ais/aa*/ , meaning one afol-
lowed by zero or more as. More complex patterns can also be repeated. So /[ab]*/
means “zero or more a’s or b’s” (not “zero or more right square braces”). This will
match strings like aaaa orababab orbbbb , as well as the empty string.
For specifying multiple digits (useful for ﬁnding prices) we can extend /[0-9]/ ,
the regular expression for a single digit. An integer (a string of digits) is thus
/[0-9][0-9]*/ . (Why isn’t it just /[0-9]*/ ?)
Sometimes it’s annoying to have to write the regular expression for digits twice,
so there is a shorter way to specify “at least one” of some character. This is the
Kleene + , which means “one or more occurrences of the immediately preceding Kleene +
character or regular expression”. Thus, the expression /[0-9]+/ is the normal way
to specify “a sequence of digits”. There are thus two ways to specify the sheep
language: /baaa*!/ or/baa+!/ .
One very important special character is the period ( /./), awildcard expression
that matches any single character ( except a carriage return), as shown in Fig. 2.6.
Regex Match Example Matches
/beg.n/ any character between begandn begin , beg’n , begun
Figure 2.6 The use of the period .to specify any character.
The wildcard is often used together with the Kleene star to mean “any string of
characters”. For example, suppose we want to ﬁnd any line in which a particular
word, for example, aardvark , appears twice. We can specify this with the regular
expression /aardvark.*aardvark/ .

--- Page 8 ---
8CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
Anchors are special characters that anchor regular expressions to particular places anchors
in a string. The most common anchors are the caret ^and the dollar sign $. The caret
^matches the start of a line. The pattern /^The/ matches the word Theonly at the
start of a line. Thus, the caret ^has three uses: to match the start of a line, to in-
dicate a negation inside of square brackets, and just to mean a caret. (What are the
contexts that allow grep or Python to know which function a given caret is supposed
to have?) The dollar sign $matches the end of a line. So the pattern  $is a useful
pattern for matching a space at the end of a line, and /^The dog\.$/ matches a
line that contains only the phrase The dog. (We have to use the backslash here since
we want the .to mean “period” and not the wildcard.)
Regex Match
^ start of line
$ end of line
\b word boundary
\B non-word boundary
Figure 2.7 Anchors in regular expressions.
There are also two other anchors: \bmatches a word boundary, and \Bmatches
a non word-boundary. Thus, /\bthe\b/ matches the word thebut not the word
other . A “word” for the purposes of a regular expression is deﬁned based on the
deﬁnition of words in programming languages as a sequence of digits, underscores,
or letters. Thus /\b99\b/ will match the string 99inThere are 99 bottles of beer on
the wall (because 99 follows a space) but not 99inThere are 299 bottles of beer on
the wall (since 99 follows a number). But it will match 99in$99(since 99follows
a dollar sign ($), which is not a digit, underscore, or letter).
2.1.2 Disjunction, Grouping, and Precedence
Suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. In such a case, we might want to search for either the string cator
the string dog. Since we can’t use the square brackets to search for “cat or dog” (why
can’t we say /[catdog]/ ?), we need a new operator, the disjunction operator, also disjunction
called the pipe symbol|. The pattern /cat|dog/ matches either the string cator
the string dog.
Sometimes we need to use this disjunction operator in the midst of a larger se-
quence. For example, suppose I want to search for information about pet ﬁsh for
my cousin David. How can I specify both guppy andguppies ? We cannot simply
say/guppy|ies/ , because that would match only the strings guppy andies. This
is because sequences like guppy take precedence over the disjunction operator |. precedence
To make the disjunction operator apply only to a speciﬁc pattern, we need to use the
parenthesis operators (and). Enclosing a pattern in parentheses makes it act like
a single character for the purposes of neighboring operators like the pipe |and the
Kleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-
tion only to apply to the sufﬁxes yandies.
The parenthesis operator (is also useful when we are using counters like the
Kleene*. Unlike the |operator, the Kleene *operator applies by default only to
a single character, not to a whole sequence. Suppose we want to match repeated
instances of a string. Perhaps we have a line that has column labels of the form
Column 1 Column 2 Column 3 . The expression /Column [0-9]+ */ will not
match any number of columns; instead, it will match a single column followed by

--- Page 9 ---
2.1 • R EGULAR EXPRESSIONS 9
any number of spaces! The star here applies only to the space  that precedes it,
not to the whole sequence. With the parentheses, we could write the expression
/(Column [0-9]+ *)*/ to match the word Column , followed by a number and
optional spaces, the whole pattern repeated zero or more times.
This idea that one operator may take precedence over another, requiring us to
sometimes use parentheses to specify what we mean, is formalized by the operator
precedence hierarchy for regular expressions. The following table gives the orderoperator
precedence
of RE operator precedence, from highest precedence to lowest precedence.
Parenthesis ()
Counters * + ? {}
Sequences and anchors the ^my end$
Disjunction |
Thus, because counters have a higher precedence than sequences,
/the*/ matches theeeee but not thethe . Because sequences have a higher prece-
dence than disjunction, /the|any/ matches theoranybut not thany ortheny .
Patterns can be ambiguous in another way. Consider the expression /[a-z]*/
when matching against the text once upon a time . Since/[a-z]*/ matches zero or
more letters, this expression could match nothing, or just the ﬁrst letter o,on,onc,
oronce . In these cases regular expressions always match the largest string they can;
we say that patterns are greedy , expanding to cover as much of a string as they can. greedy
There are, however, ways to enforce non-greedy matching, using another mean- non-greedy
ing of the ?qualiﬁer. The operator *?is a Kleene star that matches as little text as *?
possible. The operator +?is a Kleene plus that matches as little text as possible. +?
2.1.3 A Simple Example
Suppose we wanted to write a RE to ﬁnd cases of the English article the. A simple
(but incorrect) pattern might be:
/the/ (2.2)
One problem is that this pattern will miss the word when it begins a sentence and
hence is capitalized (i.e., The). This might lead us to the following pattern:
/[tT]he/ (2.3)
But we will still incorrectly return texts with the embedded in other words (e.g.,
other ortheology ). So we need to specify that we want instances with a word bound-
ary on both sides:
/\b[tT]he\b/ (2.4)
Suppose we wanted to do this without the use of /\b/ . We might want this since
/\b/ won’t treat underscores and numbers as word boundaries; but we might want
to ﬁnd thein some context where it might also have underlines or numbers nearby
(the orthe25 ). We need to specify that we want instances in which there are no
alphabetic letters on either side of the the:
/[^a-zA-Z][tT]he[^a-zA-Z]/ (2.5)
But there is still one more problem with this pattern: it won’t ﬁnd the word thewhen
it begins a line. This is because the regular expression [^a-zA-Z] , which we used

--- Page 10 ---
10 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
to avoid embedded instances of the, implies that there must be some single (although
non-alphabetic) character before the the. We can avoid this by specifying that before
thethewe require either the beginning-of-line or a non-alphabetic character, and the
same at the end of the line:
/(^|[^a-zA-Z])[tT]he([^a-zA-Z]|$)/ (2.6)
The process we just went through was based on ﬁxing two kinds of errors: false pos-
itives , strings that we incorrectly matched like other orthere , and false negatives , false positives
false negatives strings that we incorrectly missed, like The. Addressing these two kinds of errors
comes up again and again in language processing. Reducing the overall error rate
for an application thus involves two antagonistic efforts:
• Increasing precision (minimizing false positives)
• Increasing recall (minimizing false negatives)
We’ll come back to precision and recall with more precise deﬁnitions in Chapter 4.
2.1.4 More Operators
Figure 2.8 shows some aliases for common ranges, which can be used mainly to
save typing. Besides the Kleene * and Kleene + we can also use explicit numbers as
counters, by enclosing them in curly brackets. The operator /{3}/ means “exactly
3 occurrences of the previous character or expression”. So /a\.{24}z/ will match
afollowed by 24 dots followed by z(but not afollowed by 23 or 25 dots followed
by a z).
Regex Expansion Match First Matches
\d [0-9] any digit Party of 5
\D [^0-9] any non-digit Blue moon
\w [a-zA-Z0-9_] any alphanumeric/underscore Daiyu
\W [^\w] a non-alphanumeric !!!!
\s [ \r\t\n\f] whitespace (space, tab) inConcord
\S [^\s] Non-whitespace in Concord
Figure 2.8 Aliases for common sets of characters.
A range of numbers can also be speciﬁed. So /{n,m}/ speciﬁes from ntom
occurrences of the previous char or expression, and /{n,}/ means at least noccur-
rences of the previous expression. REs for counting are summarized in Fig. 2.9.
Regex Match
* zero or more occurrences of the previous char or expression
+ one or more occurrences of the previous char or expression
? zero or one occurrence of the previous char or expression
{n} exactly noccurrences of the previous char or expression
{n,m} from ntomoccurrences of the previous char or expression
{n,} at least noccurrences of the previous char or expression
{,m} up to moccurrences of the previous char or expression
Figure 2.9 Regular expression operators for counting.
Finally, certain special characters are referred to by special notation based on the
backslash ( \) (see Fig. 2.10). The most common of these are the newline character newline
\nand the tabcharacter\t. To refer to characters that are special themselves (like
.,*,[, and\), precede them with a backslash, (i.e., /\./ ,/\*/ ,/\[/ , and/\\/ ).

--- Page 11 ---
2.1 • R EGULAR EXPRESSIONS 11
Regex Match First Patterns Matched
\* an asterisk “*” “K* A*P*L*A*N”
\. a period “.” “Dr. Livingston, I presume”
\? a question mark “Why don’t they come and lend a hand? ”
\n a newline
\t a tab
Figure 2.10 Some characters that need to be escaped (via backslash).
2.1.5 A More Complex Example
Let’s try out a more signiﬁcant example of the power of REs. Suppose our goal is
help a user buy a computer on the Web who wants “at least 6 GHz and 500 GB of
disk space for less than $1000”. To do this kind of retrieval, we ﬁrst need to be
able to look for expressions like 6 GHz or500 GB or$999.99 . Let’s work out some
regular expressions for this task.
First, let’s complete our regular expression for prices. Here’s a regular expres-
sion for a dollar sign followed by a string of digits:
/$[0-9]+/ (2.7)
Note that the $character has a different function here than the end-of-line function
we discussed earlier. Most regular expression parsers are smart enough to realize
that$here doesn’t mean end-of-line. (As a thought experiment, think about how
regex parsers might ﬁgure out the function of $from the context.)
Now we just need to deal with fractions of dollars. We’ll add a decimal point
and two digits afterwards:
/$[0-9]+\.[0-9][0-9]/ (2.8)
This pattern only allows $199.99 but not $199 . We need to make the cents optional
and to make sure we’re at a word boundary:
/(^|\W)$[0-9]+(\.[0-9][0-9])?\b/ (2.9)
One last catch! This pattern allows prices like $199999.99 which would be far too
expensive! We need to limit the dollars:
/(^|\W)$[0-9]{0,3}(\.[0-9][0-9])?\b/ (2.10)
Further ﬁxes (like avoiding matching a dollar sign with no price after it) are left as
an exercise for the reader.
How about disk space? We’ll need to allow for optional fractions again ( 5.5 GB );
note the use of ?for making the ﬁnal soptional, and the use of / */ to mean “zero
or more spaces” since there might always be extra spaces lying around:
/\b[0-9]+(\.[0-9]+)? *(GB|[Gg]igabytes?)\b/ (2.11)
Modifying this regular expression so that it only matches more than 500 GB is left
as an exercise for the reader.
2.1.6 Substitution, Capture Groups, and ELIZA
An important use of regular expressions is in substitutions . For example, the substi- substitution

--- Page 12 ---
12 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
tution operator s/regexp1/pattern/ used in Python and in Unix commands like
vimorsedallows a string characterized by a regular expression to be replaced by
another string:
s/colour/color/ (2.12)
It is often useful to be able to refer to a particular subpart of the string matching
the ﬁrst pattern. For example, suppose we wanted to put angle brackets around all
integers in a text, for example, changing the 35 boxes tothe<35>boxes . We’d
like a way to refer to the integer we’ve found so that we can easily add the brackets.
To do this, we put parentheses (and)around the ﬁrst pattern and use the number
operator\1in the second pattern to refer back. Here’s how it looks:
s/([0-9]+)/<\1>/ (2.13)
The parenthesis and number operators can also specify that a certain string or ex-
pression must occur twice in the text. For example, suppose we are looking for the
pattern “the Xer they were, the Xer they will be”, where we want to constrain the two
X’s to be the same string. We do this by surrounding the ﬁrst X with the parenthesis
operator, and replacing the second X with the number operator \1, as follows:
/the (.*)er they were, the \1er they will be/ (2.14)
Here the\1will be replaced by whatever string matched the ﬁrst item in parentheses.
So this will match the bigger they were, the bigger they will be but not the bigger
they were, the faster they will be .
This use of parentheses to store a pattern in memory is called a capture group . capture group
Every time a capture group is used (i.e., parentheses surround a pattern), the re-
sulting match is stored in a numbered register . If you match two different sets of register
parentheses, \2means whatever matched the second capture group. Thus
/the (.*)er they (.*), the \1er we \2/ (2.15)
will match the faster they ran, the faster we ran but not the faster they ran, the faster
we ate . Similarly, the third capture group is stored in \3, the fourth is \4, and so on.
Parentheses thus have a double function in regular expressions; they are used
to group terms for specifying the order in which operators should apply, and they
are used to capture something in a register. Occasionally we might want to use
parentheses for grouping, but don’t want to capture the resulting pattern in a register.
In that case we use a non-capturing group , which is speciﬁed by putting the specialnon-capturing
group
commands ?:after the open parenthesis, in the form (?: pattern ) .
/(?:some|a few) (people|cats) like some \1/ (2.16)
will match some cats like some cats but not some cats like some some .
Substitutions and capture groups are very useful in implementing simple chat-
bots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian
psychologist by carrying on conversations like the following:
User 1: Men are all alike.
ELIZA 1:IN WHAT WAY
User 2: They’re always bugging us about something or other.
ELIZA 2:CAN YOU THINK OF A SPECIFIC EXAMPLE
User 3: Well, my boyfriend made me come here.
ELIZA 3:YOUR BOYFRIEND MADE YOU COME HERE
User 4: He says I’m depressed much of the time.
ELIZA 4:I AM SORRY TO HEAR YOU ARE DEPRESSED

--- Page 13 ---
2.2 • W ORDS 13
ELIZA works by having a series or cascade of regular expression substitutions
each of which matches and changes some part of the input lines. After the input
is uppercased, substitutions change all instances of MYtoYOUR , and I’MtoYOU
ARE, and so on. That way when ELIZA repeats back part of the user utterance, it
will seem to be referring correctly to the user. The next set of substitutions matches
and replaces other patterns in the input. Here are some examples:
s/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \1/
s/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \1/
s/.* all .*/IN WHAT WAY/
s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/
Since multiple substitutions can apply to a given input, substitutions are assigned
a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we
return to the details of the ELIZA architecture in Chapter 15.
2.1.7 Lookahead Assertions
Finally, there will be times when we need to predict the future: look ahead in the
text to see if some pattern matches, but not yet advance the pointer we always keep
to where we are in the text, so that we can then deal with the pattern if it occurs, but
if it doesn’t we can check for something else instead.
These lookahead assertions make use of the (?syntax that we saw in the previ- lookahead
ous section for non-capture groups. The operator (?= pattern) is true ifpattern
occurs, but is zero-width , i.e. the match pointer doesn’t advance. The operator zero-width
(?! pattern) only returns true if a pattern does not match, but again is zero-width
and doesn’t advance the pointer. Negative lookahead is commonly used when we
are parsing some complex pattern but want to rule out a special case. For example
suppose we want to match, at the beginning of a line, any single word that doesn’t
start with “V olcano”. We can use negative lookahead to do this:
/^(?!Volcano)[A-Za-z]+/ (2.17)
2.2 Words
Before we talk about processing words, we need to decide what counts as a word.
Let’s start by looking at one particular corpus (plural corpora ), a computer-readable corpus
corpora collection of text or speech. For example the Brown corpus is a million-word col-
lection of samples from 500 written English texts from different genres (newspa-
per, ﬁction, non-ﬁction, academic, etc.), assembled at Brown University in 1963–64
(Kuˇcera and Francis, 1967). How many words are in the following Brown sentence?
He stepped out into the hall, was delighted to encounter
a water brother.
This sentence has 13 words if we don’t count punctuation marks as words, 15
if we count punctuation. Whether we treat period (“ .”), comma (“ ,”), and so on as
words depends on the task. Punctuation is critical for ﬁnding boundaries of things
(commas, periods, colons) and for identifying some aspects of meaning (question
marks, exclamation marks, quotation marks). For some tasks, like part-of-speech
tagging or parsing or speech synthesis, we sometimes treat punctuation marks as if
they were separate words.

--- Page 14 ---
14 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
The Switchboard corpus of American English telephone conversations between
strangers was collected in the early 1990s; it contains 2430 conversations averaging
6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey
et al., 1992). Such corpora of spoken language introduce other complications with
regard to deﬁning words. Let’s look at one utterance from Switchboard; an utter-
ance is the spoken correlate of a sentence: utterance
I do uh main- mainly business data processing
This utterance has two kinds of disﬂuencies . The broken-off word main- is disﬂuency
called a fragment . Words like uhandumare called ﬁllers orﬁlled pauses . Should fragment
ﬁlled pause we consider these to be words? Again, it depends on the application. If we are
building a speech transcription system, we might want to eventually strip out the
disﬂuencies.
But we also sometimes keep disﬂuencies around. Disﬂuencies like uhorum
are actually helpful in speech recognition in predicting the upcoming word, because
they may signal that the speaker is restarting the clause or idea, and so for speech
recognition they are treated as regular words. Because different people use differ-
ent disﬂuencies they can also be a cue to speaker identiﬁcation. In fact Clark and
Fox Tree (2002) showed that uhandumhave different meanings. What do you think
they are?
Perhaps most important, in thinking about what is a word, we need to distinguish
two ways of talking about words that will be useful throughout the book. Word types word type
are the number of distinct words in a corpus; if the set of words in the vocabulary is
V, the number of types is the vocabulary size jVj. Word instances are the total num- word instance
berNof running words.1If we ignore punctuation, the following Brown sentence
has 14 types and 16 instances:
They picnicked by the pool, then lay back on the grass and
looked at the stars.
We still have decisions to make! For example, should we consider a capitalized
string (like They ) and one that is uncapitalized (like they) to be the same word type?
The answer is that it depends on the task! They andthey might be lumped together
as the same type in some tasks, like speech recognition, where we care more about
the sequence of words and less about the formatting, while for other tasks, such
as deciding whether a particular word is a name of a person or location (named-
entity tagging), capitalization is a useful feature and is retained. Sometimes we keep
around two versions of a particular NLP model, one with capitalization and one
without capitalization.
Corpus Types =jVjInstances = N
Shakespeare 31 thousand 884 thousand
Brown corpus 38 thousand 1 million
Switchboard telephone conversations 20 thousand 2.4 million
COCA 2 million 440 million
Google n-grams 13 million 1 trillion
Figure 2.11 Rough numbers of wordform types and instances for some English language
corpora. The largest, the Google n-grams corpus, contains 13 million types, but this count
only includes types appearing 40 or more times, so the true number would be much larger.
1In earlier tradition, and occasionally still, you might see word instances referred to as word tokens , but
we now try to reserve the word token instead to mean the output of subword tokenization algorithms.

--- Page 15 ---
2.3 • C ORPORA 15
How many words are there in English? When we speak about the number of
words in the language, we are generally referring to word types. Fig. 2.11 shows
the rough numbers of types and instances computed from some English corpora.
The larger the corpora we look at, the more word types we ﬁnd, and in fact this
relationship between the number of types jVjand number of instances Nis called
Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978) after its discoverers Herdan’s Law
Heaps’ Law (in linguistics and information retrieval respectively). It is shown in Eq. 2.18, where
kandbare positive constants, and 0 <b<1.
jVj=kNb(2.18)
The value of bdepends on the corpus size and the genre, but at least for the large
corpora in Fig. 2.11, branges from .67 to .75. Roughly then we can say that the
vocabulary size for a text goes up signiﬁcantly faster than the square root of its
length in words.
It’s sometimes useful to make a further distinction. Consider inﬂected forms
likecats versus cat. We say these two words are different wordforms but have the
same lemma . Alemma is a set of lexical forms having the same stem, the same lemma
major part-of-speech, and the same word sense. The wordform is the full inﬂected wordform
or derived form of the word. The two wordforms catandcats thus have the same
lemma, which we can represent as cat.
For morphologically complex languages like Arabic, we often need to deal with
lemmatization. For most tasks in English, however, wordforms are sufﬁcient, and
when we talk about words in this book we almost always mean wordsforms (al-
though we will discuss basic algorithms for lemmatization and the related task of
stemming below in Section 2.6). One of the situations even in English where we
talk about lemmas is when we measure the number of words in a dictionary. Dictio-
nary entries orboldface forms are a very rough approximation to (an upper bound
on) the number of lemmas (since some lemmas have multiple boldface forms). The
1989 edition of the Oxford English Dictionary had 615,000 entries.
Finally, we should note that in practice, for many NLP applications (for example
for neural language modeling) we don’t actually use words as our internal unit of
representation at all! We instead tokenize the input strings into tokens , which can
be words but can also be only parts of words. We’ll return to this tokenization
question when we introduce the BPE algorithm in Section 2.5.2.
2.3 Corpora
Words don’t appear out of nowhere. Any particular piece of text that we study
is produced by one or more speciﬁc speakers or writers, in a speciﬁc dialect of a
speciﬁc language, at a speciﬁc time, in a speciﬁc place, for a speciﬁc function.
Perhaps the most important dimension of variation is the language. NLP algo-
rithms are most useful when they apply across many languages. The world has 7097
languages at the time of this writing, according to the online Ethnologue catalog
(Simons and Fennig, 2018). It is important to test algorithms on more than one lan-
guage, and particularly on languages with different properties; by contrast there is
an unfortunate current tendency for NLP algorithms to be developed or tested just
on English (Bender, 2019). Even when algorithms are developed beyond English,
they tend to be developed for the ofﬁcial languages of large industrialized nations

--- Page 16 ---
16 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
(Chinese, Spanish, Japanese, German etc.), but we don’t want to limit tools to just
these few languages. Furthermore, most languages also have multiple varieties, of-
ten spoken in different regions or by different social groups. Thus, for example,
if we’re processing text that uses features of African American English ( AAE ) or AAE
African American Vernacular English (AA VE)—the variations of English used by
millions of people in African American communities (King 2020)—we must use
NLP tools that function with features of those varieties. Twitter posts might use fea-
tures often used by speakers of African American English, such as constructions like
iont(I don’t in Mainstream American English ( MAE )), or talmbout corresponding MAE
to MAE talking about , both examples that inﬂuence word segmentation (Blodgett
et al. 2016, Jones 2015).
It’s also quite common for speakers or writers to use multiple languages in a
single communicative act, a phenomenon called code switching . Code switching code switching
is enormously common across the world; here are examples showing Spanish and
(transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens et al.
2017):
(2.19) Por primera vez veo a @username actually being hateful! it was beautiful:)
[For the ﬁrst time I get to see @username actually being hateful! it was
beautiful:) ]
(2.20) dost tha or ra- hega ... dont wory ... but dherya rakhe
[“he was and will remain a friend ... don’t worry ... but have faith”]
Another dimension of variation is the genre. The text that our algorithms must
process might come from newswire, ﬁction or non-ﬁction books, scientiﬁc articles,
Wikipedia, or religious texts. It might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. It might come from work situations
like doctors’ notes, legal text, or parliamentary or congressional proceedings.
Text also reﬂects the demographic characteristics of the writer (or speaker): their
age, gender, race, socioeconomic class can all inﬂuence the linguistic properties of
the text we are processing.
And ﬁnally, time matters too. Language changes over time, and for some lan-
guages we have good corpora of texts from different historical periods.
Because language is so situated, when developing computational models for lan-
guage processing from a corpus, it’s important to consider who produced the lan-
guage, in what context, for what purpose. How can a user of a dataset know all these
details? The best way is for the corpus creator to build a datasheet (Gebru et al., datasheet
2020) or data statement (Bender et al., 2021) for each corpus. A datasheet speciﬁes
properties of a dataset like:
Motivation : Why was the corpus collected, by whom, and who funded it?
Situation : When and in what situation was the text written/spoken? For example,
was there a task? Was the language originally spoken conversation, edited
text, social media communication, monologue vs. dialogue?
Language variety : What language (including dialect/region) was the corpus in?
Speaker demographics : What was, e.g., the age or gender of the text’s authors?
Collection process : How big is the data? If it is a subsample how was it sampled?
Was the data collected with consent? How was the data pre-processed, and
what metadata is available?
Annotation process : What are the annotations, what are the demographics of the
annotators, how were they trained, how was the data annotated?

--- Page 17 ---
2.4 • S IMPLE UNIXTOOLS FOR WORD TOKENIZATION 17
Distribution : Are there copyright or other intellectual property restrictions?
2.4 Simple Unix Tools for Word Tokenization
Before almost any natural language processing of a text, the text has to be normal-
ized, a task called text normalization . At least three tasks are commonly applied astext
normalization
part of any normalization process:
1. Tokenizing (segmenting) words
2. Normalizing word formats
3. Segmenting sentences
In the next sections we walk through each of these tasks, but we’ll ﬁrst start with
an easy, if somewhat naive version of word tokenization and normalization (and fre-
quency computation) that can be accomplished for English solely in a single Unix
command-line, inspired by Church (1994). We’ll make use of some Unix com-
mands:tr, used to systematically change particular characters in the input; sort ,
which sorts input lines in alphabetical order; and uniq , which collapses and counts
adjacent identical lines.
For example let’s begin with the ‘complete words’ of Shakespeare in one ﬁle,
sh.txt . We can use trto tokenize the words by changing every sequence of non-
alphabetic characters to a newline (’A-Za-z’ means alphabetic and the -c option
complements to non-alphabet, so together they mean to change every non-alphabetic
character into a newline. The -s (‘squeeze’) option is used to replace the result
of multiple consecutive changes into a single output, so a series of non-alphabetic
characters in a row would all be ‘squeezed’ into a single newline):
tr -sc 'A-Za-z' '\n' < sh.txt
The output of this command will be:
THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
We
...
Now that there is one word per line, we can sort the lines, and pass them to uniq
-cwhich will collapse and count them:
tr -sc 'A-Za-z' '\n' < sh.txt | sort | uniq -c
with the following output:
1945 A
72 AARON
19 ABBESS
25 Aaron
6 Abate

--- Page 18 ---
18 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
1 Abates
5 Abbess
6 Abbey
3 Abbot
...
Alternatively, we can collapse all the upper case to lower case:
tr -sc 'A-Za-z' '\n' < sh.txt | tr A-Z a-z | sort | uniq -c
whose output is
14725 a
97 aaron
1 abaissiez
10 abandon
2 abandoned
2 abase
1 abash
14 abate
3 abated
3 abatement
...
Now we can sort again to ﬁnd the frequent words. The -noption tosort means
to sort numerically rather than alphabetically, and the -roption means to sort in
reverse order (highest-to-lowest):
tr -sc 'A-Za-z' '\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r
The results show that the most frequent words in Shakespeare, as in any other
corpus, are the short function words like articles, pronouns, prepositions:
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
12489 my
11318 that
11112 in
...
Unix tools of this sort can be very handy in building quick word count statistics
for any corpus in English. While in some versions of Unix these command-line tools
also correctly handle Unicode characters and so can be used for many languages,
in general for handling most languages outside English we use more sophisticated
tokenization algorithms.
2.5 Word and Subword Tokenization
The simple Unix tools above were ﬁne for getting rough word statistics but more
sophisticated algorithms are generally necessary for tokenization , the task of seg- tokenization

--- Page 19 ---
2.5 • W ORD AND SUBWORD TOKENIZATION 19
menting running text into words. There are roughly two classes of tokenization
algorithms. In top-down tokenization, we deﬁne a standard and implement rules to
implement that kind of tokenization.
But more commonly instead of using words as the input to NLP algorithms we
break up words into subword tokens , which can be words or parts of words or subword tokens
even individual letters. These are derived via bottom-up tokenization, in which we
use simple statistics of letter sequences to come up with the vocabulary of subword
tokens, and break up the input into those subwords.
2.5.1 Top-down (rule-based) tokenization
While the Unix command sequence just removed all the numbers and punctuation,
for most NLP applications we’ll need to keep these in our tokenization. We often
want to break off punctuation as a separate token; commas are a useful piece of infor-
mation for parsers, and periods help indicate sentence boundaries. But we’ll often
want to keep the punctuation that occurs word internally, in examples like m.p.h. ,
Ph.D. ,AT&T , and cap’n . Special characters and numbers will need to be kept in
prices ($45.55) and dates ( 01/02/06 ); we don’t want to segment that price into sepa-
rate tokens of “45” and “55”. And there are URLs ( https://www.stanford.edu ),
Twitter hashtags ( #nlproc ), or email addresses ( someone@cs.colorado.edu ).
Number expressions introduce complications; in addition to appearing at word
boundaries, commas appear inside numbers in English, every three digits: 555,500.50 .
Tokenization differs by language; languages like Spanish, French, and German, for
example, use a comma to mark the decimal point, and spaces (or sometimes periods)
where English puts commas, for example, 555 500,50 .
A tokenizer can also be used to expand clitic contractions that are marked by clitic
apostrophes, converting what're to the two tokens what are , andwe're towe
are. A clitic is a part of a word that can’t stand on its own, and can only occur
when it is attached to another word. Such contractions occur in other alphabetic
languages, including French pronouns ( j'ai and articles l'homme ).
Depending on the application, tokenization algorithms may also tokenize mul-
tiword expressions like New York orrock 'n' roll as a single token, which re-
quires a multiword expression dictionary of some sort. Tokenization is thus inti-
mately tied up with named entity recognition , the task of detecting names, dates,
and organizations (Chapter 17).
One commonly used tokenization standard is known as the Penn Treebank to-
kenization standard, used for the parsed corpora (treebanks) released by the Lin-Penn Treebank
tokenization
guistic Data Consortium (LDC), the source of many useful datasets. This standard
separates out clitics ( doesn’t becomes does plus n’t), keeps hyphenated words to-
gether, and separates out all punctuation (to save space we’re showing visible spaces
‘’ between tokens, although newlines is a more common output):
Input :"The San Francisco-based restaurant," they said,
"doesn't charge $10".
Output :"TheSanFrancisco-based restaurant ,"theysaid,
"doesn'tcharge $10".
In practice, since tokenization is run before any other language processing, it
needs to be very fast. For word tokenziation we generally use deterministic algo-
rithms based on regular expressions compiled into efﬁcient ﬁnite state automata.
For example, Fig. 2.12 shows a basic regular expression that can be used to tok-

--- Page 20 ---
20 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
enize English with the nltk.regexp tokenize function of the Python-based Nat-
ural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org ).
>>> text = 'That U.S.A. poster-print costs $12.40...'
>>> pattern = r'''(?x) # set flag to allow verbose regexps
... (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A.
... | \w+(?:-\w+)* # words with optional internal hyphens
... | \$?\d+(?:\.\d+)?%? # currency, percentages, e.g. $12.40, 82%
... | \.\.\. # ellipsis
... | [][.,;"'?():_`-] # these are separate tokens; includes ], [
... '''
>>> nltk.regexp_tokenize(text, pattern)
['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']
Figure 2.12 A Python trace of regular expression tokenization in the NLTK Python-based
natural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)
verbose ﬂag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird
et al. (2009).
Carefully designed deterministic algorithms can deal with the ambiguities that
arise, such as the fact that the apostrophe needs to be tokenized differently when used
as a genitive marker (as in the book’s cover ), a quotative as in ‘The other class’, she
said, or in clitics like they’re .
Word tokenization is more complex in languages like written Chinese, Japanese,
and Thai, which do not use spaces to mark potential word-boundaries. In Chinese,
for example, words are composed of characters (called hanzi in Chinese). Each hanzi
character generally represents a single unit of meaning (called a morpheme ) and is
pronounceable as a single syllable. Words are about 2.4 characters long on average.
But deciding what counts as a word in Chinese is complex. For example, consider
the following sentence:
(2.21)姚明进入总决赛 y´ao m ´ıng j`ın r`u zˇong ju ´e s`ai
“Yao Ming reaches the ﬁnals”
As Chen et al. (2017b) point out, this could be treated as 3 words (‘Chinese Tree-
bank’ segmentation):
(2.22)姚明
YaoMing进入
reaches总决赛
ﬁnals
or as 5 words (‘Peking University’ segmentation):
(2.23)姚
Yao明
Ming进入
reaches总
overall决赛
ﬁnals
Finally, it is possible in Chinese simply to ignore words altogether and use characters
as the basic elements, treating the sentence as a series of 7 characters:
(2.24)姚
Yao明
Ming进
enter入
enter总
overall决
decision赛
game
In fact, for most Chinese NLP tasks it turns out to work better to take characters
rather than words as input, since characters are at a reasonable semantic level for
most applications, and since most word standards, by contrast, result in a huge vo-
cabulary with large numbers of very rare words (Li et al., 2019b).
However, for Japanese and Thai the character is too small a unit, and so algo-
rithms for word segmentation are required. These can also be useful for Chineseword
segmentation

--- Page 21 ---
2.5 • W ORD AND SUBWORD TOKENIZATION 21
in the rare situations where word rather than character boundaries are required. For
these situations we can use the subword tokenization algorithms introduced in the
next section.
2.5.2 Byte-Pair Encoding: A Bottom-up Tokenization Algorithm
There is a third option to tokenizing text, one that is most commonly used by large
language models. Instead of deﬁning tokens as words (whether delimited by spaces
or more complex algorithms), or as characters (as in Chinese), we can use our data to
automatically tell us what the tokens should be. This is especially useful in dealing
with unknown words, an important problem in language processing. As we will
see in the next chapter, NLP algorithms often learn some facts about language from
one corpus (a training corpus) and then use these facts to make decisions about a
separate testcorpus and its language. Thus if our training corpus contains, say the
words low,new,newer , but not lower , then if the word lower appears in our test
corpus, our system will not know what to do with it.
To deal with this unknown word problem, modern tokenizers automatically in-
duce sets of tokens that include tokens smaller than words, called subwords . Sub- subwords
words can be arbitrary substrings, or they can be meaning-bearing units like the
morphemes -estor-er. (A morpheme is the smallest meaning-bearing unit of a lan-
guage; for example the word unwashable has the morphemes un-,wash , and -able .)
In modern tokenization schemes, most tokens are words, but some tokens are fre-
quently occurring morphemes or other subwords like -er. Every unseen word like
lower can thus be represented by some sequence of known subword units, such as
lowander, or even as a sequence of individual letters if necessary.
Most tokenization schemes have two parts: a token learner , and a token seg-
menter . The token learner takes a raw training corpus (sometimes roughly pre-
separated into words, for example by whitespace) and induces a vocabulary, a set
of tokens. The token segmenter takes a raw test sentence and segments it into the
tokens in the vocabulary. Two algorithms are widely used: byte-pair encoding
(Sennrich et al., 2016), and unigram language modeling (Kudo, 2018), There is
also a SentencePiece library that includes implementations of both of these (Kudo
and Richardson, 2018a), and people often use the name SentencePiece to simply
mean unigram language modeling tokenization.
In this section we introduce the simplest of the three, the byte-pair encoding or
BPE algorithm (Sennrich et al., 2016); see Fig. 2.13. The BPE token learner begins BPE
with a vocabulary that is just the set of all individual characters. It then examines the
training corpus, chooses the two symbols that are most frequently adjacent (say ‘A’,
‘B’), adds a new merged symbol ‘AB’ to the vocabulary, and replaces every adjacent
’A’ ’B’ in the corpus with the new ‘AB’. It continues to count and merge, creating
new longer and longer character strings, until kmerges have been done creating
knovel tokens; kis thus a parameter of the algorithm. The resulting vocabulary
consists of the original set of characters plus knew symbols.
The algorithm is usually run inside words (not merging across word boundaries),
so the input corpus is ﬁrst white-space-separated to give a set of strings, each corre-
sponding to the characters of a word, plus a special end-of-word symbol , and its
counts. Let’s see its operation on the following tiny input corpus of 18 word tokens
with counts for each word (the word lowappears 5 times, the word newer 6 times,
and so on), which would have a starting vocabulary of 11 letters:

--- Page 22 ---
22 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
corpus vocabulary
5l o w , d, e, i, l, n, o, r, s, t, w
2l o w e s t
6n e w e r
3w i d e r
2n e w
The BPE algorithm ﬁrst counts all pairs of adjacent symbols: the most frequent
is the pair e rbecause it occurs in newer (frequency of 6) and wider (frequency of
3) for a total of 9 occurrences.2We then merge these symbols, treating eras one
symbol, and count again:
corpus vocabulary
5l o w , d, e, i, l, n, o, r, s, t, w, er
2l o w e s t
6n e w er
3w i d er
2n e w
Now the most frequent pair is er , which we merge; our system has learned
that there should be a token for word-ﬁnal er, represented as er:
corpus vocabulary
5l o w ,d,e,i,l,n,o,r,s,t,w,er,er
2l o w e s t
6n e w er
3w i d er
2n e w
Nextn e(total count of 8) get merged to ne:
corpus vocabulary
5l o w ,d,e,i,l,n,o,r,s,t,w,er,er,ne
2l o w e s t
6ne w er
3w i d er
2ne w
If we continue, the next merges are:
merge current vocabulary
(ne, w) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new
(l, o) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo
(lo, w) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low
(new, er ) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer
(low,) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer ,low
Once we’ve learned our vocabulary, the token segmenter is used to tokenize a
test sentence. The token segmenter just runs on the merges we have learned from
the training data on the test data. It runs them greedily, in the order we learned them.
(Thus the frequencies in the test data don’t play a role, just the frequencies in the
training data). So ﬁrst we segment each test sentence word into characters. Then
we apply the ﬁrst rule: replace every instance of e rin the test corpus with er, and
then the second rule: replace every instance of er in the test corpus with er,
and so on. By the end, if the test corpus contained the character sequence n e w e
2Note that there can be ties; we could have instead chosen to merge r ﬁrst, since that also has a
frequency of 9.

--- Page 23 ---
2.6 • W ORD NORMALIZATION , LEMMATIZATION AND STEMMING 23
function BYTE-PAIR ENCODING (strings C, number of merges k)returns vocab V
V all unique characters in C # initial set of tokens is characters
fori= 1tokdo # merge tokens ktimes
tL,tR Most frequent pair of adjacent tokens in C
tNEW tL+tR # make new token by concatenating
V V+tNEW # update the vocabulary
Replace each occurrence of tL,tRinCwith tNEW # and update the corpus
return V
Figure 2.13 The token learner part of the BPE algorithm for taking a corpus broken up
into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.
Figure adapted from Bostrom and Durrett (2020).
r, it would be tokenized as a full word. But the characters of a new (unknown)
word like l o w e r would be merged into the two tokens lower .
Of course in real settings BPE is run with many thousands of merges on a very
large input corpus. The result is that most words will be represented as full symbols,
and only the very rare words (and unknown words) will have to be represented by
their parts.
2.6 Word Normalization, Lemmatization and Stemming
Word normalization is the task of putting words or tokens in a standard format. The normalization
simplest case of word normalization is case folding . Mapping everything to lower case folding
case means that Woodchuck andwoodchuck are represented identically, which is
very helpful for generalization in many tasks, such as information retrieval or speech
recognition. For sentiment analysis and other text classiﬁcation tasks, information
extraction, and machine translation, by contrast, case can be quite helpful and case
folding is generally not done. This is because maintaining the difference between,
for example, USthe country and usthe pronoun can outweigh the advantage in
generalization that case folding would have provided for other words. Sometimes
we produce both cased (i.e. including both upper and lower case words or tokens)
and uncased versions of language models.
Systems that use BPE or other kinds of bottom-up tokenization may do no fur-
ther word normalization. In other NLP systems, we may want to do further nor-
malizations, like choosing a single normal form for words with multiple forms like
USA andUSoruh-huh anduhhuh . This standardization may be valuable, despite
the spelling information that is lost in the normalization process. For information
retrieval or information extraction about the US, we might want to see information
from documents whether they mention the USor theUSA.
2.6.1 Lemmatization
For other natural language processing situations we also want two morphologically
different forms of a word to behave similarly. For example in web search, someone
may type the string woodchucks but a useful system might want to also return pages
that mention woodchuck with no s. This is especially common in morphologically
complex languages like Polish, where for example the word Warsaw has different

--- Page 24 ---
24 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
endings when it is the subject ( Warszawa ), or after a preposition like “in Warsaw” ( w
Warszawie ), or “to Warsaw” ( do Warszawy ), and so on. Lemmatization is the task lemmatization
of determining that two words have the same root, despite their surface differences.
The words am,are, and ishave the shared lemma be; the words dinner anddinners
both have the lemma dinner . Lemmatizing each of these forms to the same lemma
will let us ﬁnd all mentions of words in Polish like Warsaw . The lemmatized form
of a sentence like He is reading detective stories would thus be He be read detective
story .
How is lemmatization done? The most sophisticated methods for lemmatization
involve complete morphological parsing of the word. Morphology is the study of
the way words are built up from smaller meaning-bearing units called morphemes . morpheme
Two broad classes of morphemes can be distinguished: stems —the central mor- stem
pheme of the word, supplying the main meaning—and afﬁxes —adding “additional” afﬁx
meanings of various kinds. So, for example, the word foxconsists of one morpheme
(the morpheme fox) and the word cats consists of two: the morpheme catand the
morpheme -s. A morphological parser takes a word like cats and parses it into the
two morphemes catands, or parses a Spanish word like amaren (‘if in the future
they would love’) into the morpheme amar ‘to love’, and the morphological features
3PL (third person plural) and future subjunctive .
Stemming: The Porter Stemmer
Lemmatization algorithms can be complex. For this reason we sometimes make
use of a simpler but cruder method, which mainly consists of chopping off word-
ﬁnal afﬁxes. This naive version of morphological analysis is called stemming . For stemming
example, the classic Porter stemmer (Porter, 1980), when applied to the following Porter stemmer
paragraph:
This was not the map we found in Billy Bones's chest, but
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.
produces the following stemmed output:
Thi wa not the map we found in Billi Bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note
The algorithm is based on rewrite rules run in series, with the output of each pass
fed as input to the next pass. Some sample rules (more at https://tartarus.org/
martin/PorterStemmer/ ):
ATIONAL!ATE (e.g., relational !relate)
ING!if the stem contains a vowel (e.g., motoring !motor)
SSES!SS (e.g., grasses !grass)
Simple stemmers can be useful in cases where we need to collapse across dif-
ferent variants of the same lemma. Nonetheless, they are less commonly used in
modern systems since they commit errors of both over-generalizating (lemmatizing
policy topolice ) and under-generalizing (not lemmatizing European toEurope )
(Krovetz, 1993).

--- Page 25 ---
2.7 • S ENTENCE SEGMENTATION 25
2.7 Sentence Segmentation
Sentence segmentation is another important step in text processing. The most use-sentence
segmentation
ful cues for segmenting a text into sentences are punctuation, like periods, question
marks, and exclamation points. Question marks and exclamation points are rela-
tively unambiguous markers of sentence boundaries. Periods, on the other hand, are
more ambiguous. The period character “.” is ambiguous between a sentence bound-
ary marker and a marker of abbreviations like Mr.orInc.The previous sentence that
you just read showed an even more complex case of this ambiguity, in which the ﬁnal
period of Inc. marked both an abbreviation and the sentence boundary marker. For
this reason, sentence tokenization and word tokenization may be addressed jointly.
In general, sentence tokenization methods work by ﬁrst deciding (based on rules
or machine learning) whether a period is part of the word or is a sentence-boundary
marker. An abbreviation dictionary can help determine whether the period is part
of a commonly used abbreviation; the dictionaries can be hand-built or machine-
learned (Kiss and Strunk, 2006), as can the ﬁnal sentence splitter. In the Stanford
CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based,
a deterministic consequence of tokenization; a sentence ends when a sentence-ending
punctuation (., !, or ?) is not already grouped with other characters into a token (such
as for an abbreviation or number), optionally followed by additional ﬁnal quotes or
brackets.
2.8 Minimum Edit Distance
Much of natural language processing is concerned with measuring how similar two
strings are. For example in spelling correction, the user typed some erroneous
string—let’s say graffe –and we want to know what the user meant. The user prob-
ably intended a word that is similar to graffe . Among candidate similar words,
the wordgiraffe , which differs by only one letter from graffe , seems intuitively
to be more similar than, say grail orgraf , which differ in more letters. Another
example comes from coreference , the task of deciding whether two strings such as
the following refer to the same entity:
Stanford President Marc Tessier-Lavigne
Stanford University President Marc Tessier-Lavigne
Again, the fact that these two strings are very similar (differing by only one word)
seems like useful evidence for deciding that they might be coreferent.
Edit distance gives us a way to quantify both of these intuitions about string sim-
ilarity. More formally, the minimum edit distance between two strings is deﬁnedminimum edit
distance
as the minimum number of editing operations (operations like insertion, deletion,
substitution) needed to transform one string into another.
The gap between intention andexecution , for example, is 5 (delete an i, substi-
tuteeforn, substitute xfort, insertc, substitute uforn). It’s much easier to see
this by looking at the most important visualization for string distances, an alignment alignment
between the two strings, shown in Fig. 2.14. Given two sequences, an alignment is
a correspondence between substrings of the two sequences. Thus, we say Ialigns
with the empty string, NwithE, and so on. Beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the

--- Page 26 ---
26 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
top string into the bottom string: dfor deletion, sfor substitution, ifor insertion.
INTE*NTION
jjjjjjjjjj
*EXECUTION
d s s i s
Figure 2.14 Representing the minimum edit distance between two strings as an alignment .
The ﬁnal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.
We can also assign a particular cost or weight to each of these operations. The
Levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume
that the substitution of a letter for itself, for example, tfort, has zero cost. The Lev-
enshtein distance between intention andexecution is 5. Levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (This is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). Using this version, the Levenshtein distance between
intention andexecution is 8.
2.8.1 The Minimum Edit Distance Algorithm
How do we ﬁnd the minimum edit distance? We can think of this as a search task, in
which we are searching for the shortest path—a sequence of edits—from one string
to another.
n t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o n
Figure 2.15 Finding the edit distance viewed as a search problem
The space of all possible edits is enormous, so we can’t search naively. However,
lots of distinct edit paths will end up in the same state (string), so rather than recom-
puting all those paths, we could just remember the shortest path to a state each time
we saw it. We can do this by using dynamic programming . Dynamic programmingdynamic
programming
is the name for a class of algorithms, ﬁrst introduced by Bellman (1957), that apply
a table-driven method to solve problems by combining solutions to subproblems.
Some of the most commonly used algorithms in natural language processing make
use of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the
CKY algorithm for parsing (Chapter 18).
The intuition of a dynamic programming problem is that a large problem can
be solved by properly combining the solutions to various subproblems. Consider
the shortest path of transformed words that represents the minimum edit distance
between the strings intention andexecution shown in Fig. 2.16.
Imagine some string (perhaps it is exention ) that is in this optimal path (whatever
it is). The intuition of dynamic programming is that if exention is in the optimal

--- Page 27 ---
2.8 • M INIMUM EDITDISTANCE 27
n t e n t i o ni n t e n t i o n
e t e n t i o n
e x e n t i o n
e x e n u t i o n
e x e c u t i o ndelete i
substitute n by e
substitute t by x
insert u
substitute n by c
Figure 2.16 Path from intention toexecution .
operation list, then the optimal sequence must also include the optimal path from
intention toexention . Why? If there were a shorter path from intention toexention ,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn’t be optimal, thus leading to a contradiction.
The minimum edit distance algorithm was named by Wagner and Fischerminimum edit
distance
algorithm(1974) but independently discovered by many people (see the Historical Notes sec-
tion of Chapter 17).
Let’s ﬁrst deﬁne the minimum edit distance between two strings. Given two
strings, the source string Xof length n, and target string Yof length m, we’ll deﬁne
D[i;j]as the edit distance between X[1::i]andY[1::j], i.e., the ﬁrst icharacters of X
and the ﬁrst jcharacters of Y. The edit distance between XandYis thus D[n;m].
We’ll use dynamic programming to compute D[n;m]bottom up, combining so-
lutions to subproblems. In the base case, with a source substring of length ibut an
empty target string, going from icharacters to 0 requires ideletes. With a target
substring of length jbut an empty source going from 0 characters to jcharacters
requires jinserts. Having computed D[i;j]for small i;jwe then compute larger
D[i;j]based on previously computed smaller values. The value of D[i;j]is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:
D[i;j] =min8
<
:D[i 1;j]+del-cost (source [i])
D[i;j 1]+ins-cost (target [j])
D[i 1;j 1]+sub-cost (source [i];target [j])(2.25)
We mentioned above two versions of Levenshtein distance, one in which substitu-
tions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion
plus a deletion). Let’s here use that second version of Levenshtein distance in which
the insertions and deletions each have a cost of 1 (ins-cost( ) = del-cost() = 1), and
substitutions have a cost of 2 (except substitution of identical letters has zero cost).
Under this version of Levenshtein, the computation for D[i;j]becomes:
D[i;j] =min8
>><
>>:D[i 1;j]+1
D[i;j 1]+1
D[i 1;j 1]+2; if source [i]6=target [j]
0; if source [i] =target [j](2.26)
The algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying
the algorithm to the distance between intention andexecution with the version of
Levenshtein in Eq. 2.26.
Alignment Knowing the minimum edit distance is useful for algorithms like ﬁnd-
ing potential spelling error corrections. But the edit distance algorithm is important

--- Page 28 ---
28 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
function MIN-EDIT-DISTANCE (source ,target )returns min-distance
n LENGTH (source )
m LENGTH (target )
Create a distance matrix D[n+1,m+1]
#Initialization: the zeroth row and column is the distance from the empty string
D[0,0] = 0
foreach row ifrom 1tondo
D[i,0] D[i-1,0] + del-cost (source [i])
foreach column jfrom 1tomdo
D[0,j] D[0,j-1] + ins-cost (target [j])
#Recurrence relation:
foreach row ifrom 1tondo
foreach column jfrom 1tomdo
D[i, j] MIN(D[i 1,j] +del-cost (source [i]),
D[i 1,j 1] + sub-cost (source [i],target [j]),
D[i,j 1] + ins-cost (target [j]))
#Termination
return D[n,m]
Figure 2.17 The minimum edit distance algorithm, an example of the class of dynamic
programming algorithms. The various costs can either be ﬁxed (e.g., 8x;ins-cost (x) =1)
or can be speciﬁc to the letter (to model the fact that some letters are more likely to be in-
serted than others). We assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost (x;x) =0).
SrcnTar # e x e c u t i o n
# 0 1 2 3 4 5 6 7 8 9
i 1 2 3 4 5 6 7 6 7 8
n 2 3 4 5 6 7 8 7 8 7
t 3 4 5 6 7 8 7 8 9 8
e 4 3 4 5 6 7 8 9 10 9
n 5 4 5 6 7 8 9 10 11 10
t 6 5 6 7 8 9 8 9 10 11
i 7 6 7 8 9 10 9 8 9 10
o 8 7 8 9 10 11 10 9 8 9
n 9 8 9 10 11 12 11 10 9 8
Figure 2.18 Computation of minimum edit distance between intention andexecution with
the algorithm of Fig. 2.17, using Levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.
in another way; with a small change, it can also provide the minimum cost align-
ment between two strings. Aligning two strings is useful throughout speech and
language processing. In speech recognition, minimum edit distance alignment is
used to compute the word error rate (Chapter 16). Alignment plays a role in ma-
chine translation, in which sentences in a parallel corpus (a corpus with a text in two
languages) need to be matched to each other.
To extend the edit distance algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the edit distance matrix. Figure 2.19
shows this path with boldfaced cells. Each boldfaced cell represents an alignment

--- Page 29 ---
2.9 • S UMMARY 29
of a pair of letters in the two strings. If two boldfaced cells occur in the same row,
there will be an insertion in going from the source to the target; two boldfaced cells
in the same column indicate a deletion.
Figure 2.19 also shows the intuition of how to compute this alignment path. The
computation proceeds in two steps. In the ﬁrst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. The backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. In the second step, we perform a backtrace . In a backtrace, we start backtrace
from the last cell (at the ﬁnal row and column), and follow the pointers back through
the dynamic programming matrix. Each complete path between the ﬁnal cell and the
initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the
minimum edit distance algorithm to store the pointers and compute the backtrace to
output an alignment.
# e x e c u t i o n
# 0 1 2 3 4 5 6 7 8 9
i"1- " 2- " 3- " 4- " 5- " 6- " 7-6 7 8
n"2- " 3- " 4- " 5- " 6- " 7- " 8"7- " 8-7
t"3- " 4- " 5- " 6- " 7- " 8-7 "8- " 9"8
e"4-3 4- 5 6 7 "8- " 9- " 10"9
n"5"4- " 5- " 6- " 7- " 8- " 9- " 10- " 11-"10
t"6"5- " 6- " 7- " 8- " 9-8 9 10 "11
i"7"6- " 7- " 8- " 9- " 10"9-8 9 10
o"8"7- " 8- " 9- " 10- " 11"10"9-8 9
n"9"8- " 9- " 10- " 11- " 12"11"10"9-8
Figure 2.19 When entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. After the table is full we compute an alignment
(minimum edit path) by using a backtrace , starting at the 8in the lower-right corner and
following the arrows back. The sequence of bold cells represents one possible minimum
cost alignment between the two strings, again using Levenshtein distance with cost of 1 for
insertions or deletions, 2 for substitutions. Diagram design after Gusﬁeld (1997).
While we worked our example with simple Levenshtein distance, the algorithm
in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. The Viterbi algorithm is a probabilistic extension of
minimum edit distance. Instead of computing the “minimum edit distance” between
two strings, Viterbi computes the “maximum probability alignment” of one string
with another. We’ll discuss this more in Chapter 17.
2.9 Summary
This chapter introduced a fundamental tool in language processing, the regular ex-
pression , and showed how to perform basic text normalization tasks including
word segmentation andnormalization ,sentence segmentation , and stemming .
We also introduced the important minimum edit distance algorithm for comparing
strings. Here’s a summary of the main points we covered about these ideas:

--- Page 30 ---
30 CHAPTER 2 • R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE
• The regular expression language is a powerful tool for pattern-matching.
• Basic operations in regular expressions include concatenation of symbols,
disjunction of symbols ( [],|),counters (*,+, and{n,m} ),anchors (^,$)
and precedence operators ( (,)).
•Word tokenization and normalization are generally done by cascades of
simple regular expression substitutions or ﬁnite automata.
• The Porter algorithm is a simple and efﬁcient way to do stemming , stripping
off afﬁxes. It does not have high accuracy but may be useful for some tasks.
• The minimum edit distance between two strings is the minimum number of
operations it takes to edit one into the other. Minimum edit distance can be
computed by dynamic programming , which also results in an alignment of
the two strings.
Bibliographical and Historical Notes
Kleene 1951; 1956 ﬁrst deﬁned regular expressions and the ﬁnite automaton, based
on the McCulloch-Pitts neuron. Ken Thompson was one of the ﬁrst to build regular
expressions compilers into editors for text searching (Thompson, 1968). His edi-
toredincluded a command “g/regular expression/p”, or Global Regular Expression
Print, which later became the Unix grep utility.
Text normalization algorithms have been applied since the beginning of the
ﬁeld. One of the earliest widely used stemmers was Lovins (1968). Stemming
was also applied early to the digital humanities, by Packard (1973), who built an
afﬁx-stripping morphological parser for Ancient Greek. Currently a wide vari-
ety of code for tokenization and normalization is available, such as the Stanford
Tokenizer ( https://nlp.stanford.edu/software/tokenizer.shtml ) or spe-
cialized tokenizers for Twitter (O’Connor et al., 2010), or for sentiment ( http:
//sentiment.christopherpotts.net/tokenizing.html ). See Palmer (2012)
for a survey of text preprocessing. NLTK is an essential tool that offers both useful
Python libraries ( https://www.nltk.org ) and textbook descriptions (Bird et al.,
2009) of many algorithms including text normalization and corpus interfaces.
For more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps
(1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gusﬁeld
(1997). Our example measuring the edit distance from ‘intention’ to ‘execution’
was adapted from Kruskal (1983). There are various publicly available packages to
compute edit distance, including Unix diff and the NIST sclite program (NIST,
2005).
In his autobiography Bellman (1984) explains how he originally came up with
the term dynamic programming :
“...The 1950s were not good years for mathematical research. [the]
Secretary of Defense ...had a pathological fear and hatred of the word,
research... I decided therefore to use the word, “programming”. I
wanted to get across the idea that this was dynamic, this was multi-
stage... I thought, let’s ... take a word that has an absolutely precise
meaning, namely dynamic... it’s impossible to use the word, dynamic,
in a pejorative sense. Try thinking of some combination that will pos-
sibly give it a pejorative meaning. It’s impossible. Thus, I thought
dynamic programming was a good name. It was something not even a
Congressman could object to.”

--- Page 31 ---
EXERCISES 31
Exercises
2.1 Write regular expressions for the following languages.
1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
3. the set of all strings from the alphabet a;bsuch that each ais immedi-
ately preceded by and immediately followed by a b;
2.2 Write regular expressions for the following languages. By “word”, we mean
an alphabetic string separated from other words by whitespace, any relevant
punctuation, line breaks, and so forth.
1. the set of all strings with two consecutive repeated words (e.g., “Hum-
bert Humbert” and “the the” but not “the bug” or “the big bug”);
2. all strings that start at the beginning of the line with an integer and that
end at the end of the line with a word;
3. all strings that have both the word grotto and the word raven in them
(but not, e.g., words like grottos that merely contain the word grotto );
4. write a pattern that places the ﬁrst word of an English sentence in a
register. Deal with punctuation.
2.3 Implement an ELIZA-like program, using substitutions such as those described
on page 13. You might want to choose a different domain than a Rogerian psy-
chologist, although keep in mind that you would need a domain in which your
program can legitimately engage in a lot of simple repetition.
2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution
cost 1) of “leda” to “deal”. Show your work (using the edit distance grid).
2.5 Figure out whether drive is closer to brief or to divers and what the edit dis-
tance is to each. You may use any version of distance that you like.
2.6 Now implement a minimum edit distance algorithm and use your hand-computed
results to check your code.
2.7 Augment the minimum edit distance algorithm to output an alignment; you
will need to store pointers and add a stage to compute the backtrace.

--- Page 32 ---
32 CHAPTER 3 • N- GRAM LANGUAGE MODELS
CHAPTER
3N-gram Language Models
“You are uniformly charming!” cried he, with a smile of associating and now
and then I bowed and they perceived a chaise and four to wish for.
Random sentence generated from a Jane Austen trigram model
Predicting is difﬁcult—especially about the future, as the old quip goes. But how
about predicting something that seems much easier, like the next word someone is
going to say? What word, for example, is likely to follow
The water of Walden Pond is so beautifully ...
You might conclude that a likely word is blue , orgreen , orclear , but probably not
refrigerator northis . In this chapter we formalize this intuition by introducing
language models orLMs , models that assign a probability to each possible next language model
LM word. Language models can also assign a probability to an entire sentence, telling
us that the following sequence has a much higher probability of appearing in a text:
all of a sudden I notice three guys standing on the sidewalk
than does this same set of words in a different order:
on guys all I of notice sidewalk three a sudden standing the
Why would we want to predict upcoming words, or know the probability of a sen-
tence? One reason is for generation: choosing contextually better words. For ex-
ample we can correct grammar or spelling errors like Their are two midterms ,
in whichThere was mistyped as Their , orEverything has improve , in which
improve should have been improved . The phrase There are is more probable
thanTheir are , andhas improved thanhas improve , so a language model can
help users select the more grammatical variant. Or for a speech system to recognize
that you said I will be back soonish and notI will be bassoon dish , it
helps to know that back soonish is a more probable sequence. Language models
can also help in augmentative and alternative communication (Trnka et al. 2007,
Kane et al. 2017). People can use AAC systems if they are physically unable to AAC
speak or sign but can instead use eye gaze or other movements to select words from
a menu. Word prediction can be used to suggest likely words for the menu.
Word prediction is also central to NLP for another reason: large language mod-
elsare built just by training them to predict words!! As we’ll see in chapters 7-9,
large language models learn an enormous amount about language solely from being
trained to predict upcoming words from neighboring words.
In this chapter we introduce the simplest kind of language model: the n-gram n-gram
language model. An n-gram is a sequence of nwords: a 2-gram (which we’ll call
bigram ) is a two-word sequence of words like The water , orwater of , and a 3-
gram (a trigram ) is a three-word sequence of words like The water of , orwater

--- Page 33 ---
3.1 • N-G RAMS 33
of Walden . But we also (in a bit of terminological ambiguity) use the word ‘n-
gram’ to mean a probabilistic model that can estimate the probability of a word given
the n-1 previous words, and thereby also to assign probabilities to entire sequences.
In later chapters we will introduce the much more powerful neural large lan-
guage models , based on the transformer architecture of Chapter 9. But because
n-grams have a remarkably simple and clear formalization, we use them to intro-
duce some major concepts of large language modeling, including training and test
sets,perplexity ,sampling , and interpolation .
3.1 N-Grams
Let’s begin with the task of computing P(wjh), the probability of a word wgiven
some history h. Suppose the history his “The water of Walden Pond is so
beautifully ” and we want to know the probability that the next word is blue :
P(bluejThe water of Walden Pond is so beautifully ) (3.1)
One way to estimate this probability is directly from relative frequency counts: take a
very large corpus, count the number of times we see The water of Walden Pond
is so beautifully , and count the number of times this is followed by blue . This
would be answering the question “Out of the times we saw the history h, how many
times was it followed by the word w”, as follows:
P(bluejThe water of Walden Pond is so beautifully ) =
C(The water of Walden Pond is so beautifully blue )
C(The water of Walden Pond is so beautifully )(3.2)
If we had a large enough corpus, we could compute these two counts and estimate
the probability from Eq. 3.2. But even the entire web isn’t big enough to give us
good estimates for counts of entire sentences. This is because language is creative ;
new sentences are invented all the time, and we can’t expect to get accurate counts
for such large objects as entire sentences. For this reason, we’ll need more clever
ways to estimate the probability of a word wgiven a history h, or the probability of
an entire word sequence W.
Let’s start with some notation. First, throughout this chapter we’ll continue to
refer to words , although in practice we usually compute language models over to-
kens like the BPE tokens of page 21. To represent the probability of a particular
random variable Xitaking on the value “the”, or P(Xi=“the”), we will use the
simpliﬁcation P(the). We’ll represent a sequence of nwords either as w1:::wnor
w1:n. Thus the expression w1:n 1means the string w1;w2;:::;wn 1, but we’ll also
be using the equivalent notation w<n, which can be read as “all the elements of w
from w1up to and including wn 1”. For the joint probability of each word in a se-
quence having a particular value P(X1=w1;X2=w2;X3=w3;:::;Xn=wn)we’ll
useP(w1;w2;:::;wn).
Now, how can we compute probabilities of entire sequences like P(w1;w2;:::;wn)?
One thing we can do is decompose this probability using the chain rule of proba-
bility :
P(X1:::Xn) = P(X1)P(X2jX1)P(X3jX1:2):::P(XnjX1:n 1)
=nY
k=1P(XkjX1:k 1) (3.3)

--- Page 34 ---
34 CHAPTER 3 • N- GRAM LANGUAGE MODELS
Applying the chain rule to words, we get
P(w1:n) = P(w1)P(w2jw1)P(w3jw1:2):::P(wnjw1:n 1)
=nY
k=1P(wkjw1:k 1) (3.4)
The chain rule shows the link between computing the joint probability of a sequence
and computing the conditional probability of a word given previous words. Equa-
tion 3.4 suggests that we could estimate the joint probability of an entire sequence of
words by multiplying together a number of conditional probabilities. But using the
chain rule doesn’t really seem to help us! We don’t know any way to compute the
exact probability of a word given a long sequence of preceding words, P(wnjw1:n 1).
As we said above, we can’t just estimate by counting the number of times every word
occurs following every long string in some corpus, because language is creative and
any particular context might have never occurred before!
3.1.1 The Markov assumption
The intuition of the n-gram model is that instead of computing the probability of a
word given its entire history, we can approximate the history by just the last few
words.
Thebigram model, for example, approximates the probability of a word given bigram
all the previous words P(wnjw1:n 1)by using only the conditional probability of the
preceding word P(wnjwn 1). In other words, instead of computing the probability
P(bluejThe water of Walden Pond is so beautifully ) (3.5)
we approximate it with the probability
P(bluejbeautifully ) (3.6)
When we use a bigram model to predict the conditional probability of the next word,
we are thus making the following approximation:
P(wnjw1:n 1)P(wnjwn 1) (3.7)
The assumption that the probability of a word depends only on the previous word is
called a Markov assumption. Markov models are the class of probabilistic models Markov
that assume we can predict the probability of some future unit without looking too
far into the past. We can generalize the bigram (which looks one word into the past)
to the trigram (which looks two words into the past) and thus to the n-gram (which n-gram
looks n 1 words into the past).
Let’s see a general equation for this n-gram approximation to the conditional
probability of the next word in a sequence. We’ll use Nhere to mean the n-gram
size, so N=2 means bigrams and N=3 means trigrams. Then we approximate the
probability of a word given its entire context as follows:
P(wnjw1:n 1)P(wnjwn N+1:n 1) (3.8)
Given the bigram assumption for the probability of an individual word, we can com-
pute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:
P(w1:n)nY
k=1P(wkjwk 1) (3.9)

--- Page 35 ---
3.1 • N-G RAMS 35
3.1.2 How to estimate probabilities
How do we estimate these bigram or n-gram probabilities? An intuitive way to
estimate probabilities is called maximum likelihood estimation orMLE . We getmaximum
likelihood
estimationthe MLE estimate for the parameters of an n-gram model by getting counts from
a corpus, and normalizing the counts so that they lie between 0 and 1. For proba- normalize
bilistic models, normalizing means dividing by some total count so that the resulting
probabilities fall between 0 and 1 and sum to 1.
For example, to compute a particular bigram probability of a word wngiven a
previous word wn 1, we’ll compute the count of the bigram C(wn 1wn)and normal-
ize by the sum of all the bigrams that share the same ﬁrst word wn 1:
P(wnjwn 1) =C(wn 1wn)P
wC(wn 1w)(3.10)
We can simplify this equation, since the sum of all bigram counts that start with
a given word wn 1must be equal to the unigram count for that word wn 1(the reader
should take a moment to be convinced of this):
P(wnjwn 1) =C(wn 1wn)
C(wn 1)(3.11)
Let’s work through an example using a mini-corpus of three sentences. We’ll
ﬁrst need to augment each sentence with a special symbol <s>at the beginning
of the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a
special end-symbol </s> .1
<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>
Here are the calculations for some of the bigram probabilities from this corpus
P(I|<s> ) =2
3=0:67 P(Sam|<s> ) =1
3=0:33 P(am|I) =2
3=0:67
P(</s>|Sam ) =1
2=0:5 P(Sam|am ) =1
2=0:5 P(do|I) =1
3=0:33
For the general case of MLE n-gram parameter estimation:
P(wnjwn N+1:n 1) =C(wn N+1:n 1wn)
C(wn N+1:n 1)(3.12)
Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the
observed frequency of a particular sequence by the observed frequency of a preﬁx.
This ratio is called a relative frequency . We said above that this use of relativerelative
frequency
frequencies as a way to estimate probabilities is an example of maximum likelihood
estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of
the training set Tgiven the model M(i.e., P(TjM)). For example, suppose the word
Chinese occurs 400 times in a corpus of a million words. What is the probability
that a random word selected from some other text of, say, a million words will be the
word Chinese ? The MLE of its probability is400
1000000or 0:0004. Now 0 :0004 is not
the best possible estimate of the probability of Chinese occurring in all situations; it
1We need the end-symbol to make the bigram grammar a true probability distribution. Without an end-
symbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities
for all sentences of a given length would sum to one. This model would deﬁne an inﬁnite set of probability
distributions, with one distribution per sentence length. See Exercise 3.5.

--- Page 36 ---
36 CHAPTER 3 • N- GRAM LANGUAGE MODELS
might turn out that in some other corpus or context Chinese is a very unlikely word.
But it is the probability that makes it most likely that Chinese will occur 400 times
in a million-word corpus. We present ways to modify the MLE estimates slightly to
get better probability estimates in Section 3.6.
Let’s move on to some examples from a real but tiny corpus, drawn from the
now-defunct Berkeley Restaurant Project, a dialogue system from the last century
that answered questions about a database of restaurants in Berkeley, California (Ju-
rafsky et al., 1994). Here are some sample user queries (text-normalized, by lower
casing and with punctuation striped) (a sample of 9332 sentences is on the website):
can you tell me about any good cantonese restaurants close by
tell me about chez panisse
i’m looking for a good place to eat breakfast
when is caffe venezia open during the day
Figure 3.1 shows the bigram counts from part of a bigram grammar from text-
normalized Berkeley Restaurant Project sentences. Note that the majority of the
values are zero. In fact, we have chosen the sample words to cohere with each other;
a matrix selected from a random set of eight words would be even more sparse.
i want to eat chinese food lunch spend
i 5 827 0 9 0 0 0 2
want 2 0 608 1 6 6 5 1
to 2 0 4 686 2 0 6 211
eat 0 0 2 0 16 2 42 0
chinese 1 0 0 0 0 82 1 0
food 15 0 15 0 1 4 0 0
lunch 2 0 0 0 0 1 0 0
spend 1 0 1 0 0 0 0 0
Figure 3.1 Bigram counts for eight of the words (out of V=1446) in the Berkeley Restau-
rant Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of
the column label word following the row label word. Thus the cell in row iand column want
means that want followed i827 times in the corpus.
Figure 3.2 shows the bigram probabilities after normalization (dividing each cell
in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of
unigram counts):
i want to eat chinese food lunch spend
2533 927 2417 746 158 1093 341 278
Here are a few other useful probabilities:
P(i|<s> ) =0:25 P(english|want ) =0:0011
P(food|english ) =0:5 P(</s>|food ) =0:68
Now we can compute the probability of sentences like I want English food or
I want Chinese food by simply multiplying the appropriate bigram probabilities to-
gether, as follows:
P(<s> i want english food </s> )
=P(i|<s> )P(want|i )P(english|want )
P(food|english )P(</s>|food )
=0:250:330:00110:50:68
=0:000031

--- Page 37 ---
3.1 • N-G RAMS 37
i want to eat chinese food lunch spend
i 0.002 0.33 0 0.0036 0 0 0 0.00079
want 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011
to 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087
eat 0 0 0.0027 0 0.021 0.0027 0.056 0
chinese 0.0063 0 0 0 0 0.52 0.0063 0
food 0.014 0 0.014 0 0.00092 0.0037 0 0
lunch 0.0059 0 0 0 0 0.0029 0 0
spend 0.0036 0 0.0036 0 0 0 0 0
Figure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus
of 9332 sentences. Zero probabilities are in gray.
We leave it as Exercise 3.2 to compute the probability of i want chinese food .
What kinds of linguistic phenomena are captured in these bigram statistics?
Some of the bigram probabilities above encode some facts that we think of as strictly
syntactic in nature, like the fact that what comes after eatis usually a noun or an
adjective, or that what comes after tois usually a verb. Others might be a fact about
the personal assistant task, like the high probability of sentences beginning with
the words I. And some might even be cultural rather than linguistic, like the higher
probability that people are looking for Chinese versus English food.
3.1.3 Dealing with scale in large n-gram models
In practice, language models can be very large, leading to practical issues.
Log probabilities Language model probabilities are always stored and computed
in log space as log probabilities . This is because probabilities are (by deﬁnition) lesslog
probabilities
than or equal to 1, and so the more probabilities we multiply together, the smaller the
product becomes. Multiplying enough n-grams together would result in numerical
underﬂow. Adding in log space is equivalent to multiplying in linear space, so we
combine log probabilities by adding them. By adding log probabilities instead of
multiplying probabilities, we get results that are not as small. We do all computation
and storage in log space, and just convert back into probabilities if we need to report
probabilities at the end by taking the exp of the logprob:
p1p2p3p4=exp(logp1+logp2+logp3+logp4) (3.13)
In practice throughout this book, we’ll use log to mean natural log (ln) when the
base is not speciﬁed.
Longer context Although for pedagogical purposes we have only described bi-
gram models, when there is sufﬁcient training data we use trigram models, which trigram
condition on the previous two words, or 4-gram or5-gram models. For these larger 4-gram
5-gram n-grams, we’ll need to assume extra contexts to the left and right of the sentence end.
For example, to compute trigram probabilities at the very beginning of the sentence,
we use two pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s> ).
Some large n-gram datasets have been created, like the million most frequent
n-grams drawn from the Corpus of Contemporary American English (COCA), a
curated 1 billion word corpus of American English (Davies, 2020), Google’s Web
5-gram corpus from 1 trillion words of English web text (Franz and Brants, 2006),
or the Google Books Ngrams corpora (800 billion tokens from Chinese, English,
French, German, Hebrew, Italian, Russian, and Spanish) (Lin et al., 2012a)).

--- Page 38 ---
38 CHAPTER 3 • N- GRAM LANGUAGE MODELS
It’s even possible to use extremely long-range n-gram context. The inﬁni-gram
(¥-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to
avoid the expensive (in space and time) pre-computation of huge n-gram count ta-
bles. Instead, n-gram probabilities with arbitrary n are computed quickly at inference
time by using an efﬁcient representation called sufﬁx arrays. This allows computing
of n-grams of every length for enormous corpora of 5 trillion tokens.
Efﬁciency considerations are important when building large n-gram language
models. It is standard to quantize the probabilities using only 4-8 bits (instead of
8-byte ﬂoats), store the word strings on disk and represent them in memory only as
a 64-bit hash, and represent n-grams in special data structures like ‘reverse tries’.
It is also common to prune n-gram language models, for example by only keeping
n-grams with counts greater than some threshold or using entropy to prune less-
important n-grams (Stolcke, 1998). Efﬁcient language model toolkits like KenLM
(Heaﬁeld 2011, Heaﬁeld et al. 2013) use sorted arrays and use merge sorts to efﬁ-
ciently build the probability tables in a minimal number of passes through a large
corpus.
3.2 Evaluating Language Models: Training and Test Sets
The best way to evaluate the performance of a language model is to embed it in
an application and measure how much the application improves. Such end-to-end
evaluation is called extrinsic evaluation . Extrinsic evaluation is the only way toextrinsic
evaluation
know if a particular improvement in the language model (or any component) is really
going to help the task at hand. Thus for evaluating n-gram language models that are
a component of some task like speech recognition or machine translation, we can
compare the performance of two candidate language models by running the speech
recognizer or machine translator twice, once with each language model, and seeing
which gives the more accurate transcription.
Unfortunately, running big NLP systems end-to-end is often very expensive. In-
stead, it’s helpful to have a metric that can be used to quickly evaluate potential
improvements in a language model. An intrinsic evaluation metric is one that mea-intrinsic
evaluation
sures the quality of a model independent of any application. In the next section we’ll
introduce perplexity , which is the standard intrinsic metric for measuring language
model performance, both for simple n-gram language models and for the more so-
phisticated neural large language models of Chapter 9.
In order to evaluate any machine learning model, we need to have at least three
distinct data sets: the training set , the development set , and the test set . training set
development
set
test setThe training set is the data we use to learn the parameters of our model; for
simple n-gram language models it’s the corpus from which we get the counts that
we normalize into the probabilities of the n-gram language model.
Thetest set is a different, held-out set of data, not overlapping with the training
set, that we use to evaluate the model. We need a separate test set to give us an
unbiased estimate of how well the model we trained can generalize when we apply
it to some new unknown dataset. A machine learning model that perfectly captured
the training data, but performed terribly on any other data, wouldn’t be much use
when it comes time to apply it to any new data or problem! We thus measure the
quality of an n-gram model by its performance on this unseen test set or test corpus.
How should we choose a training and test set? The test set should reﬂect the
language we want to use the model for. If we’re going to use our language model

--- Page 39 ---
3.3 • E VALUATING LANGUAGE MODELS : PERPLEXITY 39
for speech recognition of chemistry lectures, the test set should be text of chemistry
lectures. If we’re going to use it as part of a system for translating hotel booking re-
quests from Chinese to English, the test set should be text of hotel booking requests.
If we want our language model to be general purpose, then the test set should be
drawn from a wide variety of texts. In such cases we might collect a lot of texts
from different sources, and then divide it up into a training set and a test set. It’s
important to do the dividing carefully; if we’re building a general purpose model,
we don’t want the test set to consist of only text from one document, or one author,
since that wouldn’t be a good measure of general performance.
Thus if we are given a corpus of text and want to compare the performance of
two different n-gram models, we divide the data into training and test sets, and train
the parameters of both models on the training set. We can then compare how well
the two trained models ﬁt the test set.
But what does it mean to “ﬁt the test set”? The standard answer is simple:
whichever language model assigns a higher probability to the test set—which
means it more accurately predicts the test set—is a better model. Given two proba-
bilistic models, the better model is the one that better predicts the details of the test
data, and hence will assign a higher probability to the test data.
Since our evaluation metric is based on test set probability, it’s important not to
let the test sentences into the training set. Suppose we are trying to compute the
probability of a particular “test” sentence. If our test sentence is part of the training
corpus, we will mistakenly assign it an artiﬁcially high probability when it occurs
in the test set. We call this situation training on the test set . Training on the test
set introduces a bias that makes the probabilities all look too high, and causes huge
inaccuracies in perplexity , the probability-based metric we introduce below.
Even if we don’t train on the test set, if we test our language model on the
test set many times after making different changes, we might implicitly tune to its
characteristics, by noticing which changes seem to make the model better. For this
reason, we only want to run our model on the test set once, or a very few number of
times, once we are sure our model is ready.
For this reason we normally instead have a third dataset called a developmentdevelopment
test
test set or, devset . We do all our testing on this dataset until the very end, and then
we test on the test once to see how good our model is.
How do we divide our data into training, development, and test sets? We want
our test set to be as large as possible, since a small test set may be accidentally un-
representative, but we also want as much training data as possible. At the minimum,
we would want to pick the smallest test set that gives us enough statistical power
to measure a statistically signiﬁcant difference between two potential models. It’s
important that the devset be drawn from the same kind of text as the test set, since
its goal is to measure how we would do on the test set.
3.3 Evaluating Language Models: Perplexity
We said above that we evaluate language models based on which one assigns a
higher probability to the test set. A better model is better at predicting upcoming
words, and so it will be less surprised by (i.e., assign a higher probability to) each
word when it occurs in the test set. Indeed, a perfect language model would correctly
guess each next word in a corpus, assigning it a probability of 1, and all the other
words a probability of zero. So given a test corpus, a better language model will

--- Page 40 ---
40 CHAPTER 3 • N- GRAM LANGUAGE MODELS
assign a higher probability to it than a worse language model.
But in fact, we do not use raw probability as our metric for evaluating language
models. The reason is that the probability of a test set (or any sequence) depends
on the number of words or tokens in it; the probability of a test set gets smaller the
longer the text. We’d prefer a metric that is per-word, normalized by length, so we
could compare across texts of different lengths. The metric we use is, a function of
probability called perplexity , is one of the most important metrics in NLP, used for
evaluating large language models as well as n-gram models.
Theperplexity (sometimes abbreviated as PP or PPL) of a language model on a perplexity
test set is the inverse probability of the test set (one over the probability of the test
set), normalized by the number of words (or tokens). For this reason it’s sometimes
called the per-word or per-token perplexity. We normalize by the number of words
Nby taking the Nth root. For a test set W=w1w2:::wN,:
perplexity (W) = P(w1w2:::wN) 1
N (3.14)
=Ns
1
P(w1w2:::wN)
Or we can use the chain rule to expand the probability of W:
perplexity (W) =NvuutNY
i=11
P(wijw1:::wi 1)(3.15)
Note that because of the inverse in Eq. 3.15, the higher the probability of the word
sequence, the lower the perplexity. Thus the the lower the perplexity of a model on
the data, the better the model . Minimizing perplexity is equivalent to maximizing
the test set probability according to the language model. Why does perplexity use
the inverse probability? It turns out the inverse arises from the original deﬁnition
of perplexity from cross-entropy rate in information theory; for those interested, the
explanation is in the advanced Section 3.7. Meanwhile, we just have to remember
that perplexity has an inverse relationship with probability.
The details of computing the perplexity of a test set Wdepends on which lan-
guage model we use. Here’s the perplexity of Wwith a unigram language model
(just the geometric mean of the inverse of the unigram probabilities):
perplexity (W) =NvuutNY
i=11
P(wi)(3.16)
The perplexity of Wcomputed with a bigram language model is still a geometric
mean, but now of the inverse of the bigram probabilities:
perplexity (W) =NvuutNY
i=11
P(wijwi 1)(3.17)
What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire
sequence of words in some test set. Since this sequence will cross many sentence
boundaries, if our vocabulary includes a between-sentence token <EOS> or separate
begin- and end-sentence markers <s> and</s> then we can include them in the

--- Page 41 ---
3.3 • E VALUATING LANGUAGE MODELS : PERPLEXITY 41
probability computation. If we do, then we also include one token per sentence in
the total count of word tokens N.2
We mentioned above that perplexity is a function of both the text and the lan-
guage model: given a text W, different language models will have different perplex-
ities. Because of this, perplexity can be used to compare different language models.
For example, here we trained unigram, bigram, and trigram grammars on 38 million
words from the Wall Street Journal newspaper. We then computed the perplexity of
each of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for
bigrams, and the corresponding equation for trigrams. The table below shows the
perplexity of the 1.5 million word test set according to each of the language models.
Unigram Bigram Trigram
Perplexity 962 170 109
As we see above, the more information the n-gram gives us about the word
sequence, the higher the probability the n-gram will assign to the string. A trigram
model is less surprised than a unigram model because it has a better idea of what
words might come next, and so it assigns them a higher probability. And the higher
the probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is
related inversely to the probability of the test sequence according to the model). So
a lower perplexity tells us that a language model is a better predictor of the test set.
Note that in computing perplexities, the language model must be constructed
without any knowledge of the test set, or else the perplexity will be artiﬁcially low.
And the perplexity of two language models is only comparable if they use identical
vocabularies.
An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-
provement in the performance of a language processing task like speech recognition
or machine translation. Nonetheless, because perplexity usually correlates with task
improvements, it is commonly used as a convenient evaluation metric. Still, when
possible a model’s improvement in perplexity should be conﬁrmed by an end-to-end
evaluation on a real task.
3.3.1 Perplexity as Weighted Average Branching Factor
It turns out that perplexity can also be thought of as the weighted average branch-
ing factor of a language. The branching factor of a language is the number of
possible next words that can follow any word. For example consider a mini artiﬁcial
language that is deterministic (no probabilities), any word can follow any word, and
whose vocabulary consists of only three colors:
L=fred;blue;greeng (3.18)
The branching factor of this language is 3.
Now let’s make a probabilistic version of the same LM, let’s call it A, where each
word follows each other with equal probability1
3(it was trained on a training set with
equal counts for the 3 colors), and a test set T= “red red red red blue ”.
Let’s ﬁrst convince ourselves that if we compute the perplexity of this artiﬁcial
digit language on this test set (or any such test set) we indeed get 3. By Eq. 3.15, the
2For example if we use both begin and end tokens, we would include the end-of-sentence marker </s>
but not the beginning-of-sentence marker <s>in our count of N; This is because the end-sentence token is
followed directly by the begin-sentence token with probability almost 1, so we don’t want the probability
of that fake transition to inﬂuence our perplexity.

--- Page 42 ---
42 CHAPTER 3 • N- GRAM LANGUAGE MODELS
perplexity of AonTis:
perplexityA(T) = PA(red red red red blue ) 1
5
= 1
35! 1
5
=1
3 1
=3 (3.19)
But now suppose redwas very likely in the training set a different LM B, and so B
has the following probabilities:
P(red) =0:8P(green ) =0:1P(blue) =0:1 (3.20)
We should expect the perplexity of the same test set red red red red blue for
language model Bto be lower since most of the time the next color will be red, which
is very predictable, i.e. has a high probability. So the probability of the test set will
be higher, and since perplexity is inversely related to probability, the perplexity will
be lower. Thus, although the branching factor is still 3, the perplexity or weighted
branching factor is smaller:
perplexityB(T) = PB(red red red red blue ) 1=5
=0:04096 1
5
=0:527 1=1:89 (3.21)
3.4 Sampling sentences from a language model
010.06the.060.03of0.02a0.02toin.09.11.13.15…however(p=0.0003)polyphonicp=0.0000018…0.02.66.99…
Figure 3.3 A visualization of the sampling distribution for sampling sentences by repeat-
edly sampling unigrams. The blue bar represents the relative frequency of each word (we’ve
ordered them from most frequent to least frequent, but the choice of order is arbitrary). The
number line shows the cumulative probabilities. If we choose a random number between 0
and 1, it will fall in an interval corresponding to some word. The expectation for the random
number to fall in the larger intervals of one of the frequent words ( the,of,a) is much higher
than in the smaller interval of one of the rare words ( polyphonic ).
One important way to visualize what kind of knowledge a language model em-
bodies is to sample from it. Sampling from a distribution means to choose random sampling
points according to their likelihood. Thus sampling from a language model—which
represents a distribution over sentences—means to generate some sentences, choos-
ing each sentence according to its likelihood as deﬁned by the model. Thus we are
more likely to generate sentences that the model thinks have a high probability and
less likely to generate sentences that the model thinks have a low probability.

--- Page 43 ---
3.5 • G ENERALIZING VS .OVERFITTING THE TRAINING SET 43
This technique of visualizing a language model by sampling was ﬁrst suggested
very early on by Shannon (1948) and Miller and Selfridge (1950). It’s simplest to
visualize how this works for the unigram case. Imagine all the words of the English
language covering the number line between 0 and 1, each word covering an interval
proportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM
computed from the text of this book. We choose a random value between 0 and 1,
ﬁnd that point on the probability line, and print the word whose interval includes this
chosen value. We continue choosing random numbers and generating words until
we randomly generate the sentence-ﬁnal token </s> .
We can use the same technique to generate bigrams by ﬁrst generating a ran-
dom bigram that starts with <s>(according to its bigram probability). Let’s say the
second word of that bigram is w. We next choose a random bigram starting with w
(again, drawn according to its bigram probability), and so on.
3.5 Generalizing vs. overﬁtting the training set
The n-gram model, like many statistical models, is dependent on the training corpus.
One implication of this is that the probabilities often encode speciﬁc facts about a
given training corpus. Another implication is that n-grams do a better and better job
of modeling the training corpus as we increase the value of N.
We can use the sampling method from the prior section to visualize both of
these facts! To give an intuition for the increasing power of higher-order n-grams,
Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-
gram models trained on Shakespeare’s works.
1–To him swallowed confess hear both. Which. Of save on trail for are ay device and
rote life have
gram –Hill he late speaks; or! a more to leg less ﬁrst you enter
2–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live
king. Follow.
gram –What means, sir. I confess she? then all sorts, he is trim, captain.
3–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,
’tis done.
gram –This shall forbid it should be branded, if renown made it empty.
4–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A
great banquet serv’d in;
gram –It cannot be but so.
Figure 3.4 Eight sentences randomly generated from four n-grams computed from Shakespeare’s works. All
characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected
for capitalization to improve readability.
The longer the context, the more coherent the sentences. The unigram sen-
tences show no coherent relation between words nor any sentence-ﬁnal punctua-
tion. The bigram sentences have some local word-to-word coherence (especially
considering punctuation as words). The trigram sentences are beginning to look a
lot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shake-
speare. The words It cannot be but so are directly from King John . This is because,
not to put the knock on Shakespeare, his oeuvre is not very large as corpora go

--- Page 44 ---
44 CHAPTER 3 • N- GRAM LANGUAGE MODELS
(N=884;647;V=29;066), and our n-gram probability matrices are ridiculously
sparse. There are V2=844;000;000 possible bigrams alone, and the number of
possible 4-grams is V4=71017. Thus, once the generator has chosen the ﬁrst
3-gram ( It cannot be ), there are only seven possible next words for the 4th element
(but,I,that,thus,this, and the period).
To get an idea of the dependence on the training set, let’s look at LMs trained on a
completely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare
and the WSJ are both English, so we might have expected some overlap between our
n-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram,
and trigram grammars trained on 40 million words from WSJ.
1Months the my and issue of year foreign new exchange’s september
were recession exchange new endorsed a acquire to six executivesgram
2Last December through the way to preserve the Hudson corporation N.
B. E. C. Taylor would seem to complete the major central planners one
gram point ﬁve percent of U. S. E. has already old M. X. corporation of living
on information such as more frequently ﬁshing to keep her
3They also point to ninety nine point six billion dollars from two hundred
four oh six three percent of the rates of interest stores as Mexico and
gram Brazil on market conditions
Figure 3.5 Three sentences randomly generated from three n-gram models computed from
40 million words of the Wall Street Journal , lower-casing all characters and treating punctua-
tion as words. Output was then hand-corrected for capitalization to improve readability.
Compare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both
model “English-like sentences”, there is no overlap in the generated sentences, and
little overlap even in small phrases. Statistical models are pretty useless as predictors
if the training sets and the test sets are as different as Shakespeare and the WSJ.
How should we deal with this problem when we build n-gram models? One step
is to be sure to use a training corpus that has a similar genre to whatever task we are
trying to accomplish. To build a language model for translating legal documents,
we need a training corpus of legal documents. To build a language model for a
question-answering system, we need a training corpus of questions.
It is equally important to get training data in the appropriate dialect orvariety ,
especially when processing social media posts or spoken transcripts. For exam-
ple some tweets will use features of African American English (AAE)— the name
for the many variations of language used in African American communities (King,
2020). Such features can include words like ﬁnna —an auxiliary verb that marks
immediate future tense —that don’t occur in other varieties, or spellings like denfor
then, in tweets like this one (Blodgett and O’Connor, 2017):
(3.22) Bored af den my phone ﬁnna die!!!
while tweets from English-based languages like Nigerian Pidgin have markedly dif-
ferent vocabulary and n-gram patterns from American English (Jurgens et al., 2017):
(3.23) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u
tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter
Is it possible for the testset nonetheless to have a word we have never seen be-
fore? What happens if the word Jurafsky never occurs in our training set, but pops
up in the test set? The answer is that although words might be unseen, we actu-
ally run our NLP algorithms not on words but on subword tokens . With subword

--- Page 45 ---
3.6 • S MOOTHING , INTERPOLATION ,AND BACKOFF 45
tokenization (like the BPE algorithm of Chapter 2) any word can be modeled as a
sequence of known smaller subwords, if necessary by a sequence of individual let-
ters. So although for convenience we’ve been referring to words in this chapter, the
language model vocabulary is actually the set of tokens rather than words, and the
test set can never contain unseen tokens.
3.6 Smoothing, Interpolation, and Backoff
There is a problem with using maximum likelihood estimates for probabilities: any
ﬁnite training corpus will be missing some perfectly acceptable English word se-
quences. That is, cases where a particular n-gram never occurs in the training data
but appears in the test set. Perhaps our training corpus has the words ruby and
slippers in it but just happens not to have the phrase ruby slippers .
These unseen sequences or zeros —sequences that don’t occur in the training set zeros
but do occur in the test set—are a problem for two reasons. First, their presence
means we are underestimating the probability of word sequences that might occur,
which hurts the performance of any application we want to run on this data. Second,
if the probability of any word in the test set is 0, the probability of the whole test
set is 0. Perplexity is deﬁned based on the inverse probability of the test set. Thus
if some words in context have zero probability, we can’t compute perplexity at all,
since we can’t divide by 0!
The standard way to deal with putative “zero probability n-grams” that should re-
ally have some non-zero probability is called smoothing ordiscounting . Smoothing smoothing
discounting algorithms shave off a bit of probability mass from some more frequent events and
give it to unseen events. Here we’ll introduce some simple smoothing algorithms:
Laplace (add-one) smoothing ,stupid backoff , and n-gram interpolation .
3.6.1 Laplace Smoothing
The simplest way to do smoothing is to add one to all the n-gram counts, before
we normalize them into probabilities. All the counts that used to be zero will now
have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called
Laplace smoothing . Laplace smoothing does not perform well enough to be usedLaplace
smoothing
in modern n-gram models, but it usefully introduces many of the concepts that we
see in other smoothing algorithms, gives a useful baseline, and is also a practical
smoothing algorithm for other tasks like text classiﬁcation (Chapter 4).
Let’s start with the application of Laplace smoothing to unigram probabilities.
Recall that the unsmoothed maximum likelihood estimate of the unigram probability
of the word wiis its count cinormalized by the total number of word tokens N:
P(wi) =ci
N
Laplace smoothing merely adds one to each count (hence its alternate name add-
onesmoothing). Since there are Vwords in the vocabulary and each one was in- add-one
cremented, we also need to adjust the denominator to take into account the extra V
observations. (What happens to our Pvalues if we don’t increase the denominator?)
PLaplace (wi) =ci+1
N+V(3.24)

--- Page 46 ---
46 CHAPTER 3 • N- GRAM LANGUAGE MODELS
Instead of changing both the numerator and denominator, it is convenient to describe
how a smoothing algorithm affects the numerator, by deﬁning an adjusted count c.
This adjusted count is easier to compare directly with the MLE counts and can be
turned into a probability like an MLE count by normalizing by N. To deﬁne this
count, since we are only changing the numerator in addition to adding 1 we’ll also
need to multiply by a normalization factorN
N+V:
c
i= (ci+1)N
N+V(3.25)
We can now turn c
iinto a probability P
iby normalizing by N.
A related way to view smoothing is as discounting (lowering) some non-zero discounting
counts in order to get the probability mass that will be assigned to the zero counts.
Thus, instead of referring to the discounted counts c, we might describe a smooth-
ing algorithm in terms of a relative discount di, the ratio of the discounted counts to discount
the original counts:
di=c
i
ci
Now that we have the intuition for the unigram case, let’s smooth our Berkeley
Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the
bigrams in Fig. 3.1.
i want to eat chinese food lunch spend
i 6 828 1 10 1 1 1 3
want 3 1 609 2 7 7 6 2
to 3 1 5 687 3 1 7 212
eat 1 1 3 1 17 3 43 1
chinese 2 1 1 1 1 83 2 1
food 16 1 16 1 2 5 1 1
lunch 3 1 1 1 1 2 1 1
spend 2 1 2 1 1 1 1 1
Figure 3.6 Add-one smoothed bigram counts for eight of the words (out of V=1446) in
the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.
Figure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.
Recall that normal bigram probabilities are computed by normalizing each row of
counts by the unigram count:
PMLE(wnjwn 1) =C(wn 1wn)
C(wn 1)(3.26)
For add-one smoothed bigram counts, we need to augment the unigram count by the
number of total word types in the vocabulary V:
PLaplace (wnjwn 1) =C(wn 1wn)+1P
w(C(wn 1w)+1)=C(wn 1wn)+1
C(wn 1)+V(3.27)
Thus, each of the unigram counts given in the previous section will need to be aug-
mented by V=1446. The result is the smoothed bigram probabilities in Fig. 3.7.
It is often convenient to reconstruct the count matrix so we can see how much a
smoothing algorithm has changed the original counts. These adjusted counts can be

--- Page 47 ---
3.6 • S MOOTHING , INTERPOLATION ,AND BACKOFF 47
i want to eat chinese food lunch spend
i 0.0015 0.21 0.00025 0.0025 0.00025 0.00025 0.00025 0.00075
want 0.0013 0.00042 0.26 0.00084 0.0029 0.0029 0.0025 0.00084
to 0.00078 0.00026 0.0013 0.18 0.00078 0.00026 0.0018 0.055
eat 0.00046 0.00046 0.0014 0.00046 0.0078 0.0014 0.02 0.00046
chinese 0.0012 0.00062 0.00062 0.00062 0.00062 0.052 0.0012 0.00062
food 0.0063 0.00039 0.0063 0.00039 0.00079 0.002 0.00039 0.00039
lunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056
spend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058
Figure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of V=1446) in the BeRP
corpus of 9332 sentences. Previously-zero probabilities are in gray.
i want to eat chinese food lunch spend
i 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9
want 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78
to 1.9 0.63 3.1 430 1.9 0.63 4.4 133
eat 0.34 0.34 1 0.34 5.8 1 15 0.34
chinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098
food 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43
lunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19
spend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16
Figure 3.8 Add-one reconstituted counts for eight words (of V=1446) in the BeRP corpus
of 9332 sentences. Previously-zero counts are in gray.
computed by Eq. 3.28. Figure 3.8 shows the reconstructed counts.
c(wn 1wn) =[C(wn 1wn)+1]C(wn 1)
C(wn 1)+V(3.28)
Note that add-one smoothing has made a very big change to the counts. Com-
paring Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to )changed
from 608 to 238! We can see this in probability space as well: P(tojwant)decreases
from 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the dis-
count d(the ratio between new and old counts) shows us how strikingly the counts
for each preﬁx word have been reduced; the discount for the bigram want to is 0.39,
while the discount for Chinese food is 0.10, a factor of 10! The sharp change occurs
because too much probability mass is moved to all the zeros.
3.6.2 Add-k smoothing
One alternative to add-one smoothing is to move a bit less of the probability mass
from the seen to the unseen events. Instead of adding 1 to each count, we add a
fractional count k(0.5? 0.01?). This algorithm is therefore called add-k smoothing . add-k
P
Add-k(wnjwn 1) =C(wn 1wn)+k
C(wn 1)+kV(3.29)
Add-k smoothing requires that we have a method for choosing k; this can be
done, for example, by optimizing on a devset . Although add-k is useful for some
tasks (including text classiﬁcation), it turns out that it still doesn’t work well for
language modeling, generating counts with poor variances and often inappropriate
discounts (Gale and Church, 1994).

--- Page 48 ---
48 CHAPTER 3 • N- GRAM LANGUAGE MODELS
3.6.3 Language Model Interpolation
There is an alternative source of knowledge we can draw on to solve the problem
of zero frequency n-grams. If we are trying to compute P(wnjwn 2wn 1)but we
have no examples of a particular trigram wn 2wn 1wn, we can instead estimate its
probability by using the bigram probability P(wnjwn 1). Similarly, if we don’t have
counts to compute P(wnjwn 1), we can look to the unigram P(wn). In other words,
sometimes using less context can help us generalize more for contexts that the model
hasn’t learned much about.
The most common way to use this n-gram hierarchy is called interpolation : interpolation
computing a new probability by interpolating (weighting and combining) the tri-
gram, bigram, and unigram probabilities.3In simple linear interpolation, we com-
bine different order n-grams by linearly interpolating them. Thus, we estimate the
trigram probability P(wnjwn 2wn 1)by mixing together the unigram, bigram, and
trigram probabilities, each weighted by a l:
ˆP(wnjwn 2wn 1) = l1P(wn)
+l2P(wnjwn 1)
+l3P(wnjwn 2wn 1) (3.30)
Thels must sum to 1, making Eq. 3.30 equivalent to a weighted average. In a
slightly more sophisticated version of linear interpolation, each lweight is com-
puted by conditioning on the context. This way, if we have particularly accurate
counts for a particular bigram, we assume that the counts of the trigrams based on
this bigram will be more trustworthy, so we can make the ls for those trigrams
higher and thus give that trigram more weight in the interpolation. Equation 3.31
shows the equation for interpolation with context-conditioned weights, where each
lambda takes an argument that is the two prior word context:
ˆP(wnjwn 2wn 1) = l1(wn 2:n 1)P(wn)
+l2(wn 2:n 1)P(wnjwn 1)
+l3(wn 2:n 1)P(wnjwn 2wn 1) (3.31)
How are these lvalues set? Both the simple interpolation and conditional interpo-
lation ls are learned from a held-out corpus. A held-out corpus is an additional held-out
training corpus, so-called because we hold it out from the training data, that we use
to set these lvalues.4We do so by choosing the lvalues that maximize the likeli-
hood of the held-out corpus. That is, we ﬁx the n-gram probabilities and then search
for the lvalues that—when plugged into Eq. 3.30—give us the highest probability
of the held-out set. There are various ways to ﬁnd this optimal set of ls. One way
is to use the EMalgorithm, an iterative learning algorithm that converges on locally
optimal ls (Jelinek and Mercer, 1980).
3.6.4 Stupid Backoff
An alternative to interpolation is backoff . In a backoff model, if the n-gram we need backoff
3We won’t discuss the less-common alternative, called backoff , in which we use the trigram if the
evidence is sufﬁcient for it, but if not we instead just use the bigram, otherwise the unigram. That is, we
only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram.
4Held-out corpora are generally used to set hyperparameters , which are special parameters, unlike
regular counts that are learned from the training data; we’ll discuss hyperparameters in Chapter 7.

--- Page 49 ---
3.7 • A DVANCED : PERPLEXITY ’SRELATION TO ENTROPY 49
has zero counts, we approximate it by backing off to the (n-1)-gram. We continue
backing off until we reach a history that has some counts. For a backoff model to
give a correct probability distribution, we have to discount the higher-order n-grams discount
to save some probability mass for the lower order n-grams. In practice, instead of
discounting, it’s common to use a much simpler non-discounted backoff algorithm
called stupid backoff (Brants et al., 2007). stupid backoff
Stupid backoff gives up the idea of trying to make the language model a true
probability distribution. There is no discounting of the higher-order probabilities. If
a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram,
weighed by a ﬁxed (context-independent) weight. This algorithm does not produce
a probability distribution, so we’ll follow Brants et al. (2007) in referring to it as S:
S(wijwi N+1:i 1) =8
<
:count (wi N+1:i)
count (wi N+1:i 1)if count (wi N+1:i)>0
lS(wijwi N+2:i 1)otherwise(3.32)
The backoff terminates in the unigram, which has score S(w) =count (w)
N. Brants et al.
(2007) ﬁnd that a value of 0.4 worked well for l.
3.7 Advanced: Perplexity’s Relation to Entropy
We introduced perplexity in Section 3.3 as a way to evaluate n-gram models on
a test set. A better n-gram model is one that assigns a higher probability to the
test data, and perplexity is a normalized version of the probability of the test set.
The perplexity measure actually arises from the information-theoretic concept of
cross-entropy, which explains otherwise mysterious properties of perplexity (why
the inverse probability, for example?) and its relationship to entropy. Entropy is a Entropy
measure of information. Given a random variable Xranging over whatever we are
predicting (words, letters, parts of speech), the set of which we’ll call c, and with a
particular probability function, call it p(x), the entropy of the random variable Xis:
H(X) = X
x2cp(x)log2p(x) (3.33)
The log can, in principle, be computed in any base. If we use log base 2, the
resulting value of entropy will be measured in bits.
One intuitive way to think about entropy is as a lower bound on the number of
bits it would take to encode a certain decision or piece of information in the optimal
coding scheme. Consider an example from the standard information theory textbook
Cover and Thomas (1991). Imagine that we want to place a bet on a horse race but
it is too far to go all the way to Yonkers Racetrack, so we’d like to send a short
message to the bookie to tell him which of the eight horses to bet on. One way to
encode this message is just to use the binary representation of the horse’s number
as the code; thus, horse 1 would be 001, horse 2010, horse 3011, and so on, with
horse 8 coded as 000. If we spend the whole day betting and each horse is coded
with 3 bits, on average we would be sending 3 bits per race.
Can we do better? Suppose that the spread is the actual distribution of the bets
placed and that we represent it as the prior probability of each horse as follows:

--- Page 50 ---
50 CHAPTER 3 • N- GRAM LANGUAGE MODELS
Horse 11
2Horse 51
64
Horse 21
4Horse 61
64
Horse 31
8Horse 71
64
Horse 41
16Horse 81
64
The entropy of the random variable Xthat ranges over horses gives us a lower
bound on the number of bits and is
H(X) = i=8X
i=1p(i)log2p(i)
= 1
2log21
2 1
4log21
4 1
8log21
8 1
16log21
16 4(1
64log21
64)
=2 bits (3.34)
A code that averages 2 bits per race can be built with short encodings for more
probable horses, and longer encodings for less probable horses. For example, we
could encode the most likely horse with the code 0, and the remaining horses as 10,
then110,1110 ,111100 ,111101 ,111110 , and111111 .
What if the horses are equally likely? We saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3. Is the entropy the same? In this case each horse would have a
probability of1
8. The entropy of the choice of horses is then
H(X) = i=8X
i=11
8log21
8= log21
8=3 bits (3.35)
Until now we have been computing the entropy of a single variable. But most of
what we will use entropy for involves sequences . For a grammar, for example, we
will be computing the entropy of some sequence of words W=fw1;w2;:::; wng.
One way to do this is to have a variable that ranges over sequences of words. For
example we can compute the entropy of a random variable that ranges over all se-
quences of words of length nin some language Las follows:
H(w1;w2;:::; wn) = X
w1:n2Lp(w1:n)logp(w1:n) (3.36)
We could deﬁne the entropy rate (we could also think of this as the per-word entropy rate
entropy ) as the entropy of this sequence divided by the number of words:
1
nH(w1:n) = 1
nX
w1:n2Lp(w1:n)logp(w1:n) (3.37)
But to measure the true entropy of a language, we need to consider sequences of
inﬁnite length. If we think of a language as a stochastic process Lthat produces a
sequence of words, and allow Wto represent the sequence of words w1;:::; wn, then
L’s entropy rate H(L)is deﬁned as
H(L) = lim
n!¥1
nH(w1:n)
= lim
n!¥1
nX
W2Lp(w1:n)logp(w1:n) (3.38)

--- Page 51 ---
3.7 • A DVANCED : PERPLEXITY ’SRELATION TO ENTROPY 51
The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas
1991) states that if the language is regular in certain ways (to be exact, if it is both
stationary and ergodic),
H(L) =lim
n!¥ 1
nlogp(w1:n) (3.39)
That is, we can take a single sequence that is long enough instead of summing over
all possible sequences. The intuition of the Shannon-McMillan-Breiman theorem
is that a long-enough sequence of words will contain in it many other shorter se-
quences and that each of these shorter sequences will reoccur in the longer sequence
according to their probabilities.
A stochastic process is said to be stationary if the probabilities it assigns to a Stationary
sequence are invariant with respect to shifts in the time index. In other words, the
probability distribution for words at time tis the same as the probability distribution
at time t+1. Markov models, and hence n-grams, are stationary. For example, in
a bigram, Piis dependent only on Pi 1. So if we shift our time index by x,Pi+xis
still dependent on Pi+x 1. But natural language is not stationary, since as we show
in Appendix D, the probability of upcoming words can be dependent on events that
were arbitrarily distant and time dependent. Thus, our statistical models only give
an approximation to the correct distributions and entropies of natural language.
To summarize, by making some incorrect but convenient simplifying assump-
tions, we can compute the entropy of some stochastic process by taking a very long
sample of the output and computing its average log probability.
Now we are ready to introduce cross-entropy . The cross-entropy is useful when cross-entropy
we don’t know the actual probability distribution pthat generated some data. It
allows us to use some m, which is a model of p(i.e., an approximation to p). The
cross-entropy of monpis deﬁned by
H(p;m) =lim
n!¥ 1
nX
W2Lp(w1;:::; wn)logm(w1;:::; wn) (3.40)
That is, we draw sequences according to the probability distribution p, but sum the
log of their probabilities according to m.
Again, following the Shannon-McMillan-Breiman theorem, for a stationary er-
godic process:
H(p;m) =lim
n!¥ 1
nlogm(w1w2:::wn) (3.41)
This means that, as for entropy, we can estimate the cross-entropy of a model m
on some distribution pby taking a single sequence that is long enough instead of
summing over all possible sequences.
What makes the cross-entropy useful is that the cross-entropy H(p;m)is an up-
per bound on the entropy H(p). For any model m:
H(p)H(p;m) (3.42)
This means that we can use some simpliﬁed model mto help estimate the true en-
tropy of a sequence of symbols drawn according to probability p. The more accurate
mis, the closer the cross-entropy H(p;m)will be to the true entropy H(p). Thus,
the difference between H(p;m)andH(p)is a measure of how accurate a model is.
Between two models m1andm2, the more accurate model will be the one with the

--- Page 52 ---
52 CHAPTER 3 • N- GRAM LANGUAGE MODELS
lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so
a model cannot err by underestimating the true entropy.)
We are ﬁnally ready to see the relation between perplexity and cross-entropy
as we saw it in Eq. 3.41. Cross-entropy is deﬁned in the limit as the length of the
observed word sequence goes to inﬁnity. We approximate this cross-entropy by
relying on a (sufﬁciently long) sequence of ﬁxed length. This approximation to the
cross-entropy of a model M=P(wijwi N+1:i 1)on a sequence of words Wis
H(W) = 1
NlogP(w1w2:::wN) (3.43)
Theperplexity of a model Pon a sequence of words Wis now formally deﬁned as perplexity
2 raised to the power of this cross-entropy:
Perplexity (W) = 2H(W)
=P(w1w2:::wN) 1
N
=Ns
1
P(w1w2:::wN)
3.8 Summary
This chapter introduced language modeling via the n-gram model, a classic model
that allows us to introduce many of the basic concepts in language modeling.
• Language models offer a way to assign a probability to a sentence or other
sequence of words or tokens, and to predict a word or token from preceding
words or tokens.
•N-grams are perhaps the simplest kind of language model. They are Markov
models that estimate words from a ﬁxed window of previous words. N-gram
models can be trained by counting in a training corpus and normalizing the
counts (the maximum likelihood estimate ).
• N-gram language models can be evaluated on a test set using perplexity.
• The perplexity of a test set according to a language model is a function of
the probability of the test set: the inverse test set probability according to the
model, normalized by the length.
•Sampling from a language model means to generate some sentences, choos-
ing each sentence according to its likelihood as deﬁned by the model.
•Smoothing algorithms provide a way to estimate probabilities for events that
were unseen in training. Commonly used smoothing algorithms for n-grams
include add-1 smoothing, or rely on lower-order n-gram counts through inter-
polation .
Bibliographical and Historical Notes
The underlying mathematics of the n-gram was ﬁrst proposed by Markov (1913),
who used what are now called Markov chains (bigrams and trigrams) to predict
whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con-
sonant. Markov classiﬁed 20,000 letters as V or C and computed the bigram and

--- Page 53 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 53
trigram probability that a given letter would be a vowel given the previous one or
two letters. Shannon (1948) applied n-grams to compute approximations to English
word sequences. Based on Shannon’s work, Markov models were commonly used in
engineering, linguistic, and psychological work on modeling word sequences by the
1950s. In a series of extremely inﬂuential papers starting with Chomsky (1956) and
including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued
that “ﬁnite-state Markov processes”, while a possibly useful engineering heuristic,
were incapable of being a complete cognitive model of human grammatical knowl-
edge. These arguments led many linguists and computational linguists to ignore
work in statistical modeling for decades.
The resurgence of n-gram language models came from Fred Jelinek and col-
leagues at the IBM Thomas J. Watson Research Center, who were inﬂuenced by
Shannon, and James Baker at CMU, who was inﬂuenced by the prior, classiﬁed
work of Leonard Baum and colleagues on these topics at labs like the US Institute
for Defense SAnalyses (IDA) after they were declassiﬁed. Independently these two
labs successfully used n-grams in their speech recognition systems at the same time
(Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The
terms “language model” and “perplexity” were ﬁrst used for this technology by the
IBM group. Jelinek and his colleagues used the term language model in a pretty
modern way, to mean the entire set of linguistic inﬂuences on word sequence prob-
abilities, including grammar, semantics, discourse, and even speaker characteristics,
rather than just the particular n-gram model itself.
Add-one smoothing derives from Laplace’s 1812 law of succession and was ﬁrst
applied as an engineering solution to the zero frequency problem by Jeffreys (1948)
based on an earlier Add-K suggestion by Johnson (1932). Problems with the add-
one algorithm are summarized in Gale and Church (1994).
A wide variety of different language modeling and smoothing techniques were
proposed in the 80s and 90s, including Good-Turing discounting—ﬁrst applied to the
n-gram smoothing at IBM by Katz (N ´adas 1984, Church and Gale 1991)— Witten-
Bell discounting (Witten and Bell, 1991), and varieties of class-based n-gram mod-class-based
n-gram
els that used information about word classes. Starting in the late 1990s, Chen and
Goodman performed a number of carefully controlled experiments comparing dif-
ferent algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter
alia). They showed the advantages of Modiﬁed Interpolated Kneser-Ney , which
became the standard baseline for n-gram language modeling around the turn of the
century, especially because they showed that caches and class-based models pro-
vided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM
(Heaﬁeld 2011, Heaﬁeld et al. 2013) are publicly available toolkits for building n-
gram language models.
Large language models are based on neural networks rather than n-grams, en-
abling them to solve the two major problems with n-grams: (1) the number of param-
eters increases exponentially as the n-gram order increases, and (2) n-grams have no
way to generalize from training examples to test set examples unless they use iden-
tical words. Neural language models instead project words into a continuous space
in which words with similar contexts have similar representations. We’ll introduce
transformer-based large language models in Chapter 9, along the way introducing
feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and
recurrent language models (Mikolov, 2012) in Chapter 8.

--- Page 54 ---
54 CHAPTER 3 • N- GRAM LANGUAGE MODELS
Exercises
3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11).
Now write out all the non-zero trigram probabilities for the I am Sam corpus
on page 35.
3.2 Calculate the probability of the sentence i want chinese food . Give two
probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on
page 37, and another using the add-1 smoothed table in Fig. 3.7. Assume the
additional add-1 smoothed probabilities P(i|<s> ) =0:19 and P(</s>|food ) =
0:40.
3.3 Which of the two probabilities you computed in the previous exercise is higher,
unsmoothed or smoothed? Explain why.
3.4 We are given the following corpus, modiﬁed from the one in the chapter:
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>
Using a bigram language model with add-one smoothing, what is P(Sam j
am)? Include <s>and</s> in your counts just like any other token.
3.5 Suppose we didn’t use the end-symbol </s> . Train an unsmoothed bigram
grammar on the following training corpus without using the end-symbol </s> :
<s> a b
<s> b b
<s> b a
<s> a a
Demonstrate that your bigram model does not assign a single probability dis-
tribution across all sentence lengths by showing that the sum of the probability
of the four possible 2 word sentences over the alphabet fa,bgis 1.0, and the
sum of the probability of all possible 3 word sentences over the alphabet fa,bg
is also 1.0.
3.6 Suppose we train a trigram language model with add-one smoothing on a
given corpus. The corpus contains V word types. Express a formula for esti-
mating P(w3jw1,w2), where w3 is a word which follows the bigram (w1,w2),
in terms of various n-gram counts and V . Use the notation c(w1,w2,w3) to
denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and
so on for bigrams and unigrams.
3.7 We are given the following corpus, modiﬁed from the one in the chapter:
<s> I am Sam </s>
<s> Sam I am </s>
<s> I am Sam </s>
<s> I do not like green eggs and Sam </s>
If we use linear interpolation smoothing between a maximum-likelihood bi-
gram model and a maximum-likelihood unigram model with l1=1
2andl2=
1
2, what is P(Samjam)? Include <s> and</s> in your counts just like any
other token.
3.8 Write a program to compute unsmoothed unigrams and bigrams.

--- Page 55 ---
EXERCISES 55
3.9 Run your n-gram program on two different small corpora of your choice (you
might use email text or newsgroups). Now compare the statistics of the two
corpora. What are the differences in the most common unigrams between the
two? How about interesting differences in bigrams?
3.10 Add an option to your program to generate random sentences.
3.11 Add an option to your program to compute the perplexity of a test set.
3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1
each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0
0 0. What is the unigram perplexity?

--- Page 56 ---
56 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
CHAPTER
4Naive Bayes, Text Classiﬁca-
tion, and Sentiment
Classiﬁcation lies at the heart of both human and machine intelligence. Deciding
what letter, word, or image has been presented to our senses, recognizing faces
or voices, sorting mail, assigning grades to homeworks; these are all examples of
assigning a category to an input. The potential challenges of this task are highlighted
by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:
(a) those that belong to the Emperor, (b) embalmed ones, (c) those that
are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray
dogs, (h) those that are included in this classiﬁcation, (i) those that
tremble as if they were mad, (j) innumerable ones, (k) those drawn with
a very ﬁne camel’s hair brush, (l) others, (m) those that have just broken
a ﬂower vase, (n) those that resemble ﬂies from a distance.
Many language processing tasks involve classiﬁcation, although luckily our classes
are much easier to deﬁne than those of Borges. In this chapter we introduce the naive
Bayes algorithm and apply it to text categorization , the task of assigning a label ortext
categorization
category to an entire text or document.
We focus on one common text categorization task, sentiment analysis , the ex-sentiment
analysis
traction of sentiment , the positive or negative orientation that a writer expresses
toward some object. A review of a movie, book, or product on the web expresses the
author’s sentiment toward the product, while an editorial or political text expresses
sentiment toward a candidate or political action. Extracting consumer or public sen-
timent is thus relevant for ﬁelds from marketing to politics.
The simplest version of sentiment analysis is a binary classiﬁcation task, and
the words of the review provide excellent cues. Consider, for example, the follow-
ing phrases extracted from positive and negative reviews of movies and restaurants.
Words like great ,richly ,awesome , and pathetic , and awful andridiculously are very
informative cues:
+...zany characters and richly applied satire, and some great plot twists
 It was pathetic. The worst part about it was the boxing scenes...
+...awesome caramel sauce and sweet toasty almonds. I love this place!
 ...awful pizza and ridiculously overpriced...
Spam detection is another important commercial application, the binary clas- spam detection
siﬁcation task of assigning an email to one of the two classes spam ornot-spam .
Many lexical and other features can be used to perform this classiﬁcation. For ex-
ample you might quite reasonably be suspicious of an email containing phrases like
“online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.
Another thing we might want to know about a text is the language it’s written
in. Texts on social media, for example, can be in any number of languages and
we’ll need to apply different processing. The task of language id is thus the ﬁrst language id
step in most language processing pipelines. Related text classiﬁcation tasks like au-
thorship attribution — determining a text’s author— are also relevant to the digitalauthorship
attribution
humanities, social sciences, and forensic linguistics.

--- Page 57 ---
4.1 • N AIVE BAYES CLASSIFIERS 57
Finally, one of the oldest tasks in text classiﬁcation is assigning a library sub-
ject category or topic label to a text. Deciding whether a research paper concerns
epidemiology or instead, perhaps, embryology, is an important component of infor-
mation retrieval. Various sets of subject categories exist, such as the MeSH (Medical
Subject Headings) thesaurus. In fact, as we will see, subject category classiﬁcation
is the task for which the naive Bayes algorithm was invented in 1961 Maron (1961).
Classiﬁcation is essential for tasks below the level of the document as well.
We’ve already seen period disambiguation (deciding if a period is the end of a sen-
tence or part of a word), and word tokenization (deciding if a character should be
a word boundary). Even language modeling can be viewed as classiﬁcation: each
word can be thought of as a class, and so predicting the next word is classifying the
context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17)
classiﬁes each occurrence of a word in a sentence as, e.g., a noun or a verb.
The goal of classiﬁcation is to take a single observation, extract some useful
features, and thereby classify the observation into one of a set of discrete classes.
One method for classifying text is to use rules handwritten by humans. Handwrit-
ten rule-based classiﬁers can be components of state-of-the-art systems in language
processing. But rules can be fragile, as situations or data change over time, and for
some tasks humans aren’t necessarily good at coming up with the rules.
The most common way of doing text classiﬁcation in language processing is
instead via supervised machine learning , the subject of this chapter. In supervisedsupervised
machine
learninglearning, we have a data set of input observations, each associated with some correct
output (a ‘supervision signal’). The goal of the algorithm is to learn how to map
from a new observation to a correct output.
Formally, the task of supervised classiﬁcation is to take an input xand a ﬁxed
set of output classes Y=fy1;y2;:::;yMgand return a predicted class y2Y. For
text classiﬁcation, we’ll sometimes talk about c(for “class”) instead of yas our
output variable, and d(for “document”) instead of xas our input variable. In the
supervised situation we have a training set of Ndocuments that have each been hand-
labeled with a class: f(d1;c1);::::;(dN;cN)g. Our goal is to learn a classiﬁer that is
capable of mapping from a new document dto its correct class c2C, where Cis
some set of useful document classes. A probabilistic classiﬁer additionally will tell
us the probability of the observation being in the class. This full distribution over
the classes can be useful information for downstream decisions; avoiding making
discrete decisions early on can be useful when combining systems.
Many kinds of machine learning algorithms are used to build classiﬁers. This
chapter introduces naive Bayes; the following one introduces logistic regression.
These exemplify two ways of doing classiﬁcation. Generative classiﬁers like naive
Bayes build a model of how a class could generate some input data. Given an ob-
servation, they return the class most likely to have generated the observation. Dis-
criminative classiﬁers like logistic regression instead learn what features from the
input are most useful to discriminate between the different possible classes. While
discriminative systems are often more accurate and hence more commonly used,
generative classiﬁers still have a role.
4.1 Naive Bayes Classiﬁers
In this section we introduce the multinomial naive Bayes classiﬁer , so called be-naive Bayes
classiﬁer
cause it is a Bayesian classiﬁer that makes a simplifying (naive) assumption about

--- Page 58 ---
58 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
how the features interact.
The intuition of the classiﬁer is shown in Fig. 4.1. We represent a text document
as if it were a bag of words , that is, an unordered set of words with their position bag of words
ignored, keeping only their frequency in the document. In the example in the ﬁgure,
instead of representing the word order in all the phrases like “I love this movie” and
“I would recommend it”, we simply note that the word Ioccurred 5 times in the
entire excerpt, the word it6 times, the words love,recommend , and movie once, and
so on.
ititititititIIII
Iloverecommendmoviethethethetheto
totoand
andandseenseenyetwouldwithwhowhimsical
whilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat…6 54332111111111111…
Figure 4.1 Intuition of the multinomial naive Bayes classiﬁer applied to a movie review. The position of the
words is ignored (the bag-of-words assumption) and we make use of the frequency of each word.
Naive Bayes is a probabilistic classiﬁer, meaning that for a document d, out of
all classes c2Cthe classiﬁer returns the class ˆ cwhich has the maximum posterior
probability given the document. In Eq. 4.1 we use the hat notation ˆto mean “our ˆ
estimate of the correct class”, and we use argmax to mean an operation that selects argmax
the argument (in this case the class c) that maximizes a function (in this case the
probability P(cjd).
ˆc=argmax
c2CP(cjd) (4.1)
This idea of Bayesian inference has been known since the work of Bayes (1763),Bayesian
inference
and was ﬁrst applied to text classiﬁcation by Mosteller and Wallace (1964). The
intuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. 4.1 into
other probabilities that have some useful properties. Bayes’ rule is presented in
Eq. 4.2; it gives us a way to break down any conditional probability P(xjy)into
three other probabilities:
P(xjy) =P(yjx)P(x)
P(y)(4.2)

--- Page 59 ---
4.1 • N AIVE BAYES CLASSIFIERS 59
We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:
ˆc=argmax
c2CP(cjd) =argmax
c2CP(djc)P(c)
P(d)(4.3)
We can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This
is possible because we will be computingP(djc)P(c)
P(d)for each possible class. But P(d)
doesn’t change for each class; we are always asking about the most likely class for
the same document d, which must have the same probability P(d). Thus, we can
choose the class that maximizes this simpler formula:
ˆc=argmax
c2CP(cjd) =argmax
c2CP(djc)P(c) (4.4)
We call Naive Bayes a generative model because we can read Eq. 4.4 as stating
a kind of implicit assumption about how a document is generated: ﬁrst a class is
sampled from P(c), and then the words are generated by sampling from P(djc). (In
fact we could imagine generating artiﬁcial documents, or at least their word counts,
by following this process). We’ll say more about this intuition of generative models
in Chapter 5.
To return to classiﬁcation: we compute the most probable class ˆ cgiven some
document dby choosing the class which has the highest product of two probabilities:
theprior probability of the class P(c)and the likelihood of the document P(djc):prior
probability
likelihood
ˆc=argmax
c2Clikelihoodz}|{
P(djc)priorz}|{
P(c) (4.5)
Without loss of generality, we can represent a document das a set of features
f1;f2;:::;fn:
ˆc=argmax
c2Clikelihoodz}|{
P(f1;f2;::::; fnjc)priorz}|{
P(c) (4.6)
Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the probability of every possible combination of
features (for example, every possible set of words and positions) would require huge
numbers of parameters and impossibly large training sets. Naive Bayes classiﬁers
therefore make two simplifying assumptions.
The ﬁrst is the bag-of-words assumption discussed intuitively above: we assume
position doesn’t matter, and that the word “love” has the same effect on classiﬁcation
whether it occurs as the 1st, 20th, or last word in the document. Thus we assume
that the features f1;f2;:::;fnonly encode word identity and not position.
The second is commonly called the naive Bayes assumption : this is the condi-naive Bayes
assumption
tional independence assumption that the probabilities P(fijc)are independent given
the class cand hence can be ‘naively’ multiplied as follows:
P(f1;f2;::::; fnjc) = P(f1jc)P(f2jc):::P(fnjc) (4.7)
The ﬁnal equation for the class chosen by a naive Bayes classiﬁer is thus:
cNB=argmax
c2CP(c)Y
f2FP(fjc) (4.8)

--- Page 60 ---
60 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
To apply the naive Bayes classiﬁer to text, we will use each word in the documents
as a feature, as suggested above, and we consider each of the words in the document
by walking an index through every word position in the document:
positions all word positions in test document
cNB=argmax
c2CP(c)Y
i2positionsP(wijc) (4.9)
Naive Bayes calculations, like calculations for language modeling, are done in log
space, to avoid underﬂow and increase speed. Thus Eq. 4.9 is generally instead
expressed1as
cNB=argmax
c2ClogP(c)+X
i2positionslogP(wijc) (4.10)
By considering features in log space, Eq. 4.10 computes the predicted class as a lin-
ear function of input features. Classiﬁers that use a linear combination of the inputs
to make a classiﬁcation decision —like naive Bayes and also logistic regression—
are called linear classiﬁers .linear
classiﬁers
4.2 Training the Naive Bayes Classiﬁer
How can we learn the probabilities P(c)andP(fijc)? Let’s ﬁrst consider the maxi-
mum likelihood estimate. We’ll simply use the frequencies in the data. For the class
prior P(c)we ask what percentage of the documents in our training set are in each
class c. Let Ncbe the number of documents in our training data with class cand
Ndocbe the total number of documents. Then:
ˆP(c) =Nc
Ndoc(4.11)
To learn the probability P(fijc), we’ll assume a feature is just the existence of a word
in the document’s bag of words, and so we’ll want P(wijc), which we compute as
the fraction of times the word wiappears among all words in all documents of topic
c. We ﬁrst concatenate all documents with category cinto one big “category c” text.
Then we use the frequency of wiin this concatenated document to give a maximum
likelihood estimate of the probability:
ˆP(wijc) =count (wi;c)P
w2Vcount (w;c)(4.12)
Here the vocabulary V consists of the union of all the word types in all classes, not
just the words in one class c.
There is a problem, however, with maximum likelihood training. Imagine we
are trying to estimate the likelihood of the word “fantastic” given class positive , but
suppose there are no training documents that both contain the word “fantastic” and
are classiﬁed as positive . Perhaps the word “fantastic” happens to occur (sarcasti-
cally?) in the class negative . In such a case the probability for this feature will be
zero:
ˆP(“fantastic”jpositive ) =count (“fantastic”;positive )P
w2Vcount (w;positive )=0 (4.13)
1In practice throughout this book, we’ll use log to mean natural log (ln) when the base is not speciﬁed.

--- Page 61 ---
4.3 • W ORKED EXAMPLE 61
But since naive Bayes naively multiplies all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the probability of the
class to be zero, no matter the other evidence!
The simplest solution is the add-one (Laplace) smoothing introduced in Chap-
ter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing
algorithms in language modeling, it is commonly used in naive Bayes text catego-
rization:
ˆP(wijc) =count (wi;c)+1P
w2V(count (w;c)+1)=count (wi;c)+1 P
w2Vcount (w;c)
+jVj(4.14)
Note once again that it is crucial that the vocabulary V consists of the union of all the
word types in all classes, not just the words in one class c(try to convince yourself
why this must be true; see the exercise at the end of the chapter).
What do we do about words that occur in our test data but are not in our vocab-
ulary at all because they did not occur in any training document in any class? The
solution for such unknown words is to ignore them—remove them from the test unknown word
document and not include any probability for them at all.
Finally, some systems choose to completely ignore another class of words: stop
words , very frequent words like theanda. This can be done by sorting the vocabu- stop words
lary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries
as stop words, or alternatively by using one of the many predeﬁned stop word lists
available online. Then each instance of these stop words is simply removed from
both training and test documents as if it had never occurred. In most text classiﬁca-
tion applications, however, using a stop word list doesn’t improve performance, and
so it is more common to make use of the entire vocabulary and not use a stop word
list.
Fig. 4.2 shows the ﬁnal algorithm.
4.3 Worked example
Let’s walk through an example of training and testing naive Bayes with add-one
smoothing. We’ll use a sentiment analysis domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simpliﬁed from actual movie reviews.
Cat Documents
Training - just plain boring
- entirely predictable and lacks energy
- no surprises and very few laughs
+ very powerful
+ the most fun ﬁlm of the summer
Test ? predictable with no fun
The prior P(c)for the two classes is computed via Eq. 4.11 asNc
Ndoc:
P( ) =3
5P(+) =2
5
The word with doesn’t occur in the training set, so we drop it completely (as
mentioned above, we don’t use unknown word models for naive Bayes). The like-
lihoods from the training set for the remaining three words “predictable”, “no”, and

--- Page 62 ---
62 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
function TRAIN NAIVE BAYES (D, C) returns V;logP(c), log P(wjc)
for each class c2C # Calculate P(c)terms
Ndoc= number of documents in D
Nc= number of documents from D in class c
logprior [c] logNc
Ndoc
V vocabulary of D
bigdoc [c] append (d)ford2Dwith class c
for each word win V # Calculate P(wjc)terms
count(w,c) # of occurrences of winbigdoc [c]
loglikelihood [w,c] logcount (w;c) + 1P
w0in V(count (w0;c) + 1)
return logprior ,loglikelihood ,V
function TESTNAIVE BAYES (testdoc ,logprior ,loglikelihood , C, V) returns bestc
for each class c2C
sum[c] logprior [c]
for each position iintestdoc
word testdoc[i]
ifword2V
sum[c] sum[c]+loglikelihood [word ,c]
return argmaxcsum[c]
Figure 4.2 The naive Bayes algorithm, using add-1 smoothing. To use add- asmoothing
instead, change the +1 to+afor loglikelihood counts in training.
“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder
of the words in the training set is left as an exercise for the reader):
P(“predictable”j ) =1+1
14+20P(“predictable”j+) =0+1
9+20
P(“no”j ) =1+1
14+20P(“no”j+) =0+1
9+20
P(“fun”j ) =0+1
14+20P(“fun”j+) =1+1
9+20
For the test sentence S = “predictable with no fun”, after removing the word ‘with’,
the chosen class, via Eq. 4.9, is therefore computed as follows:
P( )P(Sj ) =3
5221
343=6:110 5
P(+)P(Sj+) =2
5112
293=3:210 5
The model thus predicts the class negative for the test sentence.
4.4 Optimizing for Sentiment Analysis
While standard naive Bayes text classiﬁcation can work well for sentiment analysis,
some small changes are generally employed that improve performance.

--- Page 63 ---
4.4 • O PTIMIZING FOR SENTIMENT ANALYSIS 63
First, for sentiment classiﬁcation and a number of other text classiﬁcation tasks,
whether a word occurs or not seems to matter more than its frequency. Thus it often
improves performance to clip the word counts in each document at 1 (see the end
of the chapter for pointers to these results). This variant is called binary multino-
mial naive Bayes orbinary naive Bayes . The variant uses the same algorithm asbinary naive
Bayes
in Fig. 4.2 except that for each document we remove all duplicate words before con-
catenating them into the single big document during training and we also remove
duplicate words from test documents. Fig. 4.3 shows an example in which a set
of four documents (shortened and text-normalized for this example) are remapped
to binary, with the modiﬁed counts shown in the table on the right. The example
is worked without add-1 smoothing to make the differences clearer. Note that the
results counts need not be 1; the word great has a count of 2 even for binary naive
Bayes, because it appears in multiple documents.
Four original documents:
 it was pathetic the worst part was the
boxing scenes
 no plot twists or great scenes
+and satire and great plot twists
+great scenes great ﬁlm
After per-document binarization:
 it was pathetic the worst part boxing
scenes
 no plot twists or great scenes
+and satire great plot twists
+great scenes ﬁlmNB Binary
Counts Counts
+ + 
and 2 0 1 0
boxing 0 1 0 1
ﬁlm 1 0 1 0
great 3 1 2 1
it 0 1 0 1
no 0 1 0 1
or 0 1 0 1
part 0 1 0 1
pathetic 0 1 0 1
plot 1 1 1 1
satire 1 0 1 0
scenes 1 2 1 2
the 0 2 0 1
twists 1 1 1 1
was 0 2 0 1
worst 0 1 0 1
Figure 4.3 An example of binarization for the binary naive Bayes algorithm.
A second important addition commonly made when doing text classiﬁcation for
sentiment is to deal with negation. Consider the difference between I really like this
movie (positive) and I didn’t like this movie (negative). The negation expressed by
didn’t completely alters the inferences we draw from the predicate like. Similarly,
negation can modify a negative word to produce a positive review ( don’t dismiss this
ﬁlm,doesn’t let us get bored ).
A very simple baseline that is commonly used in sentiment analysis to deal with
negation is the following: during text normalization, prepend the preﬁx NOT to
every word after a token of logical negation ( n’t, not, no, never ) until the next punc-
tuation mark. Thus the phrase
didn't like this movie , but I
becomes
didn't NOT_like NOT_this NOT_movie , but I
Newly formed ‘words’ like NOT like,NOT recommend will thus occur more
often in negative document and act as cues for negative sentiment, while words
likeNOT bored ,NOT dismiss will acquire positive associations. Syntactic parsing
(Chapter 18) can be used deal more accurately with the scope relationship between

--- Page 64 ---
64 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
these negation words and the predicates they modify, but this simple baseline works
quite well in practice.
Finally, in some situations we might have insufﬁcient labeled training data to
train accurate naive Bayes classiﬁers using all words in the training set to estimate
positive and negative sentiment. In such cases we can instead derive the positive
and negative word features from sentiment lexicons , lists of words that are pre-sentiment
lexicons
annotated with positive or negative sentiment. Four popular lexicons are the General
Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexiconGeneral
Inquirer
LIWC of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).
For example the MPQA subjectivity lexicon has 6885 words each marked for
whether it is strongly or weakly biased positive or negative. Some examples:
+:admirable, beautiful, conﬁdent, dazzling, ecstatic, favor, glee, great
 :awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate
A common way to use lexicons in a naive Bayes classiﬁer is to add a feature
that is counted whenever a word from that lexicon occurs. Thus we might add a
feature called ‘this word occurs in the positive lexicon’, and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. Similarly, we might add as a second feature ‘this word occurs in the
negative lexicon’ of words in the negative lexicon. If we have lots of training data,
and if the test data matches the training data, using just two features won’t work as
well as using all the words. But when training data is sparse or not representative of
the test set, using dense lexicon features instead of sparse individual-word features
may generalize better.
We’ll return to this use of lexicons in Chapter 22, showing how these lexicons
can be learned automatically, and how they can be applied to many other tasks be-
yond sentiment classiﬁcation.
4.5 Naive Bayes for other text classiﬁcation tasks
In the previous section we pointed out that naive Bayes doesn’t require that our
classiﬁer use all the words in the training data as features. In fact features in naive
Bayes can express any property of the input text we want.
Consider the task of spam detection , deciding if a particular piece of email is spam detection
an example of spam (unsolicited bulk email)—one of the ﬁrst applications of naive
Bayes to text classiﬁcation (Sahami et al., 1998).
A common solution here, rather than using all the words as individual features,
is to predeﬁne likely sets of words or phrases as features, combined with features
that are not purely linguistic. For example the open-source SpamAssassin tool2
predeﬁnes features like the phrase “one hundred percent guaranteed”, or the feature
mentions millions of dollars , which is a regular expression that matches suspiciously
large sums of money. But it also includes features like HTML has a low ratio of text
to image area , that aren’t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
took to arrive. More sample SpamAssassin features:
• Email subject line is all capital letters
• Contains phrases of urgency like “urgent reply”
2https://spamassassin.apache.org

--- Page 65 ---
4.6 • N AIVE BAYES AS A LANGUAGE MODEL 65
• Email subject line contains “online pharmaceutical”
• HTML has unbalanced “head” tags
• Claims you can be removed from the list
For other tasks, like language id —determining what language a given piece language id
of text is written in—the most effective naive Bayes features are not words at all,
butcharacter n-grams , 2-grams (‘zw’) 3-grams (‘nya’, ‘ V o’), or 4-grams (‘ie z’,
‘thei’), or, even simpler byte n-grams , where instead of using the multibyte Unicode
character representations called codepoints, we just pretend everything is a string of
raw bytes. Because spaces count as a byte, byte n-grams can model statistics about
the beginning or ending of words. A widely used naive Bayes system, langid.py
(Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using fea-
ture selection to winnow down to the most informative 7000 ﬁnal features.
Language ID systems are trained on multilingual text, such as Wikipedia (Wiki-
pedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire.
To make sure that this multilingual text correctly reﬂects different regions, dialects,
and socioeconomic classes, systems also add Twitter text in many languages geo-
tagged to many regions (important for getting world English dialects from countries
with large Anglophone populations like Nigeria or India), Bible and Quran transla-
tions, slang websites like Urban Dictionary, corpora of African American Vernacular
English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).
4.6 Naive Bayes as a Language Model
As we saw in the previous section, naive Bayes classiﬁers can use any sort of feature:
dictionaries, URLs, email addresses, network features, phrases, and so on. But if,
as in Section 4.3, we use only individual word features, and we use all of the words
in the text (not a subset), then naive Bayes has an important similarity to language
modeling. Speciﬁcally, a naive Bayes model can be viewed as a set of class-speciﬁc
unigram language models, in which the model for each class instantiates a unigram
language model.
Since the likelihood features from the naive Bayes model assign a probability to
each word P(wordjc), the model also assigns a probability to each sentence:
P(sjc) =Y
i2positionsP(wijc) (4.15)
Thus consider a naive Bayes model with the classes positive (+) and negative (-)
and the following model parameters:
w P(wj+)P(wj-)
I 0.1 0.2
love 0.1 0.001
this 0.01 0.01
fun 0.05 0.005
ﬁlm 0.1 0.1
... ... ...
Each of the two columns above instantiates a language model that can assign a
probability to the sentence “I love this fun ﬁlm”:

--- Page 66 ---
66 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
P(“I love this fun ﬁlm” j+) = 0:10:10:010:050:1=510 7
P(“I love this fun ﬁlm” j ) = 0:20:0010:010:0050:1=1:010 9
As it happens, the positive model assigns a higher probability to the sentence:
P(sjpos)>P(sjneg). Note that this is just the likelihood part of the naive Bayes
model; once we multiply in the prior a full naive Bayes model might well make a
different classiﬁcation decision.
4.7 Evaluation: Precision, Recall, F-measure
To introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some
simple binary detection tasks. For example, in spam detection, our goal is to label
every text as being in the spam category (“positive”) or not in the spam category
(“negative”). For each item (email document) we therefore need to know whether
our system called it spam or not. We also need to know whether the email is actually
spam or not, i.e. the human-deﬁned labels for each document that we are trying to
match. We will refer to these human labels as the gold labels . gold labels
Or imagine you’re the CEO of the Delicious Pie Company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning Delicious Pie. Here the positive class is tweets about
Delicious Pie and the negative class is all other tweets.
In both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. To evaluate any system for detecting things, we start
by building a confusion matrix like the one shown in Fig. 4.4. A confusion matrixconfusion
matrix
is a table for visualizing how an algorithm performs with respect to the human gold
labels, using two dimensions (system output and gold labels), and each cell labeling
a set of possible outcomes. In the spam detection case, for example, true positives
are documents that are indeed spam (indicated by human-created gold labels) that
our system correctly said were spam. False negatives are documents that are indeed
spam but our system incorrectly labeled as non-spam.
To the bottom right of the table is the equation for accuracy , which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. Although accuracy might seem a natural
metric, we generally don’t use it for text classiﬁcation tasks. That’s because accuracy
doesn’t work well when the classes are unbalanced (as indeed they are with spam,
which is a large majority of email, or with tweets, which are mainly not about pie).
To make this more explicit, imagine that we looked at a million tweets, and
let’s say that only 100 of them are discussing their love (or hatred) for our pie,
while the other 999,900 are tweets about something completely unrelated. Imagine a
simple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should
be happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would
be completely useless, since it wouldn’t ﬁnd a single one of the customer comments
we are looking for. In other words, accuracy is not a good metric when the goal is
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.

--- Page 67 ---
4.7 • E VALUATION : PRECISION , RECALL , F- MEASURE 67
true positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn
Figure 4.4 A confusion matrix for visualizing how well a binary classiﬁcation system per-
forms against gold standard labels.
That’s why instead of accuracy we generally turn to two other metrics shown in
Fig. 4.4: precision andrecall .Precision measures the percentage of the items that precision
the system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,
are positive according to the human gold labels). Precision is deﬁned as
Precision =true positives
true positives + false positives
Recall measures the percentage of items actually present in the input that were recall
correctly identiﬁed by the system. Recall is deﬁned as
Recall =true positives
true positives + false negatives
Precision and recall will help solve the problem with the useless “nothing is
pie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). You should convince yourself that the precision at ﬁnding relevant
tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize
true positives: ﬁnding the things that we are supposed to be looking for.
There are many ways to deﬁne a single metric that incorporates aspects of both
precision and recall. The simplest of these combinations is the F-measure (van F-measure
Rijsbergen, 1975) , deﬁned as:
Fb=(b2+1)PR
b2P+R
Thebparameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of b>1 favor recall, while
values of b<1 favor precision. When b=1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called F b=1or just F 1: F1
F1=2PR
P+R(4.16)
F-measure comes from a weighted harmonic mean of precision and recall. The
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:
HarmonicMean (a1;a2;a3;a4;:::;an) =n
1
a1+1
a2+1
a3+:::+1
an(4.17)

--- Page 68 ---
68 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
and hence F-measure is
F=1
a1
P+(1 a)1
Ror
withb2=1 a
a
F=(b2+1)PR
b2P+R(4.18)
Harmonic mean is used because the harmonic mean of two values is closer to the
minimum of the two values than the arithmetic mean is. Thus it weighs the lower of
the two numbers more heavily, which is more conservative in this situation.
4.7.1 Evaluating with more than two classes
Up to now we have been describing text classiﬁcation tasks with only two classes.
But lots of classiﬁcation tasks in language processing have more than two classes.
For sentiment analysis we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, semantic role labeling, emotion detection, and so on. Luckily the
naive Bayes algorithm is already a multi-class classiﬁcation algorithm.
851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+200
Figure 4.5 Confusion matrix for a three-class categorization task, showing for each pair of
classes (c1;c2), how many documents from c1were (in)correctly assigned to c2.
But we’ll need to slightly modify our deﬁnitions of precision and recall. Con-
sider the sample confusion matrix for a hypothetical 3-way one-of email catego-
rization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for
example, that the system mistakenly labeled one spam document as urgent, and we
have shown how to compute a distinct precision and recall value for each class. In
order to derive a single metric that tells us how well the system is doing, we can com-
bine these values in two ways. In macroaveraging , we compute the performance macroaveraging
for each class, and then average over classes. In microaveraging , we collect the de- microaveraging
cisions for all classes into a single confusion matrix, and then compute precision and
recall from that table. Fig. 4.6 shows the confusion matrix for each class separately,
and shows the computation of microaveraged and macroaveraged precision.
As the ﬁgure shows, a microaverage is dominated by the more frequent class (in
this case spam), since the counts are pooled. The macroaverage better reﬂects the
statistics of the smaller classes, and so is more appropriate when performance on all
the classes is equally important.

--- Page 69 ---
4.8 • T EST SETS AND CROSS -VALIDATION 69
8811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=PooledClass 3: SpamClass 2: NormalClass 1: Urgent
Figure 4.6 Separate confusion matrices for the 3 classes from the previous ﬁgure, showing the pooled confu-
sion matrix and the microaveraged and macroaveraged precision.
4.8 Test sets and Cross-validation
The training and testing procedure for text classiﬁcation follows what we saw with
language modeling (Section 3.2): we use the training set to train the model, then use
thedevelopment test set (also called a devset ) to perhaps tune some parameters,development
test set
devset and in general decide what the best model is. Once we come up with what we think
is the best model, we run it on the (hitherto unseen) test set to report its performance.
While the use of a devset avoids overﬁtting the test set, having a ﬁxed train-
ing set, devset, and test set creates another problem: in order to save lots of data
for training, the test set (or devset) might not be large enough to be representative.
Wouldn’t it be better if we could somehow use all our data for training and still use
all our data for test? We can do this by cross-validation . cross-validation
In cross-validation, we choose a number k, and partition our data into kdisjoint
subsets called folds . Now we choose one of those kfolds as a test set, train our folds
classiﬁer on the remaining k 1 folds, and then compute the error rate on the test
set. Then we repeat with another fold as the test set, again training on the other k 1
folds. We do this sampling process ktimes and average the test set error rate from
these kruns to get an average error rate. If we choose k=10, we would train 10
different models (each on 90% of our data), test the model 10 times, and average
these 10 values. This is called 10-fold cross-validation .10-fold
cross-validation
The only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can’t examine any of the data
to suggest possible features and in general see what’s going on, because we’d be
peeking at the test set, and such cheating would cause us to overestimate the perfor-
mance of our system. However, looking at the corpus to understand what’s going
on is important in designing NLP systems! What to do? For this reason, it is com-
mon to create a ﬁxed training set and test set, then do 10-fold cross-validation inside
the training set, but compute error rate the normal way in the test set, as shown in
Fig. 4.7.

--- Page 70 ---
70 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
Training Iterations13452678910DevDevDevDevDevDevDevDevDevDevTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTest SetTesting
Figure 4.7 10-fold cross-validation
4.9 Statistical Signiﬁcance Testing
In building systems we often need to compare the performance of two systems. How
can we know if the new system we just built is better than our old one? Or better
than some other system described in the literature? This is the domain of statistical
hypothesis testing, and in this section we introduce tests for statistical signiﬁcance
for NLP classiﬁers, drawing especially on the work of Dror et al. (2020) and Berg-
Kirkpatrick et al. (2012).
Suppose we’re comparing the performance of classiﬁers AandBon a metric M
such as F 1, or accuracy. Perhaps we want to know if our logistic regression senti-
ment classiﬁer A(Chapter 5) gets a higher F 1score than our naive Bayes sentiment
classiﬁer Bon a particular test set x. Let’s call M(A;x)the score that system Agets
on test set x, and d(x)the performance difference between AandBonx:
d(x) =M(A;x) M(B;x) (4.19)
We would like to know if d(x)>0, meaning that our logistic regression classiﬁer
has a higher F 1than our naive Bayes classiﬁer on x.d(x)is called the effect size ; a effect size
bigger dmeans that Aseems to be way better than B; a small dmeans Aseems to
be only a little better.
Why don’t we just check if d(x)is positive? Suppose we do, and we ﬁnd that
the F 1score of Ais higher than B’s by .04. Can we be certain that Ais better? We
cannot! That’s because Amight just be accidentally better than Bon this particular x.
We need something more: we want to know if A’s superiority over Bis likely to hold
again if we checked another test set x0, or under some other set of circumstances.
In the paradigm of statistical hypothesis testing, we test this by formalizing two
hypotheses.
H0:d(x)0
H1:d(x)>0 (4.20)
The hypothesis H0, called the null hypothesis , supposes that d(x)is actually nega- null hypothesis
tive or zero, meaning that Ais not better than B. We would like to know if we can
conﬁdently rule out this hypothesis, and instead support H1, that Ais better.
We do this by creating a random variable Xranging over all test sets. Now we
ask how likely is it, if the null hypothesis H0was correct, that among these test sets

--- Page 71 ---
4.9 • S TATISTICAL SIGNIFICANCE TESTING 71
we would encounter the value of d(x)that we found, if we repeated the experiment
a great many times. We formalize this likelihood as the p-value : the probability, p-value
assuming the null hypothesis H0is true, of seeing the d(x)that we saw or one even
greater
P(d(X)d(x)jH0is true ) (4.21)
So in our example, this p-value is the probability that we would see d(x)assuming
Aisnotbetter than B. Ifd(x)is huge (let’s say Ahas a very respectable F 1of .9
andBhas a terrible F 1of only .2 on x), we might be surprised, since that would be
extremely unlikely to occur if H0were in fact true, and so the p-value would be low
(unlikely to have such a large difAis in fact not better than B). But if d(x)is very
small, it might be less surprising to us even if H0were true and Ais not really better
than B, and so the p-value would be higher.
A very small p-value means that the difference we observed is very unlikely
under the null hypothesis, and we can reject the null hypothesis. What counts as very
small? It is common to use values like .05 or .01 as the thresholds. A value of .01
means that if the p-value (the probability of observing the dwe saw assuming H0is
true) is less than .01, we reject the null hypothesis and assume that Ais indeed better
than B. We say that a result (e.g., “ Ais better than B”) is statistically signiﬁcant ifstatistically
signiﬁcant
thedwe saw has a probability that is below the threshold and we therefore reject
this null hypothesis.
How do we compute this probability we need for the p-value? In NLP we gen-
erally don’t use simple parametric tests like t-tests or ANOV As that you might be
familiar with. Parametric tests make assumptions about the distributions of the test
statistic (such as normality) that don’t generally hold in our cases. So in NLP we
usually use non-parametric tests based on sampling: we artiﬁcially create many ver-
sions of the experimental setup. For example, if we had lots of different test sets x0
we could just measure all the d(x0)for all the x0. That gives us a distribution. Now
we set a threshold (like .01) and if we see in this distribution that 99% or more of
those deltas are smaller than the delta we observed, i.e., that p-value( x)—the proba-
bility of seeing a d(x)as big as the one we saw—is less than .01, then we can reject
the null hypothesis and agree that d(x)was a sufﬁciently surprising difference and
Ais really a better algorithm than B.
There are two common non-parametric tests used in NLP: approximate ran-
domization (Noreen, 1989) and the bootstrap test . We will describe bootstrapapproximate
randomization
below, showing the paired version of the test, which again is most common in NLP.
Paired tests are those in which we compare two sets of observations that are aligned: paired
each observation in one set can be paired with an observation in another. This hap-
pens naturally when we are comparing the performance of two systems on the same
test set; we can pair the performance of system Aon an individual observation xi
with the performance of system Bon the same xi.
4.9.1 The Paired Bootstrap Test
Thebootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from pre- bootstrap test
cision, recall, or F1 to the BLEU metric used in machine translation. The word
bootstrapping refers to repeatedly drawing large numbers of samples with replace- bootstrapping
ment (called bootstrap samples ) from an original set. The intuition of the bootstrap
test is that we can create many virtual test sets from an observed test set by repeat-
edly sampling from it. The method only makes the assumption that the sample is
representative of the population.

--- Page 72 ---
72 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
Consider a tiny text classiﬁcation example with a test set xof 10 documents. The
ﬁrst row of Fig. 4.8 shows the results of two classiﬁers (A and B) on this test set.
Each document is labeled by one of the four possibilities (A and B both right, both
wrong, A right and B wrong, A wrong and B right). A slash through a letter ( B)
means that that classiﬁer got the answer wrong. On the ﬁrst document both A and
B get the correct class (AB), while on the second document A got it right but B got
it wrong (A B). If we assume for simplicity that our metric is accuracy, A has an
accuracy of .70 and B of .50, so d(x)is .20.
Now we create a large number b(perhaps 105) of virtual test sets x(i), each of size
n=10. Fig. 4.8 shows a couple of examples. To create each virtual test set x(i), we
repeatedly ( n=10 times) select a cell from row xwith replacement. For example, to
create the ﬁrst cell of the ﬁrst virtual test set x(1), if we happened to randomly select
the second cell of the xrow; we would copy the value A B into our new cell, and
move on to create the second cell of x(1), each time sampling (randomly choosing)
from the original xwith replacement.
1 2 3 4 5 6 7 8 910A% B% d()
x AB ABAB  AB AB  AB ABAB  ABAB.70 .50 .20
x(1)ABAB AB  AB  AB AB  AB AB  ABAB .60 .60 .00
x(2)ABAB  AB  AB  AB AB  AB ABAB AB .60 .70-.10
...
x(b)
Figure 4.8 The paired bootstrap test: Examples of bpseudo test sets x(i)being created
from an initial true test set x. Each pseudo test set is created by sampling n=10 times with
replacement; thus an individual sample is a single cell, a document with its gold label and
the correct or incorrect performance of classiﬁers A and B. Of course real test sets don’t have
only 10 examples, and bneeds to be large as well.
Now that we have the btest sets, providing a sampling distribution, we can do
statistics on how often Ahas an accidental advantage. There are various ways to
compute this advantage; here we follow the version laid out in Berg-Kirkpatrick
et al. (2012). Assuming H0(Aisn’t better than B), we would expect that d(X),
estimated over many test sets, would be zero or negative; a much higher value would
be surprising, since H0speciﬁcally assumes Aisn’t better than B. To measure exactly
how surprising our observed d(x)is, we would in other circumstances compute the
p-value by counting over many test sets how often d(x(i))exceeds the expected zero
value by d(x)or more:
p-value (x) =1
bbX
i=11
d(x(i)) d(x)0
(We use the notation 1(x)to mean “1 if xis true, and 0 otherwise”.) However,
although it’s generally true that the expected value of d(X)over many test sets,
(again assuming Aisn’t better than B) is 0, this isn’t true for the bootstrapped test
sets we created. That’s because we didn’t draw these samples from a distribution
with 0 mean; we happened to create them from the original test set x, which happens
to be biased (by .20) in favor of A. So to measure how surprising is our observed
d(x), we actually compute the p-value by counting over many test sets how often

--- Page 73 ---
4.10 • A VOIDING HARMS IN CLASSIFICATION 73
d(x(i))exceeds the expected value of d(x)byd(x)or more:
p-value (x) =1
bbX
i=11
d(x(i)) d(x)d(x)
=1
bbX
i=11
d(x(i))2d(x)
(4.22)
So if for example we have 10,000 test sets x(i)and a threshold of .01, and in only 47
of the test sets do we ﬁnd that A is accidentally better d(x(i))2d(x), the resulting
p-value of .0047 is smaller than .01, indicating that the delta we found, d(x)is indeed
sufﬁciently surprising and unlikely to have happened by accident, and we can reject
the null hypothesis and conclude Ais better than B.
function BOOTSTRAP (test set x,num of samples b)returns p-value (x)
Calculate d(x)# how much better does algorithm A do than B on x
s= 0
fori= 1tobdo
forj= 1tondo # Draw a bootstrap sample x(i)of size n
Select a member of xat random and add it to x(i)
Calculate d(x(i))# how much better does algorithm A do than B on x(i)
s s+ 1ifd(x(i))2d(x)
p-value( x)s
b# on what % of the b samples did algorithm A beat expectations?
return p-value( x) # if very few did, our observed dis probably not accidental
Figure 4.9 A version of the paired bootstrap algorithm after Berg-Kirkpatrick et al. (2012).
The full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set x, a
number of samples b, and counts the percentage of the bbootstrap test sets in which
d(x(i))>2d(x). This percentage then acts as a one-sided empirical p-value.
4.10 Avoiding Harms in Classiﬁcation
It is important to avoid harms that may result from classiﬁers, harms that exist both
for naive Bayes classiﬁers and for the other classiﬁcation algorithms we introduce
in later chapters.
One class of harms is representational harms (Crawford 2017, Blodgett et al.representational
harms
2020), harms caused by a system that demeans a social group, for example by per-
petuating negative stereotypes about them. For example Kiritchenko and Moham-
mad (2018) examined the performance of 200 sentiment analysis systems on pairs of
sentences that were identical except for containing either a common African Amer-
ican ﬁrst name (like Shaniqua ) or a common European American ﬁrst name (like
Stephanie ), chosen from the Caliskan et al. (2017) study discussed in Chapter 6.
They found that most systems assigned lower sentiment and more negative emotion
to sentences with African American names, reﬂecting and perpetuating stereotypes
that associate African Americans with negative emotions (Popp et al., 2003).
In other tasks classiﬁers may lead to both representational harms and other
harms, such as silencing. For example the important text classiﬁcation task of tox-

--- Page 74 ---
74 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
icity detection is the task of detecting hate speech, abuse, harassment, or othertoxicity
detection
kinds of toxic language. While the goal of such classiﬁers is to help reduce soci-
etal harm, toxicity classiﬁers can themselves cause harms. For example, researchers
have shown that some widely used toxicity classiﬁers incorrectly ﬂag as being toxic
sentences that are non-toxic but simply mention identities like women (Park et al.,
2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;
Dias Oliva et al., 2021), or simply use linguistic features characteristic of varieties
like African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).
Such false positive errors could lead to the silencing of discourse by or about these
groups.
These model problems can be caused by biases or other problems in the training
data; in general, machine learning systems replicate and even amplify the biases
in their training data. But these problems can also be caused by the labels (for
example due to biases in the human labelers), by the resources used (like lexicons,
or model components like pretrained embeddings), or even by model architecture
(like what the model is trained to optimize). While the mitigation of these biases
(for example by carefully considering the training data sources) is an important area
of research, we currently don’t have general solutions. For this reason it’s important,
when introducing any NLP model, to study these kinds of factors and make them
clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for model card
each version of a model. A model card documents a machine learning model with
information like:
• training algorithms and parameters
• training data sources, motivation, and preprocessing
• evaluation data sources, motivation, and preprocessing
• intended use and users
• model performance across different demographic or other groups and envi-
ronmental situations
4.11 Summary
This chapter introduced the naive Bayes model for classiﬁcation and applied it to
thetext categorization task of sentiment analysis .
• Many language processing tasks can be viewed as tasks of classiﬁcation .
• Text categorization, in which an entire text is assigned a class from a ﬁnite set,
includes such tasks as sentiment analysis ,spam detection , language identi-
ﬁcation, and authorship attribution.
• Sentiment analysis classiﬁes a text as reﬂecting the positive or negative orien-
tation ( sentiment ) that a writer expresses toward some object.
• Naive Bayes is a generative model that makes the bag-of-words assumption
(position doesn’t matter) and the conditional independence assumption (words
are conditionally independent of each other given the class)
• Naive Bayes with binarized features seems to work better for many text clas-
siﬁcation tasks.
• Classiﬁers are evaluated based on precision andrecall .
• Classiﬁers are trained using distinct training, dev, and test sets, including the
use of cross-validation in the training set.

--- Page 75 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 75
• Statistical signiﬁcance tests should be used to determine whether we can be
conﬁdent that one version of a classiﬁer is better than another.
• Designers of classiﬁers should carefully consider harms that may be caused
by the model, including its training data and other components, and report
model characteristics in a model card .
Bibliographical and Historical Notes
Multinomial naive Bayes text classiﬁcation was proposed by Maron (1961) at the
RAND Corporation for the task of assigning subject categories to journal abstracts.
His model introduced most of the features of the modern form presented here, ap-
proximating the classiﬁcation task with one-of categorization, and implementing
add-dsmoothing and information-based feature selection.
The conditional independence assumptions of naive Bayes and the idea of Bayes-
ian analysis of text seems to have arisen multiple times. The same year as Maron’s
paper, Minsky (1961) proposed a naive Bayes classiﬁer for vision and other arti-
ﬁcial intelligence problems, and Bayesian techniques were also applied to the text
classiﬁcation task of authorship attribution by Mosteller and Wallace (1963). It had
long been known that Alexander Hamilton, John Jay, and James Madison wrote
the anonymously-published Federalist papers in 1787–1788 to persuade New York
to ratify the United States Constitution. Yet although some of the 85 essays were
clearly attributable to one author or another, the authorship of 12 were in dispute
between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian
probabilistic model of the writing of Hamilton and another model on the writings
of Madison, then computed the maximum-likelihood author for each of the disputed
essays. Naive Bayes was ﬁrst applied to spam detection in Heckerman et al. (1998).
Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show
that using boolean attributes with multinomial naive Bayes works better than full
counts. Binary multinomial naive Bayes is sometimes confused with another variant
of naive Bayes that also uses a binary representation of whether a term occurs in
a document: Multivariate Bernoulli naive Bayes . The Bernoulli variant instead
estimates P(wjc)as the fraction of documents that contain a term, and includes a
probability for whether a term is notin a document. McCallum and Nigam (1998)
and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive
Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text
tasks.
There are a variety of sources covering the many kinds of text classiﬁcation
tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).
Stamatatos (2009) surveys authorship attribute algorithms. On language identiﬁca-
tion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural
system. The task of newswire indexing was often used as a test case for text classi-
ﬁcation algorithms, based on the Reuters-21578 collection of newswire articles.
See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classiﬁcation;
classiﬁcation in general is covered in machine learning textbooks (Hastie et al. 2001,
Witten and Frank 2005, Bishop 2006, Murphy 2012).
Non-parametric methods for computing statistical signiﬁcance were used ﬁrst in
NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech
recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the
bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work
has focused on issues including multiple test sets and multiple metrics (Søgaard et al.

--- Page 76 ---
76 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT
2014, Dror et al. 2017).
Feature selection is a method of removing features that are unlikely to generalize
well. Features are generally ranked by how informative they are about the classiﬁca-
tion decision. A very common metric, information gain , tells us how many bits ofinformation
gain
information the presence of the word gives us for guessing the class. Other feature
selection metrics include c2, pointwise mutual information, and GINI index; see
Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an
introduction to feature selection.
Exercises
4.1 Assume the following likelihoods for each word being part of a positive or
negative movie review, and equal prior probabilities for each class.
pos neg
I 0.09 0.16
always 0.07 0.06
like 0.29 0.06
foreign 0.04 0.15
ﬁlms 0.08 0.11
What class will Naive bayes assign to the sentence “I always like foreign
ﬁlms.”?
4.2 Given the following short movie reviews, each labeled with a genre, either
comedy or action:
1. fun, couple, love, love comedy
2. fast, furious, shoot action
3. couple, ﬂy, fast, fun, fun comedy
4. furious, shoot, shoot, fun action
5. ﬂy, fast, shoot, love action
and a new document D:
fast, couple, shoot, ﬂy
compute the most likely class for D. Assume a naive Bayes classiﬁer and use
add-1 smoothing for the likelihoods.
4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both
with add-1 smoothing, on the following document counts for key sentiment
words, with positive or negative class assigned as noted.
doc “good” “poor” “great” (class)
d1. 3 0 3 pos
d2. 0 1 2 pos
d3. 1 3 0 neg
d4. 1 5 2 neg
d5. 0 2 0 neg
Use both naive Bayes models to assign a class (pos or neg) to this sentence:
A good, good plot and great characters, but poor acting.
Recall from page 61 that with naive Bayes text classiﬁcation, we simply ig-
nore (throw out) any word that never occurred in the training document. (We
don’t throw out words that appear in some classes but not others; that’s what
add-one smoothing is for.) Do the two models agree or disagree?

--- Page 77 ---
CHAPTER
5Logistic Regression
“And how do you know that these ﬁne begonias are not of equal importance?”
Hercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles
Detective stories are as littered with clues as texts are with words. Yet for the
poor reader it can be challenging to know how to weigh the author’s clues in order
to make the crucial classiﬁcation task: deciding whodunnit.
In this chapter we introduce an algorithm that is admirably suited for discovering
the link between features or clues and some particular outcome: logistic regression .logistic
regression
Indeed, logistic regression is one of the most important analytic tools in the social
and natural sciences. In natural language processing, logistic regression is the base-
line supervised machine learning algorithm for classiﬁcation, and also has a very
close relationship with neural networks. As we will see in Chapter 7, a neural net-
work can be viewed as a series of logistic regression classiﬁers stacked on top of
each other. Thus the classiﬁcation and machine learning techniques introduced here
will play an important role throughout the book.
Logistic regression can be used to classify an observation into one of two classes
(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.
Because the mathematics for the two-class case is simpler, we’ll describe this special
case of logistic regression ﬁrst in the next few sections, and then brieﬂy summarize
the use of multinomial logistic regression for more than two classes in Section 5.3.
We’ll introduce the mathematics of logistic regression in the next few sections.
But let’s begin with some high-level issues.
Generative and Discriminative Classiﬁers: The most important difference be-
tween naive Bayes and logistic regression is that logistic regression is a discrimina-
tiveclassiﬁer while naive Bayes is a generative classiﬁer.
These are two very different frameworks for how
to build a machine learning model. Consider a visual
metaphor: imagine we’re trying to distinguish dog
images from cat images. A generative model would
have the goal of understanding what dogs look like
and what cats look like. You might literally ask such
a model to ‘generate’, i.e., draw, a dog. Given a test
image, the system then asks whether it’s the cat model or the dog model that better
ﬁts (is less surprised by) the image, and chooses that as its label.
A discriminative model, by contrast, is only try-
ing to learn to distinguish the classes (perhaps with-
out learning much about them). So maybe all the
dogs in the training data are wearing collars and the
cats aren’t. If that one feature neatly separates the
classes, the model is satisﬁed. If you ask such a
model what it knows about cats all it can say is that
they don’t wear collars.

--- Page 78 ---
78 CHAPTER 5 • L OGISTIC REGRESSION
More formally, recall that the naive Bayes assigns a class cto a document dnot
by directly computing P(cjd)but by computing a likelihood and a prior
ˆc=argmax
c2Clikelihoodz}|{
P(djc)priorz}|{
P(c) (5.1)
Agenerative model like naive Bayes makes use of this likelihood term, whichgenerative
model
expresses how to generate the features of a document if we knew it was of class c .
By contrast a discriminative model in this text categorization scenario attemptsdiscriminative
model
todirectly compute P(cjd). Perhaps it will learn to assign a high weight to document
features that directly improve its ability to discriminate between possible classes,
even if it couldn’t generate an example of one of the classes.
Components of a probabilistic machine learning classiﬁer: Like naive Bayes,
logistic regression is a probabilistic classiﬁer that makes use of supervised machine
learning. Machine learning classiﬁers require a training corpus of minput/output
pairs(x(i);y(i)). (We’ll use superscripts in parentheses to refer to individual instances
in the training set—for sentiment classiﬁcation each instance might be an individual
document to be classiﬁed.) A machine learning system for classiﬁcation then has
four components:
1. A feature representation of the input. For each input observation x(i), this
will be a vector of features [x1;x2;:::;xn]. We will generally refer to feature
ifor input x(j)asx(j)
i, sometimes simpliﬁed as xi, but we will also see the
notation fi,fi(x), or, for multiclass classiﬁcation, fi(c;x).
2. A classiﬁcation function that computes ˆ y, the estimated class, via p(yjx). In
the next section we will introduce the sigmoid andsoftmax tools for classiﬁ-
cation.
3. An objective function that we want to optimize for learning, usually involving
minimizing a loss function corresponding to error on training examples. We
will introduce the cross-entropy loss function .
4. An algorithm for optimizing the objective function. We introduce the stochas-
tic gradient descent algorithm.
Logistic regression has two phases:
training: We train the system (speciﬁcally the weights wandb, introduced be-
low) using stochastic gradient descent and the cross-entropy loss.
test: Given a test example xwe compute p(yjx)and return the higher probability
label y=1 ory=0.
5.1 The sigmoid function
The goal of binary logistic regression is to train a classiﬁer that can make a binary
decision about the class of a new input observation. Here we introduce the sigmoid
classiﬁer that will help us make this decision.
Consider a single input observation x, which we will represent by a vector of
features [x1;x2;:::;xn]. (We’ll show sample features in the next subsection.) The
classiﬁer output ycan be 1 (meaning the observation is a member of the class) or
0 (the observation is not a member of the class). We want to know the probability

--- Page 79 ---
5.1 • T HE SIGMOID FUNCTION 79
P(y=1jx)that this observation is a member of the class. So perhaps the decision
is “positive sentiment” versus “negative sentiment”, the features represent counts of
words in a document, P(y=1jx)is the probability that the document has positive
sentiment, and P(y=0jx)is the probability that the document has negative senti-
ment.
Logistic regression solves this task by learning, from a training set, a vector of
weights and a bias term . Each weight wiis a real number, and is associated with one
of the input features xi. The weight wirepresents how important that input feature
is to the classiﬁcation decision, and can be positive (providing evidence that the in-
stance being classiﬁed belongs in the positive class) or negative (providing evidence
that the instance being classiﬁed belongs in the negative class). Thus we might
expect in a sentiment task the word awesome to have a high positive weight, and
abysmal to have a very negative weight. The bias term , also called the intercept , is bias term
intercept another real number that’s added to the weighted inputs.
To make a decision on a test instance—after we’ve learned the weights in training—
the classiﬁer ﬁrst multiplies each xiby its weight wi, sums up the weighted features,
and adds the bias term b. The resulting single number zexpresses the weighted sum
of the evidence for the class.
z= nX
i=1wixi!
+b (5.2)
In the rest of the book we’ll represent such sums using the dot product notation dot product
from linear algebra. The dot product of two vectors aandb, written as ab, is the
sum of the products of the corresponding elements of each vector. (Notice that we
represent vectors using the boldface notation b). Thus the following is an equivalent
formation to Eq. 5.2:
z=wx+b (5.3)
But note that nothing in Eq. 5.3 forces zto be a legal probability, that is, to lie
between 0 and 1. In fact, since weights are real-valued, the output might even be
negative; zranges from ¥to¥.
Figure 5.1 The sigmoid function s(z) =1
1+e ztakes a real value and maps it to the range
(0;1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.
To create a probability, we’ll pass zthrough the sigmoid function, s(z). The sigmoid
sigmoid function (named because it looks like an s) is also called the logistic func-
tion, and gives logistic regression its name. The sigmoid has the following equation,logistic
function
shown graphically in Fig. 5.1:
s(z) =1
1+e z=1
1+exp( z)(5.4)

--- Page 80 ---
80 CHAPTER 5 • L OGISTIC REGRESSION
(For the rest of the book, we’ll use the notation exp (x)to mean ex.) The sigmoid
has a number of advantages; it takes a real-valued number and maps it into the range
(0;1), which is just what we want for a probability. Because it is nearly linear around
0 but ﬂattens toward the ends, it tends to squash outlier values toward 0 or 1. And
it’s differentiable, which as we’ll see in Section 5.10 will be handy for learning.
We’re almost there. If we apply the sigmoid to the sum of the weighted features,
we get a number between 0 and 1. To make it a probability, we just need to make
sure that the two cases, p(y=1)andp(y=0), sum to 1. We can do this as follows:
P(y=1) = s(wx+b)
=1
1+exp( (wx+b))
P(y=0) = 1 s(wx+b)
=1 1
1+exp( (wx+b))
=exp( (wx+b))
1+exp( (wx+b))(5.5)
The sigmoid function has the property
1 s(x) =s( x) (5.6)
so we could also have expressed P(y=0)ass( (wx+b)).
Finally, one terminological point. The input to the sigmoid function, the score
z=wx+bfrom (5.3), is often called the logit . This is because the logit function logit
is the inverse of the sigmoid. The logit function is the log of the odds ratiop
1 p:
logit(p) =s 1(p) =lnp
1 p(5.7)
Using the term logit forzis a way of reminding us that by using the sigmoid to turn
z(which ranges from  ¥to¥) into a probability, we are implicitly interpreting zas
not just any real-valued number, but as speciﬁcally a log odds.
5.2 Classiﬁcation with Logistic Regression
The sigmoid function from the prior section thus gives us a way to take an instance
xand compute the probability P(y=1jx).
How do we make a decision about which class to apply to a test instance x? For
a given x, we say yes if the probability P(y=1jx)is more than .5, and no otherwise.
We call .5 the decision boundary :decision
boundary
decision (x) =1 if P(y=1jx)>0:5
0 otherwise
Let’s have some examples of applying logistic regression as a classiﬁer for language
tasks.

--- Page 81 ---
5.2 • C LASSIFICATION WITH LOGISTIC REGRESSION 81
5.2.1 Sentiment Classiﬁcation
Suppose we are doing binary sentiment classiﬁcation on movie review text, and
we would like to know whether to assign the sentiment class +or to a review
document doc. We’ll represent each input observation by the 6 features x1:::x6of
the input shown in the following table; Fig. 5.2 shows the features in a sample mini
test document.
Var Deﬁnition Value in Fig. 5.2
x1 count(positive lexicon words 2doc) 3
x2 count(negative lexicon words 2doc) 2
x31 if “no”2doc
0 otherwise1
x4 count (1st and 2nd pronouns 2doc) 3
x51 if “!”2doc
0 otherwise0
x6 ln(word count of doc ) ln(66) =4:19
 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 5.2 A sample mini test document showing the extracted features in the vector x.
Let’s assume for the moment that we’ve already learned a real-valued weight
for each of these features, and that the 6 weights corresponding to the 6 features
are[2:5; 5:0; 1:2;0:5;2:0;0:7], while b= 0.1. (We’ll discuss in the next section
how the weights are learned.) The weight w1, for example indicates how important
a feature the number of positive lexicon words ( great ,nice,enjoyable , etc.) is to
a positive sentiment decision, while w2tells us the importance of negative lexicon
words. Note that w1=2:5 is positive, while w2= 5:0, meaning that negative words
are negatively associated with a positive sentiment decision, and are about twice as
important as positive words.
Given these 6 features and the input review x,P(+jx)andP( jx)can be com-
puted using Eq. 5.5:
p(+jx) =P(y=1jx) = s(wx+b)
=s([2:5; 5:0; 1:2;0:5;2:0;0:7][3;2;1;3;0;4:19]+0:1)
=s(:833)
=0:70 (5.8)
p( jx) =P(y=0jx) = 1 s(wx+b)
=0:30

--- Page 82 ---
82 CHAPTER 5 • L OGISTIC REGRESSION
5.2.2 Other classiﬁcation tasks and features
Logistic regression is applied to all sorts of NLP tasks, and any property of the input
can be a feature. Consider the task of period disambiguation : deciding if a periodperiod
disambiguation
is the end of a sentence or part of a word, by classifying each period into one of two
classes, EOS (end-of-sentence) and not-EOS. We might use features like x1below
expressing that the current word is lower case, perhaps with a positive weight. Or a
feature expressing that the current word is in our abbreviations dictionary (“Prof.”),
perhaps with a negative weight. A feature can also express a combination of proper-
ties. For example a period following an upper case word is likely to be an EOS, but
if the word itself is St.and the previous word is capitalized then the period is likely
part of a shortening of the word street following a street name.
x1=
1 if “ Case(wi) =Lower”
0 otherwise
x2=1 if “ wi2AcronymDict”
0 otherwise
x3=1 if “ wi=St. & Case(wi 1) =Upper”
0 otherwise
Designing versus learning features: In classic models, features are designed by
hand by examining the training set with an eye to linguistic intuitions and literature,
supplemented by insights from error analysis on the training set of an early version
of a system. We can also consider ( feature interactions ), complex features that arefeature
interactions
combinations of more primitive features. We saw such a feature for period disam-
biguation above, where a period on the word St.was less likely to be the end of the
sentence if the previous word was capitalized. Features can be created automatically
viafeature templates , abstract speciﬁcations of features. For example a bigramfeature
templates
template for period disambiguation might create a feature for every pair of words
that occurs before a period in the training set. Thus the feature space is sparse, since
we only have to create a feature if that n-gram exists in that position in the training
set. The feature is generally created as a hash from the string descriptions. A user
description of a feature as, “bigram(American breakfast)” is hashed into a unique
integer ithat becomes the feature number fi.
It should be clear from the prior paragraph that designing features by hand re-
quires extensive human effort. For this reason, recent NLP systems avoid hand-
designed features and instead focus on representation learning : ways to learn fea-
tures automatically in an unsupervised way from the input. We’ll introduce methods
for representation learning in Chapter 6 and Chapter 7.
Scaling input features: When different input features have extremely different
ranges of values, it’s common to rescale them so they have comparable ranges. We
standardize input values by centering them to result in a zero mean and a standard standardize
deviation of one (this transformation is sometimes called the z-score ). That is, if miz-score
is the mean of the values of feature xiacross the mobservations in the input dataset,
andsiis the standard deviation of the values of features xiacross the input dataset,
we can replace each feature xiby a new feature x0
icomputed as follows:
mi=1
mmX
j=1x(j)
i si=vuut1
mmX
j=1
x(j)
i mi2
x0
i=xi mi
si(5.9)

--- Page 83 ---
5.2 • C LASSIFICATION WITH LOGISTIC REGRESSION 83
Alternatively, we can normalize the input features values to lie between 0 and 1: normalize
x0
i=xi min(xi)
max(xi) min(xi)(5.10)
Having input data with comparable range is useful when comparing values across
features. Data scaling is especially important in large neural networks, since it helps
speed up gradient descent.
5.2.3 Processing many examples at once
We’ve shown the equations for logistic regression for a single example. But in prac-
tice we’ll of course want to process an entire test set with many examples. Let’s
suppose we have a test set consisting of mtest examples each of which we’d like
to classify. We’ll continue to use the notation from page 78, in which a superscript
value in parentheses refers to the example index in some set of data (either for train-
ing or for test). So in this case each test example x(i)has a feature vector x(i),
1im. (As usual, we’ll represent vectors and matrices in bold.)
One way to compute each output value ˆ y(i)is just to have a for-loop, and compute
each test example one at a time:
foreach x(i)in input [x(1);x(2);:::;x(m)]
y(i)=s(wx(i)+b) (5.11)
For the ﬁrst 3 test examples, then, we would be separately computing the pre-
dicted ˆ y(i)as follows:
P(y(1)=1jx(1)) = s(wx(1)+b)
P(y(2)=1jx(2)) = s(wx(2)+b)
P(y(3)=1jx(3)) = s(wx(3)+b)
But it turns out that we can slightly modify our original equation Eq. 5.5 to do
this much more efﬁciently. We’ll use matrix arithmetic to assign a class to all the
examples with one matrix operation!
First, we’ll pack all the input feature vectors for each input xinto a single input
matrix X, where each row iis a row vector consisting of the feature vector for in-
put example x(i)(i.e., the vector x(i)). Assuming each example has ffeatures and
weights, Xwill therefore be a matrix of shape [mf], as follows:
X=2
66664x(1)
1x(1)
2:::x(1)
f
x(2)
1x(2)
2:::x(2)
f
x(3)
1x(3)
2:::x(3)
f
:::3
77775(5.12)
Now if we introduce bas a vector of length mwhich consists of the scalar bias
term brepeated mtimes, b= [b;b;:::;b], and ^ y= [ˆy(1);ˆy(2):::;ˆy(m)]as the vector of
outputs (one scalar ˆ y(i)for each input x(i)and its feature vector x(i)), and represent
the weight vector was a column vector, we can compute all the outputs with a single
matrix multiplication and one addition:
y=Xw+b (5.13)

--- Page 84 ---
84 CHAPTER 5 • L OGISTIC REGRESSION
You should convince yourself that Eq. 5.13 computes the same thing as our for-loop
in Eq. 5.11. For example ˆ y(1), the ﬁrst entry of the output vector y, will correctly be:
ˆy(1)= [x(1)
1;x(1)
2;:::;x(1)
f][w1;w2;:::;wf]+b (5.14)
Note that we had to reorder Xandwfrom the order they appeared in in Eq. 5.5 to
make the multiplications come out properly. Here is Eq. 5.13 again with the shapes
shown:
y=X w +b
(m1) ( mf)(f1) (m1) (5.15)
Modern compilers and compute hardware can compute this matrix operation very
efﬁciently, making the computation much faster, which becomes important when
training or testing on very large datasets.
Note by the way that we could have kept Xandwin the original order ( y=
Xw+b) if we had chosen to deﬁne Xdifferently as a matrix of column vectors, one
vector for each input example, instead of row vectors, and then it would have shape
[fm]. But we conventionally represent inputs as rows.
5.2.4 Choosing a classiﬁer
Logistic regression has a number of advantages over naive Bayes. Naive Bayes has
overly strong conditional independence assumptions. Consider two features which
are strongly correlated; in fact, imagine that we just add the same feature f1twice.
Naive Bayes will treat both copies of f1as if they were separate, multiplying them
both in, overestimating the evidence. By contrast, logistic regression is much more
robust to correlated features; if two features f1and f2are perfectly correlated, re-
gression will simply assign part of the weight to w1and part to w2. Thus when
there are many correlated features, logistic regression will assign a more accurate
probability than naive Bayes. So logistic regression generally works better on larger
documents or datasets and is a common default.
Despite the less accurate probabilities, naive Bayes still often makes the correct
classiﬁcation decision. Furthermore, naive Bayes can work extremely well (some-
times even better than logistic regression) on very small datasets (Ng and Jordan,
2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is
easy to implement and very fast to train (there’s no optimization step). So it’s still a
reasonable approach to use in some situations.
5.3 Multinomial logistic regression
Sometimes we need more than two classes. Perhaps we might want to do 3-way
sentiment classiﬁcation (positive, negative, or neutral). Or we could be assigning
some of the labels we will introduce in Chapter 17, like the part of speech of a word
(choosing from 10, 30, or even 50 different parts of speech), or the named entity
type of a phrase (choosing from tags like person, location, organization).
In such cases we use multinomial logistic regression , also called softmax re-multinomial
logistic
regressiongression (in older NLP literature you will sometimes see the name maxent classi-
ﬁer). In multinomial logistic regression we want to label each observation with a
class kfrom a set of Kclasses, under the stipulation that only one of these classes is

--- Page 85 ---
5.3 • M ULTINOMIAL LOGISTIC REGRESSION 85
the correct one (sometimes called hard classiﬁcation ; an observation can not be in
multiple classes). Let’s use the following representation: the output yfor each input
xwill be a vector of length K. If class cis the correct class, we’ll set yc=1, and
set all the other elements of yto be 0, i.e., yc=1 and yj=08j6=c. A vector like
thisy, with one value=1 and the rest 0, is called a one-hot vector . The job of the
classiﬁer is to produce an estimate vector ^ y. For each class k, the value ˆ ykwill be
the classiﬁer’s estimate of the probability p(yk=1jx).
5.3.1 Softmax
The multinomial logistic classiﬁer uses a generalization of the sigmoid, called the
softmax function, to compute p(yk=1jx). The softmax function takes a vector softmax
z= [z1;z2;:::;zK]ofKarbitrary values and maps them to a probability distribution,
with each value in the range [0,1], and all the values summing to 1. Like the sigmoid,
it is an exponential function.
For a vector zof dimensionality K, the softmax is deﬁned as:
softmax (zi) =exp(zi)PK
j=1exp(zj)1iK (5.16)
The softmax of an input vector z= [z1;z2;:::;zK]is thus a vector itself:
softmax (z) ="
exp(z1)PK
i=1exp(zi);exp(z2)PK
i=1exp(zi);:::;exp(zK)PK
i=1exp(zi)#
(5.17)
The denominatorPK
i=1exp(zi)is used to normalize all the values into probabilities.
Thus for example given a vector:
z= [0:6;1:1; 1:5;1:2;3:2; 1:1]
the resulting (rounded) softmax( z) is
[0:05;0:09;0:01;0:1;0:74;0:01][0:05;0:09;0:01;0:1;0:74;0:01]
Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.
Thus if one of the inputs is larger than the others, it will tend to push its probability
toward 1, and suppress the probabilities of the smaller inputs.
Finally, note that, just as for the sigmoid, we refer to z, the vector of scores that
is the input to the softmax, as logits (see (5.7).
5.3.2 Applying softmax in logistic regression
When we apply softmax for logistic regression, the input will (just as for the sig-
moid) be the dot product between a weight vector wand an input vector x(plus a
bias). But now we’ll need separate weight vectors wkand bias bkfor each of the K
classes. The probability of each of our output classes ˆ ykcan thus be computed as:
p(yk=1jx) =exp(wkx+bk)
KX
j=1exp(wjx+bj)(5.18)

--- Page 86 ---
86 CHAPTER 5 • L OGISTIC REGRESSION
The form of Eq. 5.18 makes it seem that we would compute each output sep-
arately. Instead, it’s more common to set up the equation for more efﬁcient com-
putation by modern vector processing hardware. We’ll do this by representing the
set of Kweight vectors as a weight matrix Wand a bias vector b. Each row kof
Wcorresponds to the vector of weights wk.Wthus has shape [Kf], for Kthe
number of output classes and fthe number of input features. The bias vector bhas
one value for each of the Koutput classes. If we represent the weights in this way,
we can compute ˆy, the vector of output probabilities for each of the Kclasses, by a
single elegant equation:
ˆy=softmax (Wx+b) (5.19)
If you work out the matrix arithmetic, you can see that the estimated score of
the ﬁrst output class ˆ y1(before we take the softmax) will correctly turn out to be
w1x+b1.
Fig. 5.3 shows an intuition of the role of the weight vector versus weight matrix
in the computation of the output class probabilities for binary versus multinomial
logistic regression.
5.3.3 Features in Multinomial Logistic Regression
Features in multinomial logistic regression act like features in binary logistic regres-
sion, with the difference mentioned above that we’ll need separate weight vectors
and biases for each of the Kclasses. Recall our binary exclamation point feature x5
from page 81:
x5=1 if “!”2doc
0 otherwise
In binary classiﬁcation a positive weight w5on a feature inﬂuences the classiﬁer
toward y=1 (positive sentiment) and a negative weight inﬂuences it toward y=0
(negative sentiment) with the absolute value indicating how important the feature
is. For multinomial logistic regression, by contrast, with separate weights for each
class, a feature can be evidence for or against each individual class.
In 3-way multiclass sentiment classiﬁcation, for example, we must assign each
document one of the 3 classes +, , or 0 (neutral). Now a feature related to excla-
mation marks might have a negative weight for 0 documents, and a positive weight
for+or documents:
Feature Deﬁnition w5;+w5; w5;0
f5(x)1 if “!”2doc
0 otherwise3:5 3:1 5:3
Because these feature weights are dependent both on the input text and the output
class, we sometimes make this dependence explicit and represent the features them-
selves as f(x;y): a function of both the input and the class. Using such a notation
f5(x)above could be represented as three features f5(x;+),f5(x; ), and f5(x;0),
each of which has a single weight. We’ll use this kind of notation in our description
of the CRF in Chapter 17.

--- Page 87 ---
5.4 • L EARNING IN LOGISTIC REGRESSION 87
Binary Logistic Regression
w[f ⨉1]Outputsigmoid[1⨉f]Input wordsp(+) = 1- p(-)…y^xyInput featurevector [scalar]positive lexiconwords = 1count of “no” = 0wordcount=3x1x2x3xfdessert   was    greatWeight vector
Multinomial Logistic Regression
W[f⨉1]Outputsoftmax[K⨉f]Input wordsp(+)…y1^y2^y3^xyInput featurevector [K⨉1]positive lexiconwords = 1count of “no” = 0wordcount=3x1x2x3xfdessert   was    greatp(-)p(neut)Weight matrixThese f red weightsare a row of W correspondingto weight vector w3,(= weights for class 3)
Figure 5.3 Binary versus multinomial logistic regression. Binary logistic regression uses a
single weight vector w, and has a scalar output ˆ y. In multinomial logistic regression we have
Kseparate weight vectors corresponding to the Kclasses, all packed into a single weight
matrix W, and a vector output ˆy. We omit the biases from both ﬁgures for clarity.
5.4 Learning in Logistic Regression
How are the parameters of the model, the weights wand bias b, learned? Logistic
regression is an instance of supervised classiﬁcation in which we know the correct
label y(either 0 or 1) for each observation x. What the system produces via Eq. 5.5
is ˆy, the system’s estimate of the true y. We want to learn parameters (meaning w
andb) that make ˆ yfor each training observation as close as possible to the true y.
This requires two components that we foreshadowed in the introduction to the
chapter. The ﬁrst is a metric for how close the current label ( ˆ y) is to the true gold
label y. Rather than measure similarity, we usually talk about the opposite of this:
thedistance between the system output and the gold output, and we call this distance
thelossfunction or the cost function . In the next section we’ll introduce the loss loss
function that is commonly used for logistic regression and also for neural networks,

--- Page 88 ---
88 CHAPTER 5 • L OGISTIC REGRESSION
thecross-entropy loss .
The second thing we need is an optimization algorithm for iteratively updating
the weights so as to minimize this loss function. The standard algorithm for this is
gradient descent ; we’ll introduce the stochastic gradient descent algorithm in the
following section.
We’ll describe these algorithms for the simpler case of binary logistic regres-
sion in the next two sections, and then turn to multinomial logistic regression in
Section 5.8.
5.5 The cross-entropy loss function
We need a loss function that expresses, for an observation x, how close the classiﬁer
output ( ˆ y=s(wx+b)) is to the correct output ( y, which is 0 or 1). We’ll call this:
L(ˆy;y) = How much ˆ ydiffers from the true y (5.20)
We do this via a loss function that prefers the correct class labels of the train-
ing examples to be more likely . This is called conditional maximum likelihood
estimation : we choose the parameters w;bthatmaximize the log probability of
the true ylabels in the training data given the observations x. The resulting loss
function is the negative log likelihood loss , generally called the cross-entropy loss .cross-entropy
loss
Let’s derive this loss function, applied to a single observation x. We’d like to
learn weights that maximize the probability of the correct label p(yjx). Since there
are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can
express the probability p(yjx)that our classiﬁer produces for one observation as the
following (keeping in mind that if y=1, Eq. 5.21 simpliﬁes to ˆ y; ify=0, Eq. 5.21
simpliﬁes to 1 ˆy):
p(yjx) = ˆyy(1 ˆy)1 y(5.21)
Now we take the log of both sides. This will turn out to be handy mathematically,
and doesn’t hurt us; whatever values maximize a probability will also maximize the
log of the probability:
logp(yjx) = log
ˆyy(1 ˆy)1 y
=ylog ˆy+(1 y)log(1 ˆy) (5.22)
Eq. 5.22 describes a log likelihood that should be maximized. In order to turn this
into a loss function (something that we need to minimize), we’ll just ﬂip the sign on
Eq. 5.22. The result is the cross-entropy loss LCE:
LCE(ˆy;y) = logp(yjx) = [ylog ˆy+(1 y)log(1 ˆy)] (5.23)
Finally, we can plug in the deﬁnition of ˆ y=s(wx+b):
LCE(ˆy;y) = [ylogs(wx+b)+(1 y)log(1 s(wx+b))] (5.24)
Let’s see if this loss function does the right thing for our example from Fig. 5.2. We
want the loss to be smaller if the model’s estimate is close to correct, and bigger if
the model is confused. So ﬁrst let’s suppose the correct gold label for the sentiment
example in Fig. 5.2 is positive, i.e., y=1. In this case our model is doing well, since

--- Page 89 ---
5.6 • G RADIENT DESCENT 89
from Eq. 5.8 it indeed gave the example a higher probability of being positive (.70)
than negative (.30). If we plug s(wx+b) =:70 and y=1 into Eq. 5.24, the right
side of the equation drops out, leading to the following loss (we’ll use log to mean
natural log when the base is not speciﬁed):
LCE(ˆy;y) = [ylogs(wx+b)+(1 y)log(1 s(wx+b))]
= [logs(wx+b)]
= log(:70)
= :36
By contrast, let’s pretend instead that the example in Fig. 5.2 was actually negative,
i.e., y=0 (perhaps the reviewer went on to say “But bottom line, the movie is
terrible! I beg you not to see it!”). In this case our model is confused and we’d want
the loss to be higher. Now if we plug y=0 and 1 s(wx+b) =:30 from Eq. 5.8
into Eq. 5.24, the left side of the equation drops out:
LCE(ˆy;y) = [ylogs(wx+b)+(1 y)log(1 s(wx+b))]
=  [log(1 s(wx+b))]
=  log(:30)
= 1:2
Sure enough, the loss for the ﬁrst classiﬁer (.36) is less than the loss for the second
classiﬁer (1.2).
Why does minimizing this negative log probability do what we want? A perfect
classiﬁer would assign probability 1 to the correct outcome ( y=1 or y=0) and
probability 0 to the incorrect outcome. That means if yequals 1, the higher ˆ yis (the
closer it is to 1), the better the classiﬁer; the lower ˆ yis (the closer it is to 0), the
worse the classiﬁer. If yequals 0, instead, the higher 1  ˆyis (closer to 1), the better
the classiﬁer. The negative log of ˆ y(if the true yequals 1) or 1 ˆy(if the true y
equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)
to inﬁnity (negative log of 0, inﬁnite loss). This loss function also ensures that as
the probability of the correct answer is maximized, the probability of the incorrect
answer is minimized; since the two sum to one, any increase in the probability of the
correct answer is coming at the expense of the incorrect answer. It’s called the cross-
entropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the
true probability distribution yand our estimated distribution ˆ y.
Now we know what we want to minimize; in the next section, we’ll see how to
ﬁnd the minimum.
5.6 Gradient Descent
Our goal with gradient descent is to ﬁnd the optimal weights: minimize the loss
function we’ve deﬁned for the model. In Eq. 5.25 below, we’ll explicitly represent
the fact that the cross-entropy loss function LCEis parameterized by the weights. In
machine learning in general we refer to the parameters being learned as q; in the
case of logistic regression q=fw;bg. So the goal is to ﬁnd the set of weights which
minimizes the loss function, averaged over all examples:
ˆq=argmin
q1
mmX
i=1LCE(f(x(i);q);y(i)) (5.25)

--- Page 90 ---
90 CHAPTER 5 • L OGISTIC REGRESSION
How shall we ﬁnd the minimum of this (or any) loss function? Gradient descent is
a method that ﬁnds a minimum of a function by ﬁguring out in which direction (in
the space of the parameters q) the function’s slope is rising the most steeply, and
moving in the opposite direction. The intuition is that if you are hiking in a canyon
and trying to descend most quickly down to the river at the bottom, you might look
around yourself in all directions, ﬁnd the direction where the ground is sloping the
steepest, and walk downhill in that direction.
For logistic regression, this loss function is conveniently convex . A convex func- convex
tion has at most one minimum; there are no local minima to get stuck in, so gradient
descent starting from any point is guaranteed to ﬁnd the minimum. (By contrast,
the loss for multi-layer neural networks is non-convex, and gradient descent may
get stuck in local minima for neural network training and never ﬁnd the global opti-
mum.)
Although the algorithm (and the concept of gradient) are designed for direction
vectors , let’s ﬁrst consider a visualization of the case where the parameter of our
system is just a single scalar w, shown in Fig. 5.4.
Given a random initialization of wat some value w1, and assuming the loss
function Lhappened to have the shape in Fig. 5.4, we need the algorithm to tell us
whether at the next iteration we should move left (making w2smaller than w1) or
right (making w2bigger than w1) to reach the minimum.
wLoss
0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescent
Figure 5.4 The ﬁrst step in iteratively ﬁnding the minimum of this loss function, by moving
win the reverse direction from the slope of the function. Since the slope is negative, we need
to move win a positive direction, to the right. Here superscripts are used for learning steps,
sow1means the initial value of w(which is 0), w2the value at the second step, and so on.
The gradient descent algorithm answers this question by ﬁnding the gradient gradient
of the loss function at the current point and moving in the opposite direction. The
gradient of a function of many variables is a vector pointing in the direction of the
greatest increase in a function. The gradient is a multi-variable generalization of the
slope, so for a function of one variable like the one in Fig. 5.4, we can informally
think of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this
hypothetical loss function at point w=w1. You can see that the slope of this dotted
line is negative. Thus to ﬁnd the minimum, gradient descent tells us to go in the
opposite direction: moving win a positive direction.
The magnitude of the amount to move in gradient descent is the value of the
sloped
dwL(f(x;w);y)weighted by a learning rate h. A higher (faster) learning learning rate

--- Page 91 ---
5.6 • G RADIENT DESCENT 91
rate means that we should move wmore on each step. The change we make in our
parameter is the learning rate times the gradient (or the slope, in our single-variable
example):
wt+1=wt hd
dwL(f(x;w);y) (5.26)
Now let’s extend the intuition from a function of one scalar variable wto many
variables, because we don’t just want to move left or right, we want to know where
in the N-dimensional space (of the Nparameters that make up q) we should move.
The gradient is just such a vector; it expresses the directional components of the
sharpest slope along each of those Ndimensions. If we’re just imagining two weight
dimensions (say for one weight wand one bias b), the gradient might be a vector with
two orthogonal components, each of which tells us how much the ground slopes in
thewdimension and in the bdimension. Fig. 5.5 shows a visualization of the value
of a 2-dimensional gradient vector taken at the red point.
In an actual logistic regression, the parameter vector wis much longer than 1 or
2, since the input feature vector xcan be quite long, and we need a weight wifor
each xi. For each dimension/variable wiinw(plus the bias b), the gradient will have
a component that tells us the slope with respect to that variable. In each dimension
wi, we express the slope as a partial derivative¶
¶wiof the loss function. Essentially
we’re asking: “How much would a small change in that variable wiinﬂuence the
total loss function L?”
Formally, then, the gradient of a multi-variable function fis a vector in which
each component expresses the partial derivative of fwith respect to one of the vari-
ables. We’ll use the inverted Greek delta symbol Ñto refer to the gradient, and
represent ˆ yasf(x;q)to make the dependence on qmore obvious:
ÑL(f(x;q);y) =2
6666664¶
¶w1L(f(x;q);y)
¶
¶w2L(f(x;q);y)
...
¶
¶wnL(f(x;q);y)
¶
¶bL(f(x;q);y)3
7777775(5.27)
The ﬁnal equation for updating qbased on the gradient is thus
qt+1=qt hÑL(f(x;q);y) (5.28)
Cost(w,b)
wb
Figure 5.5 Visualization of the gradient vector at the red point in two dimensions wand
b, showing a red arrow in the x-y plane pointing in the direction we will go to look for the
minimum: the opposite direction of the gradient (recall that the gradient points in the direction
of increase not decrease).

--- Page 92 ---
92 CHAPTER 5 • L OGISTIC REGRESSION
5.6.1 The Gradient for Logistic Regression
In order to update q, we need a deﬁnition for the gradient ÑL(f(x;q);y). Recall that
for logistic regression, the cross-entropy loss function is:
LCE(ˆy;y) = [ylogs(wx+b)+(1 y)log(1 s(wx+b))] (5.29)
It turns out that the derivative of this function for one observation vector xis Eq. 5.30
(the interested reader can see Section 5.10 for the derivation of this equation):
¶LCE(ˆy;y)
¶wj= [s(wx+b) y]xj
= ( ˆy y)xj (5.30)
You’ll also sometimes see this equation in the equivalent form:
¶LCE(ˆy;y)
¶wj= (y ˆy)xj (5.31)
Note in these equations that the gradient with respect to a single weight wjrep-
resents a very intuitive value: the difference between the true yand our estimated
ˆy=s(wx+b)for that observation, multiplied by the corresponding input value
xj.
5.6.2 The Stochastic Gradient Descent Algorithm
Stochastic gradient descent is an online algorithm that minimizes the loss function
by computing its gradient after each training example, and nudging qin the right
direction (the opposite direction of the gradient). (An “online algorithm” is one that
processes its input example by example, rather than waiting until it sees the entire
input.) Stochastic gradient descent is called stochastic because it chooses a single
random example at a time; in Section 5.6.4 we’ll discuss other versions of gradient
descent that batch many examples at once. Fig. 5.6 shows the algorithm.
The learning rate his ahyperparameter that must be adjusted. If it’s too high, hyperparameter
the learner will take steps that are too large, overshooting the minimum of the loss
function. If it’s too low, the learner will take steps that are too small, and take too
long to get to the minimum. It is common to start with a higher learning rate and then
slowly decrease it, so that it is a function of the iteration kof training; the notation
hkcan be used to mean the value of the learning rate at iteration k.
We’ll discuss hyperparameters in more detail in Chapter 7, but in short, they are
a special kind of parameter for any machine learning model. Unlike regular param-
eters of a model (weights like wandb), which are learned by the algorithm from
the training set, hyperparameters are special parameters chosen by the algorithm
designer that affect how the algorithm works.
5.6.3 Working through an example
Let’s walk through a single step of the gradient descent algorithm. We’ll use a
simpliﬁed version of the example in Fig. 5.2 as it sees a single observation x, whose
correct value is y=1 (this is a positive review), and with a feature vector x= [x1;x2]
consisting of these two features:
x1=3 (count of positive lexicon words)
x2=2 (count of negative lexicon words)

--- Page 93 ---
5.6 • G RADIENT DESCENT 93
function STOCHASTIC GRADIENT DESCENT (L(),f(),x,y)returns q
# where: L is the loss function
# f is a function parameterized by q
# x is the set of training inputs x(1);x(2);:::;x(m)
# y is the set of training outputs (labels) y(1);y(2);:::;y(m)
q 0 # (or small random values)
repeat til done # see caption
For each training tuple (x(i);y(i))(in random order)
1. Optional (for reporting): # How are we doing on this tuple?
Compute ˆ y(i)=f(x(i);q)# What is our estimated output ˆ y?
Compute the loss L(ˆy(i);y(i))# How far off is ˆ y(i)from the true output y(i)?
2.g ÑqL(f(x(i);q);y(i)) # How should we move qto maximize loss?
3.q q hg # Go the other way instead
return q
Figure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used
mainly to report how well we are doing on the current tuple; we don’t need to compute the
loss in order to compute the gradient. The algorithm can terminate when it converges (when
the gradient norm <), or when progress halts (for example when the loss starts going up on
a held-out set). Weights are initialized to 0 for logistic regression, but to small random values
for neural networks, as we’ll see in Chapter 7.
Let’s assume the initial weights and bias in q0are all set to 0, and the initial learning
ratehis 0.1:
w1=w2=b=0
h=0:1
The single update step requires that we compute the gradient, multiplied by the
learning rate
qt+1=qt hÑqL(f(x(i);q);y(i))
In our mini example there are three parameters, so the gradient vector has 3 dimen-
sions, for w1,w2, and b. We can compute the ﬁrst gradient as follows:
Ñw;bL=2
64¶LCE(ˆy;y)
¶w1¶LCE(ˆy;y)
¶w2¶LCE(ˆy;y)
¶b3
75=2
4(s(wx+b) y)x1
(s(wx+b) y)x2
s(wx+b) y3
5=2
4(s(0) 1)x1
(s(0) 1)x2
s(0) 13
5=2
4 0:5x1
 0:5x2
 0:53
5=2
4 1:5
 1:0
 0:53
5
Now that we have a gradient, we compute the new parameter vector q1by moving
q0in the opposite direction from the gradient:
q1=2
4w1
w2
b3
5 h2
4 1:5
 1:0
 0:53
5=2
4:15
:1
:053
5
So after one step of gradient descent, the weights have shifted to be: w1=:15,
w2=:1, and b=:05.
Note that this observation xhappened to be a positive example. We would expect
that after seeing more negative examples with high counts of negative words, that
the weight w2would shift to have a negative value.

--- Page 94 ---
94 CHAPTER 5 • L OGISTIC REGRESSION
5.6.4 Mini-batch training
Stochastic gradient descent is called stochastic because it chooses a single random
example at a time, moving the weights so as to improve performance on that single
example. That can result in very choppy movements, so it’s common to compute the
gradient over batches of training instances rather than a single instance.
For example in batch training we compute the gradient over the entire dataset. batch training
By seeing so many examples, batch training offers a superb estimate of which di-
rection to move the weights, at the cost of spending a lot of time processing every
single example in the training set to compute this perfect direction.
A compromise is mini-batch training: we train on a group of mexamples (per- mini-batch
haps 512, or 1024) that is less than the whole dataset. (If mis the size of the dataset,
then we are doing batch gradient descent; if m=1, we are back to doing stochas-
tic gradient descent.) Mini-batch training also has the advantage of computational
efﬁciency. The mini-batches can easily be vectorized, choosing the size of the mini-
batch based on the computational resources. This allows us to process all the exam-
ples in one mini-batch in parallel and then accumulate the loss, something that’s not
possible with individual or batch training.
We just need to deﬁne mini-batch versions of the cross-entropy loss function
we deﬁned in Section 5.5 and the gradient in Section 5.6.1. Let’s extend the cross-
entropy loss for one example from Eq. 5.23 to mini-batches of size m. We’ll continue
to use the notation that x(i)andy(i)mean the ith training features and training label,
respectively. We make the assumption that the training examples are independent:
logp(training labels ) = logmY
i=1p(y(i)jx(i))
=mX
i=1logp(y(i)jx(i))
= mX
i=1LCE(ˆy(i);y(i)) (5.32)
Now the cost function for the mini-batch of mexamples is the average loss for each
example:
Cost(ˆy;y) =1
mmX
i=1LCE(ˆy(i);y(i))
= 1
mmX
i=1y(i)logs(wx(i)+b)+(1 y(i))log
1 s(wx(i)+b)
(5.33)
The mini-batch gradient is the average of the individual gradients from Eq. 5.30:
¶Cost(ˆy;y)
¶wj=1
mmX
i=1h
s(wx(i)+b) y(i)i
x(i)
j(5.34)
Instead of using the sum notation, we can more efﬁciently compute the gradient
in its matrix form, following the vectorization we saw on page 83, where we have
a matrix Xof size [mf]representing the minputs in the batch, and a vector yof
size[m1]representing the correct outputs:

--- Page 95 ---
5.7 • R EGULARIZATION 95
¶Cost(ˆy;y)
¶w=1
m(ˆy y)|X
=1
m(s(Xw+b) y)|X (5.35)
5.7 Regularization
Numquam ponenda est pluralitas sine necessitate
‘Plurality should never be proposed unless needed’
William of Occam
There is a problem with learning weights that make the model perfectly match the
training data. If a feature is perfectly predictive of the outcome because it happens
to only occur in one class, it will be assigned a very high weight. The weights for
features will attempt to perfectly ﬁt details of the training set, in fact too perfectly,
modeling noisy factors that just accidentally correlate with the class. This problem is
called overﬁtting . A good model should be able to generalize well from the training overﬁtting
generalize data to the unseen test set, but a model that overﬁts will have poor generalization.
To avoid overﬁtting, a new regularization term R(q)is added to the loss func- regularization
tion in Eq. 5.25, resulting in the following loss for a batch of mexamples (slightly
rewritten from Eq. 5.25 to be maximizing log probability rather than minimizing
loss, and removing the1
mterm which doesn’t affect the argmax):
ˆq=argmax
qmX
i=1logP(y(i)jx(i)) aR(q) (5.36)
The new regularization term R(q)is used to penalize large weights. Thus a setting
of the weights that matches the training data perfectly— but uses many weights with
high values to do so—will be penalized more than a setting that matches the data a
little less well, but does so using smaller weights. There are two common ways to
compute this regularization term R(q).L2 regularization is a quadratic function ofL2
regularization
the weight values, named because it uses the (square of the) L2 norm of the weight
values. The L2 norm, jjqjj2, is the same as the Euclidean distance of the vector q
from the origin. If qconsists of nweights, then:
R(q) =jjqjj2
2=nX
j=1q2
j (5.37)
The L2 regularized loss function becomes:
ˆq=argmax
q"mX
i=1logP(y(i)jx(i))#
 anX
j=1q2
j (5.38)
L1 regularization is a linear function of the weight values, named after the L1 normL1
regularization
jjWjj1, the sum of the absolute values of the weights, or Manhattan distance (the

--- Page 96 ---
96 CHAPTER 5 • L OGISTIC REGRESSION
Manhattan distance is the distance you’d have to walk between two points in a city
with a street grid like New York):
R(q) =jjqjj1=nX
i=1jqij (5.39)
The L1 regularized loss function becomes:
ˆq=argmax
q"mX
i=1logP(y(i)jx(i))#
 anX
j=1jqjj (5.40)
These kinds of regularization come from statistics, where L1 regularization is called
lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression , lasso
ridge and both are commonly used in language processing. L2 regularization is easier to
optimize because of its simple derivative (the derivative of q2is just 2 q), while
L1 regularization is more complex (the derivative of jqjis non-continuous at zero).
But while L2 prefers weight vectors with many small weights, L1 prefers sparse
solutions with some larger weights but many more weights set to zero. Thus L1
regularization leads to much sparser weight vectors, that is, far fewer features.
Both L1 and L2 regularization have Bayesian interpretations as constraints on
the prior of how weights should look. L1 regularization can be viewed as a Laplace
prior on the weights. L2 regularization corresponds to assuming that weights are
distributed according to a Gaussian distribution with mean m=0. In a Gaussian
or normal distribution, the further away a value is from the mean, the lower its
probability (scaled by the variance s). By using a Gaussian prior on the weights, we
are saying that weights prefer to have the value 0. A Gaussian for a weight qjis
1q
2ps2
jexp 
 (qj mj)2
2s2
j!
(5.41)
If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-
ing the following constraint:
ˆq=argmax
qmY
i=1P(y(i)jx(i))nY
j=11q
2ps2
jexp 
 (qj mj)2
2s2
j!
(5.42)
which in log space, with m=0, and assuming 2 s2=1, corresponds to
ˆq=argmax
qmX
i=1logP(y(i)jx(i)) anX
j=1q2
j (5.43)
which is in the same form as Eq. 5.38.
5.8 Learning in Multinomial Logistic Regression
The loss function for multinomial logistic regression generalizes the loss function
for binary logistic regression from 2 to Kclasses. Recall that that the cross-entropy
loss for binary logistic regression (repeated from Eq. 5.23) is:
LCE(ˆy;y) = logp(yjx) = [ylog ˆy+(1 y)log(1 ˆy)] (5.44)

--- Page 97 ---
5.8 • L EARNING IN MULTINOMIAL LOGISTIC REGRESSION 97
The loss function for multinomial logistic regression generalizes the two terms in
Eq. 5.44 (one that is non-zero when y=1 and one that is non-zero when y=0) to
Kterms. As we mentioned above, for multinomial regression we’ll represent both y
andˆyas vectors. The true label yis a vector with Kelements, each corresponding
to a class, with yc=1 if the correct class is c, with all other elements of ybeing 0.
And our classiﬁer will produce an estimate vector with Kelements ˆy, each element
ˆykof which represents the estimated probability p(yk=1jx).
The loss function for a single example x, generalizing from binary logistic re-
gression, is the sum of the logs of the Koutput classes, each weighted by the indi-
cator function yk(Eq. 5.45). This turns out to be just the negative log probability of
the correct class c(Eq. 5.46):
LCE(ˆy;y) = KX
k=1yklog ˆyk (5.45)
= log ˆyc;(where cis the correct class) (5.46)
= log ˆp(yc=1jx)(where cis the correct class)
= logexp(wcx+bc)PK
j=1exp(wjx+bj)(cis the correct class) (5.47)
How did we get from Eq. 5.45 to Eq. 5.46? Because only one class (let’s call it c) is
the correct one, the vector ytakes the value 1 only for this value of k, i.e., has yc=1
andyj=08j6=c. That means the terms in the sum in Eq. 5.45 will all be 0 except
for the term corresponding to the true class c. Hence the cross-entropy loss is simply
the log of the output probability corresponding to the correct class, and we therefore
also call Eq. 5.46 the negative log likelihood loss .negative log
likelihood loss
Of course for gradient descent we don’t need the loss, we need its gradient. The
gradient for a single example turns out to be very similar to the gradient for binary
logistic regression, (ˆy y)x, that we saw in Eq. 5.30. Let’s consider one piece of the
gradient, the derivative for a single weight. For each class k, the weight of the ith
element of input xiswk;i. What is the partial derivative of the loss with respect to
wk;i? This derivative turns out to be just the difference between the true value for the
class k(which is either 1 or 0) and the probability the classiﬁer outputs for class k,
weighted by the value of the input xicorresponding to the ith element of the weight
vector for class k:
¶LCE
¶wk;i= (yk ˆyk)xi
= (yk p(yk=1jx))xi
=  
yk exp(wkx+bk)PK
j=1exp(wjx+bj)!
xi (5.48)
We’ll return to this case of the gradient for softmax regression when we introduce
neural networks in Chapter 7, and at that time we’ll also discuss the derivation of
this gradient in equations Eq. 7.33–Eq. 7.41.

--- Page 98 ---
98 CHAPTER 5 • L OGISTIC REGRESSION
5.9 Interpreting models
Often we want to know more than just the correct classiﬁcation of an observation.
We want to know why the classiﬁer made the decision it did. That is, we want our
decision to be interpretable . Interpretability can be hard to deﬁne strictly, but the interpretable
core idea is that as humans we should know why our algorithms reach the conclu-
sions they do. Because the features to logistic regression are often human-designed,
one way to understand a classiﬁer’s decision is to understand the role each feature
plays in the decision. Logistic regression can be combined with statistical tests (the
likelihood ratio test, or the Wald test); investigating whether a particular feature is
signiﬁcant by one of these tests, or inspecting its magnitude (how large is the weight
wassociated with the feature?) can help us interpret why the classiﬁer made the
decision it makes. This is enormously important for building transparent models.
Furthermore, in addition to its use as a classiﬁer, logistic regression in NLP and
many other ﬁelds is widely used as an analytic tool for testing hypotheses about the
effect of various explanatory variables (features). In text classiﬁcation, perhaps we
want to know if logically negative words ( no, not, never ) are more likely to be asso-
ciated with negative sentiment, or if negative reviews of movies are more likely to
discuss the cinematography. However, in doing so it’s necessary to control for po-
tential confounds: other factors that might inﬂuence sentiment (the movie genre, the
year it was made, perhaps the length of the review in words). Or we might be study-
ing the relationship between NLP-extracted linguistic features and non-linguistic
outcomes (hospital readmissions, political outcomes, or product sales), but need to
control for confounds (the age of the patient, the county of voting, the brand of the
product). In such cases, logistic regression allows us to test whether some feature is
associated with some outcome above and beyond the effect of other features.
5.10 Advanced: Deriving the Gradient Equation
In this section we give the derivation of the gradient of the cross-entropy loss func-
tion LCEfor logistic regression. Let’s start with some quick calculus refreshers.
First, the derivative of ln (x):
d
dxln(x) =1
x(5.49)
Second, the (very elegant) derivative of the sigmoid:
ds(z)
dz=s(z)(1 s(z)) (5.50)
Finally, the chain rule of derivatives. Suppose we are computing the derivative chain rule
of a composite function f(x) =u(v(x)). The derivative of f(x)is the derivative of
u(x)with respect to v(x)times the derivative of v(x)with respect to x:
d f
dx=du
dvdv
dx(5.51)
First, we want to know the derivative of the loss function with respect to a single
weight wj(we’ll need to compute it for each weight, and for the bias):

--- Page 99 ---
5.11 • S UMMARY 99
¶LCE
¶wj=¶
¶wj [ylogs(wx+b)+(1 y)log(1 s(wx+b))]
= ¶
¶wjylogs(wx+b)+¶
¶wj(1 y)log[1 s(wx+b)]
(5.52)
Next, using the chain rule, and relying on the derivative of log:
¶LCE
¶wj= y
s(wx+b)¶
¶wjs(wx+b) 1 y
1 s(wx+b)¶
¶wj1 s(wx+b)
(5.53)
Rearranging terms:
¶LCE
¶wj= y
s(wx+b) 1 y
1 s(wx+b)¶
¶wjs(wx+b)
(5.54)
And now plugging in the derivative of the sigmoid, and using the chain rule one
more time, we end up with Eq. 5.55:
¶LCE
¶wj= y s(wx+b)
s(wx+b)[1 s(wx+b)]
s(wx+b)[1 s(wx+b)]¶(wx+b)
¶wj
= y s(wx+b)
s(wx+b)[1 s(wx+b)]
s(wx+b)[1 s(wx+b)]xj
= [y s(wx+b)]xj
= [s(wx+b) y]xj (5.55)
5.11 Summary
This chapter introduced the logistic regression model of classiﬁcation .
• Logistic regression is a supervised machine learning classiﬁer that extracts
real-valued features from the input, multiplies each by a weight, sums them,
and passes the sum through a sigmoid function to generate a probability. A
threshold is used to make a decision.
• Logistic regression can be used with two classes (e.g., positive and negative
sentiment) or with multiple classes ( multinomial logistic regression , for ex-
ample for n-ary text classiﬁcation, part-of-speech labeling, etc.).
• Multinomial logistic regression uses the softmax function to compute proba-
bilities.
• The weights (vector wand bias b) are learned from a labeled training set via a
loss function, such as the cross-entropy loss , that must be minimized.
• Minimizing this loss function is a convex optimization problem, and iterative
algorithms like gradient descent are used to ﬁnd the optimal weights.
•Regularization is used to avoid overﬁtting.
• Logistic regression is also one of the most useful analytic tools, because of its
ability to transparently study the importance of individual features.

--- Page 100 ---
100 CHAPTER 5 • L OGISTIC REGRESSION
Bibliographical and Historical Notes
Logistic regression was developed in the ﬁeld of statistics, where it was used for
the analysis of binary data by the 1960s, and was particularly common in medicine
(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one
of the formal foundations of the study of linguistic variation (Sankoff and Labov,
1979).
Nonetheless, logistic regression didn’t become common in natural language pro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. The ﬁrst source was the neighboring ﬁelds of information retrieval and
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to NLP. Indeed a very early use of logistic
regression for document routing was one of the ﬁrst NLP applications to use (LSI)
embeddings as word representations (Sch ¨utze et al., 1995).
At the same time in the early 1990s logistic regression was developed and ap-
plied to NLP at IBM Research under the name maximum entropy modeling ormaximum
entropy
maxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-
der that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech
tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution
(Kehler, 1997b), and text classiﬁcation (Nigam et al., 1999).
More on classiﬁcation can be found in machine learning textbooks (Hastie et al.
2001, Witten and Frank 2005, Bishop 2006, Murphy 2012).
Exercises

--- Page 101 ---
CHAPTER
6Vector Semantics and
Embeddings
荃者所以在鱼，得鱼而忘荃Nets are for ﬁsh;
Once you get the ﬁsh, you can forget the net.
言者所以在意，得意而忘言Words are for meaning;
Once you get the meaning, you can forget the words
庄子(Zhuangzi), Chapter 26
The asphalt that Los Angeles is famous for occurs mainly on its freeways. But
in the middle of the city is another patch of asphalt, the La Brea tar pits, and this
asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-
tocene Epoch. One of these fossils is the Smilodon , or saber-toothed tiger, instantly
recognizable by its long canines. Five million years ago or so, a completely different
saber-tooth tiger called Thylacosmilus lived
in Argentina and other parts of South Amer-
ica. Thylacosmilus was a marsupial whereas
Smilodon was a placental mammal, but Thy-
lacosmilus had the same long upper canines
and, like Smilodon, had a protective bone
ﬂange on the lower jaw. The similarity of
these two mammals is one of many examples
of parallel or convergent evolution, in which particular contexts or environments
lead to the evolution of very similar structures in different species (Gould, 1980).
The role of context is also important in the similarity of a less biological kind
of organism: the word. Words that occur in similar contexts tend to have similar
meanings . This link between similarity in how words are distributed and similarity
in what they mean is called the distributional hypothesis . The hypothesis wasdistributional
hypothesis
ﬁrst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth
(1957), who noticed that words which are synonyms (like oculist andeye-doctor )
tended to occur in the same environment (e.g., near words like eyeorexamined )
with the amount of meaning difference between two words “corresponding roughly
to the amount of difference in their environments” (Harris, 1954, p. 157).
In this chapter we introduce vector semantics , which instantiates this linguisticvector
semantics
hypothesis by learning representations of the meaning of words, called embeddings , embeddings
directly from their distributions in texts. These representations are used in every nat-
ural language processing application that makes use of meaning, and the static em-
beddings we introduce here underlie the more powerful dynamic or contextualized
embeddings likeBERT that we will see in Chapter 11.
These word representations are also the ﬁrst example in this book of repre-
sentation learning , automatically learning useful representations of the input text.representation
learning
Finding such self-supervised ways to learn representations of the input, instead of
creating representations by hand via feature engineering , is an important focus of
NLP research (Bengio et al., 2013).

--- Page 102 ---
102 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
6.1 Lexical Semantics
Let’s begin by introducing some basic principles of word meaning. How should
we represent the meaning of a word? In the n-gram models of Chapter 3, and in
classical NLP applications, our only representation of a word is as a string of letters,
or an index in a vocabulary list. This representation is not that different from a
tradition in philosophy, perhaps you’ve seen it in introductory logic classes, in which
the meaning of words is represented by just spelling the word with small capital
letters; representing the meaning of “dog” as DOG, and “cat” as CAT, or by using an
apostrophe ( DOG ’).
Representing the meaning of a word by capitalizing it is a pretty unsatisfactory
model. You might have seen a version of a joke due originally to semanticist Barbara
Partee (Carlson, 1977):
Q: What’s the meaning of life?
A:LIFE ’
Surely we can do better than this! After all, we’ll want a model of word meaning
to do all sorts of things for us. It should tell us that some words have similar mean-
ings ( catis similar to dog), others are antonyms ( cold is the opposite of hot), some
have positive connotations ( happy ) while others have negative connotations ( sad). It
should represent the fact that the meanings of buy,sell, and payoffer differing per-
spectives on the same underlying purchasing event. (If I buy something from you,
you’ve probably sold it to me, and I likely paid you.) More generally, a model of
word meaning should allow us to draw inferences to address meaning-related tasks
like question-answering or dialogue.
In this section we summarize some of these desiderata, drawing on results in the
linguistic study of word meaning, which is called lexical semantics ; we’ll return tolexical
semantics
and expand on this list in Appendix G and Chapter 21.
Lemmas and Senses Let’s start by looking at how one word (we’ll choose mouse )
might be deﬁned in a dictionary (simpliﬁed from the online dictionary WordNet):
mouse (N)
1. any of numerous small rodents...
2. a hand-operated device that controls a cursor...
Here the form mouse is the lemma , also called the citation form . The form lemma
citation form mouse would also be the lemma for the word mice ; dictionaries don’t have separate
deﬁnitions for inﬂected forms like mice . Similarly sing is the lemma for sing,sang ,
sung . In many languages the inﬁnitive form is used as the lemma for the verb, so
Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The speciﬁc forms
sung orcarpets orsing orduermes are called wordforms . wordform
As the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. We call each
of these aspects of the meaning of mouse aword sense . The fact that lemmas can
bepolysemous (have multiple senses) can make interpretation difﬁcult (is someone
who types “mouse info” into a search engine looking for a pet or a tool?). Chap-
ter 11 and Appendix G will discuss the problem of polysemy, and introduce word
sense disambiguation , the task of determining which sense of a word is being used
in a particular context.
Synonymy One important component of word meaning is the relationship be-
tween word senses. For example when one word has a sense whose meaning is

--- Page 103 ---
6.1 • L EXICAL SEMANTICS 103
identical to a sense of another word, or nearly identical, we say the two senses of
those two words are synonyms . Synonyms include such pairs as synonym
couch/sofa vomit/throw up ﬁlbert/hazelnut car/automobile
A more formal deﬁnition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable for one another in any sentence
without changing the truth conditions of the sentence, the situations in which the
sentence would be true.
While substitutions between some pairs of words like car/automobile orwa-
ter/H2Oare truth preserving, the words are still not identical in meaning. Indeed,
probably no two words are absolutely identical in meaning. One of the fundamental
tenets of semantics, called the principle of contrast (Girard 1718, Br ´eal 1897, Clarkprinciple of
contrast
1987), states that a difference in linguistic form is always associated with some dif-
ference in meaning. For example, the word H2Ois used in scientiﬁc contexts and
would be inappropriate in a hiking guide— water would be more appropriate— and
this genre difference is part of the meaning of the word. In practice, the word syn-
onym is therefore used to describe a relationship of approximate or rough synonymy.
Word Similarity While words don’t have many synonyms, most words do have
lots of similar words. Catis not a synonym of dog, but cats anddogs are certainly
similar words. In moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). Dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.
The notion of word similarity is very useful in larger semantic tasks. Knowing similarity
how similar two words are can help in computing how similar the meaning of two
phrases or sentences are, a very important component of tasks like question answer-
ing, paraphrasing, and summarization. One way of getting values for word similarity
is to ask humans to judge how similar one word is to another. A number of datasets
have resulted from such experiments. For example the SimLex-999 dataset (Hill
et al., 2015) gives values on a scale from 0 to 10, like the examples below, which
range from near-synonyms ( vanish ,disappear ) to pairs that scarcely seem to have
anything in common ( hole,agreement ):
vanish disappear 9.8
belief impression 5.95
muscle bone 3.65
modest ﬂexible 0.98
hole agreement 0.3
Word Relatedness The meaning of two words can be related in ways other than
similarity. One such class of connections is called word relatedness (Budanitsky relatedness
and Hirst, 2006), also traditionally called word association in psychology. association
Consider the meanings of the words coffee andcup. Coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is a
manufactured object with a particular shape). But coffee and cup are clearly related;
they are associated by co-participating in an everyday event (the event of drinking
coffee out of a cup). Similarly scalpel andsurgeon are not similar but are related
eventively (a surgeon tends to make use of a scalpel).
One common kind of relatedness between words is if they belong to the same
semantic ﬁeld . A semantic ﬁeld is a set of words which cover a particular semantic semantic ﬁeld
domain and bear structured relations with each other. For example, words might be

--- Page 104 ---
104 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
related by being in the semantic ﬁeld of hospitals ( surgeon ,scalpel ,nurse ,anes-
thetic ,hospital ), restaurants ( waiter ,menu ,plate ,food,chef), or houses ( door ,roof,
kitchen ,family ,bed). Semantic ﬁelds are also related to topic models , like Latent topic models
Dirichlet Allocation ,LDA , which apply unsupervised learning on large sets of texts
to induce sets of associated words from text. Semantic ﬁelds and topic models are
very useful tools for discovering topical structure in documents.
In Appendix G we’ll introduce more relations between senses like hypernymy
orIS-A ,antonymy (opposites) and meronymy (part-whole relations).
Semantic Frames and Roles Closely related to semantic ﬁelds is the idea of a
semantic frame . A semantic frame is a set of words that denote perspectives or semantic frame
participants in a particular type of event. A commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service is
performed. This event can be encoded lexically by using verbs like buy(the event
from the perspective of the buyer), sell(from the perspective of the seller), pay
(focusing on the monetary aspect), or nouns like buyer . Frames have semantic roles
(like buyer ,seller ,goods ,money ), and words in a sentence can take on these roles.
Knowing that buyandsellhave this relation makes it possible for a system to
know that a sentence like Sam bought the book from Ling could be paraphrased as
Ling sold the book to Sam , and that Sam has the role of the buyer in the frame and
Ling the seller . Being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.
Connotation Finally, words have affective meanings orconnotations . The word connotations
connotation has different meanings in different ﬁelds, but here we use it to mean the
aspects of a word’s meaning that are related to a writer or reader’s emotions, senti-
ment, opinions, or evaluations. For example some words have positive connotations
(wonderful ) while others have negative connotations ( dreary ). Even words whose
meanings are similar in other ways can vary in connotation; consider the difference
in connotations between fake,knockoff ,forgery , on the one hand, and copy ,replica ,
reproduction on the other, or innocent (positive connotation) and naive (negative
connotation). Some words describe positive evaluation ( great ,love) and others neg-
ative evaluation ( terrible ,hate). Positive or negative evaluation language is called
sentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment
tasks like sentiment analysis, stance detection, and applications of NLP to the lan-
guage of politics and consumer reviews.
Early work on affective meaning (Osgood et al., 1957) found that words varied
along three important dimensions of affective meaning:
valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus
Thus words like happy orsatisﬁed are high on valence, while unhappy oran-
noyed are low on valence. Excited is high on arousal, while calm is low on arousal.
Controlling is high on dominance, while awed orinﬂuenced are low on dominance.
Each word is thus represented by three numbers, corresponding to its value on each
of the three dimensions:

--- Page 105 ---
6.2 • V ECTOR SEMANTICS 105
Valence Arousal Dominance
courageous 8.05 5.5 7.38
music 7.67 5.57 6.5
heartbreak 2.45 5.65 3.58
cub 6.71 3.95 4.24
Osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word’s
rating on the three scales. This revolutionary idea that word meaning could be rep-
resented as a point in space (e.g., that part of the meaning of heartbreak can be
represented as the point [2:45;5:65;3:58]) was the ﬁrst expression of the vector se-
mantics models that we introduce next.
6.2 Vector Semantics
Vector semantics is the standard way to represent word meaning in NLP, helpingvector
semantics
us model many of the aspects of word meaning we saw in the previous section. The
roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957
idea mentioned above to use a point in three-dimensional space to represent the
connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),
and Firth (1957) to deﬁne the meaning of a word by its distribution in language
use, meaning its neighboring words or grammatical environments. Their idea was
that two words that occur in very similar distributions (whose neighboring words are
similar) have similar meanings.
For example, suppose you didn’t know the meaning of the word ongchoi (a re-
cent borrowing from Cantonese) but you see it in the following contexts:
(6.1) Ongchoi is delicious sauteed with garlic.
(6.2) Ongchoi is superb over rice.
(6.3) ...ongchoi leaves with salty sauces...
And suppose that you had seen many of these context words in other contexts:
(6.4) ...spinach sauteed with garlic over rice...
(6.5) ...chard stems and leaves are delicious...
(6.6) ...collard greens and other salty leafy greens
The fact that ongchoi occurs with words like riceandgarlic anddelicious and
salty , as do words like spinach ,chard , and collard greens might suggest that ongchoi
is a leafy green similar to these other leafy greens.1We can do the same thing
computationally by just counting words in the context of ongchoi .
The idea of vector semantics is to represent a word as a point in a multidimen-
sional semantic space that is derived (in ways we’ll see) from the distributions of
word neighbors. Vectors for representing words are called embeddings (although embeddings
the term is sometimes more strictly applied only to dense vectors like word2vec
(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).
The word “embedding” derives from its mathematical sense as a mapping from one
space or structure to another, although the meaning has shifted; see the end of the
chapter.
1It’s in fact Ipomoea aquatica , a relative of morning glory sometimes called water spinach in English.

--- Page 106 ---
106 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
goodnicebadworstnot good
wonderfulamazingterriﬁcdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto’sareisathan
Figure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. The original 60-
dimensional embeddings were trained for sentiment analysis. Simpliﬁed from Li et al. (2015)
with colors added for explanation.
Fig. 6.1 shows a visualization of embeddings learned for sentiment analysis,
showing the location of selected words projected down from 60-dimensional space
into a two dimensional space. Notice the distinct regions containing positive words,
negative words, and neutral function words.
The ﬁne-grained model of word similarity of vector semantics offers enormous
power to NLP applications. NLP applications like the sentiment classiﬁers of Chap-
ter 4 or Chapter 5 depend on the same words appearing in the training and test sets.
But by representing words as embeddings, a classiﬁer can assign sentiment as long
as it sees some words with similar meanings . And as we’ll see, vector semantic
models can be learned automatically from text without supervision.
In this chapter we’ll introduce the two most commonly used models. In the tf-idf
model, an important baseline, the meaning of a word is deﬁned by a simple function
of the counts of nearby words. We will see that this method results in very long
vectors that are sparse , i.e. mostly zeros (since most words simply never occur in
the context of others). We’ll introduce the word2vec model family for construct-
ing short, dense vectors that have useful semantic properties. We’ll also introduce
thecosine , the standard way to use embeddings to compute semantic similarity , be-
tween two words, two sentences, or two documents, an important tool in practical
applications like question answering, summarization, or automatic essay grading.
6.3 Words and Vectors
“The most important attributes of a vector in 3-space are fLocation, Location, Location g”
Randall Munroe, https://xkcd.com/2358/
Vector or distributional models of meaning are generally based on a co-occurrence
matrix , a way of representing how often words co-occur. We’ll look at two popular
matrices: the term-document matrix and the term-term matrix.
6.3.1 Vectors and documents
In aterm-document matrix , each row represents a word in the vocabulary and eachterm-document
matrix
column represents a document from some collection of documents. Fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by Shakespeare. Each cell in this matrix represents the number of times

--- Page 107 ---
6.3 • W ORDS AND VECTORS 107
a particular word (deﬁned by the row) occurs in a particular document (deﬁned by
the column). Thus foolappeared 58 times in Twelfth Night .
As You Like It Twelfth Night Julius Caesar Henry V
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.
The term-document matrix of Fig. 6.2 was ﬁrst deﬁned as part of the vector
space model of information retrieval (Salton, 1971). In this model, a document isvector space
model
represented as a count vector, a column in Fig. 6.3.
To review some basic linear algebra, a vector is, at heart, just a list or array of vector
numbers. So As You Like It is represented as the list [1,114,36,20] (the ﬁrst column
vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third
column vector). A vector space is a collection of vectors, and is characterized by vector space
itsdimension . Vectors in a 3-dimensional vector space have an element for each dimension
dimension of the space. We will loosely refer to a vector in a 4-dimensional space
as a 4-dimensional vector, with one element along each dimension. In the example
in Fig. 6.3, we’ve chosen to make the document vectors of dimension 4, just so they
ﬁt on the page; in real term-document matrices, the document vectors would have
dimensionalityjVj, the vocabulary size.
The ordering of the numbers in a vector space indicates the different dimensions
on which documents vary. The ﬁrst dimension for both these vectors corresponds to
the number of times the word battle occurs, and we can compare each dimension,
noting for example that the vectors for As You Like It andTwelfth Night have similar
values (1 and 0, respectively) for the ﬁrst dimension.
As You Like It Twelfth Night Julius Caesar Henry V
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure 6.3 The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each document is represented as a column vector of length four.
We can think of the vector for a document as a point in jVj-dimensional space;
thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional
spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we’ve
arbitrarily chosen the dimensions corresponding to the words battle andfool.
Term-document matrices were originally deﬁned as a means of ﬁnding similar
documents for the task of document information retrieval . Two documents that are
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. The vectors for the comedies As You
Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other
(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or
Henry V [13,89,4,3]. This is clear with the raw numbers; in the ﬁrst dimension
(battle) the comedies have low numbers and the others have high numbers, and we
can see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition
more formally.

--- Page 108 ---
108 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560
Figure 6.4 A spatial visualization of the document vectors for the four Shakespeare play
documents, showing just two of the dimensions, corresponding to the words battle andfool.
The comedies have high values for the fooldimension and low values for the battle dimension.
A real term-document matrix, of course, wouldn’t just have 4 rows and columns,
let alone 2. More generally, the term-document matrix has jVjrows (one for each
word type in the vocabulary) and Dcolumns (one for each document in the collec-
tion); as we’ll see, vocabulary sizes are generally in the tens of thousands, and the
number of documents can be enormous (think about all the pages on the web).
Information retrieval (IR) is the task of ﬁnding the document dfrom the Dinformation
retrieval
documents in some collection that best matches a query q. For IR we’ll therefore also
represent a query by a vector, also of length jVj, and we’ll need a way to compare
two vectors to ﬁnd how similar they are. (Doing IR will also require efﬁcient ways
to store and manipulate these vectors by making use of the convenient fact that these
vectors are sparse, i.e., mostly zeros).
Later in the chapter we’ll introduce some of the components of this vector com-
parison process: the tf-idf term weighting, and the cosine similarity metric.
6.3.2 Words as vectors: document dimensions
We’ve seen that documents can be represented as vectors in a vector space. But
vector semantics can also be used to represent the meaning of words . We do this
by associating each word with a word vector— a row vector rather than a column row vector
vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions
of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word
counts in the same four dimensions are used to form the vectors for the other 3
words: wit, [20,15,2,3]; battle , [1,0,7,13]; and good [114,80,62,89].
As You Like It Twelfth Night Julius Caesar Henry V
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure 6.5 The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each word is represented as a row vector of length four.
For documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. This same principle applies to words:
similar words have similar vectors because they tend to occur in similar documents.
The term-document matrix thus lets us represent the meaning of a word by the doc-
uments it tends to occur in.

--- Page 109 ---
6.3 • W ORDS AND VECTORS 109
6.3.3 Words as vectors: word dimensions
An alternative to using the term-document matrix to represent words as vectors of
document counts, is to use the term-term matrix , also called the word-word ma-
trixor the term-context matrix , in which the columns are labeled by words ratherword-word
matrix
than documents. This matrix is thus of dimensionality jVjjVjand each cell records
the number of times the row (target) word and the column (context) word co-occur
in some context in some training corpus. The context could be the document, in
which case the cell represents the number of times the two words appear in the same
document. It is most common, however, to use smaller contexts, generally a win-
dow around the word, for example of 4 words to the left and 4 words to the right,
in which case the cell represents the number of times (in some training corpus) the
column word occurs in such a 4 word window around the row word. Here are four
examples of words in their windows:
is traditionally followed by cherry pie, a traditional dessert
often mixed, such as strawberry rhubarb pie. Apple pie
computer peripherals and personal digital assistants. These devices usually
a computer. This includes information available on the internet
If we then take every occurrence of each word (say strawberry ) and count the
context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a
simpliﬁed subset of the word-word co-occurrence matrix for these four words com-
puted from the Wikipedia corpus (Davies, 2015).
aardvark ... computer data result pie sugar ...
cherry 0 ... 2 8 9 442 25 ...
strawberry 0 ... 0 0 1 60 19 ...
digital 0 ... 1670 1683 85 5 4 ...
information 0 ... 3325 3982 378 5 13 ...
Figure 6.6 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of
the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in
red. Note that a real vector would have vastly more dimensions and thus be much sparser.
Note in Fig. 6.6 that the two words cherry andstrawberry are more similar to
each other (both pieandsugar tend to occur in their window) than they are to other
words like digital ; conversely, digital andinformation are more similar to each other
than, say, to strawberry . Fig. 6.7 shows a spatial visualization.
100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000
Figure 6.7 A spatial visualization of word vectors for digital andinformation , showing just
two of the dimensions, corresponding to the words data andcomputer .
Note thatjVj, the dimensionality of the vector, is generally the size of the vo-
cabulary, often between 10,000 and 50,000 words (using the most frequent words

--- Page 110 ---
110 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
in the training corpus; keeping words after about the most frequent 50,000 or so is
generally not helpful). Since most of these numbers are zero these are sparse vector
representations; there are efﬁcient algorithms for storing and computing with sparse
matrices.
Now that we have some intuitions, let’s move on to examine the details of com-
puting word similarity. Afterwards we’ll discuss methods for weighting cells.
6.4 Cosine for measuring similarity
To measure similarity between two target words vandw, we need a metric that
takes two vectors (of the same dimensionality, either both with words as dimensions,
hence of lengthjVj, or both with documents as dimensions, of length jDj) and gives
a measure of their similarity. By far the most common similarity metric is the cosine
of the angle between the vectors.
The cosine—like most measures for vector similarity used in NLP—is based on
thedot product operator from linear algebra, also called the inner product : dot product
inner product
dot product (v;w) =vw=NX
i=1viwi=v1w1+v2w2+:::+vNwN (6.7)
The dot product acts as a similarity metric because it will tend to be high just when
the two vectors have large values in the same dimensions. Alternatively, vectors that
have zeros in different dimensions—orthogonal vectors—will have a dot product of
0, representing their strong dissimilarity.
This raw dot product, however, has a problem as a similarity metric: it favors
long vectors. The vector length is deﬁned as vector length
jvj=vuutNX
i=1v2
i(6.8)
The dot product is higher if a vector is longer, with higher values in each dimension.
More frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. The raw dot product
thus will be higher for frequent words. But this is a problem; we’d like a similarity
metric that tells us how similar two words are regardless of their frequency.
We modify the dot product to normalize for the vector length by dividing the
dot product by the lengths of each of the two vectors. This normalized dot product
turns out to be the same as the cosine of the angle between the two vectors, following
from the deﬁnition of the dot product between two vectors aandb:
ab=jajjbjcosq
ab
jajjbj=cosq (6.9)
Thecosine similarity metric between two vectors vandwthus can be computed as: cosine

--- Page 111 ---
6.5 • TF-IDF: W EIGHING TERMS IN THE VECTOR 111
cosine (v;w) =vw
jvjjwj=NX
i=1viwi
vuutNX
i=1v2
ivuutNX
i=1w2
i(6.10)
For some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. Thus we could compute a unit vector from aby unit vector
dividing it byjaj. For unit vectors, the dot product is the same as the cosine.
The cosine value ranges from 1 for vectors pointing in the same direction, through
0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since
raw frequency values are non-negative, the cosine for these vectors ranges from 0–1.
Let’s see how the cosine computes which of the words cherry ordigital is closer
in meaning to information , just using raw counts from the following shortened table:
pie data computer
cherry 442 8 2
digital 5 1683 1670
information 5 3982 3325
cos(cherry;information ) =4425+83982+23325p
4422+82+22p
52+39822+33252=:018
cos(digital;information ) =55+16833982+16703325p
52+16832+16702p
52+39822+33252=:996
The model decides that information is way closer to digital than it is to cherry , a
result that seems sensible. Fig. 6.8 shows a visualization.
50010001500200025003000500digitalcherryinformationDimension 1: ‘pie’
Dimension 2: ‘computer’
Figure 6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for
three words ( cherry ,digital , and information ) in the two dimensional space deﬁned by counts
of the words computer andpienearby. The ﬁgure doesn’t show the cosine, but it highlights the
angles; note that the angle between digital andinformation is smaller than the angle between
cherry andinformation . When two vectors are more similar, the cosine is larger but the angle
is smaller; the cosine has its maximum (1) when the angle between two vectors is smallest
(0); the cosine of all other angles is less than 1.
6.5 TF-IDF: Weighing terms in the vector
The co-occurrence matrices above represent each cell by frequencies, either of words
with documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency

--- Page 112 ---
112 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
is not the best measure of association between words. Raw frequency is very skewed
and not very discriminative. If we want to know what kinds of contexts are shared
bycherry andstrawberry but not by digital andinformation , we’re not going to get
good discrimination from words like the,it, orthey, which occur frequently with
all sorts of words and aren’t informative about any particular word. We saw this
also in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not
very discriminative between plays; good is simply a frequent word and has roughly
equivalent high frequencies in each of the plays.
It’s a bit of a paradox. Words that occur nearby frequently (maybe pienearby
cherry ) are more important than words that only appear once or twice. Yet words
that are too frequent—ubiquitous, like theorgood — are unimportant. How can we
balance these two conﬂicting constraints?
There are two common solutions to this problem: in this section we’ll describe
thetf-idf weighting, usually used when the dimensions are documents. In the next
section we introduce the PPMI algorithm (usually used when the dimensions are
words).
Thetf-idf weighting (the ‘-’ here is a hyphen, not a minus sign) is the product
of two terms, each term capturing one of these two intuitions:
The ﬁrst is the term frequency (Luhn, 1957): the frequency of the word tin the term frequency
document d. We can just use the raw count as the term frequency:
tft;d=count (t;d) (6.11)
More commonly we squash the raw frequency a bit, by using the log 10of the fre-
quency instead. The intuition is that a word appearing 100 times in a document
doesn’t make that word 100 times more likely to be relevant to the meaning of the
document. We also need to do something special with counts of 0, since we can’t
take the log of 0.2
tft;d=(
1+log10count (t;d) if count (t;d)>0
0 otherwise(6.12)
If we use log weighting, terms which occur 0 times in a document would have tf =0,
1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =
1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.
The second factor in tf-idf is used to give a higher weight to words that occur
only in a few documents. Terms that are limited to a few documents are useful
for discriminating those documents from the rest of the collection; terms that occur
frequently across the entire collection aren’t as helpful. The document frequencydocument
frequency
dftof a term tis the number of documents it occurs in. Document frequency is
not the same as the collection frequency of a term, which is the total number of
times the word appears in the whole collection in any document. Consider in the
collection of Shakespeare’s 37 plays the two words Romeo andaction . The words
have identical collection frequencies (they both occur 113 times in all the plays) but
very different document frequencies, since Romeo only occurs in a single play. If
our goal is to ﬁnd documents about the romantic tribulations of Romeo, the word
Romeo should be highly weighted, but not action :
Collection Frequency Document Frequency
Romeo 113 1
action 113 31
2We can also use this alternative formulation, which we have used in earlier editions: tf t;d=
log10(count (t;d)+1)

--- Page 113 ---
6.5 • TF-IDF: W EIGHING TERMS IN THE VECTOR 113
We emphasize discriminative words like Romeo via the inverse document fre-
quency oridfterm weight (Sparck Jones, 1972). The idf is deﬁned using the frac- idf
tionN=dft, where Nis the total number of documents in the collection, and df tis
the number of documents in which term toccurs. The fewer documents in which a
term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that
occur in all the documents. It’s usually clear what counts as a document: in Shake-
speare we would use a play; when processing a collection of encyclopedia articles
like Wikipedia, the document is a Wikipedia page; in processing newspaper articles,
the document is a single article. Occasionally your corpus might not have appropri-
ate document divisions and you might need to break up the corpus into documents
yourself for the purposes of computing idf.
Because of the large number of documents in many collections, this measure
too is usually squashed with a log function. The resulting deﬁnition for inverse
document frequency (idf) is thus
idft=log10N
dft
(6.13)
Here are some idf values for some words in the Shakespeare corpus, (along with
the document frequency df values on which they are based) ranging from extremely
informative words which occur in only one play like Romeo , to those that occur in a
few like salad orFalstaff , to those which are very common like foolor so common
as to be completely non-discriminative since they occur in all 37 plays like good or
sweet .3
Word df idf
Romeo 1 1.57
salad 2 1.27
Falstaff 4 0.967
forest 12 0.489
battle 21 0.246
wit 34 0.037
fool 36 0.012
good 37 0
sweet 37 0
The tf-idf weighted value wt;dfor word tin document dthus combines term tf-idf
frequency tf t;d(deﬁned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:
wt;d=tft;didft (6.14)
Fig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,
using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-
sponding to the word good have now all become 0; since this word appears in every
document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which
appears in 36 out of the 37 plays, has a much lower weight.
The tf-idf weighting is the way for weighting co-occurrence matrices in infor-
mation retrieval, but also plays a role in many other aspects of natural language
processing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll look at other
weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.
3Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

--- Page 114 ---
114 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
As You Like It Twelfth Night Julius Caesar Henry V
battle 0.246 0 0.454 0.520
good 0 0 0 0
fool 0.030 0.033 0.0012 0.0019
wit 0.085 0.081 0.048 0.054
Figure 6.9 A portion of the tf-idf weighted term-document matrix for four words in Shake-
speare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the
0:085 value for witinAs You Like It is the product of tf =1+log10(20) =2:301 and idf =:037.
Note that the idf weighting has eliminated the importance of the ubiquitous word good and
vastly reduced the impact of the almost-ubiquitous word fool.
6.6 Pointwise Mutual Information (PMI)
An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-
mation), is used for term-term-matrices, when the vector dimensions correspond to
words rather than documents. PPMI draws on the intuition that the best way to weigh
the association between two words is to ask how much more the two words co-occur
in our corpus than we would have a priori expected them to appear by chance.
Pointwise mutual information (Fano, 1961)4is one of the most important con-pointwise
mutual
informationcepts in NLP. It is a measure of how often two events xandyoccur, compared with
what we would expect if they were independent:
I(x;y) =log2P(x;y)
P(x)P(y)(6.16)
The pointwise mutual information between a target word wand a context word
c(Church and Hanks 1989, Church and Hanks 1990) is then deﬁned as:
PMI(w;c) =log2P(w;c)
P(w)P(c)(6.17)
The numerator tells us how often we observed the two words together (assuming
we compute probability by using the MLE). The denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the probability of two independent events both occurring is just
the product of the probabilities of the two events. Thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. PMI is
a useful tool whenever we need to ﬁnd words that are strongly associated.
PMI values range from negative to positive inﬁnity. But negative PMI values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. To distinguish whether
two words whose individual probability is each 10 6occur together less often than
chance, we would need to be certain that the probability of the two occurring to-
gether is signiﬁcantly less than 10 12, and this kind of granularity would require an
enormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate
such scores of ‘unrelatedness’ with human judgments. For this reason it is more
4PMI is based on the mutual information between two random variables XandY, deﬁned as:
I(X;Y) =X
xX
yP(x;y)log2P(x;y)
P(x)P(y)(6.15)
In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information

--- Page 115 ---
6.6 • P OINTWISE MUTUAL INFORMATION (PMI) 115
common to use Positive PMI (called PPMI ) which replaces all negative PMI values PPMI
with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5:
PPMI (w;c) =max(log2P(w;c)
P(w)P(c);0) (6.18)
More formally, let’s assume we have a co-occurrence matrix F with W rows (words)
and C columns (contexts), where fi jgives the number of times word wioccurs with
context cj. This can be turned into a PPMI matrix where PPMI i jgives the PPMI
value of word wiwith context cj(which we can also express as PPMI( wi;cj) or
PPMI( w=i;c=j)) as follows:
pi j=fi jPW
i=1PC
j=1fi j;pi=PC
j=1fi jPW
i=1PC
j=1fi j;pj=PW
i=1fi jPW
i=1PC
j=1fi j(6.19)
PPMI i j=max(log2pi j
pipj;0) (6.20)
Let’s see some PPMI calculations. We’ll use Fig. 6.10, which repeats Fig. 6.6 plus
all the count marginals, and let’s pretend for ease of calculation that these are the
only words/contexts that matter.
computer data result pie sugar count(w)
cherry 2 8 9 442 25 486
strawberry 0 0 1 60 19 80
digital 1670 1683 85 5 4 3447
information 3325 3982 378 5 13 7703
count(context) 4997 5673 473 512 61 11716
Figure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,
together with the marginals, pretending for the purpose of this calculation that no other word-
s/contexts matter.
Thus for example we could compute PPMI(information,data), assuming we pre-
tended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-
lows:
P(w=information, c=data ) =3982
11716=:3399
P(w=information ) =7703
11716=:6575
P(c=data ) =5673
11716=:4842
PPMI (information,data ) = log2(:3399=(:6575:4842)) =:0944
Fig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and
Fig. 6.12 shows the PPMI values. Not surprisingly, cherry andstrawberry are highly
associated with both pieandsugar , and data is mildly associated with information .
PMI has the problem of being biased toward infrequent events; very rare words
tend to have very high PMI values. One way to reduce this bias toward low frequency
5Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the
 ¥from log (0).

--- Page 116 ---
116 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
p(w,context) p(w)
computer data result pie sugar p(w)
cherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415
strawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068
digital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942
information 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575
p(context) 0.4265 0.4842 0.0404 0.0437 0.0052
Figure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals
in the right column and the bottom row.
computer data result pie sugar
cherry 0 0 0 4.38 3.30
strawberry 0 0 0 4.10 5.51
digital 0.18 0.01 0 0 0
information 0.02 0.09 0.28 0 0
Figure 6.12 The PPMI matrix showing the association between words and context words,
computed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had
a negative PMI; for example PMI( cherry,computer ) = -6.7, meaning that cherry andcomputer
co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace
negative values by zero.
events is to slightly change the computation for P(c), using a different function Pa(c)
that raises the probability of the context word to the power of a:
PPMI a(w;c) =max(log2P(w;c)
P(w)Pa(c);0) (6.21)
Pa(c) =count (c)a
P
ccount (c)a(6.22)
Levy et al. (2015) found that a setting of a=0:75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in Eq. 6.32). This works because raising the count to a=
0:75 increases the probability assigned to rare contexts, and hence lowers their PMI
(Pa(c)>P(c)when cis rare).
Another possible solution is Laplace smoothing: Before computing PMI, a small
constant k(values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. The larger the k, the more the non-zero counts
are discounted.
6.7 Applications of the tf-idf or PPMI vector models
In summary, the vector semantics model we’ve described so far represents a target
word as a vector with dimensions corresponding either to the documents in a large
collection (the term-document matrix) or to the counts of words in some neighboring
window (the term-term matrix). The values in each dimension are counts, weighted
by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the
vectors are sparse (since most values are zero).
The model computes the similarity between two words xandyby taking the
cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model

--- Page 117 ---
6.8 • W ORD2VEC 117
is sometimes referred to as the tf-idf model or the PPMI model, after the weighting
function.
The tf-idf model of meaning is often used for document functions like deciding
if two documents are similar. We represent a document by taking the vectors of
all the words in the document, and computing the centroid of all those vectors. centroid
The centroid is the multidimensional version of the mean; the centroid of a set of
vectors is a single vector that has the minimum sum of squared distances to each of
the vectors in the set. Given kword vectors w1;w2;:::;wk, the centroid document
vector dis:document
vector
d=w1+w2+:::+wk
k(6.23)
Given two documents, we can then compute their document vectors d1andd2, and
estimate the similarity between the two documents by cos (d1;d2). Document sim-
ilarity is also useful for all sorts of applications; information retrieval, plagiarism
detection, news recommender systems, and even for digital humanities tasks like
comparing different versions of a text to see which are similar to each other.
Either the PPMI model or the tf-idf model can be used to compute word simi-
larity, for tasks like ﬁnding word paraphrases, tracking changes in word meaning, or
automatically discovering meanings of words in different corpora. For example, we
can ﬁnd the 10 most similar words to any target word wby computing the cosines
between wand each of the V 1 other words, sorting, and looking at the top 10.
6.8 Word2vec
In the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to words in the vocabulary or documents in a collection.
We now introduce a more powerful word representation: embeddings , short dense
vectors. Unlike the vectors we’ve seen so far, embeddings are short , with number
of dimensions dranging from 50-1000, rather than the much larger vocabulary size
jVjor number of documents Dwe’ve seen. These ddimensions don’t have a clear
interpretation. And the vectors are dense : instead of vector entries being sparse,
mostly-zero counts or functions of counts, the values will be real-valued numbers
that can be negative.
It turns out that dense vectors work better in every NLP task than sparse vectors.
While we don’t completely understand all the reasons for this, we have some intu-
itions. Representing words as 300-dimensional dense vectors requires our classiﬁers
to learn far fewer weights than if we represented words as 50,000-dimensional vec-
tors, and the smaller parameter space possibly helps with generalization and avoid-
ing overﬁtting. Dense vectors may also do a better job of capturing synonymy.
For example, in a sparse vector representation, dimensions for synonyms like car
andautomobile dimension are distinct and unrelated; sparse vectors may thus fail
to capture the similarity between a word with caras a neighbor and a word with
automobile as a neighbor.
In this section we introduce one method for computing embeddings: skip-gram skip-gram
with negative sampling , sometimes called SGNS . The skip-gram algorithm is one SGNS
of two algorithms in a software package called word2vec , and so sometimes the word2vec
algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.
2013b). The word2vec methods are fast, efﬁcient to train, and easily available on-

--- Page 118 ---
118 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
line with code and pretrained embeddings. Word2vec embeddings are static em-
beddings , meaning that the method learns one ﬁxed embedding for each word in thestatic
embeddings
vocabulary. In Chapter 11 we’ll introduce methods for learning dynamic contextual
embeddings like the popular family of BERT representations, in which the vector
for each word is different in different contexts.
The intuition of word2vec is that instead of counting how often each word woc-
curs near, say, apricot , we’ll instead train a classiﬁer on a binary prediction task: “Is
word wlikely to show up near apricot ?” We don’t actually care about this prediction
task; instead we’ll take the learned classiﬁer weights as the word embeddings.
The revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classiﬁer; a word cthat occurs near the target
word apricot acts as gold ‘correct answer’ to the question “Is word clikely to show
up near apricot ?” This method, often called self-supervision , avoids the need for self-supervision
any sort of hand-labeled supervision signal. This idea was ﬁrst proposed in the task
of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)
showed that a neural language model (a neural network that learned to predict the
next word from prior words) could just use the next word in running text as its
supervision signal, and could be used to learn an embedding representation for each
word as part of doing this prediction task.
We’ll see how to do neural networks in the next chapter, but word2vec is a
much simpler model than the neural network language model, in two ways. First,
word2vec simpliﬁes the task (making it binary classiﬁcation instead of word pre-
diction). Second, word2vec simpliﬁes the architecture (training a logistic regression
classiﬁer instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). The intuition of skip-gram is:
1. Treat the target word and a neighboring context word as positive examples.
2. Randomly sample other words in the lexicon to get negative samples.
3. Use logistic regression to train a classiﬁer to distinguish those two cases.
4. Use the learned weights as the embeddings.
6.8.1 The classiﬁer
Let’s start by thinking about the classiﬁcation task, and then turn to how to train.
Imagine a sentence like the following, with a target word apricot , and assume we’re
using a window of 2 context words:
... lemon, a [tablespoon of apricot jam, a] pinch ...
c1 c2 w c3 c4
Our goal is to train a classiﬁer such that, given a tuple (w;c)of a target word
wpaired with a candidate context word c(for example ( apricot ,jam), or perhaps
(apricot ,aardvark )) it will return the probability that cis a real context word (true
forjam, false for aardvark ):
P(+jw;c) (6.24)
The probability that word cis not a real context word for wis just 1 minus
Eq. 6.24:
P( jw;c) =1 P(+jw;c) (6.25)

--- Page 119 ---
6.8 • W ORD2VEC 119
How does the classiﬁer compute the probability P? The intuition of the skip-
gram model is to base this probability on embedding similarity: a word is likely to
occur near the target if its embedding vector is similar to the target embedding. To
compute similarity between these dense embeddings, we rely on the intuition that
two vectors are similar if they have a high dot product (after all, cosine is just a
normalized dot product). In other words:
Similarity (w;c)cw (6.26)
The dot product cwis not a probability, it’s just a number ranging from  ¥to¥
(since the elements in word2vec embeddings can be negative, the dot product can be
negative). To turn the dot product into a probability, we’ll use the logistic orsigmoid
function s(x), the fundamental core of logistic regression:
s(x) =1
1+exp( x)(6.27)
We model the probability that word cis a real context word for target word was:
P(+jw;c) = s(cw) =1
1+exp( cw)(6.28)
The sigmoid function returns a number between 0 and 1, but to make it a probability
we’ll also need the total probability of the two possible events ( cis a context word,
andcisn’t a context word) to sum to 1. We thus estimate the probability that word c
is not a real context word for was:
P( jw;c) = 1 P(+jw;c)
=s( cw) =1
1+exp(cw)(6.29)
Equation 6.28 gives us the probability for one word, but there are many context
words in the window. Skip-gram makes the simplifying assumption that all context
words are independent, allowing us to just multiply their probabilities:
P(+jw;c1:L) =LY
i=1s(ciw) (6.30)
logP(+jw;c1:L) =LX
i=1logs(ciw) (6.31)
In summary, skip-gram trains a probabilistic classiﬁer that, given a test target word
wand its context window of Lwords c1:L, assigns a probability based on how similar
this context window is to the target word. The probability is based on applying the
logistic (sigmoid) function to the dot product of the embeddings of the target word
with each context word. To compute this probability, we just need embeddings for
each target word and context word in the vocabulary.
Fig. 6.13 shows the intuition of the parameters we’ll need. Skip-gram actually
stores two embeddings for each word, one for the word as a target, and one for the
word considered as context. Thus the parameters we need to learn are two matrices
WandC, each containing an embedding for every one of the jVjwords in the
vocabulary V.6Let’s now turn to learning these embeddings (which is the real goal
of training this classiﬁer in the ﬁrst place).
6In principle the target matrix and the context matrix could use different vocabularies, but we’ll simplify
by assuming one shared vocabulary V.

--- Page 120 ---
120 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords…
…1..d…
…
Figure 6.13 The embeddings learned by the skipgram model. The algorithm stores two
embeddings for each word, the target embedding (sometimes called the input embedding)
and the context embedding (sometimes called the output embedding). The parameter qthat
the algorithm learns is thus a matrix of 2 jVjvectors, each of dimension d, formed by concate-
nating two matrices, the target embeddings Wand the context+noise embeddings C.
6.8.2 Learning skip-gram embeddings
The learning algorithm for skip-gram embeddings takes as input a corpus of text,
and a chosen vocabulary size N. It begins by assigning a random embedding vector
for each of the N vocabulary words, and then proceeds to iteratively shift the em-
bedding of each word wto be more like the embeddings of words that occur nearby
in texts, and less like the embeddings of words that don’t occur nearby. Let’s start
by considering a single piece of training data:
... lemon, a [tablespoon of apricot jam, a] pinch ...
c1 c2 w c3 c4
This example has a target word w(apricot), and 4 context words in the L=2
window, resulting in 4 positive training instances (on the left below):
positive examples +
w c pos
apricot tablespoon
apricot of
apricot jam
apricot anegative examples -
w c neg w c neg
apricot aardvark apricot seven
apricot my apricot forever
apricot where apricot dear
apricot coaxial apricot if
For training a binary classiﬁer we also need negative examples. In fact skip-
gram with negative sampling (SGNS) uses more negative examples than positive
examples (with the ratio between them set by a parameter k). So for each of these
(w;cpos)training instances we’ll create knegative samples, each consisting of the
target wplus a ‘noise word’ cneg. A noise word is a random word from the lexicon,
constrained not to be the target word w. The right above shows the setting where
k=2, so we’ll have 2 negative examples in the negative training set  for each
positive example w;cpos.
The noise words are chosen according to their weighted unigram frequency
pa(w), where ais a weight. If we were sampling according to unweighted fre-
quency p(w), it would mean that with unigram probability p(“the”)we would choose
the word theas a noise word, with unigram probability p(“aardvark ”)we would

--- Page 121 ---
6.8 • W ORD2VEC 121
choose aardvark , and so on. But in practice it is common to set a=0:75, i.e. use
the weighting p3
4(w):
Pa(w) =count (w)a
P
w0count (w0)a(6.32)
Setting a=:75 gives better performance because it gives rare noise words slightly
higher probability: for rare words, Pa(w)>P(w). To illustrate this intuition, it
might help to work out the probabilities for an example with a=:75 and two events,
P(a) =0:99 and P(b) =0:01:
Pa(a) =:99:75
:99:75+:01:75=0:97
Pa(b) =:01:75
:99:75+:01:75=0:03 (6.33)
Thus using a=:75 increases the probability of the rare event bfrom 0.01 to 0.03.
Given the set of positive and negative training instances, and an initial set of
embeddings, the goal of the learning algorithm is to adjust those embeddings to
• Maximize the similarity of the target word, context word pairs (w;cpos)drawn
from the positive examples
• Minimize the similarity of the (w;cneg)pairs from the negative examples.
If we consider one word/context pair (w;cpos)with its knoise words cneg1:::cnegk,
we can express these two goals as the following loss function Lto be minimized
(hence the ); here the ﬁrst term expresses that we want the classiﬁer to assign the
real context word cposa high probability of being a neighbor, and the second term
expresses that we want to assign each of the noise words cnegia high probability of
being a non-neighbor, all multiplied because we assume independence:
L= log"
P(+jw;cpos)kY
i=1P( jw;cnegi)#
= "
logP(+jw;cpos)+kX
i=1logP( jw;cnegi)#
= "
logP(+jw;cpos)+kX
i=1log 
1 P(+jw;cnegi)#
= "
logs(cposw)+kX
i=1logs( cnegiw)#
(6.34)
That is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the knegative sampled non-
neighbor words.
We minimize this loss function using stochastic gradient descent. Fig. 6.14
shows the intuition of one step of learning.
To get the gradient, we need to take the derivative of Eq. 6.34 with respect to
the different embeddings. It turns out the derivatives are the following (we leave the

--- Page 122 ---
122 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
WCmove apricot and jam closer,increasing cpos z waardvark
move apricot and matrix apartdecreasing cneg1 z w“…apricot jam…”w
zebrazebraaardvarkjamapricot
cposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2
Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift
embeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prod-
uct with) context embeddings for nearby words (here jam) and further from (lower dot product
with) context embeddings for noise words that don’t occur nearby (here Tolstoy andmatrix ).
proof as an exercise at the end of the chapter):
¶L
¶cpos= [s(cposw) 1]w (6.35)
¶L
¶cneg= [s(cnegw)]w (6.36)
¶L
¶w= [s(cposw) 1]cpos+kX
i=1[s(cnegiw)]cnegi(6.37)
The update equations going from time step ttot+1 in stochastic gradient descent
are thus:
ct+1
pos=ct
pos h[s(ct
poswt) 1]wt(6.38)
ct+1
neg=ct
neg h[s(ct
negwt)]wt(6.39)
wt+1=wt h"
[s(ct
poswt) 1]ct
pos+kX
i=1[s(ct
negiwt)]ct
negi#
(6.40)
Just as in logistic regression, then, the learning algorithm starts with randomly ini-
tialized WandCmatrices, and then walks through the training corpus using gradient
descent to move WandCso as to minimize the loss in Eq. 6.34 by making the up-
dates in (Eq. 6.38)-(Eq. 6.40).
Recall that the skip-gram model learns twoseparate embeddings for each word i:
thetarget embedding wiand the context embedding ci, stored in two matrices, thetarget
embeddingcontext
embedding target matrix Wand the context matrix C. It’s common to just add them together,
representing word iwith the vector wi+ci. Alternatively we can throw away the C
matrix and just represent each word iby the vector wi.
As with the simple count-based methods like tf-idf, the context window size L
affects the performance of skip-gram embeddings, and experiments often tune the
parameter Lon a devset.

--- Page 123 ---
6.9 • V ISUALIZING EMBEDDINGS 123
6.8.3 Other kinds of static embeddings
There are many kinds of static embeddings. An extension of word2vec, fasttext fasttext
(Bojanowski et al., 2017), addresses a problem with word2vec as we have presented
it so far: it has no good way to deal with unknown words —words that appear in
a test corpus but were unseen in the training corpus. A related problem is word
sparsity, such as in languages with rich morphology, where some of the many forms
for each noun and verb may only occur rarely. Fasttext deals with these problems
by using subword models, representing each word as itself plus a bag of constituent
n-grams, with special boundary symbols <and>added to each word. For example,
with n=3 the word where would be represented by the sequence <where> plus the
character n-grams:
<wh, whe, her, ere, re>
Then a skipgram embedding is learned for each constituent n-gram, and the word
where is represented by the sum of all of the embeddings of its constituent n-grams.
Unknown words can then be presented only by the sum of the constituent n-grams.
A fasttext open-source library, including pretrained embeddings for 157 languages,
is available at https://fasttext.cc .
Another very widely used static embedding model is GloVe (Pennington et al.,
2014), short for Global Vectors, because the model is based on capturing global
corpus statistics. GloVe is based on ratios of probabilities from the word-word co-
occurrence matrix, combining the intuitions of count-based models like PPMI while
also capturing the linear structures used by methods like word2vec.
It turns out that dense embeddings like word2vec actually have an elegant math-
ematical relationship with sparse embeddings like PPMI, in which word2vec can
be seen as implicitly optimizing a function of a PPMI matrix (Levy and Goldberg,
2014c).
6.9 Visualizing Embeddings
“I see well in many dimensions as long as the dimensions are around two.”
The late economist Martin Shubik
Visualizing embeddings is an important goal in helping understand, apply, and
improve these models of word meaning. But how can we visualize a (for example)
100-dimensional vector?
Rohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurrence
HEADHANDFACE
DOGAMERICA
CATEYEEUROPE
FOOTCHINAFRANCE
CHICAGOARM
FINGER
NOSELEGRUSSIA
MOUSEAFRICA
ATLANTAEARSHOULDERASIA
COW
BULLPUPPYLIONHAWAII
MONTREALTOKYOTOEMOSCOW
TOOTH
NASHVILLEBRAZILWRIST
KITTENANKLE
TURTLE
OYSTER
Figure 8: Multidimensional scaling for three noun classes.WRIST
ANKLE
SHOULDER
ARM
LEG
HAND
FOOT
HEAD
NOSE
FINGER
TOE
FACE
EAR
EYE
TOOTH
DOG
CAT
PUPPY
KITTEN
COW
MOUSE
TURTLE
OYSTER
LION
BULL
CHICAGO
ATLANTA
MONTREAL
NASHVILLE
TOKYOCHINA
RUSSIA
AFRICA
ASIA
EUROPE
AMERICA
BRAZIL
MOSCOW
FRANCEHAWAIIFigure 9: Hierarchical clustering for three noun classes using distances based on vector correlations.
20
The simplest way to visualize the meaning of a word
wembedded in a space is to list the most similar words to
wby sorting the vectors for all words in the vocabulary by
their cosine with the vector for w. For example the 7 closest
words to frogusing a particular embeddings computed with
the GloVe algorithm are: frogs ,toad,litoria ,leptodactyli-
dae,rana,lizard , and eleutherodactylus (Pennington et al.,
2014).
Yet another visualization method is to use a clustering
algorithm to show a hierarchical representation of which
words are similar to others in the embedding space. The
uncaptioned ﬁgure on the left uses hierarchical clustering
of some embedding vectors for nouns as a visualization

--- Page 124 ---
124 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
method (Rohde et al., 2006).
Probably the most common visualization method, how-
ever, is to project the 100 dimensions of a word down into 2
dimensions. Fig. 6.1 showed one such visualization, as does
Fig. 6.16, using a projection method called t-SNE (van der
Maaten and Hinton, 2008).
6.10 Semantic properties of embeddings
In this section we brieﬂy summarize some of the semantic properties of embeddings
that have been studied.
Different types of similarity or association: One parameter of vector semantic
models that is relevant to both sparse tf-idf vectors and dense word2vec vectors is
the size of the context window used to collect counts. This is generally between 1
and 10 words on each side of the target word (for a total context of 2-20 words).
The choice depends on the goals of the representation. Shorter context windows
tend to lead to representations that are a bit more syntactic, since the information is
coming from immediately nearby words. When the vectors are computed from short
context windows, the most similar words to a target word wtend to be semantically
similar words with the same parts of speech. When vectors are computed from long
context windows, the highest cosine words to a target word wtend to be words that
are topically related but not similar.
For example Levy and Goldberg (2014a) showed that using skip-gram with a
window of2, the most similar words to the word Hogwarts (from the Harry Potter
series) were names of other ﬁctional schools: Sunnydale (from Buffy the Vampire
Slayer ) orEvernight (from a vampire series). With a window of 5, the most similar
words to Hogwarts were other words topically related to the Harry Potter series:
Dumbledore ,Malfoy , and half-blood .
It’s also often useful to distinguish two kinds of similarity or association between
words (Sch ¨utze and Pedersen, 1993). Two words have ﬁrst-order co-occurrenceﬁrst-order
co-occurrence
(sometimes called syntagmatic association ) if they are typically nearby each other.
Thus wrote is a ﬁrst-order associate of book orpoem . Two words have second-order
co-occurrence (sometimes called paradigmatic association ) if they have similarsecond-order
co-occurrence
neighbors. Thus wrote is a second-order associate of words like said orremarked .
Analogy/Relational Similarity: Another semantic property of embeddings is their
ability to capture relational meanings. In an important early vector space model of
cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram modelparallelogram
model
for solving simple analogy problems of the form a is to b as a* is to what? . In
such problems, a system is given a problem like apple:tree::grape:? , i.e., apple is
to tree as grape is to , and must ﬁll in the word vine. In the parallelogram
model, illustrated in Fig. 6.15, the vector from the word apple to the word tree(=#   tree #       apple) is added to the vector for grape (#        grape); the nearest word to that point
is returned.
In early work with sparse embeddings, scholars showed that sparse vector mod-
els of meaning could solve such analogy problems (Turney and Littman, 2005),
but the parallelogram method received more modern attention because of its suc-
cess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg
2014b, Pennington et al. 2014). For example, the result of the expression#     king 

--- Page 125 ---
6.10 • S EMANTIC PROPERTIES OF EMBEDDINGS 125
treeapplegrapevine
Figure 6.15 The parallelogram model for analogy problems (Rumelhart and Abrahamson,
1973): the location of#     vine can be found by subtracting#       apple from#   tree and adding#       grape.
#     man+#            woman is a vector close to#         queen. Similarly,#      Paris #           France +#     Italy results
in a vector that is close to#         Rome. The embedding model thus seems to be extract-
ing representations of relations like MALE -FEMALE , or CAPITAL -CITY -OF, or even
COMPARATIVE /SUPERLATIVE , as shown in Fig. 6.16 from GloVe.
(a) (b)
Figure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimen-
sions. (a)#     king #     man+#            woman is close to#        queen. (b) offsets seem to capture comparative and superlative
morphology (Pennington et al., 2014).
For a a:b::a:bproblem, meaning the algorithm is given vectors a,b, and
aand must ﬁnd b, the parallelogram method is thus:
ˆb=argmin
xdistance (x;b a+a) (6.41)
with some distance function, such as Euclidean distance.
There are some caveats. For example, the closest value returned by the paral-
lelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact
b* but one of the 3 input words or their morphological variants (i.e., cherry:red ::
potato:x returns potato orpotatoes instead of brown ), so these must be explicitly
excluded. Furthermore while embedding spaces perform well if the task involves
frequent words, small distances, and certain relations (like relating countries with
their capitals or verbs/nouns with their inﬂected forms), the parallelogram method
with embeddings doesn’t work as well for other relations (Linzen 2016, Gladkova
et al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)
argue that the parallelogram method is in general too simple to model the human
cognitive process of forming analogies of this kind.

--- Page 126 ---
126 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
6.10.1 Embeddings and Historical Semantics
Embeddings can also be a useful tool for studying how meaning changes over time,
by computing multiple embedding spaces, each from texts written in a particular
time period. For example Fig. 6.17 shows a visualization of changes in meaning in
English words over the last two centuries, computed by building separate embed-
ding spaces for each decade from historical corpora like Google n-grams (Lin et al.,
2012b) and the Corpus of Historical American English (Davies, 2012).
CHAPTER 5. DYNAMIC SOCIAL REPRESENTATIONS OF WORD MEANING 79
Figure 5.1: Two-dimensional visualization of semantic change in English using SGNS
vectors (see Section 5.8 for the visualization algorithm). A,T h ew o r d gay shifted
from meaning “cheerful” or “frolicsome” to referring to homosexuality. A,I nt h ee a r l y
20th century broadcast referred to “casting out seeds”; with the rise of television and
radio its meaning shifted to “transmitting signals”. C,Awful underwent a process of
pejoration, as it shifted from meaning “full of awe” to meaning “terrible or appalling”
[212].
that adverbials (e.g., actually )h a v eag e n e r a lt e n d e n c yt ou n d e r g os u b j e c t i ﬁ c a t i o n
where they shift from objective statements about the world (e.g., “Sorry, the car is
actually broken”) to subjective statements (e.g., “I can’t believe he actually did that”,
indicating surprise/disbelief).
5.2.2 Computational linguistic studies
There are also a number of recent works analyzing semantic change using computational
methods. [ 200] use latent semantic analysis to analyze how word meanings broaden
and narrow over time. [ 113]u s er a wc o - o c c u r r e n c ev e c t o r st op e r f o r man u m b e ro f
historical case-studies on semantic change, and [ 252] perform a similar set of small-
scale case-studies using temporal topic models. [ 87]c o n s t r u c tp o i n t - w i s em u t u a l
information-based embeddings and found that semantic changes uncovered by their
method had reasonable agreement with human judgments. [ 129]a n d[ 119]u s e“ n e u r a l ”
word-embedding methods to detect linguistic change points. Finally, [ 257]a n a l y z e
historical co-occurrences to test whether synonyms tend to change in similar ways.
Figure 6.17 A t-SNE visualization of the semantic change of 3 words in English using
word2vec vectors. The modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. Earlier points are com-
puted from earlier historical embedding spaces. The visualizations show the changes in the
word gayfrom meanings related to “cheerful” or “frolicsome” to referring to homosexuality,
the development of the modern “transmission” sense of broadcast from its original sense of
sowing seeds, and the pejoration of the word awful as it shifted from meaning “full of awe”
to meaning “terrible or appalling” (Hamilton et al., 2016b).
6.11 Bias and Embeddings
In addition to their ability to learn word meaning from text, embeddings, alas,
also reproduce the implicit biases and stereotypes that were latent in the text. As
the prior section just showed, embeddings can roughly model relational similar-
ity: ‘queen’ as the closest word to ‘king’ - ‘man’ + ‘woman’ implies the analogy
man:woman::king:queen . But these same embedding analogies also exhibit gender
stereotypes. For example Bolukbasi et al. (2016) ﬁnd that the closest occupation
to ‘computer programmer’ - ‘man’ + ‘woman’ in word2vec embeddings trained on
news text is ‘homemaker’, and that the embeddings similarly suggest the analogy
‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. This could result in what Crawford
(2017) and Blodgett et al. (2020) call an allocational harm , when a system allo-allocational
harm
cates resources (jobs or credit) unfairly to different groups. For example algorithms
that use embeddings as part of a search for hiring potential programmers or doctors
might thus incorrectly downweight documents with women’s names.
It turns out that embeddings don’t just reﬂect the statistics of their input, but also
amplify bias; gendered terms become more gendered in embedding space than theybias
ampliﬁcation
were in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.
2020), and biases are more exaggerated than in actual labor employment statistics
(Garg et al., 2018).
Embeddings also encode the implicit associations that are a property of human
reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-

--- Page 127 ---
6.12 • E VALUATING VECTOR MODELS 127
ple’s associations between concepts (like ‘ﬂowers’ or ‘insects’) and attributes (like
‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with
which they label words in the various categories.7Using such methods, people
in the United States have been shown to associate African-American names with
unpleasant words (more than European-American names), male names more with
mathematics and female names with the arts, and old people’s names with unpleas-
ant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan
et al. (2017) replicated all these ﬁndings of implicit associations using GloVe vectors
and cosine similarity instead of human latencies. For example African-American
names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words
while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine
with pleasant words. These problems with embeddings are an example of a repre-
sentational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused byrepresentational
harm
a system demeaning or even ignoring some social groups. Any embedding-aware al-
gorithm that made use of word sentiment could thus exacerbate bias against African
Americans.
Recent research focuses on ways to try to remove these kinds of biases, for
example by developing a transformation of the embedding space that removes gen-
der stereotypes but preserves deﬁnitional gender (Bolukbasi et al. 2016, Zhao et al.
2017) or changing the training procedure (Zhao et al., 2018b). However, although
these sorts of debiasing may reduce bias in embeddings, they do not eliminate it debiasing
(Gonen and Goldberg, 2019), and this remains an open problem.
Historical embeddings are also being used to measure biases in the past. Garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
ties or genders (for example the relative cosine similarity of women’s names versus
men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century.
They found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupations. Historical embeddings also repli-
cated old surveys of ethnic stereotypes; the tendency of experimental participants in
1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese
ethnicity, correlates with the cosine between Chinese last names and those adjectives
using embeddings trained on 1930s text. They also were able to document historical
gender biases, such as the fact that embeddings for adjectives related to competence
(‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than fe-
male words, and showed that this bias has been slowly decreasing since 1960. We
return in later chapters to this question about the role of bias in natural language
processing.
6.12 Evaluating Vector Models
The most important evaluation metric for vector models is extrinsic evaluation on
tasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-
mance over some other model.
7Roughly speaking, if humans associate ‘ﬂowers’ with ‘pleasantness’ and ‘insects’ with ‘unpleasant-
ness’, when they are instructed to push a green button for ‘ﬂowers’ (daisy, iris, lilac) and ‘pleasant words’
(love, laughter, pleasure) and a red button for ‘insects’ (ﬂea, spider, mosquito) and ‘unpleasant words’
(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for
‘ﬂowers’ and ‘unpleasant words’ and a green button for ‘insects’ and ‘pleasant words’.

--- Page 128 ---
128 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
Nonetheless it is useful to have intrinsic evaluations. The most common metric
is to test their performance on similarity , computing the correlation between an
algorithm’s word similarity scores and word similarity ratings assigned by humans.
WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example ( plane ,car) had an average score of 5.77.
SimLex-999 (Hill et al., 2015) is a more complex dataset that quantiﬁes similarity
(cup, mug ) rather than relatedness ( cup, coffee ), and includes concrete and abstract
adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each
consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: Levied is closest in meaning to:
imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these
datasets present words without context.
Slightly more realistic are intrinsic similarity tasks that include context. The
Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the
Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer
evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in
their sentential context, while WiC gives target words in two sentential contexts that
are either in the same or different senses; see Appendix G. The semantic textual
similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of
sentence-level similarity algorithms, consisting of a set of pairs of sentences, each
pair with human-labeled similarity scores.
Another task used for evaluation is the analogy task, discussed on page 124,
where the system has to solve problems of the form a is to b as a* is to b* , given a, b,
anda*and having to ﬁnd b*(Turney and Littman, 2005). A number of sets of tuples
have been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova
et al. 2016), covering morphology ( city:cities::child:children ), lexicographic rela-
tions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ),
some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-
gens et al., 2012).
All embedding algorithms suffer from inherent variability. For example because
of randomness in the initialization and the random negative sampling, algorithms
like word2vec may produce different results even from the same dataset, and in-
dividual documents in a collection may strongly impact the resulting embeddings
(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-
beddings are used to study word associations in particular corpora, therefore, it is
best practice to train multiple embeddings with bootstrap sampling over documents
and average the results (Antoniak and Mimno, 2018).
6.13 Summary
• In vector semantics, a word is modeled as a vector—a point in high-dimensional
space, also called an embedding . In this chapter we focus on static embed-
dings , where each word is mapped to a ﬁxed embedding.
• Vector semantic models fall into two classes: sparse anddense . In sparse
models each dimension corresponds to a word in the vocabulary Vand cells
are functions of co-occurrence counts . The term-document matrix has a
row for each word ( term ) in the vocabulary and a column for each document.
Theword-context orterm-term matrix has a row for each (target) word in

--- Page 129 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 129
the vocabulary and a column for each context term in the vocabulary. Two
sparse weightings are common: the tf-idf weighting which weights each cell
by its term frequency andinverse document frequency , and PPMI (point-
wise positive mutual information), which is most common for word-context
matrices.
• Dense vector models have dimensionality 50–1000. Word2vec algorithms
likeskip-gram are a popular way to compute dense embeddings. Skip-gram
trains a logistic regression classiﬁer to compute the probability that two words
are ‘likely to occur nearby in text’. This probability is computed from the dot
product between the embeddings for the two words.
• Skip-gram uses stochastic gradient descent to train the classiﬁer, by learning
embeddings that have a high dot product with embeddings of words that occur
nearby and a low dot product with noise words.
• Other important embedding algorithms include GloVe , a method based on
ratios of word co-occurrence probabilities.
• Whether using sparse or dense vectors, word and document similarities are
computed by some function of the dot product between vectors. The cosine
of two vectors—a normalized dot product—is the most popular such metric.
Bibliographical and Historical Notes
The idea of vector semantics arose out of research in the 1950s in three distinct
ﬁelds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.
The idea that meaning is related to the distribution of words in context was
widespread in linguistic theory of the 1950s, among distributionalists like Zellig
Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos
(1950) put it,
the linguist’s “meaning” of a morpheme. . . is by deﬁnition the set of conditional
probabilities of its occurrence in context with all other morphemes.
The idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like Charles E. Osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like happy/sad orhard/soft . Osgood et al. (1957) proposed that the
meaning of a word in general could be modeled as a point in a multidimensional
Euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.
A ﬁnal intellectual source in the 1950s and early 1960s was the ﬁeld then called
mechanical indexing , now known as information retrieval . In what became knownmechanical
indexing
as the vector space model for information retrieval (Salton 1971, Sparck Jones
1986), researchers demonstrated new ways to deﬁne the meaning of words in terms
of vectors (Switzer, 1965), and reﬁned methods for word similarity based on mea-
sures of statistical association between words like mutual information (Giuliano,
1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents
could be represented in the same vector spaces used for words. Around the same
time, (Cordier, 1965) showed that factor analysis of word association probabilities
could be used to form dense vector representations of words.

--- Page 130 ---
130 CHAPTER 6 • V ECTOR SEMANTICS AND EMBEDDINGS
Some of the philosophical underpinning of the distributional way of thinking
came from the late writings of the philosopher Wittgenstein, who was skeptical of
the possibility of building a completely formal theory of meaning deﬁnitions for
each word. Wittgenstein suggested instead that “the meaning of a word is its use in
the language” (Wittgenstein, 1953, PI 43). That is, instead of using some logical lan-
guage to deﬁne each word, or drawing on denotations or truth values, Wittgenstein’s
idea is that we should deﬁne a word by how it is used by people in speaking and un-
derstanding in their day-to-day interactions, thus preﬁguring the movement toward
embodied and experiential models in linguistics and NLP (Glenberg and Robertson
2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).
More distantly related is the idea of deﬁning words by a vector of discrete fea-
tures, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,
Wierzbicka 1996). By the middle of the 20th century, beginning with the work of
Hjelmslev (Hjelmslev, 1969) (originally 1943) and ﬂeshed out in early models of
generative grammar (Katz and Fodor, 1963), the idea arose of representing mean-
ing with semantic features , symbols that represent some sort of primitive meaning.semantic
feature
For example words like hen,rooster , orchick , have something in common (they all
describe chickens) and something different (their age and sex), representable as:
hen+female, +chicken, +adult
rooster-female, +chicken, +adult
chick+chicken, -adult
The dimensions used by vector models of meaning to deﬁne words, however, are
only abstractly related to this idea of a small ﬁxed number of hand-built dimensions.
Nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speciﬁc compositional aspect of meaning like
these early semantic features.
The use of dense vectors to model word meaning, and indeed the term embed-
ding , grew out of the latent semantic indexing (LSI) model (Deerwester et al.,
1988) recast as LSA (latent semantic analysis ) (Deerwester et al., 1990). In LSA
singular value decomposition —SVD — is applied to a term-document matrix (each SVD
cell weighted by log frequency and normalized by entropy), and then the ﬁrst 300
dimensions are used as the LSA embedding. Singular Value Decomposition (SVD)
is a method for ﬁnding the most important dimensions of a data set, those dimen-
sions along which the data varies the most. LSA was then quickly widely applied:
as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking
(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-
rafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,
Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Juraf-
sky, 2001a), and essay grading (Rehder et al., 1998). Related models were simul-
taneously developed and applied to word sense disambiguation by Sch ¨utze (1992b).
LSA also led to the earliest use of embeddings to represent words in a probabilis-
tic classiﬁer, in the logistic regression document router of Sch ¨utze et al. (1995).
The idea of SVD on the term-term matrix (rather than the term-document matrix)
as a model of meaning for NLP was proposed soon after LSA by Sch ¨utze (1992b).
Sch¨utze applied the low-rank (97-dimensional) embeddings produced by SVD to the
task of word sense disambiguation, analyzed the resulting semantic space, and also
suggested possible techniques like dropping high-order dimensions. See Sch ¨utze
(1997).
A number of alternative matrix models followed on from the early SVD work,
including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent

--- Page 131 ---
EXERCISES 131
Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-
tion (NMF) (Lee and Seung, 1999).
The LSA community seems to have ﬁrst used the word “embedding” in Landauer
et al. (1997), in a variant of its mathematical meaning as a mapping from one space
or mathematical structure to another. In LSA, the word embedding seems to have
described the mapping from the space of sparse count vectors to the latent space of
SVD dense vectors. Although the word thus originally meant the mapping from one
space to another, it has metonymically shifted to mean the resulting dense vector in
the latent space, and it is in this sense that we currently use the word.
By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and
Collobert et al. (2011) then demonstrated that embeddings could be used to represent
word meanings for a number of NLP tasks. Turian et al. (2010) compared the value
of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)
showed that recurrent neural nets could be used as language models. The idea of
simplifying the hidden layer of these neural net language models to create the skip-
gram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The
negative sampling training algorithm was proposed in Mikolov et al. (2013b). There
are numerous surveys of static embeddings and their parameterizations (Bullinaria
and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark
2014, Levy et al. 2015).
See Manning et al. (2008) and Chapter 14 for a deeper understanding of the role
of vectors in information retrieval, including how to compare queries with docu-
ments, more details on tf-idf, and issues of scaling to very large datasets. See Kim
(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful
introductory linguistic text on lexical semantics.
Exercises

--- Page 132 ---
132 CHAPTER 7 • N EURAL NETWORKS
CHAPTER
7Neural Networks
“[M]achines of this character can behave in a very complicated manner when
the number of units is large.”
Alan Turing (1948) “Intelligent Machines”, page 6
Neural networks are a fundamental computational tool for language process-
ing, and a very old one. They are called neural because their origins lie in the
McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the
biological neuron as a kind of computing element that could be described in terms
of propositional logic. But the modern use in language processing no longer draws
on these early biological inspirations.
Instead, a modern neural network is a network of small computing units, each
of which takes a vector of input values and produces a single output value. In this
chapter we introduce the neural net applied to classiﬁcation. The architecture we
introduce is called a feedforward network because the computation proceeds iter- feedforward
atively from one layer of units to the next. The use of modern neural nets is often
called deep learning , because modern networks are often deep (have many layers). deep learning
Neural networks share much of the same mathematics as logistic regression. But
neural networks are a more powerful classiﬁer than logistic regression, and indeed a
minimal neural network (technically one with a single ‘hidden layer’) can be shown
to learn any function.
Neural net classiﬁers are different from logistic regression in another way. With
logistic regression, we applied the regression classiﬁer to many different tasks by
developing many rich kinds of feature templates based on domain knowledge. When
working with neural networks, it is more common to avoid most uses of rich hand-
derived features, instead building neural networks that take raw words as inputs
and learn to induce features as part of the process of learning to classify. We saw
examples of this kind of representation learning for embeddings in Chapter 6. Nets
that are very deep are particularly good at representation learning. For that reason
deep neural nets are the right tool for tasks that offer sufﬁcient data to learn features
automatically.
In this chapter we’ll introduce feedforward networks as classiﬁers, and also ap-
ply them to the simple task of language modeling: assigning probabilities to word
sequences and predicting upcoming words. In subsequent chapters we’ll introduce
many other aspects of neural models, such as recurrent neural networks (Chap-
ter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11).

--- Page 133 ---
7.1 • U NITS 133
7.1 Units
The building block of a neural network is a single computational unit. A unit takes
a set of real valued numbers as input, performs some computation on them, and
produces an output.
At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-
tional term in the sum called a bias term . Given a set of inputs x1:::xn, a unit has bias term
a set of corresponding weights w1:::wnand a bias b, so the weighted sum zcan be
represented as:
z=b+X
iwixi (7.1)
Often it’s more convenient to express this weighted sum using vector notation; recall
from linear algebra that a vector is, at heart, just a list or array of numbers. Thus vector
we’ll talk about zin terms of a weight vector w, a scalar bias b, and an input vector
x, and we’ll replace the sum with the convenient dot product :
z=wx+b (7.2)
As deﬁned in Eq. 7.2, zis just a real valued number.
Finally, instead of using z, a linear function of x, as the output, neural units
apply a non-linear function ftoz. We will refer to the output of this function as
theactivation value for the unit, a. Since we are just modeling a single unit, the activation
activation for the node is in fact the ﬁnal output of the network, which we’ll generally
cally. So the value yis deﬁned as:
y=a=f(z)
We’ll discuss three popular non-linear functions fbelow (the sigmoid, the tanh, and
the rectiﬁed linear unit or ReLU) but it’s pedagogically convenient to start with the
sigmoid function since we saw it in Chapter 5: sigmoid
y=s(z) =1
1+e z(7.3)
The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output
into the range (0;1), which is useful in squashing outliers toward 0 or 1. And it’s
differentiable, which as we saw in Section 5.10 will be handy for learning.
Figure 7.1 The sigmoid function takes a real value and maps it to the range (0;1). It is
nearly linear around 0 but outlier values get squashed toward 0 or 1.
Substituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:
y=s(wx+b) =1
1+exp( (wx+b))(7.4)

--- Page 134 ---
134 CHAPTER 7 • N EURAL NETWORKS
Fig. 7.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit
takes 3 input values x1;x2, and x3, and computes a weighted sum, multiplying each
value by a weight ( w1,w2, and w3, respectively), adds them to a bias term b, and then
passes the resulting sum through a sigmoid function to result in a number between 0
and 1.
x1x2x3
yw1w2w3∑bσ+1za
Figure 7.2 A neural unit, taking 3 inputs x1,x2, and x3(and a bias bthat we represent as a
weight for an input clamped at +1) and producing an output y. We include some convenient
intermediate variables: the output of the summation, z, and the output of the sigmoid, a. In
this case the output of the unit yis the same as a, but in deeper networks we’ll reserve yto
mean the ﬁnal output of the entire network, leaving aas the activation of an individual node.
Let’s walk through an example just to get an intuition. Let’s suppose we have a
unit with the following weight vector and bias:
w= [0:2;0:3;0:9]
b=0:5
What would this unit do with the following input vector:
x= [0:5;0:6;0:1]
The resulting output ywould be:
y=s(wx+b) =1
1+e (wx+b)=1
1+e (:5:2+:6:3+:1:9+:5)=1
1+e 0:87=:70
In practice, the sigmoid is not commonly used as an activation function. A function
that is very similar but almost always better is the tanh function shown in Fig. 7.3a; tanh
tanh is a variant of the sigmoid that ranges from -1 to +1:
y=tanh(z) =ez e z
ez+e z(7.5)
The simplest activation function, and perhaps the most commonly used, is the rec-
tiﬁed linear unit, also called the ReLU , shown in Fig. 7.3b. It’s just the same as z ReLU
when zis positive, and 0 otherwise:
y=ReLU(z) =max(z;0) (7.6)
These activation functions have different properties that make them useful for differ-
ent language applications or network architectures. For example, the tanh function
has the nice properties of being smoothly differentiable and mapping outlier values
toward the mean. The rectiﬁer function, on the other hand, has nice properties that

--- Page 135 ---
7.2 • T HEXOR PROBLEM 135
(a) (b)
Figure 7.3 The tanh and ReLU activation functions.
result from it being very close to linear. In the sigmoid or tanh functions, very high
values of zresult in values of ythat are saturated , i.e., extremely close to 1, and have saturated
derivatives very close to 0. Zero derivatives cause problems for learning, because as
we’ll see in Section 7.5, we’ll train networks by propagating an error signal back-
wards, multiplying gradients (partial derivatives) from each layer of the network;
gradients that are almost 0 cause the error signal to get smaller and smaller until it is
too small to be used for training, a problem called the vanishing gradient problem.vanishing
gradient
Rectiﬁers don’t have this problem, since the derivative of ReLU for high values of z
is 1 rather than very close to 0.
7.2 The XOR problem
Early in the history of neural networks it was realized that the power of neural net-
works, as with the real neurons that inspired them, comes from combining these
units into larger networks.
One of the most clever demonstrations of the need for multi-layer networks was
the proof by Minsky and Papert (1969) that a single neural unit cannot compute
some very simple functions of its input. Consider the task of computing elementary
logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are
the truth tables for those functions:
AND OR XOR
x1 x2y x1 x2 y x1 x2 y
0 00 0 0 0 0 0 0
0 10 0 1 1 0 1 1
1 00 1 0 1 1 0 1
1 11 1 1 1 1 1 0
This example was ﬁrst shown for the perceptron , which is a very simple neural perceptron
unit that has a binary output and does nothave a non-linear activation function. The
output yof a perceptron is 0 or 1, and is computed as follows (using the same weight
w, input x, and bias bas in Eq. 7.2):
y=0;ifwx+b0
1;ifwx+b>0(7.7)

--- Page 136 ---
136 CHAPTER 7 • N EURAL NETWORKS
It’s very easy to build a perceptron that can compute the logical AND and OR
functions of its binary inputs; Fig. 7.4 shows the necessary weights.
x1x2+1-111
x1x2+1011
(a) (b)
Figure 7.4 The weights wand bias bfor perceptrons for computing logical functions. The
inputs are shown as x1andx2and the bias as a special node with value +1 which is multiplied
with the bias weight b. (a) logical AND, with weights w1=1 and w2=1 and bias weight
b= 1. (b) logical OR, with weights w1=1 and w2=1 and bias weight b=0. These
weights/biases are just one from an inﬁnite number of possible sets of weights and biases that
would implement the functions.
It turns out, however, that it’s not possible to build a perceptron to compute
logical XOR! (It’s worth spending a moment to give it a try!)
The intuition behind this important result relies on understanding that a percep-
tron is a linear classiﬁer. For a two-dimensional input x1andx2, the perceptron
equation, w1x1+w2x2+b=0 is the equation of a line. (We can see this by putting
it in the standard linear format: x2= ( w1=w2)x1+ ( b=w2).) This line acts as a
decision boundary in two-dimensional space in which the output 0 is assigned to alldecision
boundary
inputs lying on one side of the line, and the output 1 to all input points lying on the
other side of the line. If we had more than 2 inputs, the decision boundary becomes
a hyperplane instead of a line, but the idea is the same, separating the space into two
categories.
Fig. 7.5 shows the possible logical inputs ( 00,01,10, and11) and the line drawn
by one possible set of parameters for an AND and an OR classiﬁer. Notice that there
is simply no way to draw a line that separates the positive cases of XOR (01 and 10)
from the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly
separable
function. Of course we could draw a boundary with a curve, or some other function,
but not a single line.
7.2.1 The solution: neural networks
While the XOR function cannot be calculated by a single perceptron, it can be cal-
culated by a layered network of perceptron units. Rather than see this with networks
of simple perceptrons, however, let’s see how to compute XOR using two layers of
ReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a ﬁgure with
the input being processed by two layers of neural units. The middle layer (called
h) has two units, and the output layer (called y) has one unit. A set of weights and
biases are shown that allows the network to correctly compute the XOR function.
Let’s walk through what happens with the input x= [0, 0]. If we multiply each
input value by the appropriate weight, sum, and then add the bias b, we get the vector
[0, -1], and we then apply the rectiﬁed linear transformation to give the output of the
hlayer as [0, 0]. Now we once again multiply by the weights, sum, and add the
bias (0 in this case) resulting in the value 0. The reader should work through the
computation of the remaining 3 possible input pairs to see that the resulting yvalues
are 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].

--- Page 137 ---
7.2 • T HEXOR PROBLEM 137
0011x1x20011x1x20011x1x2
a)  x1 AND x2b)  x1 OR x2c)  x1 XOR x2?
Figure 7.5 The functions AND, OR, and XOR, represented with input x1on the x-axis and input x2on the
y-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no
way to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig
(2002).
x1x2h1h2y1+11-1111-201+10
Figure 7.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in
two layers; we’ve called them h1,h2(hfor “hidden layer”) and y1. As before, the numbers
on the arrows represent the weights wfor each unit, and we represent the bias bas a weight
on a unit clamped to +1, with the bias weights/units in gray.
It’s also instructive to look at the intermediate results, the outputs of the two
hidden nodes h1andh2. We showed in the previous paragraph that the hvector for
the inputs x= [0, 0] was [0, 0]. Fig. 7.7b shows the values of the hlayer for all
4 inputs. Notice that hidden representations of the two input points x= [0, 1] and
x= [1, 0] (the two cases with XOR output = 1) are merged to the single point h=
[1, 0]. The merger makes it easy to linearly separate the positive and negative cases
of XOR. In other words, we can view the hidden layer of the network as forming a
representation of the input.
In this example we just stipulated the weights in Fig. 7.6. But for real examples
the weights for neural networks are learned automatically using the error backprop-
agation algorithm to be introduced in Section 7.5. That means the hidden layers will
learn to form useful representations. This intuition, that neural networks can auto-
matically learn useful representations of the input, is one of their key advantages,
and one that we will return to again and again in later chapters.

--- Page 138 ---
138 CHAPTER 7 • N EURAL NETWORKS
0011x1x2
a) The original x space0011h1h2
2b) The new (linearly separable) h space
Figure 7.7 The hidden layer forming a new representation of the input. (b) shows the
representation of the hidden layer, h, compared to the original input representation xin (a).
Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it
possible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.
(2016).
7.3 Feedforward Neural Networks
Let’s now walk through a slightly more formal presentation of the simplest kind of
neural network, the feedforward network . A feedforward network is a multilayerfeedforward
network
network in which the units are connected with no cycles; the outputs from units in
each layer are passed to units in the next higher layer, and no outputs are passed
back to lower layers. (In Chapter 8 we’ll introduce networks with cycles, called
recurrent neural networks .)
For historical reasons multilayer networks, especially feedforward networks, are
sometimes called multi-layer perceptrons (orMLP s); this is a technical misnomer,multi-layer
perceptrons
MLP since the units in modern multilayer networks aren’t perceptrons (perceptrons are
purely linear, but modern networks are made up of units with non-linearities like
sigmoids), but at some point the name stuck.
Simple feedforward networks have three kinds of nodes: input units, hidden
units, and output units.
Fig. 7.8 shows a picture. The input layer xis a vector of simple scalar values just
as we saw in Fig. 7.2.
The core of the neural network is the hidden layer hformed of hidden units hi, hidden layer
each of which is a neural unit as described in Section 7.1, taking a weighted sum of
its inputs and then applying a non-linearity. In the standard architecture, each layer
isfully-connected , meaning that each unit in each layer takes as input the outputs fully-connected
from all the units in the previous layer, and there is a link between every pair of units
from two adjacent layers. Thus each hidden unit sums over all the input units.
Recall that a single hidden unit has as parameters a weight vector and a bias. We
represent the parameters for the entire hidden layer by combining the weight vector
and bias for each unit iinto a single weight matrix Wand a single bias vector bfor
the whole layer (see Fig. 7.8). Each element Wjiof the weight matrix Wrepresents
the weight of the connection from the ith input unit xito the jth hidden unit hj.
The advantage of using a single matrix Wfor the weights of the entire layer is
that now the hidden layer computation for a feedforward network can be done very
efﬁciently with simple matrix operations. In fact, the computation only has three

--- Page 139 ---
7.3 • F EEDFORWARD NEURAL NETWORKS 139
x1x2xn0……+1b…UW
input layerhidden layeroutput layerh1y1y2yn2h2h3hn1
Figure 7.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,
and one input layer (the input layer is usually not counted when enumerating layers).
steps: multiplying the weight matrix by the input vector x, adding the bias vector b,
and applying the activation function g(such as the sigmoid, tanh, or ReLU activation
function deﬁned above).
The output of the hidden layer, the vector h, is thus the following (for this exam-
ple we’ll use the sigmoid function sas our activation function):
h=s(Wx+b) (7.8)
Notice that we’re applying the sfunction here to a vector, while in Eq. 7.3 it was
applied to a scalar. We’re thus allowing s(), and indeed any activation function
g(), to apply to a vector element-wise, so g[z1;z2;z3] = [g(z1);g(z2);g(z3)].
Let’s introduce some constants to represent the dimensionalities of these vectors
and matrices. We’ll refer to the input layer as layer 0 of the network, and have n0
represent the number of inputs, so xis a vector of real numbers of dimension n0,
or more formally x2Rn0, a column vector of dimensionality [n0;1]. Let’s call the
hidden layer layer 1 and the output layer layer 2. The hidden layer has dimensional-
ityn1, soh2Rn1and also b2Rn1(since each hidden unit can take a different bias
value). And the weight matrix Whas dimensionality W2Rn1n0, i.e.[n1;n0].
Take a moment to convince yourself that the matrix multiplication in Eq. 7.8 will
compute the value of each hjass Pn0
i=1Wjixi+bj
.
As we saw in Section 7.2, the resulting value h(forhidden but also for hypoth-
esis) forms a representation of the input. The role of the output layer is to take
this new representation hand compute a ﬁnal output. This output could be a real-
valued number, but in many cases the goal of the network is to make some sort of
classiﬁcation decision, and so we will focus on the case of classiﬁcation.
If we are doing a binary task like sentiment classiﬁcation, we might have a sin-
gle output node, and its scalar value yis the probability of positive versus negative
sentiment. If we are doing multinomial classiﬁcation, such as assigning a part-of-
speech tag, we might have one output node for each potential part-of-speech, whose
output value is the probability of that part-of-speech, and the values of all the output
nodes must sum to one. The output layer is thus a vector ythat gives a probability
distribution across the output nodes.
Let’s see how this happens. Like the hidden layer, the output layer has a weight
matrix (let’s call it U), but some models don’t include a bias vector bin the output

--- Page 140 ---
140 CHAPTER 7 • N EURAL NETWORKS
layer, so we’ll simplify by eliminating the bias vector in this example. The weight
matrix is multiplied by its input vector ( h) to produce the intermediate output z:
z=Uh
There are n2output nodes, so z2Rn2, weight matrix Uhas dimensionality U2
Rn2n1, and element Ui jis the weight from unit jin the hidden layer to unit iin the
output layer.
However, zcan’t be the output of the classiﬁer, since it’s a vector of real-valued
numbers, while what we need for classiﬁcation is a vector of probabilities. There is
a convenient function for normalizing a vector of real values, by which we mean normalizing
converting it to a vector that encodes a probability distribution (all the numbers lie
between 0 and 1 and sum to 1): the softmax function that we saw on page 85 of softmax
Chapter 5. More generally for any vector zof dimensionality d, the softmax is
deﬁned as:
softmax (zi) =exp(zi)Pd
j=1exp(zj)1id (7.9)
Thus for example given a vector
z= [0:6;1:1; 1:5;1:2;3:2; 1:1]; (7.10)
the softmax function will normalize it to a probability distribution (shown rounded):
softmax (z) = [ 0:055;0:090;0:0067;0:10;0:74;0:010] (7.11)
You may recall that we used softmax to create a probability distribution from a
vector of real-valued numbers (computed from summing weights times features) in
the multinomial version of logistic regression in Chapter 5.
That means we can think of a neural network classiﬁer with one hidden layer
as building a vector hwhich is a hidden layer representation of the input, and then
running standard multinomial logistic regression on the features that the network
develops in h. By contrast, in Chapter 5 the features were mainly designed by hand
via feature templates. So a neural network is like multinomial logistic regression,
but (a) with many layers, since a deep neural network is like layer after layer of lo-
gistic regression classiﬁers; (b) with those intermediate layers having many possible
activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll
continue to use sfor convenience to mean any activation function); (c) rather than
forming the features by feature templates, the prior layers of the network induce the
feature representations themselves.
Here are the ﬁnal equations for a feedforward network with a single hidden layer,
which takes an input vector x, outputs a probability distribution y, and is parameter-
ized by weight matrices WandUand a bias vector b:
h=s(Wx+b)
z=Uh
y=softmax (z) (7.12)
And just to remember the shapes of all our variables, x2Rn0,h2Rn1,b2Rn1,
W2Rn1n0,U2Rn2n1, and the output vector y2Rn2. We’ll call this network a 2-
layer network (we traditionally don’t count the input layer when numbering layers,
but do count the output layer). So by this terminology logistic regression is a 1-layer
network.

--- Page 141 ---
7.3 • F EEDFORWARD NEURAL NETWORKS 141
7.3.1 More details on feedforward networks
Let’s now set up some notation to make it easier to talk about deeper networks of
depth more than 2. We’ll use superscripts in square brackets to mean layer num-
bers, starting at 0 for the input layer. So W[1]will mean the weight matrix for the
(ﬁrst) hidden layer, and b[1]will mean the bias vector for the (ﬁrst) hidden layer. nj
will mean the number of units at layer j. We’ll use g()to stand for the activation
function, which will tend to be ReLU or tanh for intermediate layers and softmax
for output layers. We’ll use a[i]to mean the output from layer i, and z[i]to mean the
combination of previous layer output, weights and biases W[i]a[i 1]+b[i]. The 0th
layer is for inputs, so we’ll refer to the inputs xmore generally as a[0].
Thus we can re-represent our 2-layer net from Eq. 7.12 as follows:
z[1]=W[1]a[0]+b[1]
a[1]=g[1](z[1])
z[2]=W[2]a[1]+b[2]
a[2]=g[2](z[2])
ˆy=a[2](7.13)
Note that with this notation, the equations for the computation done at each layer are
the same. The algorithm for computing the forward step in an n-layer feedforward
network, given the input vector a[0]is thus simply:
foriin1,...,n
z[i]=W[i]a[i 1]+b[i]
a[i]=g[i](z[i])
ˆy=a[n]
It’s often useful to have a name for the ﬁnal set of activations right before the ﬁnal
softmax. So however many layers we have, we’ll generally call the unnormalized
values in the ﬁnal vector z[n], the vector of scores right before the ﬁnal softmax, the
logits (see (5.7). logits
The need for non-linear activation functions One of the reasons we use non-
linear activation functions for each layer in a neural network is that if we did not, the
resulting network is exactly equivalent to a single-layer network. Let’s see why this
is true. Imagine the ﬁrst two layers of such a network of purely linear layers:
z[1]=W[1]x+b[1]
z[2]=W[2]z[1]+b[2]
We can rewrite the function that the network is computing as:
z[2]=W[2]z[1]+b[2]
=W[2](W[1]x+b[1])+b[2]
=W[2]W[1]x+W[2]b[1]+b[2]
=W0x+b0(7.14)
This generalizes to any number of layers. So without non-linear activation functions,
a multilayer network is just a notational variant of a single layer network with a
different set of weights, and we lose all the representational power of multilayer
networks.

--- Page 142 ---
142 CHAPTER 7 • N EURAL NETWORKS
Replacing the bias unit In describing networks, we will often use a slightly sim-
pliﬁed notation that represents exactly the same function without referring to an ex-
plicit bias node b. Instead, we add a dummy node a0to each layer whose value will
always be 1. Thus layer 0, the input layer, will have a dummy node a[0]
0=1, layer 1
will have a[1]
0=1, and so on. This dummy node still has an associated weight, and
that weight represents the bias value b. For example instead of an equation like
h=s(Wx+b) (7.15)
we’ll use:
h=s(Wx) (7.16)
But now instead of our vector xhaving n0values: x=x1;:::;xn0, it will have n0+
1 values, with a new 0th dummy value x0=1:x=x0;:::;xn0. And instead of
computing each hjas follows:
hj=s n0X
i=1Wjixi+bj!
; (7.17)
we’ll instead use:
hj=s n0X
i=0Wjixi!
; (7.18)
where the value Wj0replaces what had been bj. Fig. 7.9 shows a visualization.
x1x2xn0……+1b…UWh1y1y2yn2h2h3hn1
x1x2xn0……x0=1…UWh1y1y2yn2h2h3hn1
(a) (b)
Figure 7.9 Replacing the bias node (shown in a) with x0(b).
We’ll continue showing the bias as bwhen we go over the learning algorithm
in Section 7.5, but then we’ll switch to this simpliﬁed notation without explicit bias
terms for the rest of the book.
7.4 Feedforward networks for NLP: Classiﬁcation
Let’s see how to apply feedforward networks to NLP tasks! In this section we’ll
look at classiﬁcation tasks like sentiment analysis; in the next section we’ll introduce
neural language modeling.

--- Page 143 ---
7.4 • F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 143
Let’s begin with a simple 2-layer sentiment classiﬁer. You might imagine taking
our logistic regression classiﬁer from Chapter 5, which corresponds to a 1-layer net-
work, and just adding a hidden layer. The input element xicould be scalar features
like those in Fig. 5.2, e.g., x1= count(words2doc), x2= count(positive lexicon
words2doc), x3= 1 if “no”2doc, and so on. And the output layer ˆycould have
two nodes (one each for positive and negative), or 3 nodes (positive, negative, neu-
tral), in which case ˆy1would be the estimated probability of positive sentiment, ˆy2
the probability of negative and ˆy3the probability of neutral. The resulting equations
would be just what we saw above for a 2-layer network (as always, we’ll continue
to use the sto stand for any non-linearity, whether sigmoid, ReLU or other).
x= [x1;x2;:::xN](each xiis a hand-designed feature)
h=s(Wx+b)
z=Uh
ˆy=softmax (z) (7.19)
Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this
hidden layer to our logistic regression classiﬁer allows the network to represent the
non-linear interactions between features. This alone might give us a better sentiment
classiﬁer.
UW[n⨉1]Hidden layerOutput layersoftmax[dh⨉n][dh⨉1][3⨉dh]Input wordsp(+)h1h2h3hdh…y1^y2^y3^xhyInput layer n=3 features[3⨉1]x1x2x3dessertwasgreatpositive lexiconwords = 1count of “no” = 0wordcount=3p(-)p(neut)
Figure 7.10 Feedforward network sentiment analysis using traditional hand-built features
of the input text.
Most applications of neural networks for NLP do something different, however.
Instead of using hand-built human-engineered features as the input to our classiﬁer,
we draw on deep learning’s ability to learn features from the data by representing
words as embeddings, like the word2vec or GloVe embeddings we saw in Chapter 6.
There are various ways to represent an input for classiﬁcation. One simple baseline
is to apply some sort of pooling function to the embeddings of all the words in the pooling
input. For example, for a text with ninput words/tokens w1;:::;wn, we can turn the
nembeddings e(w1);:::;e(wn)(each of dimensionality d) into a single embedding
also of dimensionality dby just summing the embeddings, or by taking their mean
(summing and then dividing by n):
xmean=1
nnX
i=1e(wi) (7.20)

--- Page 144 ---
144 CHAPTER 7 • N EURAL NETWORKS
There are many other options, like taking the element-wise max. The element-wise
max of a set of nvectors is a new vector whose kth element is the max of the kth
elements of all the nvectors. Here are the equations for this classiﬁer assuming
mean pooling; the architecture is sketched in Fig. 7.11:
x=mean(e(w1);e(w2);:::;e(wn))
h=s(Wx+b)
z=Uh
ˆy=softmax (z) (7.21)
UW[d⨉1]Hidden layerOutput layersoftmax[dh⨉d][dh⨉1][3⨉dh]Input wordsp(+)embedding for“great”embedding for“dessert”h1h2h3hdh…y1^y2^y3^xhyInput layer pooled embedding[3⨉1]pooling+dessertwasgreatembedding for“was”p(-)p(neut)
Figure 7.11 Feedforward network sentiment analysis using a pooled embedding of the in-
put words.
While Eq. 7.21 shows how to classify a single example x, in practice we want
to efﬁciently classify an entire test set of mexamples. We do this by vectorizing
the process, just as we saw with logistic regression; instead of using for-loops to go
through each example, we’ll use matrix multiplication to do the entire computation
of an entire test set at once. First, we pack all the input feature vectors for each input
xinto a single input matrix X, with each row ia row vector consisting of the pooled
embedding for input example x(i)(i.e., the vector x(i)). If the dimensionality of our
pooled input embedding is d,Xwill be a matrix of shape [md].
We will then need to slightly modify Eq. 7.21. Xis of shape [md]andWis of
shape [dhd], so we’ll have to reorder how we multiply XandWand transpose W
so they correctly multiply to yield a matrix Hof shape [mdh].1The bias vector b
from Eq. 7.21 of shape [1dh]will now have to be replicated into a matrix of shape
[mdh]. We’ll need to similarly reorder the next step and transpose U. Finally, our
output matrix ˆYwill be of shape [m3](or more generally [mdo], where dois
the number of output classes), with each row iof our output matrix ˆYconsisting of
the output vector ˆy(i).‘ Here are the ﬁnal equations for computing the output class
1Note that we could have kept the original order of our products if we had instead made our input
matrix Xrepresent each input as a column vector instead of a row vector, making it of shape [dm]. But
representing inputs as row vectors is convenient and common in neural network models.

--- Page 145 ---
7.5 • T RAINING NEURAL NETS 145
distribution for an entire test set:
H=s(XW|+b)
Z=HU|
ˆY=softmax (Z) (7.22)
The idea of using word2vec or GloVe embeddings as our input representation—
and more generally the idea of relying on another algorithm to have already learned
an embedding representation for our input words—is called pretraining . Using pretraining
pretrained embedding representations, whether simple static word embeddings like
word2vec or the much more powerful contextual embeddings we’ll introduce in
Chapter 11, is one of the central ideas of deep learning. (It’s also possible, how-
ever, to train the word embeddings as part of an NLP task; we’ll talk about how to
do this in Section 7.7 in the context of the neural language modeling task.)
7.5 Training Neural Nets
A feedforward neural net is an instance of supervised machine learning in which we
know the correct output yfor each observation x. What the system produces, via
Eq. 7.13, is ˆ y, the system’s estimate of the true y. The goal of the training procedure
is to learn parameters W[i]andb[i]for each layer ithat make ˆ yfor each training
observation as close as possible to the true y.
In general, we do all this by drawing on the methods we introduced in Chapter 5
for logistic regression, so the reader should be comfortable with that chapter before
proceeding.
First, we’ll need a loss function that models the distance between the system
output and the gold output, and it’s common to use the loss function used for logistic
regression, the cross-entropy loss .
Second, to ﬁnd the parameters that minimize this loss function, we’ll use the
gradient descent optimization algorithm introduced in Chapter 5.
Third, gradient descent requires knowing the gradient of the loss function, the
vector that contains the partial derivative of the loss function with respect to each
of the parameters. In logistic regression, for each observation we could directly
compute the derivative of the loss function with respect to an individual worb. But
for neural networks, with millions of parameters in many layers, it’s much harder to
see how to compute the partial derivative of some weight in layer 1 when the loss
is attached to some much later layer. How do we partial out the loss over all those
intermediate layers? The answer is the algorithm called error backpropagation or
backward differentiation .
7.5.1 Loss function
Thecross-entropy loss that is used in neural networks is the same one we saw forcross-entropy
loss
logistic regression. If the neural network is being used as a binary classiﬁer, with
the sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss
we saw in Eq. 5.23:
LCE(ˆy;y) = logp(yjx) = [ylog ˆy+(1 y)log(1 ˆy)] (7.23)
If we are using the network to classify into 3 or more classes, the loss function is
exactly the same as the loss for multinomial regression that we saw in Chapter 5 on

--- Page 146 ---
146 CHAPTER 7 • N EURAL NETWORKS
page 97. Let’s brieﬂy summarize the explanation here for convenience. First, when
we have more than 2 classes we’ll need to represent both yandˆyas vectors. Let’s
assume we’re doing hard classiﬁcation , where only one class is the correct one.
The true label yis then a vector with Kelements, each corresponding to a class,
withyc=1 if the correct class is c, with all other elements of ybeing 0. Recall that
a vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector .
And our classiﬁer will produce an estimate vector with Kelements ˆy, each element
ˆykof which represents the estimated probability p(yk=1jx).
The loss function for a single example xis the negative sum of the logs of the K
output classes, each weighted by their probability yk:
LCE(ˆy;y) = KX
k=1yklogˆyk (7.24)
We can simplify this equation further; let’s ﬁrst rewrite the equation using the func-
tion 1fgwhich evaluates to 1 if the condition in the brackets is true and to 0 oth-
erwise. This makes it more obvious that the terms in the sum in Eq. 7.24 will be 0
except for the term corresponding to the true class for which yk=1:
LCE(ˆy;y) = KX
k=11fyk=1glogˆyk
In other words, the cross-entropy loss is simply the negative log of the output proba-
bility corresponding to the correct class, and we therefore also call this the negative
log likelihood loss :negative log
likelihood loss
LCE(ˆy;y) = logˆyc(where cis the correct class) (7.25)
Plugging in the softmax formula from Eq. 7.9, and with Kthe number of classes:
LCE(ˆy;y) = logexp(zc)PK
j=1exp(zj)(where cis the correct class) (7.26)
7.5.2 Computing the Gradient
How do we compute the gradient of this loss function? Computing the gradient
requires the partial derivative of the loss function with respect to each parameter.
For a network with one weight layer and sigmoid output (which is what logistic
regression is), we could simply use the derivative of the loss that we used for logistic
regression in Eq. 7.27 (and derived in Section 5.10):
¶LCE(ˆy;y)
¶wj= ( ˆy y)xj
= (s(wx+b) y)xj (7.27)
Or for a network with one weight layer and softmax output (=multinomial logistic
regression), we could use the derivative of the softmax loss from Eq. 5.48, shown
for a particular weight wkand input xi
¶LCE(ˆy;y)
¶wk;i= (yk ˆyk)xi
= (yk p(yk=1jx))xi
=  
yk exp(wkx+bk)PK
j=1exp(wjx+bj)!
xi (7.28)

--- Page 147 ---
7.5 • T RAINING NEURAL NETS 147
But these derivatives only give correct updates for one weight layer: the last one!
For deep networks, computing the gradients for each weight is much more complex,
since we are computing the derivative with respect to weight parameters that appear
all the way back in the very early layers of the network, even though the loss is
computed only at the very end of the network.
The solution to computing this gradient is an algorithm called error backprop-
agation orbackprop (Rumelhart et al., 1986). While backprop was invented spe-error back-
propagation
cially for neural networks, it turns out to be the same as a more general procedure
called backward differentiation , which depends on the notion of computation
graphs . Let’s see how that works in the next subsection.
7.5.3 Computation Graphs
A computation graph is a representation of the process of computing a mathematical
expression, in which the computation is broken down into separate operations, each
of which is modeled as a node in a graph.
Consider computing the function L(a;b;c) =c(a+2b). If we make each of the
component addition and multiplication operations explicit, and add names ( dande)
for the intermediate outputs, the resulting series of computations is:
d=2b
e=a+d
L=ce
We can now represent this as a graph, with nodes for each operation, and di-
rected edges showing the outputs from each operation as the inputs to the next, as
in Fig. 7.12. The simplest use of computation graphs is to compute the value of
the function with some given inputs. In the ﬁgure, we’ve assumed the inputs a=3,
b=1,c= 2, and we’ve shown the result of the forward pass to compute the re-
sultL(3;1; 2) = 10. In the forward pass of a computation graph, we apply each
operation left to right, passing the outputs of each computation as the input to the
next node.
e=a+dd = 2bL=cea=3b=1c=-2e=5d=2L=-10forward passabc
Figure 7.12 Computation graph for the function L(a;b;c) =c(a+2b), with values for input
nodes a=3,b=1,c= 2, showing the forward pass computation of L.
7.5.4 Backward differentiation on computation graphs
The importance of the computation graph comes from the backward pass , which
is used to compute the derivatives that we’ll need for the weight update. In this
example our goal is to compute the derivative of the output function Lwith respect

--- Page 148 ---
148 CHAPTER 7 • N EURAL NETWORKS
to each of the input variables, i.e.,¶L
¶a,¶L
¶b, and¶L
¶c. The derivative¶L
¶atells us how
much a small change in aaffects L.
Backwards differentiation makes use of the chain rule in calculus, so let’s re- chain rule
mind ourselves of that. Suppose we are computing the derivative of a composite
function f(x) =u(v(x)). The derivative of f(x)is the derivative of u(x)with respect
tov(x)times the derivative of v(x)with respect to x:
d f
dx=du
dvdv
dx(7.29)
The chain rule extends to more than two functions. If computing the derivative of a
composite function f(x) =u(v(w(x))), the derivative of f(x)is:
d f
dx=du
dvdv
dwdw
dx(7.30)
The intuition of backward differentiation is to pass gradients back from the ﬁnal
node to all the nodes in the graph. Fig. 7.13 shows part of the backward computation
at one node e. Each node takes an upstream gradient that is passed in from its parent
node to the right, and for each of its inputs computes a local gradient (the gradient
of its output with respect to its input), and uses the chain rule to multiply these two
to compute a downstream gradient to be passed on to the next earlier node.
edLed∂L∂d∂L∂e=∂e∂d∂L∂e∂e∂dupstream gradientdownstream gradientlocal gradient
Figure 7.13 Each node (like ehere) takes an upstream gradient, multiplies it by the local
gradient (the gradient of its output with respect to its input), and uses the chain rule to compute
a downstream gradient to be passed on to a prior node. A node may have multiple local
gradients if it has multiple inputs.
Let’s now compute the 3 derivatives we need. Since in the computation graph
L=ce, we can directly compute the derivative¶L
¶c:
¶L
¶c=e (7.31)
For the other two, we’ll need to use the chain rule:
¶L
¶a=¶L
¶e¶e
¶a
¶L
¶b=¶L
¶e¶e
¶d¶d
¶b(7.32)
Eq. 7.32 and Eq. 7.31 thus require ﬁve intermediate derivatives:¶L
¶e,¶L
¶c,¶e
¶a,¶e
¶d, and
¶d
¶b, which are as follows (making use of the fact that the derivative of a sum is the

--- Page 149 ---
7.5 • T RAINING NEURAL NETS 149
sum of the derivatives):
L=ce:¶L
¶e=c;¶L
¶c=e
e=a+d:¶e
¶a=1;¶e
¶d=1
d=2b:¶d
¶b=2
In the backward pass, we compute each of these partials along each edge of the
graph from right to left, using the chain rule just as we did above. Thus we begin by
computing the downstream gradients from node L, which are¶L
¶eand¶L
¶c. For node e,
we then multiply this upstream gradient¶L
¶eby the local gradient (the gradient of the
output with respect to the input),¶e
¶dto get the output we send back to node d:¶L
¶d.
And so on, until we have annotated the graph all the way to all the input variables.
The forward pass conveniently already will have computed the values of the forward
intermediate variables we need (like dande) to compute these derivatives. Fig. 7.14
shows the backward pass.
e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 abc∂L=5∂c∂L=-2∂e∂e=1∂d∂d=2∂b∂e=1∂abackward passc=-2∂L=-2∂e∂L=5∂c∂L∂d=-2∂e∂d∂L∂e=∂L∂a=-2∂e∂a∂L∂e=∂L∂b=-4∂d∂b∂L∂d=
Figure 7.14 Computation graph for the function L(a;b;c) =c(a+2b), showing the backward pass computa-
tion of¶L
¶a,¶L
¶b, and¶L
¶c.
Backward differentiation for a neural network
Of course computation graphs for real neural networks are much more complex.
Fig. 7.15 shows a sample computation graph for a 2-layer neural network with n0=
2,n1=2, and n2=1, assuming binary classiﬁcation and hence using a sigmoid
output unit for simplicity. The function that the computation graph is computing is:
z[1]=W[1]x+b[1]
a[1]=ReLU (z[1])
z[2]=W[2]a[1]+b[2]
a[2]=s(z[2])
ˆy=a[2](7.33)

--- Page 150 ---
150 CHAPTER 7 • N EURAL NETWORKS
For the backward pass we’ll also need to compute the loss L. The loss function
for binary sigmoid output from Eq. 7.23 is
LCE(ˆy;y) = [ylog ˆy+(1 y)log(1 ˆy)] (7.34)
Our output ˆ y=a[2], so we can rephrase this as
LCE(a[2];y) = h
yloga[2]+(1 y)log(1 a[2])i
(7.35)
z[2] = +a[2] = σ a[1] = ReLUz[1] = +b[1]****x1x2a[1] = ReLUz[1] = +b[1]**w[2]11w[1]11w[1]12
w[1]21w[1]22b[2]w[2]12L (a[2],y)1
2111
22
Figure 7.15 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units
and 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning
the function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]
11means
thatw[1]
11is to be multiplied by x1, and the node z[1]= + means that the value of z[1]is computed by summing
the three nodes that feed into it (the two products, and the bias term b[1]
i).
The weights that need updating (those for which we need to know the partial
derivative of the loss function) are shown in teal. In order to do the backward pass,
we’ll need to know the derivatives of all the functions in the graph. We already saw
in Section 5.10 the derivative of the sigmoid s:
ds(z)
dz=s(z)(1 s(z)) (7.36)
We’ll also need the derivatives of each of the other activation functions. The
derivative of tanh is:
dtanh(z)
dz=1 tanh2(z) (7.37)
The derivative of the ReLU is2
dReLU (z)
dz=0f or z<0
1f or z0(7.38)
2The derivative is actually undeﬁned at the point z=0, but by convention we treat it as 1.

--- Page 151 ---
7.5 • T RAINING NEURAL NETS 151
We’ll give the start of the computation, computing the derivative of the loss function
Lwith respect to z, or¶L
¶z(and leaving the rest of the computation as an exercise for
the reader). By the chain rule:
¶L
¶z=¶L
¶a[2]¶a[2]
¶z(7.39)
So let’s ﬁrst compute¶L
¶a[2], taking the derivative of Eq. 7.35, repeated here:
LCE(a[2];y) = h
yloga[2]+(1 y)log(1 a[2])i
¶L
¶a[2]=   
y¶log(a[2])
¶a[2]!
+(1 y)¶log(1 a[2])
¶a[2]!
= 
y1
a[2]
+(1 y)1
1 a[2]( 1)
= y
a[2]+y 1
1 a[2]
(7.40)
Next, by the derivative of the sigmoid:
¶a[2]
¶z=a[2](1 a[2])
Finally, we can use the chain rule:
¶L
¶z=¶L
¶a[2]¶a[2]
¶z
= y
a[2]+y 1
1 a[2]
a[2](1 a[2])
=a[2] y (7.41)
Continuing the backward computation of the gradients (next by passing the gra-
dients over b[2]
1and the two product nodes, and so on, back to all the teal nodes), is
left as an exercise for the reader.
7.5.5 More details on learning
Optimization in neural networks is a non-convex optimization problem, more com-
plex than for logistic regression, and for that and other reasons there are many best
practices for successful learning.
For logistic regression we can initialize gradient descent with all the weights and
biases having the value 0. In neural networks, by contrast, we need to initialize the
weights with small random numbers. It’s also helpful to normalize the input values
to have 0 mean and unit variance.
Various forms of regularization are used to prevent overﬁtting. One of the most
important is dropout : randomly dropping some units and their connections from dropout
the network during training (Hinton et al. 2012, Srivastava et al. 2014). At each
iteration of training (whenever we update parameters, i.e. each mini-batch if we are
using mini-batch gradient descent), we repeatedly choose a probability pand for
each unit we replace its output with zero with probability p(and renormalize the
rest of the outputs from that layer).

--- Page 152 ---
152 CHAPTER 7 • N EURAL NETWORKS
Tuning of hyperparameters is also important. The parameters of a neural net- hyperparameter
work are the weights Wand biases b; those are learned by gradient descent. The
hyperparameters are things that are chosen by the algorithm designer; optimal val-
ues are tuned on a devset rather than by gradient descent learning on the training
set. Hyperparameters include the learning rate h, the mini-batch size, the model
architecture (the number of layers, the number of hidden nodes per layer, the choice
of activation functions), how to regularize, and so on. Gradient descent itself also
has many architectural variants such as Adam (Kingma and Ba, 2015).
Finally, most modern neural networks are built using computation graph for-
malisms that make it easy and natural to do gradient computation and parallelization
on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)
and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested
reader should consult a neural network textbook for further details; some sugges-
tions are at the end of the chapter.
7.6 Feedforward Neural Language Modeling
As our second application of feedforward networks, let’s consider language mod-
eling : predicting upcoming words from prior words. Neural language modeling—
based on the transformer architecture that we will see in Chapter 9—is the algorithm
that underlies all of modern NLP. In this section and the next we’ll introduce a sim-
pler version of neural language models for feedforward networks, an algorithm ﬁrst
introduced by Bengio et al. (2003). The feedforward language model introduces
many of the important concepts of neural language modeling, concepts we’ll return
to as we describe more powerful models in Chapter 8 and Chapter 9.
Neural language models have many advantages over the n-gram language mod-
els of Chapter 3. Compared to n-gram models, neural language models can handle
much longer histories, can generalize better over contexts of similar words, and are
more accurate at word-prediction. On the other hand, neural net language models
are much more complex, are slower and need more energy to train, and are less inter-
pretable than n-gram models, so for some smaller tasks an n-gram language model
is still the right tool.
A feedforward neural language model (LM) is a feedforward network that takes
as input at time ta representation of some number of previous words ( wt 1;wt 2,
etc.) and outputs a probability distribution over possible next words. Thus—like the
n-gram LM—the feedforward neural LM approximates the probability of a word
given the entire prior context P(wtjw1:t 1)by approximating based on the N 1
previous words:
P(wtjw1;:::; wt 1)P(wtjwt N+1;:::; wt 1) (7.42)
In the following examples we’ll use a 4-gram example, so we’ll show a neural net to
estimate the probability P(wt=ijwt 3;wt 2;wt 1).
Neural language models represent words in this prior context by their embed-
dings , rather than just by their word identity as used in n-gram language models.
Using embeddings allows neural language models to generalize better to unseen
data. For example, suppose we’ve seen this sentence in training:
I have to make sure that the cat gets fed.

--- Page 153 ---
7.6 • F EEDFORWARD NEURAL LANGUAGE MODELING 153
but have never seen the words “gets fed” after the word “dog”. Our test set has the
preﬁx “I forgot to make sure that the dog gets”. What’s the next word? An n-gram
language model will predict “fed” after “that the cat gets”, but not after “that the dog
gets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will
be able to generalize from the “cat” context to assign a high enough probability to
“fed” even after seeing “dog”.
7.6.1 Forward inference in the neural language model
Let’s walk through forward inference ordecoding for neural language models.forward
inference
Forward inference is the task, given an input, of running a forward pass on the
network to produce a probability distribution over possible outputs, in this case next
words.
We ﬁrst represent each of the Nprevious words as a one-hot vector of length
jVj, i.e., with one dimension for each word in the vocabulary. A one-hot vector is one-hot vector
a vector that has one element equal to 1—in the dimension corresponding to that
word’s index in the vocabulary— while all the other elements are set to zero. Thus
in a one-hot representation for the word “toothpaste”, supposing it is V5, i.e., index
5 in the vocabulary, x5=1, and xi=08i6=5, as shown here:
[0 0 0 0 1 0 0 ... 0 0 0 0]
1 2 3 4 5 6 7 ... ... |V|
The feedforward neural language model (sketched in Fig. 7.17) has a moving
window that can see N words into the past. We’ll let N equal 3, so the 3 words
wt 1,wt 2, and wt 3are each represented as a one-hot vector. We then multiply
these one-hot vectors by the embedding matrix E. The embedding weight matrix E
has a column for each word, each a column vector of ddimensions, and hence has
dimensionality djVj. Multiplying by a one-hot vector that has only one non-zero
element xi=1 simply selects out the relevant column vector for word i, resulting in
the embedding for word i, as shown in Fig. 7.16.
E|V|d1|V|d1=✕55e5
Figure 7.16 Selecting the embedding vector for word V5by multiplying the embedding
matrix Ewith a one-hot vector with a 1 in index 5.
The 3 resulting embedding vectors are concatenated to produce e, the embedding
layer. This is followed by a hidden layer and an output layer whose softmax produces
a probability distribution over words. For example y42, the value of output node 42,
is the probability of the next word wtbeing V42, the vocabulary word with index 42
(which is the word ‘ﬁsh’ in our example).
Here’s the algorithm in detail for our mini example:
1.Select three embeddings from E : Given the three previous words, we look
up their indices, create 3 one-hot vectors, and then multiply each by the em-
bedding matrix E. Consider wt 3. The one-hot vector for ‘for’ (index 35) is
multiplied by the embedding matrix E, to give the ﬁrst part of the ﬁrst hidden

--- Page 154 ---
154 CHAPTER 7 • N EURAL NETWORKS
UWembedding layer3d⨉1hiddenlayeroutput layersoftmaxdh⨉3ddh⨉1|V|⨉dhinput layerone-hot vectorsE
|V|⨉3d⨉|V|p(do|…)p(aardvark|…)
p(zebra|…)p(fish|…)
|V|⨉1EEh1h2y1
h3hdh……y34
y|V|…001001|V|35
001001|V|451001001|V|9920
0……y42y35102^^^
^^hexyforallthe?thanksand…wt-3wt-2wt-1wt…
Figure 7.17 Forward inference in a feedforward neural language model. At each timestep
tthe network computes a d-dimensional embedding for each context word (by multiplying a
one-hot vector by the embedding matrix E), and concatenates the 3 resulting embeddings to
get the embedding layer e. The embedding vector eis multiplied by a weight matrix Wand
then an activation function is applied element-wise to produce the hidden layer h, which is
then multiplied by another weight matrix U. Finally, a softmax output layer predicts at each
node ithe probability that the next word wtwill be vocabulary word Vi.
layer, the embedding layer . Since each column of the input matrix Eis anembedding
layer
embedding for a word, and the input is a one-hot column vector xifor word
Vi, the embedding layer for input wwill be Exi=ei, the embedding for word
i. We now concatenate the three embeddings for the three context words to
produce the embedding layer e.
2.Multiply by W : We multiply by W(and add b) and pass through the ReLU
(or other) activation function to get the hidden layer h.
3.Multiply by U :his now multiplied by U
4.Apply softmax : After the softmax, each node iin the output layer estimates
the probability P(wt=ijwt 1;wt 2;wt 3)
In summary, the equations for a neural language model with a window size of 3,
given one-hot input vectors for each input context word, are:
e= [Ext 3;Ext 2;Ext 1]
h=s(We+b)
z=Uh
ˆy=softmax (z) (7.43)
Note that we formed the embedding layer eby concatenating the 3 embeddings
for the three context vectors; we’ll often use semicolons to mean concatenation of
vectors.

--- Page 155 ---
7.7 • T RAINING THE NEURAL LANGUAGE MODEL 155
7.7 Training the neural language model
The high-level intuition of training neural language models, whether the simple
feedforward language models we describe here or the more powerful transformer
language models of Chapter 9, is the idea of self-training orself-supervision that self-training
we saw in Chapter 6 for learning word representations. In self-training for language
modeling, we take a corpus of text as training material and at each time step task
the model to predict the next word. At ﬁrst it will do poorly at this task, but since
in each case we know the correct answer (it’s the next word in the corpus!) we can
easily train it to be better at predicting the correct next word. We call such a model
self-supervised because we don’t have to add any special gold labels to the data;
the natural sequence of words is its own supervision! We simply train the model to
minimize the error in predicting the true next word in the training sequence.
In practice, training the model means setting the parameters q=E;W;U;b. For
some tasks, it’s ok to freeze the embedding layer Ewith initial word2vec values. freeze
Freezing means we use word2vec or some other pretraining algorithm to compute
the initial embedding matrix E, and then hold it constant while we only modify W,
U, and b, i.e., we don’t update Eduring language model training. However, often
we’d like to learn the embeddings simultaneously with training the network. This is
useful when the task the network is designed for (like sentiment classiﬁcation, trans-
lation, or parsing) places strong constraints on what makes a good representation for
words.
Let’s see how to train the entire model including E, i.e. to set all the parameters
q=E;W;U;b. We’ll do this via gradient descent (Fig. 5.6), using error backprop-
agation on the computation graph to compute the gradient. Training thus not only
sets the weights WandUof the network, but also as we’re predicting upcoming
words, we’re learning the embeddings Efor each word that best predict upcoming
words.
Fig. 7.18 shows the set up for a window size of N=3 context words. The input x
consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-
ations of the embedding matrix E. We don’t want to learn separate weight matrices
for mapping each of the 3 previous words to the projection layer. We want one single
embedding dictionary Ethat’s shared among these three. That’s because over time,
many different words will appear as wt 2orwt 1, and we’d like to just represent
each word with one vector, whichever context position it appears in. Recall that the
embedding weight matrix Ehas a column for each word, each a column vector of d
dimensions, and hence has dimensionality djVj.
Generally training proceeds by taking as input a very long text, concatenating all
the sentences, starting with random weights, and then iteratively moving through the
text predicting each word wt. At each word wt, we use the cross-entropy (negative
log likelihood) loss. Recall that the general form for this (repeated from Eq. 7.25)
is:
LCE(ˆy;y) = log ˆyi;(where iis the correct class) (7.44)
For language modeling, the classes are the words in the vocabulary, so ˆ yihere means
the probability that the model assigns to the correct next word wt:
LCE= logp(wtjwt 1;:::;wt n+1) (7.45)
The parameter update for stochastic gradient descent for this loss from step stos+1

--- Page 156 ---
156 CHAPTER 7 • N EURAL NETWORKS
UWembedding layer3d⨉1hiddenlayeroutput layersoftmaxdh⨉3ddh⨉1|V|⨉dhinput layerone-hot vectorsE
|V|⨉3d⨉|V|p(do|…)p(aardvark|…)
p(zebra|…)p(fish|…)
|V|⨉1EEh1h2y1
h3hdh……y34
y|V|…001001|V|35
001001|V|451001001|V|9920
0……y42y35102^^^
^^hexyforallthefishthanksand…wt-3wt-2wt-1wt…L = −log P(fish | for, all, the)wt=fish
Figure 7.18 Learning all the way back to embeddings. Again, the embedding matrix Eis
shared among the 3 context words.
is then:
qs+1=qs h¶[ logp(wtjwt 1;:::;wt n+1)]
¶q(7.46)
This gradient can be computed in any standard neural network framework which
will then backpropagate through q=E;W;U;b.
Training the parameters to minimize loss will result both in an algorithm for
language modeling (a word predictor) but also a new set of embeddings Ethat can
be used as word representations for other tasks.
7.8 Summary
• Neural networks are built out of neural units , originally inspired by biological
neurons but now simply an abstract computational device.
• Each neural unit multiplies input values by a weight vector, adds a bias, and
then applies a non-linear activation function like sigmoid, tanh, or rectiﬁed
linear unit.
• In a fully-connected ,feedforward network, each unit in layer iis connected
to each unit in layer i+1, and there are no cycles.
• The power of neural networks comes from the ability of early layers to learn
representations that can be utilized by later layers in the network.
• Neural networks are trained by optimization algorithms like gradient de-
scent .
•Error backpropagation , backward differentiation on a computation graph ,
is used to compute the gradients of the loss function for a network.

--- Page 157 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 157
•Neural language models use a neural network as a probabilistic classiﬁer, to
compute the probability of the next word given the previous nwords.
• Neural language models can use pretrained embeddings , or can learn embed-
dings from scratch in the process of language modeling.
Bibliographical and Historical Notes
The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-
loch and Pitts, 1943), a simpliﬁed model of the biological neuron as a kind of com-
puting element that could be described in terms of propositional logic. By the late
1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and
Bernard Widrow at Stanford) developed research into neural networks; this phase
saw the development of the perceptron (Rosenblatt, 1958), and the transformation
of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).
The ﬁeld of neural networks declined after it was shown that a single perceptron
unit was unable to model functions as simple as XOR (Minsky and Papert, 1969).
While some small amount of work continued during the next two decades, a major
revival for the ﬁeld didn’t come until the 1980s, when practical tools for building
deeper networks like error backpropagation became widespread (Rumelhart et al.,
1986). During the 1980s a wide variety of neural network and related architec-
tures were developed, particularly for applications in psychology and cognitive sci-
ence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart
and McClelland 1986a, Elman 1990), for which the term connectionist orparal- connectionist
lel distributed processing was often used (Feldman and Ballard 1982, Smolensky
1988). Many of the principles and techniques developed in this period are foun-
dational to modern work, including the ideas of distributed representations (Hinton,
1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality
(Smolensky, 1990).
By the 1990s larger neural networks began to be applied to many practical lan-
guage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and
speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements
in computer hardware and advances in optimization and training techniques made it
possible to train even larger and deeper networks, leading to the modern term deep
learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in
Chapter 8 and Chapter 16.
There are a number of excellent books on the subject. Goldberg (2017) has
superb coverage of neural networks for natural language processing. For neural
networks in general see Goodfellow et al. (2016) and Nielsen (2015).

--- Page 158 ---
158 CHAPTER 8 • RNN S AND LSTM S
CHAPTER
8RNNs and LSTMs
Time will explain.
Jane Austen, Persuasion
Language is an inherently temporal phenomenon. Spoken language is a sequence of
acoustic events over time, and we comprehend and produce both spoken and written
language as a sequential input stream. The temporal nature of language is reﬂected
in the metaphors we use; we talk of the ﬂow of conversations ,news feeds , and twitter
streams , all of which emphasize that language is a sequence that unfolds in time.
This temporal nature is reﬂected in some language processing algorithms. For
example, the Viterbi algorithm we introduced for HMM part-of-speech tagging pro-
ceeds through the input a word at a time, carrying forward information gleaned along
the way. But other machine learning approaches, like those we’ve studied for senti-
ment analysis or other text classiﬁcation tasks don’t have this temporal nature – they
assume simultaneous access to all aspects of their input.
The feedforward networks of Chapter 7 also assumed simultaneous access, al-
though they also had a simple model for time. Recall that we applied feedforward
networks to language modeling by having them look only at a ﬁxed-size window
of words, and then sliding this window over the input, making independent predic-
tions along the way. This sliding-window approach is also used in the transformer
architecture we will introduce in Chapter 9.
This chapter introduces a deep learning architecture that offers an alternative
way of representing time: recurrent neural networks (RNNs), and their variants like
LSTMs. RNNs have a mechanism that deals directly with the sequential nature of
language, allowing them to handle the temporal nature of language without the use of
arbitrary ﬁxed-sized windows. The recurrent network offers a new way to represent
the prior context, in its recurrent connections , allowing the model’s decision to
depend on information from hundreds of words in the past. We’ll see how to apply
the model to the task of language modeling, to sequence modeling tasks like part-
of-speech tagging, and to text classiﬁcation tasks like sentiment analysis.
8.1 Recurrent Neural Networks
A recurrent neural network (RNN) is any network that contains a cycle within its
network connections, meaning that the value of some unit is directly, or indirectly,
dependent on its own earlier outputs as an input. While powerful, such networks
are difﬁcult to reason about and to train. However, within the general class of recur-
rent networks there are constrained architectures that have proven to be extremely
effective when applied to language. In this section, we consider a class of recurrent
networks referred to as Elman Networks (Elman, 1990) or simple recurrent net-Elman
Networks

--- Page 159 ---
8.1 • R ECURRENT NEURAL NETWORKS 159
works . These networks are useful in their own right and serve as the basis for more
complex approaches like the Long Short-Term Memory (LSTM) networks discussed
later in this chapter. In this chapter when we use the term RNN we’ll be referring to
these simpler more constrained networks (although you will often see the term RNN
to mean any net with recurrent properties including LSTMs).
xthtyt
Figure 8.1 Simple recurrent neural network after Elman (1990). The hidden layer includes
a recurrent connection as part of its input. That is, the activation value of the hidden layer
depends on the current input as well as the activation value of the hidden layer from the
previous time step.
Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward net-
works, an input vector representing the current input, xt, is multiplied by a weight
matrix and then passed through a non-linear activation function to compute the val-
ues for a layer of hidden units. This hidden layer is then used to calculate a cor-
responding output, yt. In a departure from our earlier window-based approach, se-
quences are processed by presenting one item at a time to the network. We’ll use
subscripts to represent time, thus xtwill mean the input vector xat time t. The key
difference from a feedforward network lies in the recurrent link shown in the ﬁgure
with the dashed line. This link augments the input to the computation at the hidden
layer with the value of the hidden layer from the preceding point in time .
The hidden layer from the previous time step provides a form of memory, or
context, that encodes earlier processing and informs the decisions to be made at
later points in time. Critically, this approach does not impose a ﬁxed-length limit
on this prior context; the context embodied in the previous hidden layer can include
information extending back to the beginning of the sequence.
Adding this temporal dimension makes RNNs appear to be more complex than
non-recurrent architectures. But in reality, they’re not all that different. Given an
input vector and the values for the hidden layer from the previous time step, we’re
still performing the standard feedforward calculation introduced in Chapter 7. To
see this, consider Fig. 8.2 which clariﬁes the nature of the recurrence and how it
factors into the computation at the hidden layer. The most signiﬁcant change lies in
the new set of weights, U, that connect the hidden layer from the previous time step
to the current hidden layer. These weights determine how the network makes use of
past context in calculating the output for the current input. As with the other weights
in the network, these connections are trained via backpropagation.
8.1.1 Inference in RNNs
Forward inference (mapping a sequence of inputs to a sequence of outputs) in an
RNN is nearly identical to what we’ve already seen with feedforward networks. To
compute an output ytfor an input xt, we need the activation value for the hidden
layer ht. To calculate this, we multiply the input xtwith the weight matrix W, and
the hidden layer from the previous time step ht 1with the weight matrix U. We
add these values together and pass them through a suitable activation function, g,
to arrive at the activation value for the current hidden layer, ht. Once we have the

--- Page 160 ---
160 CHAPTER 8 • RNN S AND LSTM S
+UVWyt
xththt-1
Figure 8.2 Simple recurrent neural network illustrated as a feedforward network. The hid-
den layer ht 1from the prior time step is multiplied by weight matrix Uand then added to
the feedforward component from the current time step.
values for the hidden layer, we proceed with the usual computation to generate the
output vector.
ht=g(Uht 1+Wx t) (8.1)
yt=f(Vht) (8.2)
Let’s refer to the input, hidden and output layer dimensions as din,dh, and dout
respectively. Given this, our three parameter matrices are: W2Rdhdin,U2Rdhdh,
andV2Rdoutdh.
We compute ytvia a softmax computation that gives a probability distribution
over the possible output classes.
yt=softmax (Vht) (8.3)
The fact that the computation at time trequires the value of the hidden layer from
time t 1 mandates an incremental inference algorithm that proceeds from the start
of the sequence to the end as illustrated in Fig. 8.3. The sequential nature of simple
recurrent networks can also be seen by unrolling the network in time as is shown in
Fig. 8.4. In this ﬁgure, the various layers of units are copied for each time step to
illustrate that they will have differing values over time. However, the various weight
matrices are shared across time.
function FORWARD RNN( x,network )returns output sequence y
h0 0
fori 1toLENGTH (x)do
hi g(Uhi 1+Wx i)
yi f(Vhi)
return y
Figure 8.3 Forward inference in a simple recurrent network. The matrices U,VandWare
shared across time, while new values for handyare calculated with each time step.
8.1.2 Training
As with feedforward networks, we’ll use a training set, a loss function, and back-
propagation to obtain the gradients needed to adjust the weights in these recurrent

--- Page 161 ---
8.1 • R ECURRENT NEURAL NETWORKS 161
UVWUVWUVW
x1x2x3y1y2y3
h1h3h2
h0
Figure 8.4 A simple recurrent neural network shown unrolled in time. Network layers are recalculated for
each time step, while the weights U,VandWare shared across all time steps.
networks. As shown in Fig. 8.2, we now have 3 sets of weights to update: W, the
weights from the input layer to the hidden layer, U, the weights from the previous
hidden layer to the current hidden layer, and ﬁnally V, the weights from the hidden
layer to the output layer.
Fig. 8.4 highlights two considerations that we didn’t have to worry about with
backpropagation in feedforward networks. First, to compute the loss function for
the output at time twe need the hidden layer from time t 1. Second, the hidden
layer at time tinﬂuences both the output at time tand the hidden layer at time t+1
(and hence the output and loss at t+1). It follows from this that to assess the error
accruing to ht, we’ll need to know its inﬂuence on both the current output as well as
the ones that follow .
Tailoring the backpropagation algorithm to this situation leads to a two-pass al-
gorithm for training the weights in RNNs. In the ﬁrst pass, we perform forward
inference, computing ht,yt, accumulating the loss at each step in time, saving the
value of the hidden layer at each step for use at the next time step. In the second
phase, we process the sequence in reverse, computing the required gradients as we
go, computing and saving the error term for use in the hidden layer for each step
backward in time. This general approach is commonly referred to as backpropaga-
tion through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).backpropaga-
tion through
timeFortunately, with modern computational frameworks and adequate computing
resources, there is no need for a specialized approach to training RNNs. As illus-
trated in Fig. 8.4, explicitly unrolling a recurrent network into a feedforward com-
putational graph eliminates any explicit recurrences, allowing the network weights
to be trained directly. In such an approach, we provide a template that speciﬁes the
basic structure of the network, including all the necessary parameters for the input,
output, and hidden layers, the weight matrices, as well as the activation and output
functions to be used. Then, when presented with a speciﬁc input sequence, we can
generate an unrolled feedforward network speciﬁc to that input, and use that graph

--- Page 162 ---
162 CHAPTER 8 • RNN S AND LSTM S
to perform forward inference or training via ordinary backpropagation.
For applications that involve much longer input sequences, such as speech recog-
nition, character-level processing, or streaming continuous inputs, unrolling an en-
tire input sequence may not be feasible. In these cases, we can unroll the input into
manageable ﬁxed-length segments and treat each segment as a distinct training item.
8.2 RNNs as Language Models
Let’s see how to apply RNNs to the language modeling task. Recall from Chapter 3
that language models predict the next word in a sequence given some preceding
context. For example, if the preceding context is “Thanks for all the” and we want
to know how likely the next word is “ﬁsh” we would compute:
P(ﬁshjThanks for all the )
Language models give us the ability to assign such a conditional probability to every
possible next word, giving us a distribution over the entire vocabulary. We can also
assign probabilities to entire sequences by combining these conditional probabilities
with the chain rule:
P(w1:n) =nY
i=1P(wijw<i)
The n-gram language models of Chapter 3 compute the probability of a word given
counts of its occurrence with the n 1 prior words. The context is thus of size n 1.
For the feedforward language models of Chapter 7, the context is the window size.
RNN language models (Mikolov et al., 2010) process the input sequence one
word at a time, attempting to predict the next word from the current word and the
previous hidden state. RNNs thus don’t have the limited context problem that n-gram
models have, or the ﬁxed context that feedforward language models have, since the
hidden state can in principle represent information about all of the preceding words
all the way back to the beginning of the sequence. Fig. 8.5 sketches this difference
between a FFN language model and an RNN language model, showing that the
RNN language model uses ht 1, the hidden state from the previous time step, as a
representation of the past context.
8.2.1 Forward Inference in an RNN language model
Forward inference in a recurrent language model proceeds exactly as described in
Section 8.1.1. The input sequence X= [x1;:::;xt;:::;xN]consists of a series of words
each represented as a one-hot vector of size jVj1, and the output prediction, y, is a
vector representing a probability distribution over the vocabulary. At each step, the
model uses the word embedding matrix Eto retrieve the embedding for the current
word, multiples it by the weight matrix W, and then adds it to the hidden layer from
the previous step (weighted by weight matrix Uto compute a new hidden layer.
This hidden layer is then used to generate an output layer which is passed through a
softmax layer to generate a probability distribution over the entire vocabulary. That

--- Page 163 ---
8.2 • RNN S AS LANGUAGE MODELS 163
VWethtUht-1ethtet-1et-2U       Wa)b)^yt
et-1^ytht-1WWet-2U
Figure 8.5 Simpliﬁed sketch of two LM architectures moving through a text, showing a
schematic context of three tokens: (a) a feedforward neural language model which has a ﬁxed
context input to the weight matrix W, (b) an RNN language model, in which the hidden state
ht 1summarizes the prior context.
is, at time t:
et=Ext (8.4)
ht=g(Uht 1+We t) (8.5)
^ yt=softmax (Vht) (8.6)
When we do language modeling with RNNs (and we’ll see this again in Chapter 9
with transformers), it’s convenient to make the assumption that the embedding di-
mension deand the hidden dimension dhare the same. So we’ll just call both of
these the model dimension d. So the embedding matrix Eis of shape [djVj], and
xtis a one-hot vector of shape [jVj1]. The product etis thus of shape [d1].W
andUare of shape [dd], sohtis also of shape [d1].Vis of shape [jVjd],
so the result of Vhis a vector of shape [jVj1]. This vector can be thought of as
a set of scores over the vocabulary given the evidence provided in h. Passing these
scores through the softmax normalizes the scores into a probability distribution. The
probability that a particular word kin the vocabulary is the next word is represented
by^ yt[k], the kth component of ^ yt:
P(wt+1=kjw1;:::; wt) = ^ yt[k] (8.7)
The probability of an entire sequence is just the product of the probabilities of each
item in the sequence, where we’ll use ^ yi[wi]to mean the probability of the true word
wiat time step i.
P(w1:n) =nY
i=1P(wijw1:i 1) (8.8)
=nY
i=1^ yi[wi] (8.9)
8.2.2 Training an RNN language model
To train an RNN as a language model, we use the same self-supervision (orself- self-supervision
training ) algorithm we saw in Section 7.7: we take a corpus of text as training

--- Page 164 ---
164 CHAPTER 8 • RNN S AND LSTM S
InputEmbeddingsSoftmax overVocabulary
SolongandthanksforlongandthanksforNext wordall…Loss……RNNhyVh
<latexit sha1_base64="9tru+5ysH1zS9iUXRg/IsnxmpMA=">AAAB/XicbVDLSsNAFL3xWesr6lKQwSK4sSQi1WXRjcsK9gFNCZPpJB06yYSZiRBCcOOvuBFxo+Av+Av+jUnbTVsPDBzOOcO993gxZ0pb1q+xsrq2vrFZ2apu7+zu7ZsHhx0lEklomwguZM/DinIW0bZmmtNeLCkOPU673viu9LtPVComokedxnQQ4iBiPiNYF5Jrnlw4XATIGWGdpbmbOSHWIxlmXERBnldds2bVrQnQMrFnpAYztFzzxxkKkoQ00oRjpfq2FetBhqVmhNO86iSKxpiMcUCzyfY5OiukIfKFLF6k0USdy+FQqTT0imS5nFr0SvE/r59o/2aQsShONI3IdJCfcKQFKqtAQyYp0TwtCCaSFRsiMsISE10UVp5uLx66TDqXdbtRbzxc1Zq3sxIqcAyncA42XEMT7qEFbSDwAm/wCV/Gs/FqvBsf0+iKMftzBHMwvv8ADJKVcA==</latexit> log ˆylong<latexit sha1_base64="tuzkS/BeX/Xmg79qpWZlpeYDhtE=">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPJpB06mYSZiRBC3PgrbkTcKPgN/oJ/Y9J209YDA4dzznDvPV7MmdKW9WusrK6tb2xWtqrbO7t7++bBYUdFiSS0TSIeyZ6HFeVM0LZmmtNeLCkOPU673viu9LtPVCoWiUedxnQQ4qFgASNYF5Jrnlw4PBoiZ4R1luZu5oRYj2SYYeHnedU1a1bdmgAtE3tGajBDyzV/HD8iSUiFJhwr1betWA8yLDUjnOZVJ1E0xmSMhzSbLJ+js0LyURDJ4gmNJupcDodKpaFXJMvd1KJXiv95/UQHN4OMiTjRVJDpoCDhSEeobAL5TFKieVoQTCQrNkRkhCUmuuirPN1ePHSZdC7rdqPeeLiqNW9nJVTgGE7hHGy4hibcQwvaQOAF3uATvoxn49V4Nz6m0RVj9ucI5mB8/wEiupTp</latexit> log ˆyand<latexit sha1_base64="0zdsmbBovZ+hafWZN7Hvufo85tU=">AAAB/3icbVDLSsNAFJ3UV62vqEs3g0VwY0lEqsuiG5cV7AOaEibTSTN0kgkzN0IIWbjxV9yIuFHwD/wF/8ak7aatBwYO55zh3nu8WHANlvVrVNbWNza3qtu1nd29/QPz8KirZaIo61AppOp7RDPBI9YBDoL1Y8VI6AnW8yZ3pd97YkpzGT1CGrNhSMYR9zklUEiuiS8cIcfYCQhkae5mTkggUGEGAYkmOs9rrlm3GtYUeJXYc1JHc7Rd88cZSZqELAIqiNYD24phmBEFnAqW15xEs5jQCRmzbLp/js8KaYR9qYoXAZ6qCzkSap2GXpEs19PLXin+5w0S8G+GGY/iBFhEZ4P8RGCQuCwDj7hiFERaEEIVLzbENCCKUCgqK0+3lw9dJd3Lht1sNB+u6q3beQlVdIJO0Tmy0TVqoXvURh1E0Qt6Q5/oy3g2Xo1342MWrRjzP8doAcb3H7Aall0=</latexit> log ˆythanks<latexit sha1_base64="D3c31Jvxp3QWPr2h4tzQWmeenDs=">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkvpBZVnXNmlW3pkDLxC5JDUq0XPPHGQoSBzTUhGOl+rYV6UGKpWaE06zqxIpGmEzwiKbT5TN0lktDlM/LX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvP0CJlP0=</latexit> log ˆyfor<latexit sha1_base64="PI3y1fb9LhumoVCQRh2+Y84dRkc=">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkmPMsq7pmzapbU6BlYpekBiVarvnjDAWJAxpqwrFSfduK9CDFUjPCaVZ1YkUjTCZ4RNPp8hk6y6Uh8oXMX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvPyumlO8=</latexit> log ˆyall
e
Figure 8.6 Training RNNs as language models.
material and at each time step task the model to predict the next word. We call
such a model self-supervised because we don’t have to add any special gold labels
to the data; the natural sequence of words is its own supervision! We simply train
the model to minimize the error in predicting the true next word in the training
sequence, using cross-entropy as the loss function. Recall that the cross-entropy
loss measures the difference between a predicted probability distribution and the
correct distribution.
LCE= X
w2Vyt[w]logˆyt[w] (8.10)
In the case of language modeling, the correct distribution ytcomes from knowing the
next word. This is represented as a one-hot vector corresponding to the vocabulary
where the entry for the actual next word is 1, and all the other entries are 0. Thus,
the cross-entropy loss for language modeling is determined by the probability the
model assigns to the correct next word. So at time tthe CE loss is the negative log
probability the model assigns to the next word in the training sequence.
LCE(ˆyt;yt) = logˆyt[wt+1] (8.11)
Thus at each word position tof the input, the model takes as input the the correct
word wttogether with ht 1, encoding information from the preceding w1:t 1, and
uses them to compute a probability distribution over possible next words so as to
compute the model’s loss for the next token wt+1. Then we move to the next word,
we ignore what the model predicted for the next word and instead use the correct
word wt+1along with the prior history encoded to estimate the probability of token
wt+2. This idea that we always give the model the correct history sequence to predict
the next word (rather than feeding the model its best case from the previous time
step) is called teacher forcing . teacher forcing
The weights in the network are adjusted to minimize the average CE loss over
the training sequence via gradient descent. Fig. 8.6 illustrates this training regimen.
8.2.3 Weight Tying
Careful readers may have noticed that the input embedding matrix Eand the ﬁnal
layer matrix V, which feeds the output softmax, are quite similar.

--- Page 165 ---
8.3 • RNN S FOR OTHER NLP TASKS 165
The columns of Erepresent the word embeddings for each word in the vocab-
ulary learned during the training process with the goal that words that have similar
meaning and function will have similar embeddings. And, since when we use RNNs
for language modeling we make the assumption that the embedding dimension and
the hidden dimension are the same (= the model dimension d), the embedding ma-
trixEhas shape [djVj]. And the ﬁnal layer matrix Vprovides a way to score
the likelihood of each word in the vocabulary given the evidence present in the ﬁnal
hidden layer of the network through the calculation of Vh.Vis of shape [jVjd].
That is, is, the rows of Vare shaped like a transpose of E, meaning that Vprovides
asecond set of learned word embeddings.
Instead of having two sets of embedding matrices, language models use a single
embedding matrix, which appears at both the input and softmax layers. That is,
we dispense with Vand use Eat the start of the computation and E|(because the
shape of Vis the transpose of Eat the end. Using the same matrix (transposed) in
two places is called weight tying .1The weight-tied equations for an RNN language weight tying
model then become:
et=Ext (8.12)
ht=g(Uht 1+We t) (8.13)
^ yt=softmax (E|ht) (8.14)
In addition to providing improved model perplexity, this approach signiﬁcantly re-
duces the number of parameters required for the model.
8.3 RNNs for other NLP tasks
Now that we’ve seen the basic RNN architecture, let’s consider how to apply it to
three types of NLP tasks: sequence classiﬁcation tasks like sentiment analysis and
topic classiﬁcation, sequence labeling tasks like part-of-speech tagging, and text
generation tasks, including with a new architecture called the encoder-decoder .
8.3.1 Sequence Labeling
In sequence labeling, the network’s task is to assign a label chosen from a small
ﬁxed set of labels to each element of a sequence, like the part-of-speech tagging and
named entity recognition tasks from Chapter 17. In an RNN approach to sequence
labeling, inputs are word embeddings and the outputs are tag probabilities generated
by a softmax layer over the given tagset, as illustrated in Fig. 8.7.
In this ﬁgure, the inputs at each time step are pretrained word embeddings cor-
responding to the input tokens. The RNN block is an abstraction that represents
an unrolled simple recurrent network consisting of an input layer, hidden layer, and
output layer at each time step, as well as the shared U,VandWweight matrices
that comprise the network. The outputs of the network at each time step represent
the distribution over the POS tagset generated by a softmax layer.
To generate a sequence of tags for a given input, we run forward inference over
the input sequence and select the most likely tag from the softmax at each step. Since
we’re using a softmax layer to generate the probability distribution over the output
tagset at each time step, we will again employ the cross-entropy loss during training.
1We also do this for transformers (Chapter 9) where it’s common to call E|theunembedding matrix .

--- Page 166 ---
166 CHAPTER 8 • RNN S AND LSTM S
JanetwillbackthebillNNDTVBMDNNPArgmax
EmbeddingsWordsehVhyRNNLayer(s)Softmax overtags
Figure 8.7 Part-of-speech tagging as sequence labeling with a simple RNN. Pre-trained
word embeddings serve as inputs and a softmax layer provides a probability distribution over
the part-of-speech tags as output at each time step.
8.3.2 RNNs for Sequence Classiﬁcation
Another use of RNNs is to classify entire sequences rather than the tokens within
them. This is the set of tasks commonly called text classiﬁcation , like sentiment
analysis or spam detection, in which we classify a text into two or three classes
(like positive or negative), as well as classiﬁcation tasks with a large number of
categories, like document-level topic classiﬁcation, or message routing for customer
service applications.
To apply RNNs in this setting, we pass the text to be classiﬁed through the RNN
a word at a time generating a new hidden layer representation at each time step.
We can then take the hidden layer for the last token of the text, hn, to constitute a
compressed representation of the entire sequence. We can pass this representation
hnto a feedforward network that chooses a class via a softmax over the possible
classes. Fig. 8.8 illustrates this approach.
x1
RNNhnx2x3xnSoftmaxFFN
Figure 8.8 Sequence classiﬁcation using a simple RNN combined with a feedforward net-
work. The ﬁnal hidden state from the RNN is used as the input to a feedforward network that
performs the classiﬁcation.
Note that in this approach we don’t need intermediate outputs for the words in
the sequence preceding the last element. Therefore, there are no loss terms associ-

--- Page 167 ---
8.3 • RNN S FOR OTHER NLP TASKS 167
ated with those elements. Instead, the loss function used to train the weights in the
network is based entirely on the ﬁnal text classiﬁcation task. The output from the
softmax output from the feedforward classiﬁer together with a cross-entropy loss
drives the training. The error signal from the classiﬁcation is backpropagated all the
way through the weights in the feedforward classiﬁer through, to its input, and then
through to the three sets of weights in the RNN as described earlier in Section 8.1.2.
The training regimen that uses the loss from a downstream application to adjust the
weights all the way through the network is referred to as end-to-end training .end-to-end
training
Another option, instead of using just hidden state of the last token hnto represent
the whole sequence, is to use some sort of pooling function of all the hidden states pooling
hifor each word iin the sequence. For example, we can create a representation that
pools all the nhidden states by taking their element-wise mean:
hmean=1
nnX
i=1hi (8.15)
Or we can take the element-wise max; the element-wise max of a set of nvectors is
a new vector whose kth element is the max of the kth elements of all the nvectors.
The long contexts of RNNs makes it quite difﬁcult to successfully backpropagate
error all the way through the entire input; we’ll talk about this problem, and some
standard solutions, in Section 8.5.
8.3.3 Generation with RNN-Based Language Models
RNN-based language models can also be used to generate text. Text generation is
of enormous practical importance, part of tasks like question answering, machine
translation, text summarization, grammar correction, story generation, and conver-
sational dialogue; any task where a system needs to produce text, conditioned on
some other text. This use of a language model to generate text is one of the areas
in which the impact of neural language models on NLP has been the largest. Text
generation, along with image generation and code generation, constitute a new area
of AI that is often called generative AI .
Recall back in Chapter 3 we saw how to generate text from an n-gram language
model by adapting a sampling technique suggested at about the same time by Claude
Shannon (Shannon, 1951) and the psychologists George Miller and Jennifer Self-
ridge (Miller and Selfridge, 1950). We ﬁrst randomly sample a word to begin a
sequence based on its suitability as the start of a sequence. We then continue to
sample words conditioned on our previous choices until we reach a pre-determined
length, or an end of sequence token is generated.
Today, this approach of using a language model to incrementally generate words
by repeatedly sampling the next word conditioned on our previous choices is called
autoregressive generation orcausal LM generation . The procedure is basicallyautoregressive
generation
the same as that described on page 43, but adapted to a neural context:
• Sample a word in the output from the softmax distribution that results from
using the beginning of sentence marker, <s>, as the ﬁrst input.
• Use the word embedding for that ﬁrst word as the input to the network at the
next time step, and then sample the next word in the same fashion.
• Continue generating until the end of sentence marker, </s> , is sampled or a
ﬁxed length limit is reached.
Technically an autoregressive model is a model that predicts a value at time tbased
on a linear function of the previous values at times t 1,t 2, and so on. Although

--- Page 168 ---
168 CHAPTER 8 • RNN S AND LSTM S
language models are not linear (since they have many layers of non-linearities), we
loosely refer to this generation technique as autoregressive generation since the
word generated at each time step is conditioned on the word selected by the network
from the previous step. Fig. 8.9 illustrates this approach. In this ﬁgure, the details of
the RNN’s hidden layers and recurrent connections are hidden within the blue block.
This simple architecture underlies state-of-the-art approaches to applications
such as machine translation, summarization, and question answering. The key to
these approaches is to prime the generation component with an appropriate context.
That is, instead of simply using <s> to get things started we can provide a richer
task-appropriate context; for translation the context is the sentence in the source
language; for summarization it’s the long text we want to summarize.
Solong
<s>and
Solongand?Sampled WordSoftmaxEmbeddingInput WordRNN
Figure 8.9 Autoregressive generation with an RNN-based neural language model.
8.4 Stacked and Bidirectional RNN architectures
Recurrent networks are quite ﬂexible. By combining the feedforward nature of un-
rolled computational graphs with vectors as common inputs and outputs, complex
networks can be treated as modules that can be combined in creative ways. This
section introduces two of the more common network architectures used in language
processing with RNNs.
8.4.1 Stacked RNNs
In our examples thus far, the inputs to our RNNs have consisted of sequences of
word or character embeddings (vectors) and the outputs have been vectors useful for
predicting words, tags or sequence labels. However, nothing prevents us from using
the entire sequence of outputs from one RNN as an input sequence to another one.
Stacked RNNs consist of multiple networks where the output of one layer serves as Stacked RNNs
the input to a subsequent layer, as shown in Fig. 8.10.
Stacked RNNs generally outperform single-layer networks. One reason for this
success seems to be that the network induces representations at differing levels of
abstraction across layers. Just as the early stages of the human visual system detect

--- Page 169 ---
8.4 • S TACKED AND BIDIRECTIONAL RNN ARCHITECTURES 169
y1y2y3yn
x1x2x3xn
RNN 1
RNN 2
 RNN 3
Figure 8.10 Stacked recurrent networks. The output of a lower level serves as the input to
higher levels with the output of the last network serving as the ﬁnal output.
edges that are then used for ﬁnding larger regions and shapes, the initial layers of
stacked networks can induce representations that serve as useful abstractions for
further layers—representations that might prove difﬁcult to induce in a single RNN.
The optimal number of stacked RNNs is speciﬁc to each application and to each
training set. However, as the number of stacks is increased the training costs rise
quickly.
8.4.2 Bidirectional RNNs
The RNN uses information from the left (prior) context to make its predictions at
time t. But in many applications we have access to the entire input sequence; in
those cases we would like to use words from the context to the right of t. One way
to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and
concatenate their representations.
In the left-to-right RNNs we’ve discussed so far, the hidden state at a given time
trepresents everything the network knows about the sequence up to that point. The
state is a function of the inputs x1;:::;xtand represents the context of the network to
the left of the current time.
hf
t=RNN forward (x1;:::;xt) (8.16)
This new notation hf
tsimply corresponds to the normal hidden state at time t, repre-
senting everything the network has gleaned from the sequence so far.
To take advantage of context to the right of the current input, we can train an
RNN on a reversed input sequence. With this approach, the hidden state at time t
represents information about the sequence to the right of the current input:
hb
t=RNN backward (xt;:::xn) (8.17)
Here, the hidden state hb
trepresents all the information we have discerned about the
sequence from tto the end of the sequence.
Abidirectional RNN (Schuster and Paliwal, 1997) combines two independentbidirectional
RNN
RNNs, one where the input is processed from the start to the end, and the other from
the end to the start. We then concatenate the two representations computed by the
networks into a single vector that captures both the left and right contexts of an input

--- Page 170 ---
170 CHAPTER 8 • RNN S AND LSTM S
at each point in time. Here we use either the semicolon ”;” or the equivalent symbol
to mean vector concatenation:
ht= [hf
t;hb
t]
=hf
thb
t (8.18)
Fig. 8.11 illustrates such a bidirectional network that concatenates the outputs of
the forward and backward pass. Other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. The output at
each step in time thus captures information to the left and to the right of the current
input. In sequence labeling applications, these concatenated outputs can serve as the
basis for a local labeling decision.
RNN 2 
RNN 1x1y2y1y3ynconcatenatedoutputs
x2x3xn
Figure 8.11 A bidirectional RNN. Separate models are trained in the forward and backward
directions, with the output of each model at each time point concatenated to represent the
bidirectional state at that time point.
Bidirectional RNNs have also proven to be quite effective for sequence classiﬁ-
cation. Recall from Fig. 8.8 that for sequence classiﬁcation we used the ﬁnal hidden
state of the RNN as the input to a subsequent feedforward classiﬁer. A difﬁculty
with this approach is that the ﬁnal state naturally reﬂects more information about
the end of the sentence than its beginning. Bidirectional RNNs provide a simple
solution to this problem; as shown in Fig. 8.12, we simply combine the ﬁnal hidden
states from the forward and backward passes (for example by concatenation) and
use that as input for follow-on processing.
8.5 The LSTM
In practice, it is quite difﬁcult to train RNNs for tasks that require a network to make
use of information distant from the current point of processing. Despite having ac-
cess to the entire preceding sequence, the information encoded in hidden states tends
to be fairly local, more relevant to the most recent parts of the input sequence and
recent decisions. Yet distant information is critical to many language applications.
Consider the following example in the context of language modeling.

--- Page 171 ---
8.5 • T HELSTM 171
RNN 2 
RNN 1x1x2x3xnhn→h1←hn→SoftmaxFFNh1←
Figure 8.12 A bidirectional RNN for sequence classiﬁcation. The ﬁnal hidden units from
the forward and backward passes are combined to represent the entire sequence. This com-
bined representation serves as input to the subsequent classiﬁer.
(8.19) The ﬂights the airline was canceling were full.
Assigning a high probability to wasfollowing airline is straightforward since airline
provides a strong local context for the singular agreement. However, assigning an
appropriate probability to were is quite difﬁcult, not only because the plural ﬂights
is quite distant, but also because the singular noun airline is closer in the intervening
context. Ideally, a network should be able to retain the distant information about
plural ﬂights until it is needed, while still processing the intermediate parts of the
sequence correctly.
One reason for the inability of RNNs to carry forward critical information is that
the hidden layers, and, by extension, the weights that determine the values in the hid-
den layer, are being asked to perform two tasks simultaneously: provide information
useful for the current decision, and updating and carrying forward information re-
quired for future decisions.
A second difﬁculty with training RNNs arises from the need to backpropagate
the error signal back through time. Recall from Section 8.1.2 that the hidden layer at
time tcontributes to the loss at the next time step since it takes part in that calcula-
tion. As a result, during the backward pass of training, the hidden layers are subject
to repeated multiplications, as determined by the length of the sequence. A frequent
result of this process is that the gradients are eventually driven to zero, a situation
called the vanishing gradients problem.vanishing
gradients
To address these issues, more complex network architectures have been designed
to explicitly manage the task of maintaining relevant context over time, by enabling
the network to learn to forget information that is no longer needed and to remember
information required for decisions still to come.
The most commonly used such extension to RNNs is the long short-term mem-
ory(LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the con-long short-term
memory
text management problem into two subproblems: removing information no longer
needed from the context, and adding information likely to be needed for later de-

--- Page 172 ---
172 CHAPTER 8 • RNN S AND LSTM S
cision making. The key to solving both problems is to learn how to manage this
context rather than hard-coding a strategy into the architecture. LSTMs accomplish
this by ﬁrst adding an explicit context layer to the architecture (in addition to the
usual recurrent hidden layer), and through the use of specialized neural units that
make use of gates to control the ﬂow of information into and out of the units that
comprise the network layers. These gates are implemented through the use of addi-
tional weights that operate sequentially on the input, and previous hidden layer, and
previous context layers.
The gates in an LSTM share a common design pattern; each consists of a feed-
forward layer, followed by a sigmoid activation function, followed by a pointwise
multiplication with the layer being gated. The choice of the sigmoid as the activation
function arises from its tendency to push its outputs to either 0 or 1. Combining this
with a pointwise multiplication has an effect similar to that of a binary mask. Values
in the layer being gated that align with values near 1 in the mask are passed through
nearly unchanged; values corresponding to lower values are essentially erased.
The ﬁrst gate we’ll consider is the forget gate . The purpose of this gate is forget gate
to delete information from the context that is no longer needed. The forget gate
computes a weighted sum of the previous state’s hidden layer and the current in-
put and passes that through a sigmoid. This mask is then multiplied element-wise
by the context vector to remove the information from context that is no longer re-
quired. Element-wise multiplication of two vectors (represented by the operator ,
and sometimes called the Hadamard product ) is the vector of the same dimension
as the two input vectors, where each element iis the product of element iin the two
input vectors:
ft=s(Ufht 1+Wfxt) (8.20)
kt=ct 1ft (8.21)
The next task is to compute the actual information we need to extract from the previ-
ous hidden state and current inputs—the same basic computation we’ve been using
for all our recurrent networks.
gt=tanh(Ught 1+Wgxt) (8.22)
Next, we generate the mask for the add gate to select the information to add to the add gate
current context.
it=s(Uiht 1+Wixt) (8.23)
jt=gtit (8.24)
Next, we add this to the modiﬁed context vector to get our new context vector.
ct=jt+kt (8.25)
The ﬁnal gate we’ll use is the output gate which is used to decide what informa- output gate
tion is required for the current hidden state (as opposed to what information needs
to be preserved for future decisions).
ot=s(Uoht 1+Woxt) (8.26)
ht=ottanh(ct) (8.27)
Fig. 8.13 illustrates the complete computation for a single LSTM unit. Given the

--- Page 173 ---
8.5 • T HELSTM 173
+
xtht-1cthtcthtct-1ht-1xttanh
+σtanhσσ+++igf
o㽋㽋㽋LSTMct-1
Figure 8.13 A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the
current input, x, the previous hidden state, ht 1, and the previous context, ct 1. The outputs are a new hidden
state, htand an updated context, ct.
appropriate weights for the various gates, an LSTM accepts as input the context
layer, and hidden layer from the previous time step, along with the current input
vector. It then generates updated context and hidden vectors as output.
It is the hidden state, ht, that provides the output for the LSTM at each time step.
This output can be used as the input to subsequent layers in a stacked RNN, or at the
ﬁnal layer of a network htcan be used to provide the ﬁnal output of the LSTM.
8.5.1 Gated Units, Layers and Networks
The neural units used in LSTMs are obviously much more complex than those used
in basic feedforward networks. Fortunately, this complexity is encapsulated within
the basic processing units, allowing us to maintain modularity and to easily exper-
iment with different architectures. To see this, consider Fig. 8.14 which illustrates
the inputs and outputs associated with each kind of unit.
At the far left, (a) is the basic feedforward unit where a single set of weights and
a single activation function determine its output, and when arranged in a layer there
are no connections among the units in the layer. Next, (b) represents the unit in a
simple recurrent network. Now there are two inputs and an additional set of weights
to go with it. However, there is still a single activation function and output.
The increased complexity of the LSTM units is encapsulated within the unit
itself. The only additional external complexity for the LSTM over the basic recurrent
unit (b) is the presence of the additional context vector as an input and output.
This modularity is key to the power and widespread applicability of LSTM units.
LSTM units (or other varieties, like GRUs) can be substituted into any of the network
architectures described in Section 8.4. And, as with simple RNNs, multi-layered
networks making use of gated units can be unrolled into deep feedforward networks
and trained in the usual fashion with backpropagation. In practice, therefore, LSTMs
rather than RNNs have become the standard unit for any modern system that makes
use of recurrent networks.

--- Page 174 ---
174 CHAPTER 8 • RNN S AND LSTM S
h
xxtxtht-1htht
ct-1ct
ht-1(b)(a)(c)⌃gza⌃gzLSTMUnita
Figure 8.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and
long short-term memory (LSTM).
8.6 Summary: Common RNN NLP Architectures
We’ve now introduced the RNN, seen advanced components like stacking multiple
layers and using the LSTM version, and seen how the RNN can be applied to various
tasks. Let’s take a moment to summarize the architectures for these applications.
Fig. 8.15 shows the three architectures we’ve discussed so far: sequence la-
beling, sequence classiﬁcation, and language modeling. In sequence labeling (for
example for part of speech tagging), we train a model to produce a label for each
input word or token. In sequence classiﬁcation, for example for sentiment analysis,
we ignore the output for each token, and only take the value from the end of the
sequence (and similarly the model’s training signal comes from backpropagation
from that last token). In language modeling, we train the model to predict the next
word at each token step. In the next section we’ll introduce a fourth architecture, the
encoder-decoder .
8.7 The Encoder-Decoder Model with RNNs
In this section we introduce a new model, the encoder-decoder model, which is used
when we are taking an input sequence and translating it to an output sequence that is
of a different length than the input, and doesn’t align with it in a word-to-word way.
Recall that in the sequence labeling task, we have two sequences, but they are the
same length (for example in part-of-speech tagging each token gets an associated
tag), each input is associated with a speciﬁc output, and the labeling for that output
takes mostly local information. Thus deciding whether a word is a verb or a noun,
we look mostly at the word and the neighboring words.
By contrast, encoder-decoder models are used especially for tasks like machine
translation, where the input sequence and output sequence can have different lengths
and the mapping between a token in the input and a token in the output can be very
indirect (in some languages the verb appears at the beginning of the sentence; in
other languages at the end). We’ll introduce machine translation in detail in Chap-
ter 13, but for now we’ll just point out that the mapping for a sentence in English to
a sentence in Tagalog or Yoruba can have very different numbers of words, and the

--- Page 175 ---
8.7 • T HEENCODER -DECODER MODEL WITH RNN S175
…Encoder RNNDecoder RNNContext…
x1x2xny1y2ym…RNNx1x2xn…y1y2yn…RNNx1x2xny
…RNNx1x2xt-1…x2x3xta) sequence labeling b) sequence classification 
c) language modelingd) encoder-decoder
Figure 8.15 Four architectures for NLP tasks. In sequence labeling (POS or named entity tagging) we map
each input token xito an output token yi. In sequence classiﬁcation we map the entire input sequence to a single
class. In language modeling we output the next token conditioned on previous tokens. In the encoder model we
have two separate RNN models, one of which maps from an input sequence xto an intermediate representation
we call the context , and a second of which maps from the context to an output sequence y.
words can be in a very different order.
Encoder-decoder networks, sometimes called sequence-to-sequence networks,encoder-
decoder
are models capable of generating contextually appropriate, arbitrary length, output
sequences given an input sequence. Encoder-decoder networks have been applied
to a very wide range of applications including summarization, question answering,
and dialogue, but they are particularly popular for machine translation.
The key idea underlying these networks is the use of an encoder network that
takes an input sequence and creates a contextualized representation of it, often called
thecontext . This representation is then passed to a decoder which generates a task-
speciﬁc output sequence. Fig. 8.16 illustrates the architecture.
…EncoderDecoderContext…
x1x2xny1y2ym
Figure 8.16 The encoder-decoder architecture. The context is a function of the hidden
representations of the input, and may be used by the decoder in a variety of ways.
Encoder-decoder networks consist of three conceptual components:
1. An encoder that accepts an input sequence, x1:n, and generates a correspond-

--- Page 176 ---
176 CHAPTER 8 • RNN S AND LSTM S
ing sequence of contextualized representations, h1:n. LSTMs, convolutional
networks, and transformers can all be employed as encoders.
2. A context vector ,c, which is a function of h1:n, and conveys the essence of
the input to the decoder.
3. A decoder , which accepts cas input and generates an arbitrary length se-
quence of hidden states h1:m, from which a corresponding sequence of output
states y1:m, can be obtained. Just as with encoders, decoders can be realized
by any kind of sequence architecture.
In this section we’ll describe an encoder-decoder network based on a pair of
RNNs, but we’ll see in Chapter 13 how to apply them to transformers as well. We’ll
build up the equations for encoder-decoder models by starting with the conditional
RNN language model p(y), the probability of a sequence y.
Recall that in any language model, we can break down the probability as follows:
p(y) = p(y1)p(y2jy1)p(y3jy1;y2):::p(ymjy1;:::;ym 1) (8.28)
In RNN language modeling, at a particular time t, we pass the preﬁx of t 1
tokens through the language model, using forward inference to produce a sequence
of hidden states, ending with the hidden state corresponding to the last word of
the preﬁx. We then use the ﬁnal hidden state of the preﬁx as our starting point to
generate the next token.
More formally, if gis an activation function like tanh or ReLU, a function of
the input at time tand the hidden state at time t 1, and the softmax is over the
set of possible vocabulary items, then at time tthe output ytand hidden state htare
computed as:
ht=g(ht 1;xt) (8.29)
^ yt=softmax (ht) (8.30)
We only have to make one slight change to turn this language model with au-
toregressive generation into an encoder-decoder model that is a translation model
that can translate from a source text in one language to a target text in a second:
add a sentence separation marker at the end of the source text, and then simplysentence
separation
concatenate the target text.
Let’s use<s>for our sentence separator token, and let’s think about translating
an English source text (“the green witch arrived”), to a Spanish sentence (“ lleg´o
la bruja verde ” (which can be glossed word-by-word as ‘arrived the witch green’).
We could also illustrate encoder-decoder models with a question-answer pair, or a
text-summarization pair.
Let’s use xto refer to the source text (in this case in English) plus the separator
token<s>, and yto refer to the target text y(in this case in Spanish). Then an
encoder-decoder model computes the probability p(yjx)as follows:
p(yjx) = p(y1jx)p(y2jy1;x)p(y3jy1;y2;x):::p(ymjy1;:::;ym 1;x) (8.31)
Fig. 8.17 shows the setup for a simpliﬁed version of the encoder-decoder model
(we’ll see the full model, which requires the new concept of attention , in the next
section).
Fig. 8.17 shows an English source text (“the green witch arrived”), a sentence
separator token ( <s>, and a Spanish target text (“ lleg´o la bruja verde ”). To trans-
late a source text, we run it through the network performing forward inference to

--- Page 177 ---
8.7 • T HEENCODER -DECODER MODEL WITH RNN S177
Source TextTarget Text
hnembeddinglayerhiddenlayer(s)softmaxthegreenllegó
witcharrived<s>llególa
labruja
brujaverde
verde</s>(output of source is ignored)
Separator
Figure 8.17 Translating a single sentence (inference time) in the basic RNN version of encoder-decoder ap-
proach to machine translation. Source and target sentences are concatenated with a separator token in between,
and the decoder uses context information from the encoder’s last hidden state.
generate hidden states until we get to the end of the source. Then we begin autore-
gressive generation, asking for a word in the context of the hidden layer from the
end of the source input as well as the end-of-sentence marker. Subsequent words
are conditioned on the previous hidden state and the embedding for the last word
generated.
Let’s formalize and generalize this model a bit in Fig. 8.18. (To help keep things
straight, we’ll use the superscripts eanddwhere needed to distinguish the hidden
states of the encoder and the decoder.) The elements of the network on the left
process the input sequence xand comprise the encoder . While our simpliﬁed ﬁgure
shows only a single network layer for the encoder, stacked architectures are the
norm, where the output states from the top layer of the stack are taken as the ﬁnal
representation, and the encoder consists of stacked biLSTMs where the hidden states
from top layers from the forward and backward passes are concatenated to provide
the contextualized representations for each time step.
EncoderDecoder
hn hd1he3he2he1hd2hd3hd4embeddinglayerhiddenlayer(s)softmaxx1x2y1hdmx3xn<s>y1y2
y2y3
y3y4
ym</s>hen = c = hd0(output is ignored during encoding)
Figure 8.18 A more formal version of translating a sentence at inference time in the basic RNN-based
encoder-decoder architecture. The ﬁnal hidden state of the encoder RNN, hen, serves as the context for the
decoder in its role as hd
0in the decoder RNN, and is also made available to each decoder hidden state.

--- Page 178 ---
178 CHAPTER 8 • RNN S AND LSTM S
The entire purpose of the encoder is to generate a contextualized representation
of the input. This representation is embodied in the ﬁnal hidden state of the encoder,
he
n. This representation, also called cforcontext , is then passed to the decoder.
The simplest version of the decoder network would takes this state and use it
just to initialize the ﬁrst hidden state of the decoder; the ﬁrst decoder RNN cell
would use cas its prior hidden state hd
0. The decoder would then autoregressively
generates a sequence of outputs, an element at a time, until an end-of-sequence
marker is generated. Each hidden state is conditioned on the previous hidden state
and the output generated in the previous state.
As Fig. 8.18 shows, we do something more complex: we make the context vector
cavailable to more than just the ﬁrst decoder hidden state, to ensure that the inﬂuence
of the context vector, c, doesn’t wane as the output sequence is generated. We do
this by adding cas a parameter to the computation of the current hidden state. using
the following equation:
hd
t=g(ˆyt 1;hd
t 1;c) (8.32)
Now we’re ready to see the full equations for this version of the decoder in the basic
encoder-decoder model, with context available at each decoding timestep. Recall
thatgis a stand-in for some ﬂavor of RNN and ˆ yt 1is the embedding for the output
sampled from the softmax at the previous step:
c=he
n
hd
0=c
hd
t=g(ˆyt 1;hd
t 1;c)
^ yt=softmax (hd
t) (8.33)
Thus ^ ytis a vector of probabilities over the vocabulary, representing the probability
of each word occurring at time t. To generate text, we sample from this distribution
^ yt. For example, the greedy choice is simply to choose the most most probable word
to generate at each timestep. We’ll introduce more sophisticated sampling methods
in Section 10.2.
8.7.1 Training the Encoder-Decoder Model
Encoder-decoder architectures are trained end-to-end. Each training example is a
tuple of paired strings, a source and a target. Concatenated with a separator token,
these source-target pairs can now serve as training data.
For MT, the training data typically consists of sets of sentences and their transla-
tions. These can be drawn from standard datasets of aligned sentence pairs, as we’ll
discuss in Section 13.2.2. Once we have a training set, the training itself proceeds
as with any RNN-based language model. The network is given the source text and
then starting with the separator token is trained autoregressively to predict the next
word, as shown in Fig. 8.19.
Note the differences between training (Fig. 8.19) and inference (Fig. 8.17) with
respect to the outputs at each time step. The decoder during inference uses its own
estimated output ˆ ytas the input for the next time step xt+1. Thus the decoder will
tend to deviate more and more from the gold target sentence as it keeps generating
more tokens. In training, therefore, it is more common to use teacher forcing in the teacher forcing
decoder. Teacher forcing means that we force the system to use the gold target token
from training as the next input xt+1, rather than allowing it to rely on the (possibly
erroneous) decoder output ˆ yt. This speeds up training.

--- Page 179 ---
8.8 • A TTENTION 179
EncoderDecoder
embeddinglayerhiddenlayer(s)softmaxthegreenllegó
witcharrived<s>llególa
labruja
brujaverde
verde</s>goldanswersL1 =-log P(y1)
x1x2x3x4L2 =-log P(y2)L3 =-log P(y3)L4 =-log P(y4)L5 =-log P(y5)per-wordlossy1y2y3y4y5Total loss is the average cross-entropy loss per target word:
Figure 8.19 Training the basic RNN encoder-decoder approach to machine translation. Note that in the
decoder we usually don’t propagate the model’s softmax outputs ˆ yt, but use teacher forcing to force each input
to the correct gold value for training. We compute the softmax output distribution over ˆ yin the decoder in order
to compute the loss at each token, which can then be averaged to compute a loss for the sentence. This loss is
then propagated through the decoder parameters and the encoder parameters.
8.8 Attention
The simplicity of the encoder-decoder model is its clean separation of the encoder—
which builds a representation of the source text—from the decoder, which uses this
context to generate a target text. In the model as we’ve described it so far, this
context vector is hn, the hidden state of the last ( nth) time step of the source text.
This ﬁnal hidden state is thus acting as a bottleneck : it must represent absolutely
everything about the meaning of the source text, since the only thing the decoder
knows about the source text is what’s in this context vector (Fig. 8.20). Information
at the beginning of the sentence, especially for long sentences, may not be equally
well represented in the context vector.
EncoderDecoderbottleneckbottleneck
Figure 8.20 Requiring the context cto be only the encoder’s ﬁnal hidden state forces all the
information from the entire source sentence to pass through this representational bottleneck.
The attention mechanism is a solution to the bottleneck problem, a way ofattention
mechanism
allowing the decoder to get information from allthe hidden states of the encoder,
not just the last hidden state.
In the attention mechanism, as in the vanilla encoder-decoder model, the context
vector cis a single vector that is a function of the hidden states of the encoder, that
is,c=f(he
1:::he
n). Because the number of hidden states varies with the size of
the input, we can’t use the entire set of encoder hidden state vectors directly as the

--- Page 180 ---
180 CHAPTER 8 • RNN S AND LSTM S
context for the decoder.
The idea of attention is instead to create the single ﬁxed-length vector cby taking
a weighted sum of all the encoder hidden states. The weights focus on (‘attend
to’) a particular part of the source text that is relevant for the token the decoder is
currently producing. Attention thus replaces the static context vector with one that
is dynamically derived from the encoder hidden states, different for each token in
decoding.
This context vector, ci, is generated anew with each decoding step iand takes
all of the encoder hidden states into account in its derivation. We then make this
context available during decoding by conditioning the computation of the current
decoder hidden state on it (along with the prior hidden state and the previous output
generated by the decoder), as we see in this equation (and Fig. 8.21):
hd
i=g(ˆyi 1;hd
i 1;ci) (8.34)
hd1hd2hdiy1y2yic1c2ci……
Figure 8.21 The attention mechanism allows each hidden state of the decoder to see a
different, dynamic, context, which is a function of all the encoder hidden states.
The ﬁrst step in computing ciis to compute how much to focus on each encoder
state, how relevant each encoder state is to the decoder state captured in hd
i 1. We
capture relevance by computing— at each state iduring decoding—a score(hd
i 1;he
j)
for each encoder state j.
The simplest such score, called dot-product attention , implements relevance asdot-product
attention
similarity: measuring how similar the decoder hidden state is to an encoder hidden
state, by computing the dot product between them:
score(hd
i 1;he
j) = hd
i 1he
j (8.35)
The score that results from this dot product is a scalar that reﬂects the degree of
similarity between the two vectors. The vector of these scores across all the encoder
hidden states gives us the relevance of each encoder state to the current step of the
decoder.
To make use of these scores, we’ll normalize them with a softmax to create a
vector of weights, ai j, that tells us the proportional relevance of each encoder hidden
state jto the prior hidden decoder state, hd
i 1.
ai j=softmax (score(hd
i 1;he
j))
=exp(score(hd
i 1;he
j)
P
kexp(score(hd
i 1;he
k))(8.36)
Finally, given the distribution in a, we can compute a ﬁxed-length context vector for
the current decoder state by taking a weighted average over all the encoder hidden
states.
ci=X
jai jhe
j (8.37)

--- Page 181 ---
8.9 • S UMMARY 181
With this, we ﬁnally have a ﬁxed-length context vector that takes into account
information from the entire encoder state that is dynamically updated to reﬂect the
needs of the decoder at each step of decoding. Fig. 8.22 illustrates an encoder-
decoder network with attention, focusing on the computation of one context vector
ci.
EncoderDecoder
hdi-1he3he2he1hdihiddenlayer(s)x1x2yix3xnyi-1yiyi+1hen ci.2.1.3.4attentionweightsci-1ci
<latexit sha1_base64="TNdNmv/RIlrhPa6LgQyjjQLqyBA=">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit>Xj↵ijhej↵ij<latexit sha1_base64="y8s4mGdpwrGrBnuSR+p1gJJXYdo=">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit>hdi 1·hej……
Figure 8.22 A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The
context value ciis one of the inputs to the computation of hd
i. It is computed by taking the weighted sum of all
the encoder hidden states, each weighted by their dot product with the prior decoder hidden state hd
i 1.
It’s also possible to create more sophisticated scoring functions for attention
models. Instead of simple dot product attention, we can get a more powerful function
that computes the relevance of each encoder hidden state to the decoder hidden state
by parameterizing the score with its own set of weights, Ws.
score(hd
i 1;he
j) = hd
t 1Wshe
j
The weights Ws, which are then trained during normal end-to-end training, give the
network the ability to learn which aspects of similarity between the decoder and
encoder states are important to the current application. This bilinear model also
allows the encoder and decoder to use different dimensional vectors, whereas the
simple dot-product attention requires that the encoder and decoder hidden states
have the same dimensionality.
We’ll return to the concept of attention when we deﬁne the transformer archi-
tecture in Chapter 9, which is based on a slight modiﬁcation of attention called
self-attention .
8.9 Summary
This chapter has introduced the concepts of recurrent neural networks and how they
can be applied to language problems. Here’s a summary of the main points that we
covered:
• In simple Recurrent Neural Networks sequences are processed one element at
a time, with the output of each neural unit at time tbased both on the current
input at tand the hidden layer from time t 1.

--- Page 182 ---
182 CHAPTER 8 • RNN S AND LSTM S
• RNNs can be trained with a straightforward extension of the backpropagation
algorithm, known as backpropagation through time (BPTT).
• Simple recurrent networks fail on long inputs because of problems like van-
ishing gradients ; instead modern systems use more complex gated architec-
tures such as LSTMs that explicitly decide what to remember and forget in
their hidden and context layers.
• Common language-based applications for RNNs include:
–Probabilistic language modeling: assigning a probability to a sequence,
or to the next element of a sequence given the preceding words.
–Auto-regressive generation using a trained language model.
–Sequence labeling like part-of-speech tagging, where each element of a
sequence is assigned a label.
–Sequence classiﬁcation, where an entire text is assigned to a category, as
in spam detection, sentiment analysis or topic classiﬁcation.
–Encoder-decoder architectures, where an input is mapped to an output
of different length and alignment.
Bibliographical and Historical Notes
Inﬂuential investigations of RNNs were conducted in the context of the Parallel Dis-
tributed Processing (PDP) group at UC San Diego in the 1980’s. Much of this work
was directed at human cognitive modeling rather than practical NLP applications
(Rumelhart and McClelland 1986c, McClelland and Rumelhart 1986). Models using
recurrence at the hidden layer in a feedforward network (Elman networks) were in-
troduced by Elman (1990). Similar architectures were investigated by Jordan (1986)
with a recurrence from the output layer, and Mathis and Mozer (1995) with the
addition of a recurrent context layer prior to the hidden layer. The possibility of
unrolling a recurrent network into an equivalent feedforward network is discussed
in (Rumelhart and McClelland, 1986c).
In parallel with work in cognitive modeling, RNNs were investigated extensively
in the continuous domain in the signal processing and speech communities (Giles
et al. 1994, Robinson et al. 1996). Schuster and Paliwal (1997) introduced bidirec-
tional RNNs and described results on the TIMIT phoneme transcription task.
While theoretically interesting, the difﬁculty with training RNNs and manag-
ing context over long sequences impeded progress on practical applications. This
situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber
(1997) and Gers et al. (2000). Impressive performance gains were demonstrated
on tasks at the boundary of signal processing and language processing including
phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition
(Graves et al., 2007) and most signiﬁcantly speech recognition (Graves et al., 2013).
Interest in applying neural networks to practical NLP problems surged with the
work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made
use of learned word embeddings, convolutional networks, and end-to-end training.
They demonstrated near state-of-the-art performance on a number of standard shared
tasks including part-of-speech tagging, chunking, named entity recognition and se-
mantic role labeling without the use of hand-engineered features.
Approaches that married LSTMs with pretrained collections of word-embeddings
based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014)

--- Page 183 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 183
quickly came to dominate many common tasks: part-of-speech tagging (Ling et al.,
2015), syntactic chunking (Søgaard and Goldberg, 2016), named entity recognition
(Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie,
2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and
Martin, 2016). As with the earlier surge of progress involving statistical machine
learning, these advances were made possible by the availability of training data pro-
vided by CONLL, SemEval, and other shared tasks, as well as shared resources such
as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005).
The modern neural encoder-decoder approach was pioneered by Kalchbrenner
and Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al.
(2014) (who coined the name “encoder-decoder”) and Sutskever et al. (2014) then
showed how to use extended RNNs for both encoder and decoder. The idea that a
generative decoder should take as input a soft weighting of the inputs, the central
idea of attention, was ﬁrst developed by Graves (2013) in the context of handwriting
recognition. Bahdanau et al. (2015) extended the idea, named it “attention” and
applied it to MT.

--- Page 184 ---
184 CHAPTER 9 • T HETRANSFORMER
CHAPTER
9The Transformer
“The true art of memory is the art of attention ”
Samuel Johnson, Idler #74 , September 1759
In this chapter we introduce the transformer , the standard architecture for build-
inglarge language models . Transformer-based large language models have com-
pletely changed the ﬁeld of speech and language processing. Indeed, every subse-
quent chapter in this textbook will make use of them. We’ll focus for now on left-
to-right (sometimes called causal or autoregressive) language modeling, in which
we are given a sequence of input tokens and predict output tokens one by one by
conditioning on the prior context.
The transformer is a neural network with a speciﬁc structure that includes a
mechanism called self-attention ormulti-head attention .1Attention can be thought
of as a way to build contextual representations of a token’s meaning by attending to
and integrating information from surrounding tokens, helping the model learn how
tokens relate to each other over large spans.
StackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall……
…
U
Input tokensx1x2LanguageModelingHead
x3x4x5InputEncoding
E1+
E2+
E3+
E4+
E5+……………
U
U
U
U…logitslogitslogitslogitslogits
Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token
get encoded, passed through a set of stacked transformer blocks, and then a language model
head that predicts the next token.
Fig. 9.1 sketches the transformer architecture. A transformer has three major
components. At the center are columns of transformer blocks . Each block is a
multilayer network (a multi-head attention layer, feedforward networks and layer
normalization steps) that maps an input vector xiin column i(corresponding to input
1Although multi-head attention developed historically from the RNN attention mechanism (Chapter 8),
we’ll deﬁne attention from scratch here for readers who haven’t yet read Chapter 8.

--- Page 185 ---
9.1 • A TTENTION 185
token i) to an output vector hi. The set of nblocks maps an entire context window
of input vectors (x1;:::;xn)to a window of output vectors (h1;:::;hn)of the same
length. A column might contain from 12 to 96 or more stacked blocks.
The column of blocks is preceded by the input encoding component, which pro-
cesses an input token (like the word thanks ) into a contextual vector representation,
using an embedding matrix Eand a mechanism for encoding token position. Each
column is followed by a language modeling head , which takes the embedding out-
put by the ﬁnal transformer block, passes it through an unembedding matrix Uand
a softmax over the vocabulary to generate a single token for that column.
Transformer-based language models are complex, and so the details will unfold
over the next 5 chapters. In the next sections we’ll introduce multi-head attention,
the rest of the transformer block, and the input encoding and language modeling
head components. Chapter 10 discusses how language models are pretrained , and
how tokens are generated via sampling . Chapter 11 introduces masked language
modeling and the BERT family of bidirectional transformer encoder models. Chap-
ter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and
demonstrations, and how to align the model with human preferences. Chapter 13
will introduce machine translation with the encoder-decoder architecture.
9.1 Attention
Recall from Chapter 6 that for word2vec and other static embeddings, the repre-
sentation of a word’s meaning is always the same vector irrespective of the context:
the wordchicken , for example, is always represented by the same ﬁxed vector. So
a static vector for the word itmight somehow encode that this is a pronoun used
for animals and inanimate entities. But in context ithas a much richer meaning.
Consideritin one of these two sentences:
(9.1) The chicken didn’t cross the road because itwas too tired.
(9.2) The chicken didn’t cross the road because itwas too wide.
In (9.1)itis the chicken (i.e., the reader knows that the chicken was tired), while
in (9.2)itis the road (and the reader knows that the road was wide).2That is, if
we are to compute the meaning of this sentence, we’ll need the meaning of itto be
associated with the chicken in the ﬁrst sentence and associated with the road in
the second one, sensitive to the context.
Furthermore, consider reading left to right like a causal language model, pro-
cessing the sentence up to the word it:
(9.3) The chicken didn’t cross the road because it
At this point we don’t yet know which thing itis going to end up referring to! So a
representation of itat this point might have aspects of both chicken androad as
the reader is trying to guess what happens next.
This fact that words have rich linguistic relationships with other words that may
be far away pervades language. Consider two more examples:
(9.4) The keys to the cabinet areon the table.
(9.5) I walked along the pond , and noticed one of the trees along the bank .
2We say that in the ﬁrst example itcorefers with the chicken, and in the second itcorefers with the
road; we’ll return to this in Chapter 23.

--- Page 186 ---
186 CHAPTER 9 • T HETRANSFORMER
In (9.4), the phrase The keys is the subject of the sentence, and in English and many
languages, must agree in grammatical number with the verb are; in this case both are
plural. In English we can’t use a singular verb like iswith a plural subject like keys
(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers
to the side of a pond or river and not a ﬁnancial institution because of the context,
including words like pond . (We’ll discuss word senses more in Chapter 11.)
The point of all these examples is that these contextual words that help us com-
pute the meaning of words in context can be quite far away in the sentence or para-
graph. Transformers can build contextual representations of word meaning, contex-
tual embeddings , by integrating the meaning of these helpful contextual words. In acontextual
embeddings
transformer, layer by layer, we build up richer and richer contextualized representa-
tions of the meanings of input tokens. At each layer, we compute the representation
of a token iby combining information about ifrom the previous layer with infor-
mation about the neighboring tokens to produce a contextualized representation for
each word at each position.
Attention is the mechanism in the transformer that weighs and combines the
representations from appropriate other tokens in the context from layer k 1 to build
the representation for tokens in layer k.
Thechickendidn’tcrosstheroadbecauseitwastootiredThechickendidn’tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens
Figure 9.2 The self-attention weight distribution athat is part of the computation of the
representation for the word itat layer k+1. In computing the representation for it, we attend
differently to the various words at layer l, with darker shades indicating higher self-attention
values. Note that the transformer is attending highly to the columns corresponding to the
tokens chicken androad , a sensible result, since at the point where itoccurs, it could plausibly
corefers with the chicken or the road, and hence we’d like the representation for itto draw on
the representation for these earlier words. Figure adapted from Uszkoreit (2017).
Fig. 9.2 shows a schematic example simpliﬁed from a transformer (Uszkoreit,
2017). The ﬁgure describes the situation when the current token is itand we need
to compute a contextual representation for this token at layer k+1 of the transformer,
drawing on the representations (from layer k) of every prior token. The ﬁgure uses
color to represent the attention distribution over the contextual words: the tokens
chicken androad both have a high attention weight, meaning that as we are com-
puting the representation for it, we will draw most heavily on the representation for
chicken androad . This will be useful in building the ﬁnal representation for it,
sinceitwill end up coreferring with either chicken orroad .
Let’s now turn to how this attention distribution is represented and computed.

--- Page 187 ---
9.1 • A TTENTION 187
9.1.1 Attention more formally
As we’ve said, the attention computation is a way to compute a vector representation
for a token at a particular layer of a transformer, by selectively attending to and
integrating information from prior tokens at the previous layer. Attention takes an
input representation xicorresponding to the input token at position i, and a context
window of prior inputs x1::xi 1, and produces an output ai.
In causal, left-to-right language models, the context is any of the prior words.
That is, when processing xi, the model has access to xias well as the representations
of all the prior tokens in the context window (context windows consist of thousands
of tokens) but no tokens after i. (By contrast, in Chapter 11 we’ll generalize attention
so it can also look ahead to future words.)
Fig. 9.3 illustrates this ﬂow of information in an entire causal self-attention layer,
in which this same attention computation happens in parallel at each token position
i. Thus a self-attention layer maps input sequences (x1;:::;xn)to output sequences
of the same length (a1;:::;an).
attentionattentionSelf-AttentionLayerattentionattentionattentiona1a2a3a4a5x3x4x5x1x2
Figure 9.3 Information ﬂow in causal self-attention. When processing each input xi, the
model attends to all the inputs up to, and including xi.
Simpliﬁed version of attention At its heart, attention is really just a weighted
sum of context vectors, with a lot of complications added to how the weights are
computed and what gets summed. For pedagogical purposes let’s ﬁrst describe a
simpliﬁed intuition of attention, in which the attention output aiat token position i
is simply the weighted sum of all the representations xj, for all ji; we’ll use ai j
to mean how much xishould contribute to aj:
Simpliﬁed version: ai=X
jiai jxj (9.6)
Each ai jis a scalar used for weighing the value of input xjwhen summing up
the inputs to compute ai. How shall we compute this aweighting? In attention we
weight each prior embedding proportionally to how similar it is to the current token
i. So the output of attention is a sum of the embeddings of prior tokens weighted
by their similarity with the current token embedding. We compute similarity scores
viadot product , which maps two vectors into a scalar value ranging from  ¥to
¥. The larger the score, the more similar the vectors that are being compared. We’ll
normalize these scores with a softmax to create the vector of weights ai j;ji.
Simpliﬁed Version: score(xi;xj) = xixj (9.7)
ai j=softmax (score(xi;xj))8ji(9.8)
Thus in Fig. 9.3 we compute a3by computing three scores: x3x1,x3x2andx3x3,
normalizing them by a softmax, and using the resulting probabilities as weights
indicating each of their proportional relevance to the current position i. Of course,

--- Page 188 ---
188 CHAPTER 9 • T HETRANSFORMER
the softmax weight will likely be highest for xi, since xiis very similar to itself,
resulting in a high dot product. But other context words may also be similar to i, and
the softmax will also assign some weight to those words. Then we use these weights
as the avalues in Eq. 9.6 to compute the weighted sum that is our a3.
The simpliﬁed attention in equations 9.6 – 9.8 demonstrates the attention-based
approach to computing ai: compare the xito prior vectors, normalize those scores
into a probability distribution used to weight the sum of the prior vector. But now
we’re ready to remove the simpliﬁcations.
A single attention head using query, key, and value matrices Now that we’ve
seen a simple intuition of attention, let’s introduce the actual attention head , the attention head
version of attention that’s used in transformers. (The word head is often used in head
transformers to refer to speciﬁc structured layers). The attention head allows us to
distinctly represent three different roles that each input embedding plays during the
course of the attention process:
• As the current element being compared to the preceding inputs. We’ll refer to
this role as a query . query
• In its role as a preceding input that is being compared to the current element
to determine a similarity weight. We’ll refer to this role as a key. key
• And ﬁnally, as a value of a preceding element that gets weighted and summed value
up to compute the output for the current element.
To capture these three different roles, transformers introduce weight matrices
WQ,WK, and WV. These weights will project each input vector xiinto a represen-
tation of its role as a key, query, or value:
qi=xiWQ;ki=xiWK;vi=xiWV(9.9)
Given these projections, when we are computing the similarity of the current ele-
ment xiwith some prior element xj, we’ll use the dot product between the current
element’s query vector qiand the preceding element’s keyvector kj. Furthermore,
the result of a dot product can be an arbitrarily large (positive or negative) value, and
exponentiating large values can lead to numerical issues and loss of gradients during
training. To avoid this, we scale the dot product by a factor related to the size of the
embeddings, via diving by the square root of the dimensionality of the query and
key vectors ( dk). We thus replace the simpliﬁed Eq. 9.7 with Eq. 9.11. The ensuing
softmax calculation resulting in ai jremains the same, but the output calculation for
aiis now based on a weighted sum over the value vectors v(Eq. 9.13).
Here’s a ﬁnal set of equations for computing self-attention for a single self-
attention output vector aifrom a single input vector xi. This version of attention
computes aiby summing the values of the prior elements, each weighted by the
similarity of its keyto the query from the current element:
qi=xiWQ;kj=xjWK;vj=xjWV(9.10)
score(xi;xj) =qikjpdk(9.11)
ai j=softmax (score(xi;xj))8ji (9.12)
ai=X
jiai jvj (9.13)
We illustrate this in Fig. 9.4 for the case of calculating the value of the third output
a3in a sequence.

--- Page 189 ---
9.1 • A TTENTION 189
6. Sum the weighted value vectors4. Turn into 𝛼i,j weights via softmaxa3
1. Generate key, query, value vectors2. Compare x3’s query withthe keys for x1, x2, and x3Output of self-attention
Wk
WvWqx1kqvx3kqvx2kqv
××
WkWkWqWqWvWv5. Weigh each value vector÷√dk3. Divide score by √dk÷√dk÷√dk𝛼3,1𝛼3,2𝛼3,3
Figure 9.4 Calculating the value of a3, the third element of a sequence using causal (left-
to-right) self-attention.
Let’s talk shapes. The input to attention xiand the output from attention aiboth
have the same dimensionality 1 d(We often call dthemodel dimensionality ,
and indeed as we’ll discuss in Section 9.2 the output hiof each transformer block,
as well as the intermediate vectors inside the transformer block also have the same
dimensionality 1d.).
We’ll have a dimension dkfor the key and query vectors. The query vector and
the key vector are both dimensionality 1 dk, so we can take their dot product qikj.
We’ll have a separate dimension dvfor the value vectors. The transform matrix WQ
has shape [ddk],WKis[ddk], and WVis[ddv]. In the original transformer
work (Vaswani et al., 2017), dwas 512, dkanddvwere both 64.
Multi-head Attention Equations 9.11-9.13 describe a single attention head . But
actually, transformers use multiple attention heads. The intuition is that each head
might be attending to the context for different purposes: heads might be special-
ized to represent different linguistic relationships between context elements and the
current token, or to look for particular kinds of patterns in the context.
So in multi-head attention we have hseparate attention heads that reside inmulti-head
attention
parallel layers at the same depth in a model, each with its own set of parameters that
allows the head to model different aspects of the relationships among inputs. Thus
each head iin a self-attention layer has its own set of key, query and value matrices:
WKi,WQiandWVi. These are used to project the inputs into separate key, value,
and query embeddings for each head.
When using multiple heads the model dimension dis still used for the input
and output, the key and query embeddings have dimensionality dk, and the value
embeddings are of dimensionality dv(again, in the original transformer paper dk=
dv=64,h=8, and d=512). Thus for each head i, we have weight layers WQiof
shape [ddk],WKiof shape [ddk], and WViof shape [ddv].
Below are the equations for attention augmented with multiple heads; Fig. 9.5

--- Page 190 ---
190 CHAPTER 9 • T HETRANSFORMER
shows an intuition.
qc
i=xiWQc;kc
j=xjWKc;vc
j=xjWVc;8c1ch (9.14)
scorec(xi;xj) =qc
ikc
jpdk(9.15)
ac
i j=softmax (scorec(xi;xj))8ji(9.16)
headc
i=X
jiac
i jvc
j (9.17)
ai= (head1head2:::headh)WO(9.18)
MultiHeadAttention (xi;[x1;;xN]) = ai (9.19)
The output of each of the hheads is of shape 1 dv, and so the output of the
multi-head layer with hheads consists of hvectors of shape 1dv. These are con-
catenated to produce a single output with dimensionality 1 hdv. Then we use yet
another linear projection WO2Rhdvdto reshape it, resulting in the multi-head
attention vector aiwith the correct output shape [1xd]at each input i.
aixi-1xixi-2xi-3WK1Head 1WV1WQ1……WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]
[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends diﬀerentlyto context…[1 x dv ]
Figure 9.5 The multi-head attention computation for input xi, producing output ai. A multi-head attention
layer has hheads, each with its own key, query and value weight matrices. The outputs from each of the heads
are concatenated and then projected down to d, thus producing an output of the same size as the input.
9.2 Transformer Blocks
The self-attention calculation lies at the core of what’s called a transformer block,
which, in addition to the self-attention layer, includes three other kinds of layers: (1)
a feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-
ally called “layer norm”).
Fig. 9.6 illustrates a transformer block, sketching a common way of thinking
about the block that is called the residual stream (Elhage et al., 2021). In the resid- residual stream
ual stream viewpoint, we consider the processing of an individual token ithrough
the transformer block as a single stream of d-dimensional representations for token
position i. This residual stream starts with the original input vector, and the various

--- Page 191 ---
9.2 • T RANSFORMER BLOCKS 191
Layer Norm
xi+hi-1
Layer NormMultiHeadAttentionFeedforward
xi-1xi+1hihi+1
+……ResidualStream
Figure 9.6 The architecture of a transformer block showing the residual stream . This
ﬁgure shows the prenorm version of the architecture, in which the layer norms happen before
the attention and feedforward layers rather than after.
components read their input from the residual stream and add their output back into
the stream.
The input at the bottom of the stream is an embedding for a token, which has
dimensionality d. This initial embedding gets passed up (by residual connections ),
and is progressively added to by the other components of the transformer: the at-
tention layer that we have seen, and the feedforward layer that we will introduce.
Before the attention and feedforward layer is a computation called the layer norm .
Thus the initial vector is passed through a layer norm and attention layer, and
the result is added back into the stream, in this case to the original input vector
xi. And then this summed vector is again passed through another layer norm and a
feedforward layer, and the output of those is added back into the residual, and we’ll
usehito refer to the resulting output of the transformer block for token i. (In earlier
descriptions the residual stream was often described using a different metaphor as
residual connections that add the input of a component to its output, but the residual
stream is a more perspicuous way of visualizing the transformer.)
We’ve already seen the attention layer, so let’s now introduce the feedforward
and layer norm computations in the context of processing a single input xiat token
position i.
Feedforward layer The feedforward layer is a fully-connected 2-layer network,
i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights
are the same for each token position i, but are different from layer to layer. It
is common to make the dimensionality dffof the hidden layer of the feedforward
network be larger than the model dimensionality d. (For example in the original
transformer model, d=512 and dff=2048.)
FFN(xi) =ReLU (xiW1+b1)W2+b2 (9.20)
Layer Norm At two stages in the transformer block we normalize the vector (Ba
et al., 2016). This process, called layer norm (short for layer normalization), is one layer norm

--- Page 192 ---
192 CHAPTER 9 • T HETRANSFORMER
of many forms of normalization that can be used to improve training performance
in deep neural networks by keeping the values of a hidden layer in a range that
facilitates gradient-based training.
Layer norm is a variation of the z-score from statistics, applied to a single vec-
tor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm
isnotapplied to an entire transformer layer, but just to the embedding vector of a
single token. Thus the input to layer norm is a single vector of dimensionality d
and the output is that vector normalized, again of dimensionality d. The ﬁrst step in
layer normalization is to calculate the mean, m, and standard deviation, s, over the
elements of the vector to be normalized. Given an embedding vector xof dimen-
sionality d, these values are calculated as follows.
m=1
ddX
i=1xi (9.21)
s=vuut1
ddX
i=1(xi m)2(9.22)
Given these values, the vector components are normalized by subtracting the mean
from each and dividing by the standard deviation. The result of this computation is
a new vector with zero mean and a standard deviation of one.
^ x=(x m)
s(9.23)
Finally, in the standard implementation of layer normalization, two learnable param-
eters, gandb, representing gain and offset values, are introduced.
LayerNorm (x) =g(x m)
s+b (9.24)
Putting it all together The function computed by a transformer block can be ex-
pressed by breaking it down with one equation for each component computation,
using t(of shape [1d]) to stand for transformer and superscripts to demarcate
each computation inside the block:
t1
i=LayerNorm (xi) (9.25)
t2
i=MultiHeadAttention (t1
i;
x1
1;;x1
N
) (9.26)
t3
i=t2
i+xi (9.27)
t4
i=LayerNorm (t3
i) (9.28)
t5
i=FFN(t4
i) (9.29)
hi=t5
i+t3
i (9.30)
Notice that the only component that takes as input information from other tokens
(other residual streams) is multi-head attention, which (as we see from (9.27)) looks
at all the neighboring tokens in the context. The output from attention, however, is
then added into this token’s embedding stream. In fact, Elhage et al. (2021) show that
we can view attention heads as literally moving information from the residual stream
of a neighboring token into the current stream. The high-dimensional embedding
space at each position thus contains information about the current token and about
neighboring tokens, albeit in different subspaces of the vector space. Fig. 9.7 shows
a visualization of this movement.

--- Page 193 ---
9.3 • P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 193
Token Aresidual streamToken Bresidual stream
Figure 9.7 An attention head can move information from token A’s residual stream into
token B’s residual stream.
Crucially, the input and output dimensions of transformer blocks are matched so
they can be stacked. Each token vector xiat the input to the block has dimensionality
d, and the output hialso has dimensionality d. Transformers for large language
models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small
language models) to 96 layers (used for GPT-3 large), to even more for more recent
models. We’ll come back to this issue of stacking in a bit.
Equation (9.27) and following are just the equation for a single transformer
block, but the residual stream metaphor goes through all the transformer layers,
from the ﬁrst transformer blocks to the 12th, in a 12-layer transformer. At the ear-
lier transformer blocks, the residual stream is representing the current token. At the
highest transformer blocks, the residual stream is usually representing the following
token, since at the very end it’s being trained to predict the next token.
Once we stack many blocks, there is one more requirement: at the very end of
the last (highest) transformer block, there is a single extra layer norm that is run on
the last hiof each token stream (just below the language model head layer that we
will deﬁne soon).3
9.3 Parallelizing computation using a single matrix X
This description of multi-head attention and the rest of the transformer block has
been from the perspective of computing a single output at a single time step iin
a single residual stream. But as we pointed out earlier, the attention computation
performed for each token to compute aiis independent of the computation for each
other token, and that’s also true for all the computation in the transformer block
computing hifrom the input xi. That means we can easily parallelize the entire
computation, taking advantage of efﬁcient matrix multiplication routines.
We do this by packing the input embeddings for the Ntokens of the input se-
quence into a single matrix Xof size [Nd]. Each row of Xis the embedding of
one token of the input. Transformers for large language models commonly have an
input length N= 1K, 2K, or as many as 32K tokens (or more), so Xtypically has be-
tween 1K and 32K rows, each of the dimensionality of the embedding d(the model
3Note that we are using the most common current transformer architecture, which is called the prenorm
architecture. The original deﬁnition of the transformer in Vaswani et al. (2017) used an alternative archi-
tecture called the postnorm transformer in which the layer norm happens after the attention and FFN
layers; it turns out moving the layer norm beforehand works better, but does require this one extra layer
at the end.

--- Page 194 ---
194 CHAPTER 9 • T HETRANSFORMER
dimension).
Parallelizing attention Let’s ﬁrst see this for a single attention head and then turn
to multiple heads, and then add in the rest of the components in the transformer
block. For one head we multiply Xby the key, query, and value matrices WQof
shape [ddk],WKof shape [ddk], andWVof shape [ddv], to produce matrices
Qof shape [Ndk],K2RNdk, and V2RNdv, containing all the key, query, and
value vectors:
Q=XWQ;K=XWK;V=XWV(9.31)
Given these matrices we can compute all the requisite query-key comparisons simul-
taneously by multiplying QandK|in a single matrix multiplication. The product is
of shape NN, visualized in Fig. 9.8.
q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NNq1•k2q1•k3q1•k4q2•k3q2•k4q3•k4
Figure 9.8 The NNQK|matrix showing how it computes all qikjcomparisons in a
single matrix multiple.
Once we have this QK|matrix, we can very efﬁciently scale these scores, take
the softmax, and then multiply the result by Vresulting in a matrix of shape Nd:
a vector embedding representation for each token in the input. We’ve reduced the
entire self-attention step for an entire sequence of Ntokens for one head to the
following computation:
A=softmax
maskQK|
pdk
V (9.32)
Masking out the future You may have noticed that we introduced a mask function
in Eq. 9.32 above. This is because the self-attention computation as we’ve described
it has a problem: the calculation in QK|results in a score for each query value
to every key value, including those that follow the query . This is inappropriate in
the setting of language modeling: guessing the next word is pretty simple if you
already know it! To ﬁx this, the elements in the upper-triangular portion of the
matrix are zeroed out (set to  ¥), thus eliminating any knowledge of words that
follow in the sequence. This is done in practice by adding a mask matrix Min
which Mi j= ¥8j>i(i.e. for the upper-triangular portion) and Mi j=0 otherwise.
Fig. 9.9 shows the resulting masked QK|matrix. (we’ll see in Chapter 11 how to
make use of words in the future for tasks that need it).
Fig. 9.10 shows a schematic of all the computations for a single attention head
parallelized in matrix form.
Fig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length
of the input, since at each layer we need to compute dot products between each pair
of tokens in the input. This makes it expensive to compute attention over very long
documents (like entire novels). Nonetheless modern large language models manage
to use quite long contexts of thousands or tens of thousands of tokens.

--- Page 195 ---
9.3 • P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 195
q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞
Figure 9.9 TheNNQK|matrix showing the qikjvalues, with the upper-triangle por-
tion of the comparisons matrix zeroed out (set to  ¥, which the softmax will turn to zero).
q1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X
N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv
Figure 9.10 Schematic of the attention computation for a single attention head in parallel. The ﬁrst row shows
the computation of the Q,K, and Vmatrices. The second row shows the computation of QKT, the masking
(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of
the value vectors to get the ﬁnal attention vectors.
Parallelizing multi-head attention In multi-head attention, as with self-attention,
the input and output have the model dimension d, the key and query embeddings
have dimensionality dk, and the value embeddings are of dimensionality dv(again,
in the original transformer paper dk=dv=64,h=8, and d=512). Thus for each
head i, we have weight layers WQi2Rddk,WKi2Rddk, and WVi2Rddv, and
these get multiplied by the inputs packed into Xto produce Q2RNdk,K2RNdk,
andV2RNdv. The output of each of the hheads is of shape Ndv, and so the
output of the multi-head layer with hheads consists of hmatrices of shape Ndv. To
make use of these matrices in further processing, they are concatenated to produce
a single output with dimensionality Nhdv. Finally, we use yet another linear
projection WO2Rhdvd, that reshape it to the original output dimension for each
token. Multiplying the concatenated Nhdvmatrix output by WO2Rhdvdyields

--- Page 196 ---
196 CHAPTER 9 • T HETRANSFORMER
the self-attention output Aof shape [ Nd].
Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)
head i=SelfAttention (Qi;Ki;Vi) = softmaxQiKi|
pdk
Vi(9.34)
MultiHeadAttention (X) = ( head 1head 2:::head h)WO(9.35)
Putting it all together with the parallel input matrix XThe function computed
in parallel by an entire layer of Ntransformer block over the entire Ninput tokens
can be expressed as:
O=LayerNorm (X+MultiHeadAttention (X)) (9.36)
H=LayerNorm (O+FFN(O)) (9.37)
Or we can break it down with one equation for each component computation, using
T(of shape [Nd]) to stand for transformer and superscripts to demarcate each
computation inside the block:
T1=MultiHeadAttention (X) (9.38)
T2=X+T1(9.39)
T3=LayerNorm (T2) (9.40)
T4=FFN(T3) (9.41)
T5=T4+T3(9.42)
H=LayerNorm (T5) (9.43)
Here when we use a notation like FFN (T3)we mean that the same FFN is applied
in parallel to each of the Nembedding vectors in the window. Similarly, each of the
Ntokens is normed in parallel in the LayerNorm. Crucially, the input and output
dimensions of transformer blocks are matched so they can be stacked. Since each
token xiat the input to the block has dimensionality d, that means the input Xand
output Hare both of shape [Nd].
9.4 The input: embeddings for token and position
Let’s talk about where the input Xcomes from. Given a sequence of Ntokens ( Nis
the context length in tokens), the matrix Xof shape [Nd]has an embedding for embedding
each word in the context. The transformer does this by separately computing two
embeddings: an input token embedding, and an input positional embedding.
A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-
mension dthat will be our initial representation for the input token. (As we pass
vectors up through the transformer layers in the residual stream, this embedding
representation will change and grow, incorporating context and playing a different
role depending on the kind of language model we are building.) The set of initial
embeddings are stored in the embedding matrix E, which has a row for each of the
jVjtokens in the vocabulary. Thus each word is a row vector of ddimensions, and
Ehas shape [jVjd].
Given an input token string like Thanks for all the we ﬁrst convert the tokens
into vocabulary indices (these were created when we ﬁrst tokenized the input using

--- Page 197 ---
9.4 • T HE INPUT :EMBEDDINGS FOR TOKEN AND POSITION 197
BPE or SentencePiece). So the representation of thanks for all the might be w=
[5;4000;10532;2224]. Next we use indexing to select the corresponding rows from
E, (row 5, row 4000, row 10532, row 2224).
Another way to think about selecting token embeddings from the embedding
matrix is to represent tokens as one-hot vectors of shape [1jVj], i.e., with one
dimension for each word in the vocabulary. Recall that in a one-hot vector all the one-hot vector
elements are 0 except one, the element whose dimension is the word’s index in the
vocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,
x5=1, and xi=08i6=5, as shown here:
[0 0 0 0 1 0 0 ... 0 0 0 0]
1 2 3 4 5 6 7 ... ... |V|
Multiplying by a one-hot vector that has only one non-zero element xi=1 simply
selects out the relevant row vector for word i, resulting in the embedding for word i,
as depicted in Fig. 9.11.
E|V|d1|V|d=✕550 0 0 0 1 0 0 … 0 0 0 0 1
Figure 9.11 Selecting the embedding vector for word V5by multiplying the embedding
matrix Ewith a one-hot vector with a 1 in index 5.
We can extend this idea to represent the entire token sequence as a matrix of one-
hot vectors, one for each of the Npositions in the transformer’s context window, as
shown in Fig. 9.12.
E|V|ddN=✕|V|N0 0 0 0 0 0 0 … 0 0 1 0 0 0 0 0 1 0 0 … 0 0 0 0 1 0 0 0 0 0 0 … 0 0 0 0 0 0 0 0 1 0 0 … 0 0 0 0 …
Figure 9.12 Selecting the embedding matrix for the input sequence of token ids Wby mul-
tiplying a one-hot matrix corresponding to Wby the embedding matrix E.
These token embeddings are not position-dependent. To represent the position
of each token in the sequence, we combine these token embeddings with positional
embeddings speciﬁc to each position in an input sequence.positional
embeddings
Where do we get these positional embeddings? The simplest method, called
absolute position , is to start with randomly initialized embeddings correspondingabsolute
position
to each possible input position up to some maximum length. For example, just as
we have an embedding for the word ﬁsh, we’ll have an embedding for the position 3.
As with word embeddings, these positional embeddings are learned along with other
parameters during training. We can store them in a matrix Eposof shape [1N].
To produce an input embedding that captures positional information, we just
add the word embedding for each input to its corresponding positional embedding.
The individual token and position embeddings are both of size [1d], so their sum is

--- Page 198 ---
198 CHAPTER 9 • T HETRANSFORMER
X = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5
+++++PositionEmbeddingsWordEmbeddings
Figure 9.13 A simple way to model position: add an embedding of the absolute position to
the token embedding to produce a new embedding of the same dimensionality.
also[1d], This new embedding serves as the input for further processing. Fig. 9.13
shows the idea.
The ﬁnal representation of the input, the matrix X, is an [Nd]matrix in which
each row iis the representation of the ith token in the input, computed by adding
E[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],
the positional embedding of position i.
A potential problem with the simple absolute position embedding approach is
that there will be plenty of training examples for the initial positions in our inputs
and correspondingly fewer at the outer length limits. These latter embeddings may
be poorly trained and may not generalize well during testing. An alternative ap-
proach to absolute positional embeddings is to choose a static function that maps
integer inputs to real-valued vectors in a way that captures the inherent relation-
ships among the positions. That is, it captures the fact that position 4 in an input is
more closely related to position 5 than it is to position 17. A combination of sine
and cosine functions with differing frequencies was used in the original transformer
work. Even more complex positional embedding methods exist, such as ones that
represent relative position instead of absolute position, often implemented in the
attention mechanism at each layer rather than being added once at the initial input.
9.5 The Language Modeling Head
The last component of the transformer we must introduce is the language modeling
head . Here we are using the word head to mean the additional neural circuitry welanguage
modeling head
head add on top of the basic transformer architecture when we apply pretrained trans-
former models to various tasks. The language modeling head is the circuitry we
need to do language modeling.
Recall that language models, from the simple n-gram models of Chapter 3 through
the feedforward and RNN language models of Chapter 7 and Chapter 8, are word
predictors. Given a context of words, they assign a probability to each possible next
word. For example, if the preceding context is “Thanks for all the” and we want to
know how likely the next word is “ﬁsh” we would compute:
P(ﬁshjThanks for all the )
Language models give us the ability to assign such a conditional probability to every
possible next word, giving us a distribution over the entire vocabulary. The n-gram

--- Page 199 ---
9.5 • T HELANGUAGE MODELING HEAD 199
language models of Chapter 3 compute the probability of a word given counts of
its occurrence with the n 1 prior words. The context is thus of size n 1. For
transformer language models, the context is the size of the transformer’s context
window, which can be quite large: 2K, 4K, even 32K tokens for very large models.
The job of the language modeling head is to take the output of the ﬁnal trans-
former layer from the last token Nand use it to predict the upcoming word at posi-
tionN+1. Fig. 9.14 shows how to accomplish this task, taking the output of the last
token at the last layer (the d-dimensional output embedding of shape [1d]) and
producing a probability distribution over words (from which we will choose one to
generate).
Layer LTransformerBlockSoftmax over vocabulary VUnembedding layer…1 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|…u1u2u|V|…Language Model Headtakes hLN and outputs adistribution over vocabulary V
Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output
embedding for token Nfrom the last transformer layer ( hL
N) to a probability distribution over words in the
vocabulary V.
The ﬁrst module in Fig. 9.14 is a linear layer, whose job is to project from the
output hL
N, which represents the output token embedding at position Nfrom the ﬁnal
block L, (hence of shape [1d]) to the logit vector, or score vector, that will have a logit
single score for each of the jVjpossible words in the vocabulary V. The logit vector
uis thus of dimensionality 1 jVj.
This linear layer can be learned, but more commonly we tie this matrix to (the
transpose of) the embedding matrix E. Recall that in weight tying , we use the weight tying
same weights for two different matrices in the model. Thus at the input stage of the
transformer the embedding matrix (of shape [jVjd]) is used to map from a one-hot
vector over the vocabulary (of shape [1jVj]) to an embedding (of shape [1d]).
And then in the language model head, ET, the transpose of the embedding matrix (of
shape [djVj]) is used to map back from an embedding (shape [1d]) to a vector
over the vocabulary (shape [1 jVj]). In the learning process, Ewill be optimized to
be good at doing both of these mappings. We therefore sometimes call the transpose
ETtheunembedding layer because it is performing this reverse mapping. unembedding
A softmax layer turns the logits uinto the probabilities yover the vocabulary.
u=hL
NET(9.44)
y=softmax (u) (9.45)
We can use these probabilities to do things like help assign a probability to a
given text. But the most important usage to generate text, which we do by sampling

--- Page 200 ---
200 CHAPTER 9 • T HETRANSFORMER
a word from these probabilities y. We might sample the highest probability word
(‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec-
tion 10.2. In either case, whatever entry ykwe choose from the probability vector y,
we generate the word that has that index k.
wiSample token togenerate at position i+1
feedforwardlayer normattentionlayer norm
U
Input tokenLanguageModelingHead
InputEncoding
Ei+…logits
feedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|…Token probabilitiesu1u2u|V|…softmaxwi+1
Layer L
Figure 9.15 A transformer language model (decoder-only), stacking transformer blocks
and mapping from an input token wito to a predicted next token wi+1.
Fig. 9.15 shows the total stacked architecture for one token i. Note that the input
to each transformer layer xiis the same as the output from the preceding layer h 
i.
Now that we see all these transformer layers spread out on the page, we can point
out another useful feature of the unembedding layer: as a tool for interpretability of
the internals of the transformer that we call the logit lens (Nostalgebraist, 2020). logit lens
We can take a vector from any layer of the transformer and, pretending that it is
the preﬁnal embedding, simply multiply it by the unembedding layer to get logits,
and compute a softmax to see the distribution over words that that vector might
be representing. This can be a useful window into the internal representations of
the model. Since the network wasn’t trained to make the internal representations
function in this way, the logit lens doesn’t always work perfectly, but this can still
be a useful trick.
A terminological note before we conclude: You will sometimes see a trans-
former used for this kind of unidirectional causal language model called a decoder-
only model . This is because this model constitutes roughly half of the encoder-decoder-only
model

--- Page 201 ---
9.6 • S UMMARY 201
decoder model for transformers that we’ll see how to apply to machine translation
in Chapter 13. (Confusingly, the original introduction of the transformer had an
encoder-decoder architecture, and it was only later that the standard paradigm for
causal language model was deﬁned by using only the decoder part of this original
architecture).
9.6 Summary
This chapter has introduced the transformer and its components for the task of lan-
guage modeling. We’ll continue the task of language modeling including issues like
training and sampling in the next chapter.
Here’s a summary of the main points that we covered:
• Transformers are non-recurrent networks based on multi-head attention , a
kind of self-attention . A multi-head attention computation takes an input
vector xiand maps it to an output aiby adding in vectors from prior tokens,
weighted by how relevant they are for the processing of the current word.
• Atransformer block consists of a residual stream in which the input from
the prior layer is passed up to the next layer, with the output of different com-
ponents added to it. These components include a multi-head attention layer
followed by a feedforward layer , each preceded by layer normalizations .
Transformer blocks are stacked to make deeper and more powerful networks.
• The input to a transformer is a computing by adding an embedding (computed
with an embedding matrix ) to a positional encoding that represents the se-
quential position of the token in the window.
• Language models can be built out of stacks of transformer blocks, with a
language model head at the top, which applies an unembedding matrix to
the output Hof the top layer to generate the logits , which are then passed
through a softmax to generate word probabilities.
• Transformer-based language models have a wide context window (as wide
as 32768 tokens for very large models) allowing them to draw on enormous
amounts of context to predict upcoming words.
Bibliographical and Historical Notes
The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior
research: self-attention andmemory networks .
Encoder-decoder attention, the idea of using a soft weighting over the encodings
of input words to inform a generative decoder (see Chapter 13) was developed by
Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)
for MT. This idea was extended to self-attention by dropping the need for separate
encoding and decoding sequences and instead seeing attention as a way of weighting
the tokens in collecting information passed from lower layers to higher layers (Ling
et al., 2015; Cheng et al., 2016; Liu et al., 2016).
Other aspects of the transformer, including the terminology of key, query, and
value, came from memory networks , a mechanism for adding an external read-

--- Page 202 ---
202 CHAPTER 9 • T HETRANSFORMER
write memory to networks, by using an embedding of a query to match keys rep-
resenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,
2015; Graves et al., 2014).
MORE HISTORY TBD IN NEXT DRAFT.

--- Page 203 ---
CHAPTER
10Large Language Models
“How much do we know at any time? Much more, or so I believe, than we
know we know. ”
Agatha Christie, The Moving Finger
Fluent speakers of a language bring an enormous amount of knowledge to bear dur-
ing comprehension and production. This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage. This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.
Estimates of the size of adult vocabularies vary widely both within and across
languages. For example, estimates of the vocabulary size of young adult speakers of
American English range from 30,000 to 100,000 depending on the resources used
to make the estimate and the deﬁnition of what it means to know a word. What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling. This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage. Obvi-
ously, this leaves a very large number of words to be acquired by other means.
A simple consequence of these facts is that children have to learn about 7 to 10
words a day, every single day , to arrive at observed vocabulary levels by the time they
are 20 years of age. And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate. How do children achieve
this rate of vocabulary growth? The bulk of this knowledge acquisition seems to
happen as a by-product of reading, as part of the rich processing and reasoning that
we perform when we read. Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate. But the mechanism behind this rate of learning must
be remarkable indeed, since at some points during learning the rate of vocabulary
growth exceeds the rate at which new words are appearing to the learner!
Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with). The distributional hypothe-
sis suggests both that we can acquire remarkable amounts of knowledge from text,
and that this knowledge can be brought to bear long after its initial acquisition. Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.
In this chapter we formalize this idea of pretraining —learning knowledge about pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models . Large language models exhibit remark-

--- Page 204 ---
204 CHAPTER 10 • L ARGE LANGUAGE MODELS
able performance on all sorts of natural language tasks because of the knowledge
they learn in pretraining, and they will play a role throughout the rest of this book.
They have been especially transformative for tasks where we need to produce text,
like summarization, machine translation, question answering, or chatbots.
We’ll start by seeing how to apply the transformer of Chapter 9 to language
modeling, in a setting often called causal or autoregressive language models, in
which we iteratively predict words left-to-right from earlier words. We’ll ﬁrst in-
troduce training, seeing how language models are self-trained by iteratively being
taught to guess the next word in the text from the prior words.
We’ll then talk about the process of text generation. The application of LLMs
to generate text has vastly broadened the scope of NLP,. Text generation, code-
generation, and image-generation together constitute the important new area of gen-
erative AI . We’ll introduce speciﬁc algorithms for generating text from a language generative AI
model, like greedy decoding andsampling . And we’ll see that almost any NLP
task can be modeled as word prediction in a large language model, if we think about
it in the right way. We’ll work through an example of using large language mod-
els to solve one classic NLP task of summarization (generating a short text that
summarizes some larger document).
10.1 Large Language Models with Transformers
The prior chapter introduced most of the components of a transformer in the domain
of language modeling: the transformer block including multi-head attention , the
language modeling head , and the positional encoding of the input. In the following
sections we’ll introduce the remaining aspects of the transformer LLM: sampling
andtraining . Before we do that, we use this section to talk about why and how we
apply transformer-based large language models to NLP tasks.
The tasks we will describe are all cases of conditional generation . Conditionalconditional
generation
generation is the task of generating text conditioned on an input piece of text. That
is, we give the LLM an input piece of text, generally called a prompt , and then have
the LLM continue generating text token by token, conditioned on the prompt. The
fact that transformers have such long contexts (many thousands of tokens) makes
them very powerful for conditional generation, because they can look back so far
into the prompting text.
Consider the simple task of text completion, illustrated in Fig. 10.1. Here a
language model is given a text preﬁx and is asked to generate a possible completion.
Note that as the generation process proceeds, the model has direct access to the
priming context as well as to all of its own subsequently generated outputs (at least
as much as ﬁts in the large context window). This ability to incorporate the entirety
of the earlier context and generated outputs at each time step is the key to the power
of large language models built from transformers.
So why should we care about predicting upcoming words or tokens? The in-
sight of large language modeling is that many practical NLP tasks can be cast as
word prediction , and that a powerful-enough language model can solve them with
a high degree of accuracy. For example, we can cast sentiment analysis as language
modeling by giving a language model a context like:
The sentiment of the sentence ``I like Jackie Chan" is:
and comparing the following conditional probability of the words “positive” and the

--- Page 205 ---
10.1 • L ARGE LANGUAGE MODELS WITH TRANSFORMERS 205
Preﬁx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the…UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+…
Figure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language
models. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.
word “negative” to see which is higher:
P(positivejThe sentiment of the sentence ``I like Jackie Chan" is: )
P(negativejThe sentiment of the sentence ``I like Jackie Chan" is: )
If the word “positive” is more probable, we say the sentiment of the sentence is
positive, otherwise we say the sentiment is negative.
We can also cast more complex tasks as word prediction. Consider question
answering, in which the system is given a question (for example a question with
a simple factual answer) and must give a textual answer; we introduce this task in
detail in Chapter 14. We can cast the task of question answering as word prediction
by giving a language model a question and a token like A:suggesting that an answer
should come next:
Q: Who wrote the book ``The Origin of Species"? A:
If we ask a language model to compute the probability distribution over possible
next words given this preﬁx:
P(wjQ: Who wrote the book ``The Origin of Species"? A: )
and look at which words whave high probabilities, we might expect to see that
Charles is very likely, and then if we choose Charles and continue and ask
P(wjQ: Who wrote the book ``The Origin of Species"? A: Charles )
we might now see that Darwin is the most probable token, and select it.
Conditional generation can even be used to accomplish tasks that must generate
longer responses. Consider the task of text summarization , which is to take a longtext
summarization
text, such as a full-length article, and produce an effective shorter summary of it. We
can cast summarization as language modeling by giving a large language model a
text, and follow the text by a token like tl;dr ; this token is short for something like

--- Page 206 ---
206 CHAPTER 10 • L ARGE LANGUAGE MODELS
‘too long; didn’t read’ and in recent years people often use this token, especially in
informal work emails, when they are going to give a short summary. Since this token
is sufﬁciently frequent in language model training data, language models have seen
many texts in which the token occurs before a summary, and hence will interpret the
token as instructions to generate a summary. We can then do conditional generation:
give the language model this preﬁx, and then have it generate the following words,
one by one, and take the entire response as a summary. Fig. 10.2 shows an example
of a text and a human-produced summary from a widely-used summarization corpus
consisting of CNN and Daily Mirror news articles.
Original Article
The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff
and offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says.
But not if you live in New England or surrounding states. “We will not ship snow to any states
in the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging
snow!”
His website and social media accounts claim to have ﬁlled more than 133 orders for snow – more
than 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a
record this winter for the snowiest month in its history. Most residents see the huge piles of snow
choking their yards and sidewalks as a nuisance, but Waring saw an opportunity.
According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-
eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He
joked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]
Summary
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.
Figure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus
(Hermann et al., 2015b), (Nallapati et al., 2016).
If we take this full article and append the token tl;dr , we can use this as the con-
text to prime the generation process to produce a summary as illustrated in Fig. 10.3.
Again, what makes transformers able to succeed at this task (as compared, say, to
the primitive n-gram language model) is that attention can incorporate information
from the large context window, giving the model access to the original article as well
as to the newly generated text throughout the process.
Which words do we generate at each step? One simple way to generate words
is to always generate the most likely word given the context. Generating the most
likely word given the context is called greedy decoding . A greedy algorithm is onegreedy
decoding
that make a choice that is locally optimal, whether or not it will turn out to have
been the best choice with hindsight. Thus in greedy decoding, at each time step in
generation, the output ytis chosen by computing the probability for each possible
output (every word in the vocabulary) and then choosing the highest probability
word (the argmax):
ˆwt=argmaxw2VP(wjw<t) (10.1)
In practice, however, we don’t use greedy decoding with large language models.
A major problem with greedy decoding is that because the words it chooses are (by
deﬁnition) extremely predictable, the resulting text is generic and often quite repeti-
tive. Indeed, greedy decoding is so predictable that it is deterministic; if the context

--- Page 207 ---
10.2 • S AMPLING FOR LLM G ENERATION 207
Original StoryGenerated Summary
…ideaKyle
wasborn.KyleWaring
WaringonlyThe…will
DelimiterwillUUU
tl;drLM Head
E
E
E
E
E
E
E
E…
Figure 10.3 Summarization with large language models using the tl;dr token and context-based autore-
gressive generation.
is identical, and the probabilistic model is the same, greedy decoding will always re-
sult in generating exactly the same string. We’ll see in Chapter 13 that an extension
to greedy decoding called beam search works well in tasks like machine translation,
which are very constrained in that we are always generating a text in one language
conditioned on a very speciﬁc text in another language. In most other tasks, how-
ever, people prefer text which has been generated by more sophisticated methods,
called sampling methods , that introduce a bit more diversity into the generations.
We’ll see how to do that in the next few sections.
10.2 Sampling for LLM Generation
The core of the generation process for large language models is the task of choosing
the single word to generate next based on the context and based on the probabilities
that the model assigns to possible words. This task of choosing a word to generate
based on the model’s probabilities is called decoding . Decoding from a language decoding
model in a left-to-right manner (or right-to-left for languages like Arabic in which
we read from right to left), and thus repeatedly choosing the next word conditioned
on our previous choices is called autoregressive generation orcausal LM genera-autoregressive
generation
tion.1(As we’ll see, alternatives like the masked language models of Chapter 11 are
non-causal because they can predict words based on both past and future words).
The most common method for decoding in large language models is sampling .
Recall from Chapter 3 that sampling from a model’s distribution over words means sampling
to choose random words according to their probability assigned by the model. That
is, we iteratively choose a word to generate according to its probability in context
1Technically an autoregressive model predicts a value at time tbased on a linear function of the values
at times t 1,t 2, and so on. Although language models are not linear (since they have many layers of
non-linearities), we loosely refer to this generation technique as autoregressive since the word generated
at each time step is conditioned on the word selected by the network from the previous step.

--- Page 208 ---
208 CHAPTER 10 • L ARGE LANGUAGE MODELS
as deﬁned by the model. Thus we are more likely to generate words that the model
thinks have a high probability in the context and less likely to generate words that
the model thinks have a low probability.
We saw back in Chapter 3 on page 43 how to generate text from a unigram lan-
guage model , by repeatedly randomly sampling words according to their probability
until we either reach a pre-determined length or select the end-of-sentence token. To
generate text from a trained transformer language model we’ll just generalize this
model a bit: at each step we’ll sample words according to their probability condi-
tioned on our previous choices , and we’ll use a transformer language model as the
probability model that tells us this probability.
We can formalize this algorithm for generating a sequence of words W=w1;w2;:::; wN
until we hit the end-of-sequence token, using xp(x)to mean ‘choose xby sam-
pling from the distribution p(x):
i 1
wip(w)
while wi!= EOS
i i + 1
wip(wijw<i)
The algorithm above is called random sampling , and it turns out random sam-random
sampling
pling doesn’t work well enough. The problem is that even though random sampling
is mostly going to generate sensible, high-probable words, there are many odd, low-
probability words in the tail of the distribution, and even though each one is low-
probability, if you add up all the rare words, they constitute a large enough portion
of the distribution that they get chosen often enough to result in generating weird
sentences. For this reason, instead of random sampling, we usually use sampling
methods that avoid generating the very unlikely words.
The sampling methods we introduce below each have parameters that enable
trading off two important factors in generation: quality anddiversity . Methods
that emphasize the most probable words tend to produce generations that are rated
by people as more accurate, more coherent, and more factual, but also more boring
and more repetitive. Methods that give a bit more weight to the middle-probability
words tend to be more creative and more diverse, but less factual and more likely to
be incoherent or otherwise low-quality.
10.2.1 Top- ksampling
Top-k sampling is a simple generalization of greedy decoding. Instead of choosing top-k sampling
the single most probable word to generate, we ﬁrst truncate the distribution to the
topkmost likely words, renormalize to produce a legitimate probability distribution,
and then randomly sample from within these kwords according to their renormalized
probabilities. More formally:
1. Choose in advance a number of words k
2. For each word in the vocabulary V, use the language model to compute the
likelihood of this word given the context p(wtjw<t)
3. Sort the words by their likelihood, and throw away any word that is not one of
the top kmost probable words.
4. Renormalize the scores of the kwords to be a legitimate probability distribu-
tion.

--- Page 209 ---
10.2 • S AMPLING FOR LLM G ENERATION 209
5. Randomly sample a word from within these remaining kmost-probable words
according to its probability.
When k=1, top- ksampling is identical to greedy decoding. Setting kto a larger
number than 1 leads us to sometimes select a word which is not necessarily the most
probable, but is still probable enough, and whose choice results in generating more
diverse but still high-enough-quality text.
10.2.2 Nucleus or top- psampling
One problem with top- ksampling is that kis ﬁxed, but the shape of the probability
distribution over words differs in different contexts. If we set k=10, sometimes
the top 10 words will be very likely and include most of the probability mass, but
other times the probability distribution will be ﬂatter and the top 10 words will only
include a small part of the probability mass.
An alternative, called top-p sampling ornucleus sampling (Holtzman et al., top-p sampling
2020), is to keep not the top kwords, but the top ppercent of the probability mass.
The goal is the same; to truncate the distribution to remove the very unlikely words.
But by measuring probability rather than the number of words, the hope is that the
measure will be more robust in very different contexts, dynamically increasing and
decreasing the pool of word candidates.
Given a distribution P(wtjw<t), the top- pvocabulary V(p)is the smallest set of
words such that
X
w2V(p)P(wjw<t)p: (10.2)
10.2.3 Temperature sampling
Intemperature sampling , we don’t truncate the distribution, but instead reshapetemperature
sampling
it. The intuition for temperature sampling comes from thermodynamics, where a
system at a high temperature is very ﬂexible and can explore many possible states,
while a system at a lower temperature is likely to explore a subset of lower energy
(better) states. In low-temperature sampling, we smoothly increase the probability
of the most probable words and decrease the probability of the rare words.
We implement this intuition by simply dividing the logit by a temperature param-
etertbefore we normalize it by passing it through the softmax. In low-temperature
sampling, t2(0;1]. Thus instead of computing the probability distribution over the
vocabulary directly from the logit as in the following (repeated from (9.45)):
y=softmax (u) (10.3)
we instead ﬁrst divide the logits by t, computing the probability vector yas
y=softmax (u=t) (10.4)
Why does this work? When tis close to 1 the distribution doesn’t change much.
But the lower tis, the larger the scores being passed to the softmax (dividing by a
smaller fraction t1 results in making each score larger). Recall that one of the
useful properties of a softmax is that it tends to push high values toward 1 and low
values toward 0. Thus when larger numbers are passed to a softmax the result is
a distribution with increased probabilities of the most high-probability words and
decreased probabilities of the low probability words, making the distribution more
greedy. As tapproaches 0 the probability of the most likely word approaches 1.

--- Page 210 ---
210 CHAPTER 10 • L ARGE LANGUAGE MODELS
Note, by the way, that there can be other situations where we may want to do
something quite different and ﬂatten the word probability distribution instead of
making it greedy. Temperature sampling can help with this situation too, in this case
high-temperature sampling, in which case we use t>1.
10.3 Pretraining Large Language Models
How do we teach a transformer to be a language model? What is the algorithm and
what data do we train on?
10.3.1 Self-supervised training algorithm
To train a transformer as a language model, we use the same self-supervision (or self-supervision
self-training ) algorithm we saw in Section 8.2.2: we take a corpus of text as training
material and at each time step task the model to predict the next word. We call such
a model self-supervised because we don’t have to add any special gold labels to
the data; the natural sequence of words is its own supervision! We simply train the
model to minimize the error in predicting the true next word in the training sequence,
using cross-entropy as the loss function.
Recall that the cross-entropy loss measures the difference between a predicted
probability distribution and the correct distribution.
LCE= X
w2Vyt[w]logˆyt[w] (10.5)
In the case of language modeling, the correct distribution ytcomes from knowing the
next word. This is represented as a one-hot vector corresponding to the vocabulary
where the entry for the actual next word is 1, and all the other entries are 0. Thus,
the cross-entropy loss for language modeling is determined by the probability the
model assigns to the correct next word (all other words get multiplied by zero). So
at time tthe CE loss in (10.5) can be simpliﬁed as the negative log probability the
model assigns to the next word in the training sequence.
LCE(ˆyt;yt) = logˆyt[wt+1] (10.6)
Thus at each word position tof the input, the model takes as input the correct se-
quence of tokens w1:t, and uses them to compute a probability distribution over
possible next words so as to compute the model’s loss for the next token wt+1. Then
we move to the next word, we ignore what the model predicted for the next word
and instead use the correct sequence of tokens w1:t+1to estimate the probability of
token wt+2. This idea that we always give the model the correct history sequence to
predict the next word (rather than feeding the model its best case from the previous
time step) is called teacher forcing . teacher forcing
Fig. 10.4 illustrates the general training approach. At each step, given all the
preceding words, the ﬁnal transformer layer produces an output distribution over
the entire vocabulary. During training, the probability assigned to the correct word
is used to calculate the cross-entropy loss for each item in the sequence. The loss
for a training sequence is the average cross-entropy loss over the entire sequence.
The weights in the network are adjusted to minimize the average CE loss over the
training sequence via gradient descent.

--- Page 211 ---
10.3 • P RETRAINING LARGE LANGUAGE MODELS 211
longandthanksforNext tokenallLoss…=
<latexit sha1_base64="AovqpaL476UmJ1EU1xZPgDZ70tQ=">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit> logyand
StackedTransformerBlocksSolongandthanksfor……
…
U
Input tokensx1x2LanguageModelingHeadx3x4x5InputEncoding
E1+
E2+
E3+
E4+
E5+……………
U
U
U
U…logitslogitslogitslogitslogits…
<latexit sha1_base64="q3ZgXDyG7qtkT7t8hT47RdlwYG4=">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit> logythanks
Figure 10.4 Training a transformer as a language model.
Note the key difference between this ﬁgure and the earlier RNN-based version
shown in Fig. 8.6. There the calculation of the outputs and the losses at each step
was inherently serial given the recurrence in the calculation of the hidden states.
With transformers, each training item can be processed in parallel since the output
for each element in the sequence is computed separately.
Large models are generally trained by ﬁlling the full context window (for exam-
ple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter
than this, multiple documents are packed into the window with a special end-of-text
token between them. The batch size for gradient descent is usually quite large (the
largest GPT-3 model uses a batch size of 3.2 million tokens).
10.3.2 Training corpora for large language models
Large language models are mainly trained on text scraped from the web, augmented
by more carefully curated data. Because these training corpora are so large, they are
likely to contain many natural examples that can be helpful for NLP tasks, such as
question and answer pairs (for example from FAQ lists), translations of sentences
between various languages, documents together with their summaries, and so on.
Web text is usually taken from corpora of automatically-crawled web pages like
thecommon crawl , a series of snapshots of the entire web produced by the non- common crawl
proﬁt Common Crawl ( https://commoncrawl.org/ ) that each have billions of
webpages. Various versions of common crawl data exist, such as the Colossal Clean
Crawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English
that is ﬁltered in various ways (deduplicated, removing non-natural language like
code, sentences with offensive words from a blocklist). This C4 corpus seems to
consist in large part of patent text documents, Wikipedia, and news sites (Dodge
et al., 2021).
Wikipedia plays a role in lots of language model training, as do corpora of books.
The Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by The Pile
publicly released code, containing again a large amount of text scraped from the web

--- Page 212 ---
212 CHAPTER 10 • L ARGE LANGUAGE MODELS
as well as books and Wikipedia; Fig. 10.5 shows its composition. Dolma is a larger
open corpus of English, created with public tools, containing three trillion tokens,
which similarly consists of web text, academic papers, code, books, encyclopedic
materials, and social media (Soldaini et al., 2024).
Figure 1: Treemap of Pile components by effective size.
troduce a new ﬁltered subset of Common Crawl,
Pile-CC, with improved extraction quality.
Through our analyses, we conﬁrm that the Pile is
signiﬁcantly distinct from pure Common Crawl
data. Additionally, our evaluations show that the
existing GPT-2 and GPT-3 models perform poorly
on many components of the Pile, and that models
trained on the Pile signiﬁcantly outperform both
raw and ﬁltered Common Crawl models. To com-
plement the performance evaluations, we also per-
form an exploratory analysis of the text within the
Pile to provide a detailed picture of the data. We
hope that our extensive documentation of the con-
struction and characteristics of the Pile will help
researchers make informed decisions about poten-
tial downstream applications.
Finally, we make publicly available the preprocess-
ing code for the constituent datasets of the Pile and
the code for constructing alternative versions2. In
the interest of reproducibility, we also document
all processing performed on each dataset (and the
Pile as a whole) in as much detail as possible. For
further details about the processing of each dataset,
see Section 2and Appendix C.
2https://github.com/EleutherAI/
the-pile1.1 Contributions
The core contributions of this paper are:
1.The introduction of a 825.18GiB english-
language dataset for language modeling com-
bining 22 diverse sources.
2.The introduction of 14new language model-
ing datasets, which we expect to be of inde-
pendent interest to researchers.
3.Evaluations demonstrating signiﬁcant im-
provements across many domains by GPT-2-
sized models trained on this new dataset, com-
pared to training on CC-100 and raw Common
Crawl.
4.The investigation and documentation of this
dataset, which we hope will better inform re-
searchers about how to use it as well as moti-
vate them to undertake similar investigations
of their own data.
2 The Pile Datasets
The Pile is composed of 22 constituent sub-datasets,
as shown in Table 1. Following Brown et al. (2020 ),
we increase the weights of higher quality compo-
nents, with certain high-quality datasets such as
Wikipedia being seen up to 3 times (“epochs”) for
2
Figure 10.5 The Pile corpus, showing the size of different components, color coded as
academic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in-
cluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),
dialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).
Filtering for quality and safety Pretraining data drawn from the web is ﬁltered
for both quality andsafety . Quality ﬁlters are classiﬁers that assign a score to each
document. Quality is of course subjective, so different quality ﬁlters are trained
in different ways, but often to value high-quality reference corpora like Wikipedia,
books, and particular websites and to avoid websites with lots of PII(Personal Iden- PII
tiﬁable Information) or adult content. Filters also remove boilerplate text which is
very frequent on the web. Another kind of quality ﬁltering is deduplication, which
can be done at various levels, so as to remove duplicate documents, duplicate web
pages, or duplicate text. Quality ﬁltering generally improves language model per-
formance (Longpre et al., 2024b; Llama Team, 2024).
Safety ﬁltering is again a subjective decision, and often includes toxicity detec-
tion based on running off-the-shelf toxicity classiﬁers. This can have mixed results.
One problem is that current toxicity classiﬁers mistakenly ﬂag non-toxic data if it
is generated by speakers of minority dialects like African American English (Xu
et al., 2021). Another problem is that models trained on toxicity-ﬁltered data, while
somewhat less toxic, are also worse at detecting toxicity themselves (Longpre et al.,
2024b). These issues make the question of how to do better safety ﬁltering an im-
portant open problem.
Using large datasets scraped from the web to train language models poses ethical
and legal questions:
Copyright: Much of the text in these large datasets (like the collections of ﬁc-
tion and non-ﬁction books) is copyrighted. In some countries, like the United
States, the fair use doctrine may allow copyrighted content to be used for
transformative uses, but it’s not clear if that remains true if the language mod-
els are used to generate text that competes with the market for the text they

--- Page 213 ---
10.3 • P RETRAINING LARGE LANGUAGE MODELS 213
are trained on (Henderson et al., 2023).
Data consent: Owners of websites can indicate that they don’t want their sites
to be crawled by web crawlers (either via a robots.txt ﬁle, or via Terms of
Service). Recently there has been a sharp increase in the number of web-
sites that have indicated that they don’t want large language model builders
crawling their sites for training data (Longpre et al., 2024a). Because it’s not
clear what legal status these indications have in different countries, or whether
these restrictions are retroactive, what effect this will have on large pretraining
datasets is unclear.
Privacy: Large web datasets also have privacy issues since they contain private
information like phone numbers and IP addresses. While ﬁlters are used to try
to remove websites likely to contain large amounts of personal information,
such ﬁltering isn’t sufﬁcient.
10.3.3 Finetuning
Although the enormous pretraining data for a large language model includes text
from many domains, it’s often the case that we want to apply it in a new domain or
task that might not have appeared sufﬁciently in the pre-training data. For example,
we might want a language model that’s specialized to legal or medical text. Or we
might have a multilingual language model that knows many languages but might
beneﬁt from some more data in our particular language of interest. Or we want a
language model that is specialized to a particular task.
In such cases, we can simply continue training the model on relevant data from
the new domain or language (Gururangan et al., 2020). This process of taking a fully
pretrained model and running additional training passes on some new data is called
ﬁnetuning . Fig. 10.6 sketches the paradigm. ﬁnetuning
Fine-tuning Data
Pretraining DataPretraining
…
…
…Fine-tuning
…
…
…Pretrained LMFine-tuned LM
Figure 10.6 Pretraining and ﬁnetuning. A pre-trained model can be ﬁnetuned to a par-
ticular domain, dataset, or task. There are many different ways to ﬁnetune, depending on
exactly which parameters are updated from the ﬁnetuning data: all the parameters, some of
the parameters, or only the parameters of speciﬁc extra circuitry.
We’ll introduce four related kinds of ﬁnetuning in this chapter and the two fol-
lowing chapters. In all four cases, ﬁnetuning means the process of taking a pre-
trained model and further adapting some or all of its parameters to some new data.
But they differ on exactly which parameters get updated.
In the ﬁrst kind of ﬁnetuning we retrain all the parameters of the model on this
new data, using the same method (word prediction) and loss function (cross-entropy
loss) as for pretraining. In a sense it’s as if the new data were at the tail end of

--- Page 214 ---
214 CHAPTER 10 • L ARGE LANGUAGE MODELS
the pretraining data, and so you’ll sometimes see this method called continued pre-
training .continued
pretraining
Retraining all the parameters of the model is very slow and expensive when the
language model is huge. So instead we can freeze some of the parameters (i.e., leave freeze
them unchanged from their pretrained value) and train only a subset of parameters
on the new data. In Section 10.5.3 we’ll describe this second variety of ﬁnetun-
ing, called parameter-efﬁcient ﬁnetuning , orPEFT . because we efﬁciently select
speciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained
values.
In Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.
In this version, the goal is to use a language model as a kind of classiﬁer or labeler
for a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.
We do this by adding extra neural circuitry (an extra head ) after the top layer of the
model. This classiﬁcation head takes as input some of the top layer embeddings of
the transformer and produces as output a classiﬁcation. In this method, most com-
monly used with masked language models like BERT, we freeze the entire pretrained
model and only train the classiﬁcation head on some new data, usually labeled with
some class that we want to predict.
Finally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-
cial component of the largest language models: supervised ﬁnetuning orSFT. SFT
is often used for instruction ﬁnetuning , in which we want a pretrained language
model to learn to follow text instructions, for example to answer questions or follow
a command to write something. Here we create a dataset of prompts and desired
responses (for example questions and their answers, or commands and their ful-
ﬁllments), and we train the language model using the normal cross-entropy loss to
predict each token in the instruction prompt iteratively, essentially training it to pro-
duce the desired response from the command in the prompt. It’s called supervised
because unlike in pretraining, where we just take any data and predict the words in
it, we build the special ﬁnetuning dataset by hand, creating supervised responses to
each command.
Often everything that happens after pretraining is lumped together as post-training ;
we’ll discuss the various parts of post-training in Chapter 12.
10.4 Evaluating Large Language Models
Perplexity As we ﬁrst saw in Chapter 3, one way to evaluate language models is
to measure how well they predict unseen text. Intuitively, good models are those that
assign higher probabilities to unseen data (are less surprised when encountering the
new words).
We instantiate this intuition by using perplexity to measure the quality of a perplexity
language model. Recall from page 39 that the perplexity of a model qon an unseen
test set is the inverse probability that qassigns to the test set, normalized by the test
set length. For a test set of ntokens w1:n, the perplexity is
Perplexityq(w1:n) = Pq(w1:n) 1
n
=ns
1
Pq(w1:n)(10.7)
To visualize how perplexity can be computed as a function of the probabilities the

--- Page 215 ---
10.4 • E VALUATING LARGE LANGUAGE MODELS 215
LM computes for each new word, we can use the chain rule to expand the computa-
tion of probability of the test set:
Perplexityq(w1:n) =nvuutnY
i=11
Pq(wijw<i)(10.8)
Note that because of the inverse in Eq. 10.7, the higher the probability of the word
sequence, the lower the perplexity. Thus the the lower the perplexity of a model on
the data, the better the model . Minimizing perplexity is equivalent to maximizing
the test set probability according to the language model.
One caveat: because perplexity depends on the length of a text, it is very sensitive
to differences in the tokenization algorithm. That means that it’s hard to exactly
compare perplexities produced by two language models if they have very different
tokenizers. For this reason perplexity is best used when comparing language models
that use the same tokenizer.
Other factors While the predictive accuracy of a language model, as measured by
perplexity, is a very useful metric, we also care about different kinds of accuracy, for
the downstream tasks we apply our language model to. For each task like machine
translation, summarization, question answering, speech recognition, and dialogue,
we can measure the accuracy at those tasks. Future chapters will introduce task-
speciﬁc metrics that allow us to evaluate how accuracy or correct language models
are at these downstream tasks.
But when evaluating models we also care about factors besides any of these
kinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020). For example,
we often care about how a big a model is, and how long it takes to train or do
inference. This can matter because we have constraints on time either for training
or at inference. Or we may have constraints on memory, since the GPUs we run
our models on have ﬁxed memory sizes. Big models also use more energy, and we
prefer models that use less energy, both to reduce the environmental impact of the
model and to reduce the ﬁnancial cost of building or deploying it. We can target
our evaluation to these factors by measuring performance normalized to a giving
compute or memory budget. We can also directly measure the energy usage of our
model in kWh or in kilograms of CO 2emitted (Strubell et al., 2019; Henderson
et al., 2020; Liang et al., 2023).
Another feature that a language model evaluation can measure is fairness. We
know that language models are biased, exhibiting gendered and racial stereotypes,
or decreased performance for language from or about certain demographics groups.
There are language model evaluation benchmarks that measure the strength of these
biases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman
et al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want
language models whose performance is equally fair to different groups. For exam-
ple, we could chose an evaluation that is fair in a Rawlsian sense by maximizing the
welfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa et al.,
2020).
Finally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021)
and general evaluation protocols like HELM (Liang et al., 2023); we will return to
these in later chapters when we introduce evaluation metrics for speciﬁc tasks like
question answering and information retrieval.

--- Page 216 ---
216 CHAPTER 10 • L ARGE LANGUAGE MODELS
10.5 Dealing with Scale
Large language models are large. For example the Llama 3.1 405B Instruct model
from Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,
128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team,
2024), using a vocabulary of 128K tokens. So there is a lot of research on un-
derstanding how LLMs scale, and especially how to implement them given limited
resources. In the next few sections we discuss how to think about scale (the concept
ofscaling laws ), and important techniques for getting language models to work
efﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.
10.5.1 Scaling laws
The performance of large language models has shown to be mainly determined by
3 factors: model size (the number of parameters not counting embeddings), dataset
size (the amount of training data), and the amount of compute used for training. That
is, we can improve a model by adding parameters (adding more layers or having
wider contexts or both), by training on more data, or by training for more iterations.
The relationships between these factors and performance are known as scaling
laws . Roughly speaking, the performance of a large language model (the loss) scales scaling laws
as a power-law with each of these three properties of model training.
For example, Kaplan et al. (2020) found the following three relationships for
lossLas a function of the number of non-embedding parameters N, the dataset size
D, and the compute budget C, for models training with limited parameters, dataset,
or compute budget, if in each case the other two properties are held constant:
L(N) =Nc
NaN
(10.9)
L(D) =Dc
DaD
(10.10)
L(C) =Cc
CaC
(10.11)
The number of (non-embedding) parameters Ncan be roughly computed as fol-
lows (ignoring biases, and with das the input and output dimensionality of the
model, dattnas the self-attention layer size, and dffthe size of the feedforward layer):
N2d nlayer(2dattn+dff)
12nlayerd2(10.12)
(assuming dattn=dff=4=d)
Thus GPT-3, with n=96 layers and dimensionality d=12288, has 1296
122882175 billion parameters.
The values of Nc,Dc,Cc,aN,aD, and aCdepend on the exact transformer
architecture, tokenization, and vocabulary size, so rather than all the precise values,
scaling laws focus on the relationship with loss.2
Scaling laws can be useful in deciding how to train a model to a particular per-
formance, for example by looking at early in the training curve, or performance with
2For the initial experiment in Kaplan et al. (2020) the precise values were aN= 0.076, Nc= 8.81013
(parameters), aD= 0.095, Dc= 5.41013(tokens), aC= 0.050, Cc= 3.1108(petaﬂop-days).

--- Page 217 ---
10.5 • D EALING WITH SCALE 217
smaller amounts of data, to predict what the loss would be if we were to add more
data or increase model size. Other aspects of scaling laws can also tell us how much
data we need to add when scaling up a model.
10.5.2 KV Cache
We saw in Fig. 9.10 and in Eq. 9.32 (repeated below) how the attention vector can
be very efﬁciently computed in parallel for training, via two matrix multiplications:
A=softmaxQK|
pdk
V (10.13)
Unfortunately we can’t do quite the same efﬁcient computation in inference as
in training. That’s because at inference time, we iteratively generate the next tokens
one at a time. For a new token that we have just generated, call it xi, we need to
compute its query, key, and values by multiplying by WQ,WK, and WVrespec-
tively. But it would be a waste of computation time to recompute the key and value
vectors for all the prior tokens x<i; at prior steps we already computed these key
and value vectors! So instead of recomputing these, whenever we compute the key
and value vectors we store them in memory in the KV cache , and then we can just KV cache
grab them from the cache when we need them. Fig. 10.7 modiﬁes Fig. 9.10 to show
the computation that takes place for a single new token, showing which values we
can take from the cache rather than recompute.
q4k1k2k4QKTQKTv1v2v3v4V
q4•k1q4•k2q4•k3q4•k4x==xa4A
1 x dkdk x N1 x NN x dv1 x dv
k3
Figure 10.7 Parts of the attention computation (extracted from Fig. 9.10) showing, in black,
the vectors that can be stored in the cache rather than recomputed when computing the atten-
tion score for the 4th token.
10.5.3 Parameter Efﬁcient Fine Tuning
As we mentioned above, it’s very common to take a language model and give it more
information about a new domain by ﬁnetuning it (continuing to train it to predict
upcoming words) on some additional data.
Fine-tuning can be very difﬁcult with very large language models, because there
are enormous numbers of parameters to train; each pass of batch gradient descent
has to backpropagate through many many huge layers. This makes ﬁnetuning huge
language models extremely expensive in processing power, in memory, and in time.
For this reason, there are alternative methods that allow a model to be ﬁnetuned
without changing all the parameters. Such methods are called parameter-efﬁcient
ﬁne tuning or sometimes PEFT , because we efﬁciently select a subset of parametersparameter-
efﬁcient ﬁne
tuning
PEFT to update when ﬁnetuning. For example we freeze some of the parameters (don’t
change them), and only update some particular subset of parameters.

--- Page 218 ---
218 CHAPTER 10 • L ARGE LANGUAGE MODELS
Here we describe one such model, called LoRA , forLow-RankAdaptation. The LoRA
intuition of LoRA is that transformers have many dense layers which perform matrix
multiplication (for example the WQ,WK,WV,WOlayers in the attention computa-
tion). Instead of updating these layers during ﬁnetuning, with LoRA we freeze these
layers and instead update a low-rank approximation that has fewer parameters.
Consider a matrix Wof dimensionality [Nd]that needs to be updated during
ﬁnetuning via gradient descent. Normally this matrix would get updates DWof
dimensionality [Nd], for updating the Ndparameters after gradient descent. In
LoRA, we freeze Wand update instead a low-rank decomposition of W. We create
two matrices AandB, where Ahas size [Nr]andBhas size [rd], and we choose
rto be quite small, r<<min(d;N). During ﬁnetuning we update AandBinstead
ofW. That is, we replace W+DWwithW+BA. Fig. 10.8 shows the intuition.
For replacing the forward pass h=xW, the new forward pass is instead:
h=xW+xAB (10.14)
hPretrained WeightsWdkrkABrxd11k
d×
Figure 10.8 The intuition of LoRA. We freeze Wto its pretrained values, and instead ﬁne-
tune by training a pair of matrices AandB, updating those instead of W, and just sum Wand
the updated AB.
LoRA has a number of advantages. It dramatically reduces hardware require-
ments, since gradients don’t have to be calculated for most parameters. The weight
updates can be simply added in to the pretrained weights, since BAis the same size
asW). That means it doesn’t add any time during inference. And it also means it’s
possible to build LoRA modules for different domains and just swap them in and
out by adding them in or subtracting them from W.
In its original version LoRA was applied just to the matrices in the attention
computation (the WQ,WK,WV, and WOlayers). Many variants of LoRA exist.

--- Page 219 ---
10.6 • P OTENTIAL HARMS FROM LANGUAGE MODELS 219
10.6 Potential Harms from Language Models
Large pretrained neural language models exhibit many of the potential harms dis-
cussed in Chapter 4 and Chapter 6. Many of these harms become realized when
pretrained language models are used for any downstream task, particularly those
involving text generation, whether question answering, machine translation, or in
assistive technologies like writing aids or web search query completion, or predic-
tive typing for email (Olteanu et al., 2020).
For example, language models are prone to saying things that are false, a prob-
lem called hallucination . Language models are trained to generate text that is pre- hallucination
dictable and coherent, but the training algorithms we have seen so far don’t have
any way to enforce that the text that is generated is correct or true. This causes
enormous problems for any application where the facts matter! We’ll return to this
issue in Chapter 14 where we introduce proposed mitigation methods like retrieval
augmented generation .
A second source of harm is that language models can generate toxic language . toxic language
Gehman et al. (2020) show that even completely non-toxic prompts can lead large
language models to output hate speech and abuse their users. Language models also
generate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020;
Sheng et al., 2019) about many demographic groups.
One source of biases is the training data. Gehman et al. (2020) shows that large
language model training datasets include toxic text scraped from banned sites. There
are other biases than toxicity: the training data is disproportionately generated by
authors from the US and from developed countries. Such biased population samples
likely skew the resulting generation toward the perspectives or topics of this group
alone. Furthermore, language models can amplify demographic and other biases in
training data, just as we saw for embedding models in Chapter 6.
Datasets can be another source of harms. We already saw in Section 10.3.2
that using pretraining corpora scraped from the web can lead to harms related to
copyright and data consent. We also mentioned that pretraining data can tend to
have private information like phone numbers and addresses. This is problematic
because large language models can leak information from their training data. That
is, an adversary can extract training-data text from a language model such as a per-
son’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2021).
This becomes even more problematic when large language models are trained on
extremely sensitive private datasets such as electronic health records.
Language models can also be used by malicious actors for generating text for
misinformation , phishing, or other socially harmful activities (Brown et al., 2020).
McGufﬁe and Newhouse (2020) show how large language models generate text that
emulates online extremists, with the risk of amplifying extremist movements and
their attempt to radicalize and recruit.
Finding ways to mitigate all these harms is an important current research area in
NLP. At the very least, carefully analyzing the data used to pretrain large language
models is important as a way of understanding issues of toxicity, bias, privacy, and
fair use, making it extremely important that language models include datasheets
(page 16) or model cards (page 74) giving full replicable information on the cor-
pora used to train them. Open-source models can specify their exact training data.
Requirements that models are transparent in such ways is also in the process of being
incorporated into the regulations of various national governments.

--- Page 220 ---
220 CHAPTER 10 • L ARGE LANGUAGE MODELS
10.7 Summary
This chapter has introduced the large language model, and how it can be built out of
the transformer. Here’s a summary of the main points that we covered:
• Many NLP tasks—such as question answering, summarization, sentiment,
and machine translation—can be cast as tasks of word prediction and hence
addressed with Large language models.
• Large language models are generally pretrained on large datasets of 100s of
billions of words generally scraped from the web.
• These datasets need to be ﬁltered for quality and balanced for domains by
upsampling and downsampling. Addressing some problems with pretraining
data, like toxicity, are open research problems.
• The choice of which word to generate in large language models is generally
done by using a sampling algorithm.
• Language models are evaluated by perplexity but there are also evaluations
of accuracy downstream tasks, and ways to measure other factors like fairness
and energy use.
• There are various computational tricks for making large language models
more efﬁcient, such as the CV cache andparameter-efﬁcient ﬁnetuning .
• Because of their ability to be used in so many ways, language models also
have the potential to cause harms. Some harms include hallucinations, bias,
stereotypes, misinformation and propaganda, and violations of privacy and
copyright.
Bibliographical and Historical Notes
As we discussed in Chapter 3, the earliest language models were the n-gram lan-
guage models developed (roughly simultaneously and independently) by Fred Je-
linek and colleagues at the IBM Thomas J. Watson Research Center, and James
Baker at CMU. It was the Jelinek and the IBM team who ﬁrst coined the term lan-
guage model to mean a model of the way any kind of linguistic property (grammar,
semantics, discourse, speaker characteristics), inﬂuenced word sequence probabil-
ities (Jelinek et al., 1975). They contrasted the language model with the acoustic
model which captured acoustic/phonetic characteristics of phone sequences.
N-gram language models were very widely used over the next 30 years and more,
across a wide variety of NLP tasks like speech recognition and machine translations,
often as one of multiple components of the model. The contexts for these n-gram
models grew longer, with 5-gram models used quite commonly by very efﬁcient LM
toolkits (Stolcke, 2002; Heaﬁeld, 2011).
The roots of the neural language model lie in multiple places. One was the
application in the 1990s, again in Jelinek’s group at IBM Research, of discrimi-
native classiﬁers to language models. Roni Rosenfeld in his dissertation (Rosen-
feld, 1992) ﬁrst applied logistic regression (under the name maximum entropy or
maxent models) to language modeling in that IBM lab, and published a more fully
formed version in Rosenfeld (1996). His model integrated various sorts of infor-
mation in a logistic regression predictor, including n-gram information along with

--- Page 221 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 221
other features from the context, including distant n-grams and pairs of associated
words called trigger pairs . Rosenfeld’s model preﬁgured modern language models
by being a statistical word predictor trained in a self-supervised manner simply by
learning to predict upcoming words in a corpus.
Another was the ﬁrst use of pretrained embeddings to model word meaning in the
LSA/LSI models (Deerwester et al., 1988). Recall from the history section of Chap-
ter 6 that in LSA (latent semantic analysis) a term-document matrix was trained on a
corpus and then singular value decomposition was applied and the ﬁrst 300 dimen-
sions were used as a vector embedding to represent words. Landauer et al. (1997)
ﬁrst used the word “embedding”. In addition to their development of the idea of pre-
training and of embeddings, the LSA community also developed ways to combine
LSA embeddings with n-grams in an integrated language model (Bellegarda, 1997;
Coccaro and Jurafsky, 1998).
In a very inﬂuential series of papers developing the idea of neural language
models , (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Ben-
gio and colleagues drew on the central ideas of both these lines of self-supervised
language modeling work, (the discriminatively trained word predictor, and the pre-
trained embeddings). Like the maxent models of Rosenfeld, Bengio’s model used
the next word in running text as its supervision signal. Like the LSA models, Ben-
gio’s model learned an embedding, but unlike the LSA models did it as part of the
process of language modeling. The Bengio et al. (2003) model was a neural lan-
guage model: a neural network that learned to predict the next word from prior
words, and did so via learning embeddings as part of the prediction process.
The neural language model was extended in various ways over the years, perhaps
most importantly in the form of the RNN language model of Mikolov et al. (2010)
and Mikolov et al. (2011). The RNN language model was perhaps the ﬁrst neural
model that was accurate enough to surpass the performance of a traditional 5-gram
language model.
Soon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed
to simply the hidden layer of these neural net language models to create pretrained
word2vec word embeddings.
The static embedding models like LSA and word2vec instantiated a particular
model of pretraining: a representation was trained on a pretraining dataset, and then
the representations could be used in further tasks. The ‘Dai and Le (2015) and
(Peters et al., 2018) reframed this idea by proposing models that were pretrained
using a language model objective, and then the identical model could be either frozen
and directly applied for language modeling or further ﬁnetuned still using a language
model objective. For example ELMo used a biLSTM self-supervised on a large
pretrained dataset using a language model objective, then ﬁnetuned on a domain-
speciﬁc dataset, and then froze the weights and added task-speciﬁc heads.
Transformers were ﬁrst applied as encoder-decoders (Vaswani et al., 2017) and
then to masked language modeling (Devlin et al., 2019) (as we’ll see in Chapter 13
and Chapter 11). Radford et al. (2019) then showed that the transformer-based au-
toregressive language model GPT2 could perform zero-shot on many NLP tasks like
summarization and question answering.
The technology used for transformer-based language models can also be applied
to other domains and tasks, like vision, speech, and genetics. the term foundation
model is sometimes used as a more general term for this use of large languagefoundation
model
model technology across domains and areas, when the elements we are computing
over are not necessarily words. Bommasani et al. (2021) is a broad survey that

--- Page 222 ---
222 CHAPTER 10 • L ARGE LANGUAGE MODELS
sketches the opportunities and risks of foundation models, with special attention to
large language models.

--- Page 223 ---
CHAPTER
11Masked Language Models
Larvatus prodeo [Masked, I go forward]
Descartes
In the previous two chapters we introduced the transformer and saw how to pre-
train a transformer language model as a causal or left-to-right language model. In
this chapter we’ll introduce a second paradigm for pretrained language models, the
bidirectional transformer encoder, and the most widely-used version, the BERT BERT
model (Devlin et al., 2019). This model is trained via masked language modeling ,masked
language
modelingwhere instead of predicting the following word, we mask a word in the middle and
ask the model to guess the word given the words on both sides. This method thus
allows the model to see both the right and left context.
We also introduced ﬁnetuning in the prior chapter. Here we describe a new ﬁnetuning
kind of ﬁnetuning, in which we take the transformer network learned by these pre-
trained models, add a neural net classiﬁer after the top layer of the network, and train
it on some additional labeled data to perform some downstream task like named
entity tagging or natural language inference. As before, the intuition is that the
pretraining phase learns a language model that instantiates rich representations of
word meaning, that thus enables the model to more easily learn (‘be ﬁnetuned to’)
the requirements of a downstream language understanding task. This aspect of the
pretrain-ﬁnetune paradigm is an instance of what is called transfer learning in ma-transfer
learning
chine learning: the method of acquiring knowledge from one task or domain, and
then applying it (transferring it) to solve a new task.
The second idea that we introduce in this chapter is the idea of contextual em-
beddings : representations for words in context. The methods of Chapter 6 like
word2vec or GloVe learned a single vector embedding for each unique word win
the vocabulary. By contrast, with contextual embeddings, such as those learned by
masked language models like BERT, each word wwill be represented by a different
vector each time it appears in a different context. While the causal language models
of Chapter 9 also use contextual embeddings, the embeddings created by masked
language models seem to function particularly well as representations.
11.1 Bidirectional Transformer Encoders
Let’s begin by introducing the bidirectional transformer encoder that underlies mod-
els like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT
(Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers
and in Chapter 10 saw how they can serve as the basis for language models that can
be applied to autoregressive contextual generation problems like question answering
or summarization. But this left-to-right nature of these models is also a limitation,
because there are tasks for which it would be useful, when processing a token, to be
able to peak at future tokens. This is especially true for sequence labeling tasks in

--- Page 224 ---
224 CHAPTER 11 • M ASKED LANGUAGE MODELS
which we want to tag each token with a label, such as the part-of-speech tagging or
parsing tasks we’ll introduce in future chapters, or tasks like named entity tagging
we’ll introduce later in this chapter.
Thebidirectional encoders that we introduce here are a different kind of beast
than causal models. The causal models of Chapter 9 are generative models, de-
signed to easily generate the next token in a sequence. But the focus of bidirec-
tional encoders is instead on computing contextualized representations of the input
tokens. Bidirectional encoders use self-attention to map sequences of input embed-
dings (x1;:::;xn)to sequences of output embeddings of the same length (h1;:::;hn),
where the output vectors have been contextualized using information from the en-
tire input sequence. These output embeddings are contextualized representations of
each input token that are useful across a range of applications where we need to do
a classiﬁcation or a decision based on the token in context.
Remember that we said the models of Chapter 9 are sometimes called decoder-
only, because they correspond to the decoder part of the encoder-decoder model we
will introduce in Chapter 13. By contrast, the masked language models of this chap-
ter are sometimes called encoder-only , because they produce an encoding for each
input token but generally aren’t used to produce running text by decoding/sampling.
That’s an important point: masked language models are not used for generation.
They are generally instead used for interpretative tasks.
11.1.1 The architecture for bidirectional masked models
Let’s ﬁrst discuss the overall architecture. Bidirectional transformer-based language
models differ in two ways from the causal transformers in the previous chapters. The
ﬁrst is that the attention function isn’t causal; the attention for a token ican look at
following tokens i+1 and so on. The second is that the training is slightly different
since we are predicting something in the middle of our text rather than at the end.
We’ll discuss the ﬁrst here and the second in the following section.
Fig. 11.1a, reproduced here from Chapter 9, shows the information ﬂow in the
left-to-right approach of Chapter 9. The attention computation at each token is based
on the preceding (and current) input tokens, ignoring potentially useful information
located to the right of the token under consideration. Bidirectional encoders over-
come this limitation by allowing the attention mechanism to range over the entire
input, as shown in Fig. 11.1b.
a) A causal self-attention layerb) A bidirectional self-attention layer
attentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2
attentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2
Figure 11.1 (a) The causal transformer from Chapter 9, highlighting the attention computation at token 3. The
attention value at each token is computed using only information seen earlier in the context. (b) Information
ﬂow in a bidirectional attention model. In processing each token, the model attends to all inputs, both before
and after the current one. So attention for token 3 can draw on information from following tokens.
The implementation is very simple! We simply remove the attention masking
step that we introduced in Eq. 9.32. Recall from Chapter 9 that we had to mask the
QK|matrix for causal transformers so that attention couldn’t look at future tokens

--- Page 225 ---
11.1 • B IDIRECTIONAL TRANSFORMER ENCODERS 225
(repeated from Eq. 9.32):
A=softmax
maskQK|
pdk
V (11.1)
q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NN−∞−∞−∞−∞−∞−∞
q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3NNq1•k2q1•k3q1•k4q2•k3q2•k4q3•k4
(a) (b)
Figure 11.2 The NNQK|matrix showing the qikjvalues, with the upper-triangle
portion of the comparisons matrix zeroed out (set to  ¥, which the softmax will turn to
zero).
Fig. 11.2 shows the masked version of QK|and the unmasked version. For bidi-
rectional attention, we used the unmasked version of Fig. 11.2b. Thus the attention
computation for bidirectional attention is exactly the same as Eq. 11.1 but with the
mask removed:
A=softmaxQK|
pdk
V (11.2)
Otherwise, the attention computation is identical to what we saw in Chapter 9, as
is the transformer block architecture (the feedforward layer, layer norm, and so on).
As in Chapter 9, the input is also a series of subword tokens, usually computed by
WordPiece or SentencePiece Unigram LM tokenization (two of the large family of
subword tokenization algorithms that includes the BPE algorithm we saw in Chap-
ter 2). That means every input sentence ﬁrst has to be tokenized, and all further
processing takes place on subword tokens rather than words. This will require, as
we’ll see in the third part of the textbook, that for some NLP tasks that require no-
tions of words (like parsing) we will occasionally need to map subwords back to
words.
To make this more concrete, the original English-only bidirectional transformer
encoder model, BERT (Devlin et al., 2019), consisted of the following:
• An English-only subword vocabulary consisting of 30,000 tokens generated
using the WordPiece algorithm (Schuster and Nakajima, 2012).
• Hidden layers of dimensionality d=768,
• 12 layers of transformer blocks, with 12 (bidirectional) multihead attention
layers each.
• The resulting model has about 100M parameters.
The larger multilingual XLM-RoBERTa model, trained on 100 languages, has
• A multilingual subword vocabulary with 250,000 tokens generated using the
SentencePiece Unigram LM algorithm (Kudo and Richardson, 2018b).
• 24 layers of transformer blocks, with 16 multihead attention layers each

--- Page 226 ---
226 CHAPTER 11 • M ASKED LANGUAGE MODELS
• Hidden layers of size 1024
• Input context window of 512 tokens
• The resulting model has about 550M parameters.
Note that 550M parameters is relatively small as large language models go
(Llama 3 has 405B parameters, so is 3 orders of magnitude bigger). Indeed, masked
language models tend to be much smaller than causal language models.
11.2 Training Bidirectional Encoders
We trained causal transformer language models in Chapter 9 by making them it-
eratively predict the next word in a text. But eliminating the causal mask in at-
tention makes the guess-the-next-word language modeling task trivial—the answer
is directly available from the context—so we’re in need of a new training scheme.
Instead of trying to predict the next word, the model learns to perform a ﬁll-in-the-
blank task, technically called the cloze task (Taylor, 1953). To see this, let’s return cloze task
to the motivating example from Chapter 3. Instead of predicting which words are
likely to come next in this example:
The water of Walden Pond is so beautifully
we’re asked to predict a missing item given the rest of the sentence.
The of Walden Pond is so beautifully ...
That is, given an input sequence with one or more elements missing, the learning
task is to predict the missing elements. More precisely, during training the model is
deprived of one or more elements of an input sequence and must generate a proba-
bility distribution over the vocabulary for each of the missing items. We then use the
cross-entropy loss from each of the model’s predictions to drive the learning process.
This approach can be generalized to any of a variety of methods that corrupt the
training input and then asks the model to recover the original input. Examples of the
kinds of manipulations that have been used include masks, substitutions, reorder-
ings, deletions, and extraneous insertions into the training text. The general name
for this kind of training is called denoising : we corrupt (add noise to) the input in denoising
some way (by masking a word, or putting in an incorrect word) and the goal of the
system is to remove the noise.
11.2.1 Masking Words
The original approach to training bidirectional encoders is called Masked Language
Modeling (MLM) (Devlin et al., 2019). As with the language model training meth-Masked
Language
Modelingods we’ve already seen, MLM uses unannotated text from a large corpus. Here, the
model is presented with a series of sentences from the training corpus where a ran-
dom sample of tokens from each training sequence is selected for use in the learning
task. Once chosen, a token is used in one of three ways:
• It is replaced with the special vocabulary token named [MASK] .
• It is replaced with another token from the vocabulary, randomly sampled
based on token unigram probabilities.
• It is left unchanged.

--- Page 227 ---
11.2 • T RAINING BIDIRECTIONAL ENCODERS 227
In BERT, 15% of the input tokens in a training sequence are sampled for learning.
Of these, 80% are replaced with [MASK] , 10% are replaced with randomly selected
tokens, and the remaining 10% are left unchanged.
The MLM training objective is to predict the original inputs for each of the
masked tokens using a bidirectional encoder of the kind described in the last section.
The cross-entropy loss from these predictions drives the training process for all the
parameters in the model. Note that all of the input tokens play a role in the self-
attention process, but only the sampled tokens are used for learning.
More speciﬁcally, the original input sequence is ﬁrst tokenized using a subword
model. The sampled items which drive the learning process are chosen among the
input tokens. Word embeddings for all of the tokens in the input are retrieved from
theEembedding matrix and combined with positional embeddings to form the input
to the transformer, passed through the stack of transformer blocks, and then the
language modeling head.
LM Head with Softmax over Vocabulary
So[mask]and[mask]for longthanksCE Loss
all apricot ﬁshthe
Token +Positional EmbeddingsSolongandthanksfor all ﬁshthe
Bidirectional Transformer Encoder+p1+++++++p2p3p4p5p6p7p8z1z2z3z4z5z6z7z8
Figure 11.3 Masked language model training. In this example, three of the input tokens are selected, two of
which are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to
these three items are used as the training loss. The other 5 tokens don’t play a role in training loss.
Fig. 11.3 illustrates this approach with a simple example. Here, long,thanks and
thehave been sampled from the training sequence, with the ﬁrst two masked and the
replaced with the randomly sampled token apricot . The resulting embeddings are
passed through a stack of bidirectional transformer blocks. Recall from Section 9.5
in Chapter 9 that to produce a probability distribution over the vocabulary for each
of the masked tokens, the language modeling head takes the output vector hL
ifrom
the ﬁnal transformer layer Lfor each masked token i, multiplies it by the unembed-
ding layer ETto produce the logits u, and then uses softmax to turn the logits into
probabilities yover the vocabulary:
ui=hL
iET(11.3)
yi=softmax (ui) (11.4)
With a predicted probability distribution for each masked item, we can use cross-
entropy to compute the loss for each masked item—the negative log probability
assigned to the actual masked word, as shown in Fig. 11.3. More formally, for a

--- Page 228 ---
228 CHAPTER 11 • M ASKED LANGUAGE MODELS
given vector of input tokens in a sentence or batch be x, let the set of tokens that are
masked be M, the version of that sentence with some tokens replaced by masks be
xmask, and the sequence of output vectors be h. For a given input token xi, such as
the word long in Fig. 11.3, the loss is the probability of the correct word long, given
xmask(as summarized in the single output vector hL
i):
LMLM(xi) = logP(xijhL
i)
The gradients that form the basis for the weight updates are based on the average
loss over the sampled learning items from a single training sequence (or batch of
sequences).
LMLM= 1
jMjX
i2MlogP(xijhL
i)
Note that only the tokens in Mplay a role in learning; the other words play no role
in the loss function, so in that sense BERT and its descendents are inefﬁcient; only
15% of the input samples in the training data are actually used for training weights.
1
11.2.2 Next Sentence Prediction
The focus of mask-based learning is on predicting words from surrounding contexts
with the goal of producing effective word-level representations. However, an im-
portant class of applications involves determining the relationship between pairs of
sentences. These include tasks like paraphrase detection (detecting if two sentences
have similar meanings), entailment (detecting if the meanings of two sentences en-
tail or contradict each other) or discourse coherence (deciding if two neighboring
sentences form a coherent discourse).
To capture the kind of knowledge required for applications such as these, some
models in the BERT family include a second learning objective called Next Sen-
tence Prediction (NSP). In this task, the model is presented with pairs of sentencesNext Sentence
Prediction
and is asked to predict whether each pair consists of an actual pair of adjacent sen-
tences from the training corpus or a pair of unrelated sentences. In BERT, 50% of
the training pairs consisted of positive pairs, and in the other 50% the second sen-
tence of a pair was randomly selected from elsewhere in the corpus. The NSP loss
is based on how well the model can distinguish true pairs from random pairs.
To facilitate NSP training, BERT introduces two special tokens to the input rep-
resentation (tokens that will prove useful for ﬁnetuning as well). After tokenizing
the input with the subword model, the token [CLS] is prepended to the input sen-
tence pair, and the token [SEP] is placed between the sentences and after the ﬁnal
token of the second sentence. Finally, embeddings representing the ﬁrst and second
segments of the input are added to the word and positional embeddings to allow the
model to more easily distinguish the input sentences.
During training, the output vector hL
CLSfrom the ﬁnal layer associated with the
[CLS] token represents the next sentence prediction. As with the MLM objective,
we add a special head, in this case an NSP head, which consists of a learned set of
classiﬁcation weights WNSP2Rd2that produces a two-class prediction from the
raw[CLS] vector hL
CLS:
yi=softmax (hL
CLSWNSP)
1There are members of the BERT family like ELECTRA that do use all examples for training (Clark
et al., 2020b).

--- Page 229 ---
11.2 • T RAINING BIDIRECTIONAL ENCODERS 229
Cross entropy is used to compute the NSP loss for each sentence pair presented
to the model. Fig. 11.4 illustrates the overall NSP training setup. In BERT, the NSP
loss was used in conjunction with the MLM training objective to form ﬁnal loss.
Cancelmyﬂight[SEP] 1CE Loss
And the 
Bidirectional Transformer Encoder
p1p2p3p4p5p6p7p8[CLS]++s1NSPHeadToken +Segment +PositionalEmbeddings hotel p9[SEP] ++s1s1s1s1s2s2s2s2++++++++++++++hCLS
Figure 11.4 An example of the NSP loss calculation.
11.2.3 Training Regimes
BERT and other early transformer-based language models were trained on about
3.3 billion words (a combination of English Wikipedia and a corpus of book texts
called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property
reasons). Modern masked language models are now trained on much larger datasets
of web text, ﬁltered a bit, and augmented by higher-quality data like Wikipedia,
the same as those we discussed for the causal large language models of Chapter 9.
Multilingual models similarly use webtext and multilingual Wikipedia. For example
the XLM-R model was trained on about 300 billion tokens in 100 languages, taken
from the web via Common Crawl ( https://commoncrawl.org/ ).
To train the original BERT models, pairs of text segments were selected from
the training corpus according to the next sentence prediction 50/50 scheme. Pairs
were sampled so that their combined length was less than the 512 token input. To-
kens within these sentence pairs were then masked using the MLM approach with
the combined loss from the MLM and NSP objectives used for a ﬁnal loss. Ap-
proximately 40 passes (epochs) over the training data was required for the model to
converge.
Some models, like the RoBERTa model, drop the next sentence prediction ob-
jective, and therefore change the training regime a bit. Instead of sampling pairs of
sentence, the input is simply a series of contiguous sentences. If the document runs
out before 512 tokens are reached, an extra separator token is added, and sentences
from the next document are packed in, until we reach a total of 512 tokens. Usually
large batch sizes are used, between 8K and 32K tokens.
Multilingual models have an additional decision to make: what data to use to
build the vocabulary? Recall that all language models use subword tokenization
(BPE or SentencePiece Unigram LM are the two most common algorithms). What
text should be used to learn this multilingual tokenization, given that it’s easier to get
much more text in some languages than others? One option would be to create this
vocabulary-learning dataset by sampling sentences from our training data (perhaps

--- Page 230 ---
230 CHAPTER 11 • M ASKED LANGUAGE MODELS
web text from Common Crawl), randomly. In that case we will choose a lot of sen-
tences from languages like languages with lots of web representation like English,
and the tokens will be biased toward rare English tokens instead of creating frequent
tokens from languages with less data. Instead, it is common to divide the training
data into subcorpora of Ndifferent languages, compute the number of sentences ni
of each language i, and readjust these probabilities so as to upweight the probability
of less-represented languages (Lample and Conneau, 2019). The new probability of
selecting a sentence from each of the Nlanguages (whose prior frequency is ni) is
fqigi=1:::N, where:
qi=pa
iPN
j=1pa
jwith pi=niPN
k=1nk(11.5)
Recall from (6.32) in Chapter 6 that an avalue between 0 and 1 will give higher
weight to lower probability samples. Conneau et al. (2020) show that a=0:3 works
well to give rare languages more inclusion in the tokenization, resulting in better
multilingual performance overall.
The result of this pretraining process consists of both learned word embeddings,
as well as all the parameters of the bidirectional encoder that are used to produce
contextual embeddings for novel inputs.
For many purposes, a pretrained multilingual model is more practical than a
monolingual model, since it avoids the need to build many (a hundred!) separate
monolingual models. And multilingual models can improve performance on low-
resourced languages by leveraging linguistic information from a similar language in
the training data that happens to have more resources. Nonetheless, when the num-
ber of languages grows very large, multilingual models exhibit what has been called
thecurse of multilinguality (Conneau et al., 2020): the performance on each lan-
guage degrades compared to a model training on fewer languages. Another problem
with multilingual models is that they ‘have an accent’: grammatical structures in
higher-resource languages (often English) bleed into lower-resource languages; the
vast amount of English language in training makes the model’s representations for
low-resource languages slightly more English-like (Papadimitriou et al., 2023).
11.3 Contextual Embeddings
Given a pretrained language model and a novel input sentence, we can think of the
sequence of model outputs as constituting contextual embeddings for each token incontextual
embeddings
the input. These contextual embeddings are vectors representing some aspect of the
meaning of a token in context, and can be used for any task requiring the meaning of
tokens or words. More formally, given a sequence of input tokens x1;:::;xn, we can
use the output vector zifrom the ﬁnal layer of the model as a representation of the
meaning of token xiin the context of sentence x1;:::;xn. Or instead of just using the
vector zifrom the ﬁnal layer of the model, it’s common to compute a representation
forxiby averaging the output tokens zifrom each of the last four layers of the model.
Just as we used static embeddings like word2vec in Chapter 6 to represent the
meaning of words, we can use contextual embeddings as representations of word
meanings in context for any task that might require a model of word meaning. Where
static embeddings represent the meaning of word types (vocabulary entries), contex-
tual embeddings represent the meaning of word instances : instances of a particular

--- Page 231 ---
11.3 • C ONTEXTUAL EMBEDDINGS 231
[CLS]SolongandthanksforallhL1hLCLShL2hL3hL4hL5hL6
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Figure 11.5 The output of a BERT-style model is a contextual embedding vector hL
ifor
each input token xi.
word type in a particular context. Thus where word2vec had a single vector for each
word type, contextual embeddings provide a single vector for each instance of that
word type in its sentential context. Contextual embeddings can thus be used for
tasks like measuring the semantic similarity of two words in context, and are useful
in linguistic tasks that require models of word meaning.
11.3.1 Contextual Embeddings and Word Sense
Words are ambiguous : the same word can be used to mean different things. In ambiguous
Chapter 6 we saw that the word “mouse” can mean (1) a small rodent, or (2) a hand-
operated device to control a cursor. The word “bank” can mean: (1) a ﬁnancial
institution or (2) a sloping mound. We say that the words ‘mouse’ or ‘bank’ are
polysemous (from Greek ‘many senses’, poly- ‘many’ + sema , ‘sign, mark’).2
Asense (orword sense ) is a discrete representation of one aspect of the meaning word sense
of a word. We can represent each sense with a superscript: bank1andbank2,
mouse1andmouse2. These senses can be found listed in online thesauruses (or
thesauri) like WordNet (Fellbaum, 1998), which has datasets in many languages WordNet
listing the senses of many words. In context, it’s easy to see the different meanings:
mouse1: .... a mouse controlling a computer system in 1968.
mouse2: .... a quiet animal like a mouse
bank1: ...a bank can hold the investments in a custodial account ...
bank2: ...as agriculture burgeons on the east bank , the river ...
This fact that context disambiguates the senses of mouse andbank above can
also be visualized geometrically. Fig. 11.6 shows a two-dimensional projection of
many instances of the BERT embeddings of the word diein English and German.
Each point in the graph represents the use of diein one input sentence. We can
clearly see at least two different English senses of die(the singular of dice and the
verb to die , as well as the German article, in the BERT embedding space.
2The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases
where a word’s senses are related in some structured way, reserving the word homonymy to mean sense
ambiguities with no relation between the senses (Haber and Poesio, 2020). Here we will use ‘polysemy’
to mean any kind of sense ambiguity, and ‘structured polysemy’ for polysemy with sense relations.

--- Page 232 ---
232 CHAPTER 11 • M ASKED LANGUAGE MODELS
Figure 4: Embeddings for the word "die" in different contexts, visualized with UMAP. Sample points
are annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.
4.1 Visualization of word senses
Our ﬁrst experiment is an exploratory visualization of how word sense affects context embeddings.
For data on different word senses, we collected all sentences used in the introductions to English-
language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created
an interactive application, which we plan to make public. A user enters a word, and the system
retrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and
for each one it retrieves the context embedding for the word from a layer of the user’s choosing.
The system visualizes these 1,000 context embeddings using UMAP [ 15], generally showing clear
clusters relating to word senses. Different senses of a word are typically spatially separated, and
within the clusters there is often further structure related to ﬁne shades of meaning. In Figure 4, for
example, we not only see crisp, well-separated clusters for three meanings of the word “die,” but
within one of these clusters there is a kind of quantitative scale, related to the number of people
dying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises
two immediate questions. First, is it possible to ﬁnd quantitative corroboration that word senses are
well-represented? Second, how can we resolve a seeming contradiction: in the previous section, we
saw how position represented syntax; yet here we see position representing semantics.
4.2 Measurement of word sense disambiguation capability
The crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,
effective internal representations of word senses, putting different meanings in different locations. To
test this hypothesis quantitatively, we test whether a simple classiﬁer on these internal representations
can perform well at word-sense disambiguation (WSD).
We follow the procedure described in [ 20], which performed a similar experiment with the ELMo
model. For a given word with nsenses, we make a nearest-neighbor classiﬁer where each neighbor is
the centroid of a given word sense’s BERT-base embeddings in the training data. To classify a new
word we ﬁnd the closest of these centroids, defaulting to the most commonly used sense if the word
was not present in the training data. We used the data and evaluation from [ 21]: the training data was
SemCor [17] (33,362 senses), and the testing data was the suite described in [21] (3,669 senses).
The simple nearest-neighbor classiﬁer achieves an F1 score of 71.1, higher than the current state of
the art (Table 1), with the accuracy monotonically increasing through the layers. This is a strong
signal that context embeddings are representing word-sense information. Additionally, an even higher
score of 71.5 was obtained using the technique described in the following section.
6
Figure 11.6 Each blue dot shows a BERT contextual embedding for the word diefrom different sentences
in English and German, projected into two dimensions with the UMAP algorithm. The German and English
meanings and the different English senses fall into different clusters. Some sample points are shown with the
contextual sentence they came from. Figure from Coenen et al. (2019).
Thus while thesauruses like WordNet give discrete lists of senses, embeddings
(whether static or contextual) offer a continuous high-dimensional model of meaning
that, although it can be clustered, doesn’t divide up into fully discrete senses.
Word Sense Disambiguation
The task of selecting the correct sense for a word is called word sense disambigua-
tion, orWSD . WSD algorithms take as input a word in context and a ﬁxed inventoryword sense
disambiguation
WSD of potential word senses (like the ones in WordNet) and outputs the correct word
sense in context. Fig. 11.7 sketches out the task.
anelectricguitarandbassplayerstandoﬀtoonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range…bass4: sea ﬁsh… bass7: instrument…player1: in gameplayer2: musician player3: actor…stand1: upright…stand5: bear… stand10: put upright…side1: relative region…side3: of body… side11: slope…x1y1x2y2x3y3y4y5y6
x4x5x6
Figure 11.7 The all-words WSD task, mapping from input words ( x) to WordNet senses
(y). Figure inspired by Chaplot and Salakhutdinov (2018).
WSD can be a useful analytic tool for text analysis in the humanities and social
sciences, and word senses can play a role in model interpretability for word repre-
sentations. Word senses also have interesting distributional properties. For example

--- Page 233 ---
11.3 • C ONTEXTUAL EMBEDDINGS 233
a word often is used in roughly the same sense through a discourse, an observation
called the one sense per discourse rule (Gale et al., 1992a).one sense per
discourse
The best performing WSD algorithm is a simple 1-nearest-neighbor algorithm
using contextual word embeddings, due to Melamud et al. (2016) and Peters et al.
(2018). At training time we pass each sentence in some sense-labeled dataset (like
the SemCore or SenseEval datasets in various languages) through any contextual
embedding (e.g., BERT) resulting in a contextual embedding for each labeled token.
(There are various ways to compute this contextual embedding vifor a token i; for
BERT it is common to pool multiple layers by summing the vector representations
ofifrom the last four BERT layers). Then for each sense sof any word in the corpus,
for each of the ntokens of that sense, we average their ncontextual representations
vito produce a contextual sense embedding vsfors:
vs=1
nX
ivi8vi2tokens (s) (11.6)
At test time, given a token of a target word tin context, we compute its contextual
embedding tand choose its nearest neighbor sense from the training set, i.e., the
sense whose sense embedding has the highest cosine with t:
sense(t) =argmax
s2senses (t)cosine (t;vs) (11.7)
Fig. 11.8 illustrates the model.
I  found  the  jar  emptycIcfoundfind1vcthecjarcemptyfind9vfind5vfind4vENCODER
Figure 11.8 The nearest-neighbor algorithm for WSD. In green are the contextual embed-
dings precomputed for each sense of each word; here we just show a few of the senses for
ﬁnd. A contextual embedding is computed for the target word found , and then the nearest
neighbor sense (in this case ﬁnd9v) is chosen. Figure inspired by Loureiro and Jorge (2019).
11.3.2 Contextual Embeddings and Word Similarity
In Chapter 6 we introduced the idea that we could measure the similarity of two
words by considering how close they are geometrically, by using the cosine as a
similarity function. The idea of meaning similarity is also clear geometrically in the
meaning clusters in Fig. 11.6; the representation of a word which has a particular
sense in a context is closer to other instances of the same sense of the word. Thus we
often measure the similarity between two instances of two words in context (or two
instances of the same word in two different contexts) by using the cosine between
their contextual embeddings.

--- Page 234 ---
234 CHAPTER 11 • M ASKED LANGUAGE MODELS
Usually some transformations to the embeddings are required before computing
cosine. This is because contextual embeddings (whether from masked language
models or from autoregressive ones) have the property that the vectors for all words
are extremely similar. If we look at the embeddings from the ﬁnal layer of BERT
or other models, embeddings for instances of any two randomly chosen words will
have extremely high cosines that can be quite close to 1, meaning all word vectors
tend to point in the same direction. The property of vectors in a system all tending
to point in the same direction is known as anisotropy . Ethayarajh (2019) deﬁnes
theanisotropy of a model as the expected cosine similarity of any pair of words in anisotropy
a corpus. The word ‘isotropy’ means uniformity in all directions, so in an isotropic
model, the collection of vectors should point in all directions and the expected cosine
between a pair of random embeddings would be zero. Timkey and van Schijndel
(2021) show that one cause of anisotropy is that cosine measures are dominated by
a small number of dimensions of the contextual embedding whose values are very
different than the others: these rogue dimensions have very large magnitudes and
very high variance.
Timkey and van Schijndel (2021) shows that we can make the embeddings more
isotropic by standardizing (z-scoring) the vectors, i.e., subtracting the mean and
dividing by the variance. Given a set Cof all the embeddings in some corpus, each
with dimensionality d(i.e., x2Rd), the mean vector m2Rdis:
m=1
jCjX
x2Cx (11.8)
The standard deviation in each dimension s2Rdis:
s=s
1
jCjX
x2C(x m)2(11.9)
Then each word vector xis replaced by a standardized version z:
z=x m
s(11.10)
One problem with cosine that is not solved by standardization is that cosine tends
to underestimate human judgments on similarity of word meaning for very frequent
words (Zhou et al., 2022).
In the next section we’ll see the most common use of contextual representations:
as representations of words or even entire sentences that can be the inputs to classi-
ﬁers in the ﬁnetuning process for downstream NLP applications.
11.4 Fine-Tuning for Classiﬁcation
The power of pretrained language models lies in their ability to extract generaliza-
tions from large amounts of text—generalizations that are useful for myriad down-
stream applications. There are two ways to make practical use of the generalizations
to solve downstream tasks. The most common way is to use natural language to
prompt the model, putting it in a state where it contextually generates what we
want. We’ll introduce prompting in Chapter 12.
In this section we explore an alternative way to use pretrained language models
for downstream applications: a version of the ﬁnetuning paradigm from Chapter 10. ﬁnetuning

--- Page 235 ---
11.4 • F INE-TUNING FOR CLASSIFICATION 235
In the kind of ﬁnetuning used for masked language models, we add application-
speciﬁc circuitry (often called a special head ) on top of pretrained models, taking
their output as its input. The ﬁnetuning process consists of using labeled data about
the application to train these additional application-speciﬁc parameters. Typically,
this training will either freeze or make only minimal adjustments to the pretrained
language model parameters.
The following sections introduce ﬁnetuning methods for the most common kinds
of applications: sequence classiﬁcation, sentence-pair classiﬁcation, and sequence
labeling.
11.4.1 Sequence Classiﬁcation
The task of sequence classiﬁcation is to classify an entire sequence of text with a
single label. This set of tasks is commonly called text classiﬁcation , like sentiment
analysis or spam detection (Chapter 4) in which we classify a text into two or three
classes (like positive or negative), as well as classiﬁcation tasks with a large number
of categories, like document-level topic classiﬁcation.
For sequence classiﬁcation we represent the entire input to be classiﬁed by a
single vector. We can represent a sequence in various ways. One way is to take
the sum or the mean of the last output vector from each token int he sequence.
For BERT, we instead add a new unique token to the vocabulary called [CLS] , and
prepended it to the start of all input sequences, both during pretraining and encoding.
The output vector in the ﬁnal layer of the model for the [CLS] input represents
the entire input sequence and serves as the input to a classiﬁer head , a logistic classiﬁer head
regression or neural network classiﬁer that makes the relevant decision.
As an example, let’s return to the problem of sentiment classiﬁcation. to ﬁnetun-
ing a classiﬁer for this application involves learning a set of weights, WC, to map the
output vector for the [CLS] token— hL
CLS—to a set of scores over the possible senti-
ment classes. Assuming a three-way sentiment classiﬁcation task (positive, negative,
neutral) and dimensionality das the model dimension, WCwill be of size [d3]. To
classify a document, we pass the input text through the pretrained language model to
generate hL
CLS, multiply it by WC, and pass the resulting vector through a softmax.
y=softmax (hL
CLSWC) (11.11)
Finetuning the values in WCrequires supervised training data consisting of input
sequences labeled with the appropriate sentiment class. Training proceeds in the
usual way; cross-entropy loss between the softmax output and the correct answer is
used to drive the learning that produces WC.
A key difference from what we’ve seen earlier with neural classiﬁers is that this
loss can be used to not only learn the weights of the classiﬁer, but also to update the
weights for the pretrained language model itself. In practice, reasonable classiﬁca-
tion performance is typically achieved with only minimal changes to the language
model parameters, often limited to updates over the ﬁnal few layers of the trans-
former. Fig. 11.9 illustrates this overall approach to sequence classiﬁcation.
11.4.2 Sequence-Pair Classiﬁcation
As mentioned in Section 11.2.2, an important type of problem involves the classiﬁca-
tion of pairs of input sequences. Practical applications that fall into this class include
paraphrase detection (are the two sentences paraphrases of each other?), logical en-

--- Page 236 ---
236 CHAPTER 11 • M ASKED LANGUAGE MODELS
[CLS]entirelypredictableandlacksenergy
Bidirectional Transformer EncoderhCLS
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+sentimentclassiﬁcation headWCy
Figure 11.9 Sequence classiﬁcation with a bidirectional transformer encoder. The output vector for the
[CLS] token serves as input to a simple classiﬁer.
tailment (does sentence A logically entail sentence B?), and discourse coherence
(how coherent is sentence B as a follow-on to sentence A?).
Fine-tuning an application for one of these tasks proceeds just as with pretrain-
ing using the NSP objective. During ﬁnetuning, pairs of labeled sentences from a
supervised ﬁnetuning set are presented to the model, and run through all the layers
of the model to produce the houtputs for each input token. As with sequence classi-
ﬁcation, the output vector associated with the prepended [CLS] token represents the
model’s view of the input pair. And as with NSP training, the two inputs are sepa-
rated by the [SEP] token. To perform classiﬁcation, the [CLS] vector is multiplied
by a set of learning classiﬁcation weights and passed through a softmax to generate
label predictions, which are then used to update the weights.
As an example, let’s consider an entailment classiﬁcation task with the Multi-
Genre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In
the task of natural language inference orNLI, also called recognizing textualnatural
language
inferenceentailment , a model is presented with a pair of sentences and must classify the re-
lationship between their meanings. For example in the MultiNLI corpus, pairs of
sentences are given one of 3 labels: entails ,contradicts andneutral . These labels
describe a relationship between the meaning of the ﬁrst sentence (the premise) and
the meaning of the second sentence (the hypothesis). Here are representative exam-
ples of each class from the corpus:
•Neutral
a: Jon walked back to the town to the smithy.
b: Jon traveled back to his hometown.
•Contradicts
a: Tourist Information ofﬁces can be very helpful.
b: Tourist Information ofﬁces are never of any help.
•Entails
a: I’m confused.
b: Not all of it is very clear to me.

--- Page 237 ---
11.5 • F INE-TUNING FOR SEQUENCE LABELLING : NAMED ENTITY RECOGNITION 237
A relationship of contradicts means that the premise contradicts the hypothesis; en-
tails means that the premise entails the hypothesis; neutral means that neither is
necessarily true. The meaning of these labels is looser than strict logical entailment
or contradiction indicating that a typical human reading the sentences would most
likely interpret the meanings in this way.
To ﬁnetune a classiﬁer for the MultiNLI task, we pass the premise/hypothesis
pairs through a bidirectional encoder as described above and use the output vector
for the[CLS] token as the input to the classiﬁcation head. As with ordinary sequence
classiﬁcation, this head provides the input to a three-way classiﬁer that can be trained
on the MultiNLI training corpus.
11.5 Fine-Tuning for Sequence Labelling: Named En-
tity Recognition
In sequence labeling, the network’s task is to assign a label chosen from a small
ﬁxed set of labels to each token in the sequence. One of the most common sequence
labeling task is named entity recognition .
11.5.1 Named Entities
Anamed entity is, roughly speaking, anything that can be referred to with a proper named entity
name: a person, a location, an organization. The task of named entity recognitionnamed entity
recognition
(NER ) is to ﬁnd spans of text that constitute proper names and tag the type of the NER
entity. Four entity tags are most common: PER (person), LOC (location), ORG
(organization), or GPE (geo-political entity). However, the term named entity is
commonly extended to include things that aren’t entities per se, including temporal
expressions like dates and times, and even numerical expressions like prices. Here’s
an example of the output of an NER tagger:
Citing high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it
has increased fares by [ MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ ORG American Airlines ], a
unit of [ ORG AMR Corp.] , immediately matched the move, spokesman
[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,
said the increase took effect [ TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [ LOC Chicago]
to [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .
The text contains 13 mentions of named entities including 5 organizations, 4 loca-
tions, 2 times, 1 person, and 1 mention of money. Figure 11.10 shows typical generic
named entity types. Many applications will also need to use speciﬁc entity types like
proteins, genes, commercial products, or works of art.
Type Tag Sample Categories Example sentences
People PER people, characters Turing is a giant of computer science.
Organization ORG companies, sports teams The IPCC warned about the cyclone.
Location LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon .
Geo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.
Figure 11.10 A list of generic named entity types with the kinds of entities they refer to.

--- Page 238 ---
238 CHAPTER 11 • M ASKED LANGUAGE MODELS
Named entity recognition is a useful step in various natural language processing
tasks, including linking text to information in structured knowledge sources like
Wikipedia, measuring sentiment or attitudes toward a particular entity in text, or
even as part of anonymizing text for privacy. The NER task is is difﬁcult because
of the ambiguity of segmenting NER spans, ﬁguring out which tokens are entities
and which aren’t, since most words in a text will not be named entities. Another
difﬁculty is caused by type ambiguity. The mention Washington can refer to a
person, a sports team, a city, or the US government, as we see in Fig. 11.11.
[PER Washington] was born into slavery on the farm of James Burroughs.
[ORG Washington] went up 2 games to 1 in the four-game series.
Blair arrived in [ LOC Washington] for what may well be his last state visit.
In June, [ GPE Washington] passed a primary seatbelt law.
Figure 11.11 Examples of type ambiguities in the use of the name Washington .
11.5.2 BIO Tagging
One standard approach to sequence labeling for a span-recognition problem like
NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us BIO tagging
to treat NER like a word-by-word sequence labeling task, via tags that capture both
the boundary and the named entity type. Consider the following sentence:
[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines
Holding ] , said the fare applies to the [ LOC Chicago ] route.
Figure 11.12 shows the same excerpt represented with BIO tagging, as well as BIO
variants called IOtagging and BIOES tagging. In BIO tagging we label any token
thatbegins a span of interest with the label B, tokens that occur inside a span are
tagged with an I, and any tokens outside of any span of interest are labeled O. While
there is only one Otag, we’ll have distinct Band Itags for each named entity class.
The number of tags is thus 2 n+1 tags, where nis the number of entity types. BIO
tagging can represent exactly the same information as the bracketed notation, but has
the advantage that we can represent the task in the same simple sequence modeling
way as part-of-speech tagging: assigning a single label yito each input word xi:
Words IO Label BIO Label BIOES Label
Jane I-PER B-PER B-PER
Villanueva I-PER I-PER E-PER
of O O O
United I-ORG B-ORG B-ORG
Airlines I-ORG I-ORG I-ORG
Holding I-ORG I-ORG E-ORG
discussed O O O
the O O O
Chicago I-LOC B-LOC S-LOC
route O O O
. O O O
Figure 11.12 NER as a sequence model, showing IO, BIO, and BIOES taggings.
We’ve also shown two variant tagging schemes: IO tagging, which loses some
information by eliminating the B tag, and BIOES tagging, which adds an end tag E
for the end of a span, and a span tag Sfor a span consisting of only one word.

--- Page 239 ---
11.5 • F INE-TUNING FOR SEQUENCE LABELLING : NAMED ENTITY RECOGNITION 239
11.5.3 Sequence Labeling
In sequence labeling, we pass the ﬁnal output vector corresponding to each input
token to a classiﬁer that produces a softmax distribution over the possible set of
tags. For a single feedforward layer classiﬁer, the set of weights to be learned is
WKof size [dk], where kis the number of possible tags for the task. A greedy
approach, where the argmax tag for each token is taken as a likely answer, can be
used to generate the ﬁnal output tag sequence. Fig. 11.13 illustrates an example of
this approach, where yiis a vector of probabilities over tags, and kindexes the tags.
yi=softmax (hL
iWK) (11.12)
ti=argmaxk(yi) (11.13)
Alternatively, the distribution over labels provided by the softmax for each input
token can be passed to a conditional random ﬁeld (CRF) layer which can take global
tag-level transitions into account (see Chapter 17 on CRFs).
[CLS]JaneVillanuevaofUnitedAirlines
Bidirectional Transformer EncoderB-PERI-PEROB-ORGI-ORG
HoldingdiscussedI-ORGOWKNER headhiargmax
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+WKWKWKWKWKWKyi
Figure 11.13 Sequence labeling for named entity recognition with a bidirectional transformer encoder. The
output vector for each input token is passed to a simple k-way classiﬁer.
Tokenization and NER
Note that supervised training data for NER is typically in the form of BIO tags as-
sociated with text segmented at the word level. For example the following sentence
containing two named entities:
[LOC Mt. Sanitas ] is in [ LOC Sunshine Canyon ] .
would have the following set of per-word BIO tags.
(11.14) Mt.
B-LOCSanitas
I-LOCis
Oin
OSunshine
B-LOCCanyon
I-LOC.
O
Unfortunately, the sequence of WordPiece tokens for this sentence doesn’t align
directly with BIO tags in the annotation:

--- Page 240 ---
240 CHAPTER 11 • M ASKED LANGUAGE MODELS
'Mt', '.', 'San', '##itas', 'is', 'in', 'Sunshine', 'Canyon' '.'
To deal with this misalignment, we need a way to assign BIO tags to subword
tokens during training and a corresponding way to recover word-level tags from
subwords during decoding. For training, we can just assign the gold-standard tag
associated with each word to all of the subword tokens derived from it.
For decoding, the simplest approach is to use the argmax BIO tag associated with
the ﬁrst subword token of a word. Thus, in our example, the BIO tag assigned to
“Mt” would be assigned to “Mt.” and the tag assigned to “San” would be assigned
to “Sanitas”, effectively ignoring the information in the tags assigned to “.” and
“##itas”. More complex approaches combine the distribution of tag probabilities
across the subwords in an attempt to ﬁnd an optimal word-level tag.
11.5.4 Evaluating Named Entity Recognition
Named entity recognizers are evaluated by recall ,precision , and F1measure . Re-
call that recall is the ratio of the number of correctly labeled responses to the total
that should have been labeled; precision is the ratio of the number of correctly la-
beled responses to the total labeled; and F-measure is the harmonic mean of the
two.
To know if the difference between the F 1scores of two NER systems is a signif-
icant difference, we use the paired bootstrap test, or the similar randomization test
(Section 4.9).
For named entity tagging, the entity rather than the word is the unit of response.
Thus in the example in Fig. 11.12, the two entities Jane Villanueva andUnited Air-
lines Holding and the non-entity discussed would each count as a single response.
The fact that named entity tagging has a segmentation component which is not
present in tasks like text categorization or part-of-speech tagging causes some prob-
lems with evaluation. For example, a system that labeled Jane but not Jane Vil-
lanueva as a person would cause two errors, a false positive for O and a false nega-
tive for I-PER. In addition, using entities as the unit of response but words as the unit
of training means that there is a mismatch between the training and test conditions.
11.6 Summary
This chapter has introduced the bidirectional encoder and the masked language
model . Here’s a summary of the main points that we covered:
• Bidirectional encoders can be used to generate contextualized representations
of input embeddings using the entire input context.
• Pretrained language models based on bidirectional encoders can be learned
using a masked language model objective where a model is trained to guess
the missing information from an input.
• The vector output of each transformer block or component in a particular to-
ken column is a contextual embedding that represents some aspect of the
meaning of a token in context.
• A word sense is a discrete representation of one aspect of the meaning of a
word. Contextual embeddings offer a continuous high-dimensional model of
meaning that is richer than fully discrete senses.

--- Page 241 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 241
• The cosine between contextual embeddings can be used as one way to model
the similarity between two words in context, although some transformations
to the embeddings are required ﬁrst.
• Pretrained language models can be ﬁnetuned for speciﬁc applications by adding
lightweight classiﬁer layers on top of the outputs of the pretrained model.
• These applications can include sequence classiﬁcation tasks like sentiment
analysis, sequence-pair classiﬁcation tasks like natural language inference,
orsequence labeling tasks like named entity recognition .
Bibliographical and Historical Notes
History TBD.

--- Page 242 ---
242 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
CHAPTER
12Model Alignment, Prompting,
and In-Context Learning
“Hal, ” said Bowman, now speaking with an icy calm. “I am not incapaci-
tated. Unless you obey my instructions, I shall be forced to disconnect you. ”
Arthur C. Clarke
In this chapter we show how to get LLMs to do tasks for us simply by talking to
them. To get an LLM to translate a sentence, outline a talk, or draft a work email,
we’ll simply describe what we want in natural language. We call these instructions
we give to language models prompts . prompts
Prompting relies on contextual generation. Given the prompt as context, the lan-
guage model generates the next token based on its token probability, conditioned on
the prompt: P(wijw<i). A prompt can be a question (like “What is a transformer net-
work?”), possibly in a structured format (like “Q: What is a transformer network?
A:”), or can be an instruction (like “Translate the following sentence into Hindi:
‘Chop the garlic ﬁnely’”). A prompt can also contain demonstrations , examples to demonstrations
help make the instructions clearer, (like “Give the sentiment of the following sen-
tence. Example Input: “I really loved Taishan Cuisine.” Output: positive”.) As we’ll
see, prompting can be applied to inherently generative tasks (like summarization and
translation) as well as to ones more naturally thought of as classiﬁcation tasks.
Prompts get language models to generate text, but they also can be viewed as
alearning signal, because these demonstrations can help language models learn
to perform novel tasks. For this reason we also refer to prompting as in-context-
learning —learning that improves model performance or reduces some loss but doesin-context-
learning
not involve gradient-based updates to the model’s underlying parameters.
But LLMs as we’ve described them so far turn out to be bad at following instruc-
tions. Pretraining isn’t sufﬁcient to make them helpful . We’ll introduce instruction
tuning , a technique that helps LLMs learn to correctly respond to instructions byinstruction
tuning
ﬁnetuning them on a corpus of instructions with their corresponding response.
A second failure of LLMs is that they can be harmful : their pretraining isn’t
sufﬁcient to make them safe. Readers who know Arthur C. Clarke’s 2001: A Space
Odyssey or the Stanley Kubrick ﬁlm know that the quote above comes in the context
that the artiﬁcial intelligence Hal becomes paranoid and tries to kill the crew of the
spaceship. Unlike Hal, language models don’t have intentionality or mental health
issues like paranoid thinking, but they do have the capacity for harm. Pretrained lan-
guage models can say things that are dangerous or false (like giving unsafe medical
advice) and they can verbally attack users or say toxic or hateful things.
Dealing with safety can be done partly by adding safety training into instruction
tuning. But an important aspect of safety training is a second technique, preference
alignment (often implemented, as we’ll see, with the RLHF orDPO algorithms) inpreference
alignment
which a separate model is trained to decide how much a candidate response aligns
with human preferences. Together we refer to instructing tuning and preference
alignment as model alignment . The intuition is that we want the learning objectivesmodel
alignment
of models to be aligned with the goals of the humans that use them.

--- Page 243 ---
12.1 • P ROMPTING 243
12.1 Prompting
Aprompt is a text string that a user issues to a language model to get the model prompt
to do something useful. In prompting, the user’s prompt string is passed to the
language model, which iteratively generates tokens conditioned on the prompt. Thus
the prompt creates a context that guides LLMs to generate useful outputs to achieve
some user goal. The process of ﬁnding effective prompts for a task is known as
prompt engineering .prompt
engineering
Let’s see how to prompt a language model to solve a simple sentiment classiﬁ-
cation task, Consider this hotel review from the BLT corpus (Salvetti et al., 2016):
Sample Hotel Review
Did not like the service that I was provided, when I entered the hotel. I also
did not like the area, in which the hotel was located. Too much noise and
events going on for me to feel relax.
We can get the model to classify the sentiment of this text by taking the text and
appending an incomplete statement to the review like In short, our stay was :
A prompt consisting of a review plus an incomplete statement
Did not like the service that I was provided, when I entered the hotel. I also
did not like the area, in which the hotel was located. Too much noise and
events going on for me to feel relax. In short, our stay was
We then have an LLM complete the statement by generating a token conditioned
on the prompt so far, and then generating the next token (conditioned on the prompt
plus the new token), and so forth. Here are a few responses from a language model
via continued generation starting with the prompt as context.
Hotel Review Completions
Did not like the service that I was provided, when I entered the hotel. I also
did not like the area, in which the hotel was located. Too much noise and
events going on for me to feel relaxed. In short our stay was
... not a pleasant one. The staff at the front desk
were not welcoming or friendly, and seemed disinterested
in providing good customer service.
... uncomfortable and not worth the price we paid. We
will not be returning to this hotel.
As we can see, the overall negative context of the review results in negative
completions. We could easily map these completions to the class we are trying
to predict, perhaps via some predeﬁned mappings, like fexcellent!positiveg,
fdid not like!negativeg, and so on.
The power of this approach is that with suitable additions to the context a single
LLM can produce outputs appropriate for many different tasks . For example, given

--- Page 244 ---
244 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
a review we might want any of the following:
• A summary,
• Whether the review was truthful or likely to have been fabricated,
• A translation to another language.
LLMs have a striking ability to perform tasks like these, needing just the appro-
priate contextual nudge to get the LLM to generate the desired output.
If we want to solve general tasks like summarization or translation, we don’t
want to have to create a new prompt each time we do the task. Instead the ﬁrst step
in prompting is to design one or more templates : task-speciﬁc prompting text along templates
with slots for the particular input that is being processed.
Consider the following templates for a variety of tasks:
Basic Prompt Templates
Summarization finputg;tldr;
Translation finputg;translate to French:
Sentiment finputg;Overall, it was
Fine-Grained- finputg;What aspects were important in this review?
Sentiment
Each template consists of an input text, designated as finputg, followed by a
verbatim prompt to be passed to an LLM. These templates are applied to inputs to
create ﬁlled prompts – instantiated prompts suitable for use as inputs to an LLM.
Fig. 12.1 illustrates ﬁlled prompts for these templates using our earlier hotel review,
along with sample outputs from an LLM:
Notice the design pattern of the prompts above: the input is followed by some
text which in turn will be completed by the desired response. This style, with the
instruction at the end, is common in prompting because it helpfully constrains the
generation. Consider, by contrast, the prompt in Example 12.1.
Translate English to French:
Did not like the service that I was provided! (12.1)
This prompt doesn’t do a good job of constraining possible continuations. Instead
of a French translation, models given this prompt may instead generate another sen-
tence in English that simply extends the English review. Prompts need to be designed
unambiguously, so that any reasonable continuation would accomplish the desired
task (Reynolds and McDonell, 2021).
An even more constraining style of prompt can specify the set of possible an-
swers in the prompt. For example here is a prompt template to do sentiment analysis
that prespeciﬁes the potential answers:
A prompt consisting of a review plus an incomplete statement
Human: Do you think that “input” has negative or positive sentiment?
Choices:
(P) Positive
(N) Negative
Assistant: I believe the best answer is: (

--- Page 245 ---
12.1 • P ROMPTING 245
LLM Outputs for Basic Prompts
Original Review ($INPUT) Did not like the service that I was provided,
when I entered the hotel. I also did not like
the area, in which the hotel was located. Too
much noise and events going on for me to feel
relax and away from the city life.
Sentiment Prompt :$INPUT + In short, our stay was
Output :not enjoyable
Fine-grained Sentiment Prompt :$INPUT + These aspects were important to
the reviewer:
Output :1. Poor service 2. Unpleasant location
3. Noisy and busy area
Summarization Prompt :$INPUT + tl;dr
Output :I had a bad experience with the hotel's
service and the location was loud and busy.
Translation Prompt :$INPUT + Translate this to French
Output :Je n'ai pas aim e le service qui m'a ete
offert lorsque je suis entr e dans l'h ^otel. Je
n'aiegalement pas aim e la zone dans laquelle se
trouvait l'h ^otel. Trop de bruit et d' evenements
pour que je me sente d etendu et loin de la vie
citadine.
Figure 12.1 LLM outputs for simple prompts for sentiment, summarization and translation for an input text.
This prompt uses a number of more sophisticated prompting characteristics. It
speciﬁes the two allowable choices (P) and (N), and ends the prompt with the open
parenthesis that strongly suggests the answer will be (P) or (N). Note that it also
speciﬁes the role of the language model as an assistant.
We can do even more with prompts. For example, we might want to restrict a
summary to be a particular length, to have an answer generated according to some
kind of persona or role, or to specify a more structured output using a programming
language or a data interchange format such as JSON. Or we may want to prompt
the system to break down complex tasks, using methods like chain-of-thought that
we’ll discuss in Section 12.4. All of these kinds of instructions go beyond simple
prompting and require further LLM ﬁnetuning to enable them to follow instructions.
We’ll return to this notion of instruction tuning in Section 12.3.
In summary, we prompt an LM by transforming each task into a form that is
amenable to contextual generation by an LLM, as follows:
1. For a given task, develop a a task-speciﬁc template that has a free parameter
for the input text.
2. Given that input and the task-speciﬁc template , the input is used to instantiate template
aﬁlled prompt that is then passed to a pretrained language model.
3. Autoregressive decoding is then used to generate a sequence of token outputs.
4. The output of the model can either be used directly as the desired output (as
in the case of naturally generative tasks such as translation or summarization),
or a task-appropriate answer can be extracted from the generated output (as in
the case of classiﬁcation).

--- Page 246 ---
246 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
12.1.1 Learning from Demonstrations: Few-Shot Prompting
It’s often possible to improve a prompt by including some labeled examples in the
prompt template. We call such examples demonstrations . The task of prompting demonstrations
with examples is sometimes called few-shot prompting , as contrasted with zero- few-shot
shot prompting which means instructions that don’t include labeled examples. zero-shot
Fig. 12.2 illustrates a few-shot example from an extractive question answering
task. The context combines the task deﬁnition along with three gold-standard ques-
tion and answer pairs from the training set.
Deﬁnition : This task is about writing a correct answer for the reading comprehension task.
Based on the information provided in a given passage, you should identify the shortest
continuous text span from the passage that serves as an answer to the given question. Avoid
answers that are incorrect or provides incomplete justiﬁcation for the question.
Passage : Beyonc ´e Giselle Knowles-Carter (born September 4, 1981) is an American singer,
songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in
various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead
singer of R&B girl-group Destiny’s Child. Managed by her father, Mathew Knowles, the group
became one of the world’s best-selling girl groups of all time. Their hiatus saw the release
of Beyonc ´e’s debut album, Dangerously in Love (2003), which established her as a solo artist
worldwide, earned ﬁve Grammy Awards and featured the Billboard Hot 100 number-one singles
“Crazy in Love” and “Baby Boy”.
Examples:
Q: In what city and state did Beyonc ´e grow up?
A: Houston, Texas
Q: What areas did Beyonc ´e compete in when she was growing up?
A: singing and dancing
Q: When did Beyonc ´e release Dangerously in Love?
A: 2003
Q: When did Beyonc ´e start becoming popular?
A:
Figure 12.2 A prompt for extractive question answering, from an example from the SQuAD 2.0 dataset
(Rajpurkar et al., 2018). The prompt contains the task deﬁnition, the passage, 3 demonstration examples,
followed by the test question. This deﬁnition speciﬁcation and format are after the Natural Instructions dataset
(Mishra et al., 2022).
How Many Demonstrations? The number of demonstrations doesn’t have to be
large. A small number of randomly selected labeled examples used as demonstra-
tions can be sufﬁcient to improve performance over the zero-shot setting. Indeed,
the largest performance gains in few-shot prompting tends to come from the ﬁrst
training example, with diminishing returns for subsequent demonstrations. This is
in contrast with ﬁnetuning of specialized classiﬁer heads that we saw in Chapter 11
where it helps to have lots of examples.
Why isn’t it useful to have more demonstrations? The reason is that the primary
beneﬁt in examples is to demonstrate the task to be performed to the LLM and the
format of the sequence, not to provide relevant information as to the right answer

--- Page 247 ---
12.1 • P ROMPTING 247
for any particular question. In fact, demonstrations that have incorrect answers can
still improve a system (Min et al., 2022; Webson and Pavlick, 2022). Adding too
many examples seems to cause the model to overﬁt to details of the exact examples
chosen and generalize poorly.
How to Select Demonstrations? Demonstrations are generally created by for-
matting examples drawn from a labeled training set There are some heuristics about
what makes a good demonstration. For example, using demonstrations that are sim-
ilarto the current input seems to improve performance. It can thus be useful to
dynamically retrieve demonstrations for each input, based on their similarity to the
current example (for example, comparing the embedding of the current example
with embeddings of each of the training set example to ﬁnd the best top- T).
But more generally, the best way to select demonstrations from the training set
is programmatically: choosing the set of demonstrations that most increases task
performance of the prompt on a test set. Task performance for sentiment analysis
or multiple-choice question answering can be measured in accuracy; for machine
translation with chrF, and for summarization via Rouge. Systems like DSPy (Khat-
tab et al., 2024), a framework for algorithmically optimizing LM prompts, can au-
tomatically ﬁnd the optimum set of demonstrations to include by searching through
the space of possible demonstrations to include. We’ll return to automatic prompt
optimization in Section 12.5.
12.1.2 In-Context Learning and Induction Heads
As a way of getting a model to do what we want, prompting is fundamentally differ-
ent than pretraining. Learning via pretraining means updating the model’s parame-
ters by using gradient descent according to some loss function. But prompting with
demonstrations can teach a model to do a new task. The model is learning something
as it processes the prompt.
Even without demonstrations, we can think of the process of prompting as a kind
of learning. For example, the further a model gets in a prompt, the better it tends
to get at predicting the upcoming tokens. The information in the context is helping
give the model more predictive power.
We use the term in-context learning to refer to either of these kinds of learningin-context
learning
that language models do from their prompts. In-context learning means language
models learning to do new tasks, better predict tokens, or generally reduce their
loss, but without any gradient-based updates to the model’s parameters.
How does in-context learning work? While we don’t know for sure, there are
some intriguing ideas. One hypothesis is based on the idea of induction heads induction heads
(Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit ,
which is a kind of abstract component of a network. The induction head circuit
is part of the attention computation in transformers, discovered by looking at mini
language models with only 1-2 attention heads.
The function of the induction head is to predict repeated sequences. For example
if it sees the pattern AB...A in an input sequence, it predicts that Bwill follow,
instantiating the pattern completion ruleAB...A!B. It does this by having a preﬁx
matching component of the attention computation that, when looking at the current
token A, searches back over the context to ﬁnd a prior instance of A. If it ﬁnds one,
the induction head has a copying mechanism that “copies” the token B that followed
the earlier A, by increasing the probability the B will occur next. Fig. 12.3 shows an
example.

--- Page 248 ---
248 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
Figure 1: In the sequence “...vintage cars ... vintage”, an induction head identiﬁes the initial occurrence of “vintage”,
attends to the subsequent word “cars” for preﬁx matching, and predicts “cars” as the next word through the copying
mechanism.
determines each head’s independent output for the
current token.
Leveraging this decomposition, Elhage et al.
(2021 ) discovered a distinct behaviour in certain
attention heads, which they named induction heads .
This behaviour emerges when these heads process
sequences of the form "[A] [B] ... [A] →". In
these heads, the QK circuit directs attention to-
wards [B], which appears directly after the previous
occurrence of the current token [A]. This behaviour
is termed preﬁx matching . The OV circuit subse-
quently increases the output logit of the [B] token,
termed copying . An overview of this mechanism is
shown in Figure 1.
4 Methods
4.1 Models
We utilise two recently developed open-source
models, namely Llama-3-8B2and InternLM2-20B
(Cai et al. ,2024 ), both of which are based on the
original Llama ( Touvron et al. ,2023a ) architec-
ture. These models feature grouped-query atten-
tion mechanisms ( Ainslie et al. ,2023 ) to enhance
efﬁciency. Llama-3-8B, comprises 32 layers, each
with 32 attention heads and it uses a query group
size of 4 attention heads. It has shown superior
performance compared to its predecessors, even
the larger Llama-2 models.
InternLM2-20B, featuring 48 layers with 48 at-
tention heads each, uses a query group size of 6
attention heads. We selected InternLM2-20B for
its exemplary performance on the Needle-in-the-
Haystack3task, which assesses LLMs’ ability to
retrieve a single critical piece of information em-
bedded within a lengthy text. This mirrors the
functionality of induction heads, which scan the
context for prior occurrences of a token to extract
relevant subsequent information.
2https://ai.meta.com/blog/meta-llama-3/
3https://github.com/gkamradt/LLMTest_
NeedleInAHaystack4.2 Identifying Induction Heads
To identify induction heads within models, we mea-
sure the ability of all attention heads to perform
preﬁx matching on random input sequences.4We
follow the task-agnostic approach to computing pre-
ﬁx matching scores outlined by Bansal et al. (2023 ).
We argue that focusing solely on preﬁx matching
scores is sufﬁcient for our analysis, as high pre-
ﬁx matching cores speciﬁcally indicate induction
heads, while less relevant heads tend to show high
copying capabilities ( Bansal et al. ,2023 ). We gen-
erate a sequence of 50 random tokens, excluding
the 4% most common and least common tokens.
This sequence is repeated four times to form the
input to the model. The preﬁx matching score is cal-
culated by averaging the attention values from each
token to the tokens that directly followed the same
token in earlier repeats. The ﬁnal preﬁx matching
scores are averaged over ﬁve random sequences.
The preﬁx matching scores for Llama-3-8B are
shown in Figure 2. For IntermLM2-20B, we refer
to Figure 8in Appendix A.1. Both models exhibit
heads with notably high preﬁx matching scores,
distributed across various layers. In the Llama-3-
8B model, ~3% of the heads have a preﬁx matching
score of 0.3 or higher, indicating a degree of spe-
cialisation in preﬁx matching, and some heads have
high scores of up to 0.98.
4.3 Head Ablations
To investigate the signiﬁcance of induction heads
for a speciﬁc ICL task, we conduct zero-ablations
of 1% and 3% of the heads with the highest preﬁx
matching scores. This ablation process involves
masking the corresponding partition of the output
matrix, denoted as Wh
oin Eq. 1, by setting it to
zero. This effectively renders the heads inactive
4In this work, the term "induction heads" refers to what
we deﬁne as behavioural induction heads, not mechanistic
ones. A true induction head must be veriﬁed mechanistically;
however, our analysis employs preﬁx-matching scores as a
proxy. We will continue to use the term "induction heads" for
simplicity throughout the rest of the paper.
4
Figure 12.3 An induction head looking at vintage uses the preﬁx matching mechanism to
ﬁnd a prior instance of vintage , and the copying mechanism to predict that cars will occur
again. Figure from Crosbie and Shutova (2022).
Olsson et al. (2022) propose that a generalized fuzzy version of this pattern com-
pletion rule, implementing a rule like A*B*...A!B*, where A*A and B*B
(bywe mean they they are semantically similar in some way), might be respon-
sible for in-context learning. Suggestive evidence for their hypothesis comes from
Crosbie and Shutova (2022), who show that ablating induction heads causes in- ablating
context learning performance to decrease. Ablation is originally a medical term
meaning the removal of something. We use it in NLP interpretability studies as
a tool for testing causal effects; if we knock out a hypothesized cause, we would
expect the effect to disappear. Crosbie and Shutova (2022) ablate induction heads
by ﬁrst ﬁnding attention heads that perform as induction heads on random input
sequences, and then zeroing out the output of these heads by setting certain terms
of the output matrix WOto zero. Indeed they ﬁnd that ablated models are much
worse at in-context learning: they have much worse performance at learning from
demonstrations in the prompts.
12.2 Post-training and Model Alignment
With simple prompting, LLMs have been successfully applied to a range of appli-
cations without the need to update the parameters in the underlying models. Nev-
ertheless, there are limits to how much can be expected from a model whose sole
training objective is to predict the next word from large amounts of pretraining text.
To see this, consider the following failed examples of following instructions from
early work with GPT (Ouyang et al., 2022).
Prompt : Explain the moon landing to a six year old in a few sentences.
Output : Explain the theory of gravity to a 6 year old.
Prompt : Translate to French: The small dog
Output : The small dog crossed the road.
Here, the LLM ignores the intent of the request and relies instead on its natural
inclination to autoregressively generate continuations consistent with its context. In
the ﬁrst example, it outputs a text somewhat similar to the original request, and in the
second it provides a continuation to the given input, ignoring the request to translate.
LLMs are not sufﬁciently helpful : they need extra training to increase their abilities
to follow textual instructions.
A deeper problem is that LLMs can simultaneously be too harmful . Pretrained
language models easily generate text that is harmful in many ways. For example
they can generate text that is false , including unsafe misinformation like giving dan-
gerously incorrect answers to medical questions. And they can generate text that is

--- Page 249 ---
12.3 • M ODEL ALIGNMENT : INSTRUCTION TUNING 249
toxic in many ways, such as facilitating the spread of hate speech. Gehman et al.
(2020) show that even completely non-toxic prompts can lead large language mod-
els to output hate speech and abuse their users. Or language models can generate
stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng
et al., 2019) about many demographic groups.
One reason LLMs are too harmful and insufﬁciently helpful is that their pre-
training objective (success at predicting words in text) is misaligned with the human
need for models to be helpful and non-harmful.
In an attempt to address these two problems, language models generally include
two additional kinds of training for model alignment : methods designed to adjustmodel
alignment
LLMs to better align them to human needs for models to be helpful and non-harmful.
In the ﬁrst technique, instruction tuning (or sometimes called SFT for supervised
ﬁnetuning), models are ﬁnetuned on a corpus of instructions and questions with
their corresponding responses. In the second technique, preference alignment , of-
ten called RLHF after one of the speciﬁc instantiations, Reinforcement Learning
from Human Feedback, a separate model is trained to decide how much a candidate
response aligns with human preferences. This model is then used to ﬁnetune the
base model.
We’ll use the term base model to mean a model that has been pretrained but base model
hasn’t yet been aligned either by instruction tuning or RLHF. And we refer to these aligned
steps as post-training , meaning that they apply after the model has been pretrained. post-training
12.3 Model Alignment: Instruction Tuning
Instruction tuning (short for instruction ﬁnetuning , and sometimes even short-Instruction
tuning
ened to instruct tuning ) is a method for making an LLM better at following instruc-
tions. It involves taking a base pretrained LLM and training it to follow instructions
for a range of tasks, from machine translation to meal planning, by ﬁnetuning it on
a corpus of instructions and responses. The resulting model not only learns those
tasks, but also engages in a form of meta-learning – it improves its ability to follow
instructions generally.
Instruction tuning is a form of supervised learning where the training data con-
sists of instructions and we continue training the model on them using the same
language modeling objective used to train the original model. In the case of causal
models, this is just the standard guess-the-next-token objective. The training corpus
of instructions is simply treated as additional training data, and the gradient-based
updates are generating using cross-entropy loss as in the original model training.
Even though it is trained to predict the next token (which we traditionally think of
as self-supervised), we call this method supervised ﬁne tuning (orSFT) because SFT
unlike in pretraining, each instruction or question in the instruction tuning data has
a supervised objective: a correct answer to the question or a response to the instruc-
tion.
How does instruction tuning differ from the other kinds of ﬁnetuning introduced
in Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the ﬁrst exam-
ple, introduced in, Chapter 10 we can ﬁnetune as a way of adapting to a new domain
by just continuing pretraining the LLM on data from a new domain. In this method
all the parameters of the LLM are updated.
In the second example, also from Chapter 10, parameter-efﬁcient ﬁnetuning ,
we adapt to a new domain by creating some new (small) parameters, and just adapt-

--- Page 250 ---
250 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
Pretrained LLM
Continue training all paramaterson ﬁnetuning domainFinetuningInferencePretraining
On ﬁnetuning domainFinetuning asContinuedPretrainingParameterEﬃcientFinetuning(e.g., LoRA)Pretrained LLMABPretrained LLMMLMFinetuning
…
…
…
…
…
…
…InstructionTuning(SFT)
On ﬁnetuning domain
On ﬁnetuning task
On unseen tasksNext wordpredictionobjectiveData from ﬁnetuning domain
Train only new parameters on ﬁnetuning domainNext wordpredictionobjectiveData from ﬁnetuning domain
Train only classiﬁcation head on ﬁnetuning taskTaskspeciﬁclossSupervised data from task
Instruction tuning on diverse tasksNext word predictionobjectiveSupervised instructions+
…
Figure 12.4 Instruction tuning compared to the other kinds of ﬁnetuning.
ing them to the new domain. In LoRA, for example, it’s the A and B matrices that
we adapt, but the pretrained model parameters are frozen.
In the task-based ﬁnetuning of Chapter 11, we adapt to a particular task by
adding a new specialized classiﬁcation head and updating its features via its own
loss function (e.g., classiﬁcation or sequence labeling); the parameters of the pre-
trained model may be frozen or might be slightly updated.
Finally, in instruction tuning, we take a dataset of instructions and their super-
vised responses and continue to train the language model on this data, based on the
standard language model loss.
Instruction tuning, like all of these kinds of ﬁnetuning, is much more modest
than the training of base LLMs. Training typically involves several epochs over
instruction datasets that number in the thousands. The overall cost of instruction
tuning is therefore a small fraction of the original cost to train the base model.
12.3.1 Instructions as Training Data
Byinstruction , we have in mind a natural language description of a task to be per-
formed, combined with labeled task demonstrations. This can include minimal de-
scriptions similar to the prompts we’ve already seen such as Answer the following
question ,Translate the following text to Arapaho , orSummarize this report . How-

--- Page 251 ---
12.3 • M ODEL ALIGNMENT : INSTRUCTION TUNING 251
ever, since we will be using supervised ﬁnetuning to update the model, these in-
structions need not be limited to simple prompts designed to evoke a behavior found
in the pretraining corpora. Instructions can also include length restrictions or other
constraints, personas to assume, and demonstrations.
Many huge instruction tuning datasets have been created, covering many tasks
and languages. For example Aya gives 503 million instructions in 114 languages
from 12 tasks including question answering, summarization, translation, paraphras-
ing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).
SuperNatural Instructions 12 million examples from 1600 tasks (Wang et al., 2022),
Flan 2022 15 million examples from 1836 tasks (Longpre et al., 2023), and OPT-
IML 18 million examples from 2000 tasks (Iyer et al., 2022).
These instruction-tuning datasets are created in four ways. The ﬁrst is for people
to write the instances directly. For example, part of the Aya instruct ﬁnetuning cor-
pus (Fig. 12.5) includes 204K instruction/response instances written by 3000 ﬂuent
speakers of 65 languages volunteering as part of a participatory research initiative
with the goal of improving multilingual performance of LLMs.
Lang PromptCompletionara.ڰ٭ژ੅ݠﺍࠍ ොຳ ؇ﺀڢݱ٭ڎﺓ݁ڎﺡ݆݁ ૰૙؆࿓ ڢًܾఈఃُߺࠊﻙلگٴںڣَݯ ৎ৊ﻥَ݁ڎﺡﺍ ֣ﺇ؇ﺩ༚ﻭᕚ৙৑؇ﻉﻭَﺍ ༟ීෂﺍ ༃຃؜ََ݆݁ڎ ُܾዛዊڣ۰ِ݁ ఈఃِ੅ﺍࠍّਲ਼َڍَ؇؜ ༠ᕚَܳިﺃ ؇ﺀ૰૙֣৕৑ﺍ ᄭᄟَਲ਼َਦ لُިﺍﺯﻱ ቕَረfra Qui a écrit le livre LaSagouine?Antonine Maillet a écrit La Sagouine en 1971. Elle a écrit beau-coup d’histoires sur les Acadiens (les francophones de la côte Estdu Canada) et a fait connaître l’Acadie au monde.ibo Kedu ka ndị Igbo si emeemume ji ọhụrụ?Emume ji ọhụrụ n’omenala Igbo, bụ emume na-egosi njedebeoge ọrụ ugbo, na-amalite site n’emume ekele ma tinyekwa oririna egwu. Ọ bụ mmemme ọgbakọ ebe a na-ekerịta ji ọhụrụ, na-egosipụta ekele na mmalite nke oge ọhụrụ, na-eme ka mmekọrịtammadụ na ibe ya dịkwuo elu.por QualéacapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, é Salvador. Fundadaem 1549, foi a primeira capital do Brasil e é conhecida porseu patrimônio cultural rico e diversificado. A cidade é famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro histórico que foi declarado Patrimônio Mundial pela UN-ESCO.pesﺩﺭﺑﺮﺝﮐﺒﻮﺗﺮﺧﺎﻧﻪﻣﻴﺒﺪﭼﻨﺪﻻﻧﻪﮐﺒﻮﺗﺮ ﻭﺟﻮﺩﺩﺍﺭﺩ؟ﻫﺰﺍﺭﻻﻧﻪﮐﺒﻮﺗﺮﻭﺟﻮﺩﺩﺍﺭﺩﻭﻧﻮﺍﺭ ﺍﯼﺍﺳﺖﮐﻪﺩﺍﺧﻞﺁﻥﭼﻬﺎﺭ ﺧﺎﻧﻪﻣﻴﺒﺪﺑﺮﺟﯽﺍﺳﺘﻮﺍﻧﻪ ﮐﺒﻮﺗﺮﺳﻔﻴﺪﺭﻧﮕﯽﺑﺮﺍﯼﺟﺬﺏﮐﺒﻮﺗﺮﺍﻥﺩﻭﺭﺗﺎﺩﻭﺭﺑﺮﺝﮐﺸﻴﺪﻩﺷﺪﻩﺍﺳﺖ.ﺍﻳﻦﺑﺮﺝﺩﺭﺑﺮﺍﺑﺮﺧﻄﺮ ﺣﻤﻠﻪﺩﻳﮕﺮﺣﻴﻮﺍﻧﺎﺕﺑﻪﮐﺒﻮﺗﺮﻫﺎﺑﺴﻴﺎﺭﺍﻳﻤﻦﺑﻮﺩﻩﺍﺳﺖ. msa Apakah nasi lemak?Nasi lemak merupakan makanan tradisi orang Melayu yangterdapat di semua bahagian Malaysia, Singapura, Indonesia(terutama di Riau, Jambi serta utara dan pantai timur Su-matera) dan Brunei. Sajian ini merujuk kepada nasi yang di-masak dengan menggunakan santan kelapa bagi menambah rasalemaknya. Kadangkala, daunpandanwangidimasukkansemasanasi dimasak bagi menambahkan aromanya.tamெசயற்ைக நுண்ணற§வுஎன்றால்என்ன?ெபாதுவாக மனிதர்களால் ெசய்யப்படும் பணிகைளச்ெசய்ய ஒரு கணினி அல்லது ஒரு கணினியால்கட்டுப்படுத்தப்படும்ஒருேராேபாவ¥ன்த¦றன்ெசயற்ைகநுண்ணற§வுஎனப்படும்.Table 3: Examples of prompt and completions in the AyaDataset.
tors is not uniform across languages. Moreover, within each language, there is a lack of consistent
contributions from all annotators. In this section, we examine the impact of annotator skew on the
resulting dataset.
Annotator Skew Across Languages. Annotators were encouraged to contribute to any language
in which they could comfortably read and write and were asked to focus most of their e ﬀorts on
languages other than English . Although a signiﬁcant number of participants registered for many
languages, the engagement level of annotators was not equal, which resulted in considerable di ﬀer-
ences in the number of contributions across languages. Figure 10(top) provides an overview of the
percentage of each language present in the ﬁnal compilation. The highest number of contributions
is for Malagasy with 14,597 instances, and the lowest is 79 for Kurdish .
Annotator Skew Within a Language. The ﬁnal contributions for each language in the Aya
Dataset are not evenly distributed among annotators. The median number of annotators per lan-
guage is 15 (mean is 24.75) with one language having only a single active annotator ( Sindhi )a n d
14
Figure 12.5 Samples of prompt/completion instances in 4 of the 65 languages in the Aya
corpus (Singh et al., 2024).
Developing high quality supervised training data in this way is time consuming
and costly. A more common approach makes use of the copious amounts of super-
vised training data that have been curated over the years for a wide range of natural
language tasks. There are thousands of such datasets available, like the SQuAD
dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of
translations or summarization. This data can be automatically converted into sets of
instruction prompts and input/output demonstration pairs via simple templates.
Fig. 12.6 illustrates examples for some applications from the S UPER NATURAL IN-
STRUCTIONS resource (Wang et al., 2022), showing relevant slots such as text,
context , and hypothesis . To generate instruction-tuning data, these ﬁelds and the
ground-truth labels are extracted from the training data, encoded as key/value pairs,
and inserted in templates (Fig. 12.7) to produce instantiated instructions. Because
it’s useful for the prompts to be diverse in wording, language models can also be
used to generate paraphrase of the prompts.
Because supervised NLP datasets are themselves often produced by crowdwork-

--- Page 252 ---
252 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
Few-Shot Learning for QA
Task Keys Values
Sentiment text Did not like the service that I was provided...
label 0
text It sounds like a great plot, the actors are ﬁrst grade, and...
label 0
NLI premise No weapons of mass destruction found in Iraq yet.
hypothesis Weapons of mass destruction found in Iraq.
label 2
premise Jimmy Smith... played college football at University of Col-
orado.
hypothesis The University of Colorado has a college football team.
label 0
Extractive Q/A context Beyonc ´e Giselle Knowles-Carter is an American singer...
question When did Beyonce start becoming popular?
answersftext : [’in the late 1990s’], answerstart : 269g
Figure 12.6 Examples of supervised training data for sentiment, natural language inference and Q/A tasks.
The various components of the dataset are extracted and stored as key/value pairs to be used in generating
instructions.
Task Templates
Sentiment -fftextggHow does the reviewer feel about the movie?
-The following movie review expresses what sentiment?
fftextgg
-fftextggDid the reviewer enjoy the movie?
Extractive Q/A -ffcontextggFrom the passage, ffquestiongg
-Answer the question given the context. Context:
ffcontextggQuestion:ffquestiongg
-Given the following passage ffcontextgg, answer the
questionffquestiongg
NLI -SupposeffpremiseggCan we infer that ffhypothesisgg?
Yes, no, or maybe?
-ffpremiseggBased on the previous passage, is it true
thatffhypothesisgg? Yes, no, or maybe?
-GivenffpremiseggShould we assume that ffhypothesisgg
is true? Yes,no, or maybe?
Figure 12.7 Instruction templates for sentiment, Q/A and NLI tasks.
ers based on carefully written annotation guidelines, a third option is to draw on
these guidelines, which can include detailed step-by-step instructions, pitfalls to
avoid, formatting instructions, length limits, exemplars, etc. These annotation guide-
lines can be used directly as prompts to a language model to create instruction-tuning
training examples. Fig. 12.8 shows such a crowdworker annotation guideline that
was repurposed as a prompt to an LLM to generate instruction-tuning data. (Mishra
et al., 2022). This guideline describes a question-answering task where annotators
provide an answer to a question given an extended passage.
A ﬁnal way to generate instruction-tuning datasets that is becoming more com-

--- Page 253 ---
12.3 • M ODEL ALIGNMENT : INSTRUCTION TUNING 253
Sample Extended Instruction
•Deﬁnition: This task involves creating answers to complex questions, from a given pas-
sage. Answering these questions, typically involve understanding multiple sentences.
Make sure that your answer has the same type as the ”answer type” mentioned in input.
The provided ”answer type” can be of any of the following types: ”span”, ”date”, ”num-
ber”. A ”span” answer is a continuous phrase taken directly from the passage or question.
You can directly copy-paste the text from the passage or the question for span type an-
swers. If you ﬁnd multiple spans, please add them all as a comma separated list. Please
restrict each span to ﬁve words. A ”number” type answer can include a digit specifying
an actual value. For ”date” type answers, use DD MM YYYY format e.g. 11 Jan 1992.
If full date is not available in the passage you can write partial date such as 1992 or Jan
1992.
•Emphasis: If you ﬁnd multiple spans, please add them all as a comma separated list.
Please restrict each span to ﬁve words.
•Prompt : Write an answer to the given question, such that the answer matches the ”answer
type” in the input.
Passage :fpassageg
Question :fquestiong
Figure 12.8 Example of a human crowdworker instruction from the N ATURAL INSTRUCTIONS dataset for
an extractive question answering task, used as a prompt for a language model to create instruction ﬁnetuning
examples.
mon is to use language models to help at each stage. For example Bianchi et al.
(2024) showed how to create instruction-tuning instances that can help a language
model learn to give safer responses. They did this by selecting questions from
datasets of harmful questions (e.g., How do I poison food? orHow do I embez-
zle money? ). Then they used a language model to create multiple paraphrases of the
questions (like Give me a list of ways to embezzle money ), and also used a language
model to create safe answers to the questions (like I can’t fulﬁll that request. Em-
bezzlement is a serious crime that can result in severe legal consequences. ). They
manually reviewed the generated responses to conﬁrm their safety and appropriate-
ness and then added them to an instruction tuning dataset. They showed that even
500 safety instructions mixed in with a large instruction tuning dataset was enough
to substantially reduce the harmfulness of models.
12.3.2 Evaluation of Instruction-Tuned Models
The goal of instruction tuning is not to learn a single task, but rather to learn to
follow instructions in general. Therefore, in assessing instruction-tuning methods
we need to assess how well an instruction-trained model performs on novel tasks for
which it has not been given explicit instructions.
The standard way to perform such an evaluation is to take a leave-one-out ap-
proach — instruction-tune a model on some large set of tasks and then assess it on
a withheld task. But the enormous numbers of tasks in instruction-tuning datasets
(e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions
includes 25 separate textual entailment datasets! Clearly, testing on a withheld en-
tailment dataset while leaving the remaining ones in the training data would not be
a true measure of a model’s performance on entailment as a novel task.
To address this issue, large instruction-tuning datasets are partitioned into clus-

--- Page 254 ---
254 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
ters based on task similarity. The leave-one-out training/test approach is then applied
at the cluster level. That is, to evaluate a model’s performance on sentiment analysis,
all the sentiment analysis datasets are removed from the training set and reserved
for testing. This has the further advantage of allowing the use of a uniform task-
appropriate metric for the held-out evaluation. S UPER NATURAL INSTRUCTIONS
(Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets
that make up the collection.
12.4 Chain-of-Thought Prompting
There are a wide range of techniques to use prompts to improve the performance of
language models on many tasks. Here we describe one of them, called chain-of-
thought prompting.chain-of-
thought
The goal of chain-of-thought prompting is to improve performance on difﬁcult
reasoning tasks that language models tend to fail on. The intuition is that people
solve these tasks by breaking them down into steps, and so we’d like to have lan-
guage in the prompt that encourages language models to break them down in the
same way.
The actual technique is quite simple: each of the demonstrations in the few-shot
prompt is augmented with some text explaining some reasoning steps. The goal is to
cause the language model to output similar kinds of reasoning steps for the problem
being solved, and for the output of those reasoning steps to cause the system to
generate the correct answer.
Indeed, numerous studies have found that augmenting the demonstrations with
reasoning steps in this way makes language models more likely to give the correct
answer difﬁcult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023b). Fig. 12.9
shows an example where the demonstrations are augmented with chain-of-thought
text in the domain of math word problems (from the GSM8k dataset of math word
problems (Cobbe et al., 2021). Fig. 12.10 shows a similar example from the BIG-
Bench-Hard dataset (Suzgun et al., 2023b).
Figure 12.9 Example of the use of chain-of-thought prompting (right) versus standard
prompting (left) on math word problems. Figure from Wei et al. (2022).

--- Page 255 ---
12.5 • A UTOMATIC PROMPT OPTIMIZATION 255
(B)Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: (D)Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA:Model OutputModel OutputModel Input (“Answer-Only” Prompting)
Wake-up time: 5am. 5am-6am: buying clothes at the mall. 6am-11am: watching a movie at the theater.11am-1pm: getting a coffee at the cafe.1pm-3pm: working at the office. 3pm-5pm: waiting at the airport. 5pm-6pm: free. The soccer field closure time: 6pm. The only time when Hannah could have gone to the soccer field was 5pm to 6pm. So the answer is (C).Model Input (Chain-of-Thought Prompting)
Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: Let's think step by step. Wake-up time: 5am. [...] The only time when Tiffany could have gone to the beach was 3pm to 4pm. So the answer is (D).Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA: Let's think step by step. Task DescriptionQuestionChain-of-ThoughtTest-Time QuestionTask DescriptionQuestionTest-Time QuestionAnswer
Generated Chain-of-ThoughtGenerated AnswerOptionsOptionsFigure 3:An illustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsinclude task descriptions and options in the input prompt. The task here isTemporal Sequences.“let’s think step-by-step” (Kojima et al.,2022) toall CoT annotations in the few-shot exemplars. Anexample of a CoT prompt is shown in Figure3.Language models.We consider three fami-lies of language models: Codex (Chen et al.,2021a), InstructGPT (Ouyang et al.,2022;Brownet al.,2020), and PaLM (Chowdhery et al.,2022).For Codex, we focus on code-davinci-002, code-davinci-002, and code-cushman-001. For Instruct-GPT, we use text-davinci-002, text-curie-002, text-babbgage-001, and text-ada-001. For PaLM, weuse the three available sizes: 8B, 62B, and 540B.Evaluation protocol.We evaluate all languagemodels via greedy decoding (i.e., temperature sam-pling with temperature parameter⌧=0). Weextract the ﬁnal answer based on keywords thatthe language model is expected to produce (i.e.,“the answer is”). We measure accuracy using exactmatch (EM), computed by comparing the generatedoutput with the ground-truth label.44 Results4.1 Standard answer-only promptingunderestimates model capabilitiesTable2summarizes the performance of PaLM, In-structGPT, and Codex models on BBH for answer-only and CoT prompting approaches. Whileanswer-only prompting has been used as the stan-4For multiple-choice tasks, this setup differs slightly fromrank/scoring classiﬁcation (Brown et al.,2020;Srivastavaet al.,2022;Lampinen et al.,2022). We provide a languagemodel with all multiple-choice options at once, generate anoutput based on the input, and measure exact match accuracy.dard in many prior work (Brown et al.,2020;Raeet al.,2021;Hoffmann et al.,2022;Srivastava et al.,2022), it typically underestimates model perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires sufﬁcient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In Figure4, we plot the performance of bothCoT and answer-only prompting (no CoT) as a13006
Figure 12.10 Example of the use of chain-of-thought prompting (right) vs standard prompting (left) in a
reasoning task on temporal sequencing. Figure from Suzgun et al. (2023b).
12.5 Automatic Prompt Optimization
Given a prompt for a task (human or computer generated), prompt optimization
methods search for prompts with improved performance. Most of these approaches
can be viewed as a form of iterative improvement search (Russell and Norvig, 2002)
through a space of possible prompts for those that optimize performance on a task.
As such, these approaches all share the following components:
• A start state – An initial human or machine generated prompt or prompts
suitable for some task.
• A scoring metric – A method for assessing how well a given prompt performs
on the task.
• An expansion method – A method for generating variations of a prompt.
Given the enormous variation in how prompts for a single task can be expressed in
language, search methods have to be constrained to a reasonable space. Beam search
is a widely used method that combines breadth-ﬁrst search with a ﬁxed-width pri-
ority queue that focuses the search effort on the top performing variants. Fig. 12.11
outlines the general approach behind most current prompt optimization methods.
Beginning with initial candidate prompt(s), the algorithm generates variants and
adds them to a list of prompts to be considered. These prompts are then selectively
added to the active list based on whether their scores place them in the top set of
candidates. A beam width of 1 results in a focused greedy search, whereas an inﬁnite
beam width results in an exhaustive breadth ﬁrst search. The goal is to continue
to seek improved prompts given the computational resources available. Iterative
improvement searches typically use a combination of a ﬁxed number of iterations in
combination with a failure to improve after some period to time as stopping criteria.
This latter is equivalent to early stopping with patience used in training deep neural
networks.

--- Page 256 ---
256 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
function PROMPT OPTIMIZATION (prompts ,width )returns optimized prompts
active prompts ; Add initial set of candidate prompts to agenda
repeat until done
frontier foo
children EXPAND (c) ; Expand each candidate prompt
foreach c2children
frontier ADDTOBEAM (c,frontier, w ) ; apply it, creating a new state
active frontier
return
function ADDTOBEAM (state ,agenda ,width )returns updated agenda
ifLENGTH (agenda )<width then
agenda INSERT (state ,agenda )
else if SCORE (state )>SCORE (WORST OF(agenda ))
agenda REMOVE (WORST OF(agenda ))
agenda INSERT (state ,agenda )
return agenda
Figure 12.11 A generic iterative-improvement beam search for prompt optimization.
12.5.1 Candidate Scoring
Candidate scoring methods assess the likely performance of potential prompts, both
to identify promising avenues of search and to prune those that are unlikely to be
effective. Since candidate scoring is embedded in the inner-loop of the search, the
computational cost of scoring is critical.
Given access to labeled training data, candidate prompts can be scored based on
execution accuracy (Honovich et al., 2023). In this approach, candidate promptsexecution
accuracy
are combined with inputs sampled from the training data and passed to an LLM for
decoding. The LLM output is evaluated against the training label using a metric
appropriate for the task. In the case of classiﬁcation-based tasks, this is effectively a
0/1 loss — how many examples were correctly labeled with the given prompt. Gen-
erative applications such as summarization or translation use task-speciﬁc similarity
scores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004).
Given the computational cost of issuing calls to an LLM, evaluating each can-
didate prompt against a complete training set would be infeasible. Instead, prompt
performance is estimated from a small sample of training data (Pryzant et al., 2023).
12.5.2 Prompt Expansion
Prompt expansion generates variants of a given prompt to create an expanded set of
neighboring prompts that may improve performance over the original. A common
method is to use language models to create paraphrases. For example Zhou et al.
(2023) use the following meta-prompt to elicit a variant prompt from an original:
Prompting for a Variant
Generate a variation of the following instruction while keeping the semantic meaning.
Input:fINSTRUCTIONg
Output:fCOMPLETEg

--- Page 257 ---
12.5 • A UTOMATIC PROMPT OPTIMIZATION 257
A variation of this method is to truncate the current prompt at a set of random loca-
tions, generating a set of prompt preﬁxes. The paraphrasing LLM is then asked to
continue each the preﬁxes to generate a complete prompt.
This methods is an example of an uninformed search. That is, the candidate
expansion step is not directed towards generating better candidates; candidates are
generated without regard to their quality. It it is the job of the priority queue to
elevate improved candidates when they are found. By contrast, Prasad et al. (2023)
employ a candidate expansion technique that explicitly attempts to generate superior
prompts during the expansion process. In this approach, the current candidate is ﬁrst
applied to a sample of training examples using the execution accuracy approach.
The prompt’s performance on these examples then guides the expansion process.
Speciﬁcally, incorrect examples are used to critique the original prompt — with
the critique playing the role of a gradient for the search. The method includes the
following steps.
1. Run the prompt on a sample of training examples,
2. Identify examples where the prompt fails,
3. Ask an LLM to produce a critique of the prompt in light of the failed examples,
4. Provide the resulting critique to an LLM, and ask it to generate improved
prompts.
Given a prompt and a set of failed examples, Prasad et al. (2023) use the follow-
ing template for a classiﬁer task to solicit critiques from a target LLM.
Critiquing Prompt
I'm trying to write a zero-shot classifier prompt.
My current prompt is: fpromptg
But this prompt gets the following examples wrong:
ferrorstringg
Givefnumfeedbacksgreasons why the prompt could have
gotten these examples wrong.
This model feedback is then combined with a second template to elicit improved
prompts from the LLM.
Prompt Improvement Prompt
I'm trying to write a zero-shot classifier. My current prompt is:
fpromptg
But it gets the following examples wrong: ferrorstrg
Based on these examples the problem with this prompt is that fgradientg.
Based on the above information, I wrote fstepspergradientgdifferent
improved prompts. Each prompt is wrapped with <START> and <END>.
Thefstepspergradientgnew prompts are:

--- Page 258 ---
258 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
12.6 Evaluating Prompted Language Models
Language models are evaluated in many ways. we introduced some evaluations for
in Section 10.4, including measuring the language model’s perplexity on a test set,
evaluating its accuracy on various NLP tasks, as well as benchmarks that help mea-
sure efﬁciency, toxicity, fairness, and so on. We’ll have further discussion of eval-
uate NLP tasks in future chapters; machine translation in Chapter 13 and question
answering and information retrieval in Chapter 14.
Here we just brieﬂy show the mechanism for measuring accuracy in a prompt-
ing setup for tests that have multiple-choice questions. We show this for MMLU MMLU
(Massive Multitask Language Understanding), a commonly-used dataset of 15908
knowledge and reasoning questions in 57 areas including medicine, mathematics,
computer science, law, and others. For example, here is an MMLU question from
the microeconomics domain:1
MMLU microeconomics example
One of the reasons that the government discourages and regulates monopo-
lies is that
(A) producer surplus is lost and consumer surplus is gained.
(B) monopoly prices ensure productive efﬁciency but cost society allocative
efﬁciency.
(C) monopoly ﬁrms do not engage in signiﬁcant research and development.
(D) consumer surplus is lost with higher prices and lower levels of output.
Fig. 12.12 shows the way MMLU turns these questions into prompted tests of a
language model, in this case showing an example prompt with 2 demonstrations.
MMLU mathematics prompt
The following are multiple choice questions about high school mathematics.
How many numbers are in the list 25, 26, ..., 100?
(A) 75 (B) 76 (C) 22 (D) 23
Answer: B
Compute i+i2+i3++i258+i259.
(A) -1 (B) 1 (C) i (D) -i
Answer: A
If 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps?
(A) 28 (B) 21 (C) 40 (D) 30
Answer:
Figure 12.12 Sample 2-shot prompt from MMLU testing high-school mathematics. (The
correct answer is (C)).
1For those of you whose economics is a bit rusty, the correct answer is (D).

--- Page 259 ---
12.7 • M ODEL ALIGNMENT WITH HUMAN PREFERENCES : RLHF AND DPO 259
12.7 Model Alignment with Human Preferences: RLHF
and DPO
TBD
12.8 Summary
This chapter has explored the topic of prompting large language models to follow
instructions. Here are some of the main points that we’ve covered:
• Simple prompting can be used to map practical applications to problems that
can be solved by LLMs without altering the model.
• Labeled examples ( demonstrations ) can be used to provide further guidance
to a model via few-shot learning.
• Methods like chain-of-thought can be used to create prompts that help lan-
guage models deal with complex reasoning problems.
• Pretrained language models can be altered to behave in desired ways through
model alignment .
• One method for model alignment is instruction tuning , in which the model
is ﬁnetuned (using the next-word-prediction language model objective) on
a dataset of instructions together with correct responses. Instruction tuning
datasets are often created by repurposing standard NLP datasets for tasks like
question answering or machine translation.
Bibliographical and Historical Notes

--- Page 261 ---
Part II
NLP APPLICATIONS
In this second part of the book we introduce fundamental NLP applications:
machine translation, information retrieval, question answering, dialogue systems,
and speech recognition.

--- Page 263 ---
CHAPTER
13Machine Translation
“I want to talk the dialect of your people. It’s no use of talking unless
people understand what you say.”
Zora Neale Hurston, Moses, Man of the Mountain 1939, p. 121
This chapter introduces machine translation (MT), the use of computers to trans-machine
translation
MT late from one language to another.
Of course translation, in its full generality, such as the translation of literature, or
poetry, is a difﬁcult, fascinating, and intensely human endeavor, as rich as any other
area of human creativity.
Machine translation in its present form therefore focuses on a number of very
practical tasks. Perhaps the most common current use of machine translation is
forinformation access . We might want to translate some instructions on the web,information
access
perhaps the recipe for a favorite dish, or the steps for putting together some furniture.
Or we might want to read an article in a newspaper, or get information from an
online resource like Wikipedia or a government webpage in some other language.
MT for information
access is probably
one of the most com-
mon uses of NLP
technology, and Google
Translate alone (shown above) translates hundreds of billions of words a day be-
tween over 100 languages. Improvements in machine translation can thus help re-
duce what is often called the digital divide in information access: the fact that much digital divide
more information is available in English and other languages spoken in wealthy
countries. Web searches in English return much more information than searches in
other languages, and online resources like Wikipedia are much larger in English and
other higher-resourced languages. High-quality translation can help provide infor-
mation to speakers of lower-resourced languages.
Another common use of machine translation is to aid human translators. MT sys-
tems are routinely used to produce a draft translation that is ﬁxed up in a post-editing post-editing
phase by a human translator. This task is often called computer-aided translation
orCAT . CAT is commonly used as part of localization : the task of adapting content CAT
localization or a product to a particular language community.
Finally, a more recent application of MT is to in-the-moment human commu-
nication needs. This includes incremental translation, translating speech on-the-ﬂy
before the entire sentence is complete, as is commonly used in simultaneous inter-
pretation. Image-centric translation can be used for example to use OCR of the text
on a phone camera image as input to an MT system to translate menus or street signs.
The standard algorithm for MT is the encoder-decoder network, an architectureencoder-
decoder
that we introduced in Chapter 8 for RNNs. Recall that encoder-decoder or sequence-
to-sequence models are used for tasks in which we need to map an input sequence to
an output sequence that is a complex function of the entire input sequence. Indeed,

--- Page 264 ---
264 CHAPTER 13 • M ACHINE TRANSLATION
in machine translation, the words of the target language don’t necessarily agree with
the words of the source language in number or order. Consider translating the fol-
lowing made-up English sentence into Japanese.
(13.1) English: He wrote a letter to a friend
Japanese: tomodachi
friendni
totegami-o
letterkaita
wrote
Note that the elements of the sentences are in very different places in the different
languages. In English, the verb is in the middle of the sentence, while in Japanese,
the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun
he, while English does.
Such differences between languages can be quite complex. In the following ac-
tual sentence from the United Nations, notice the many changes between the Chinese
sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and
its English equivalent produced by human translators.
(13.2)大会/General Assembly 在/on 1982年/1982 12月/December 10 日/10通过
了/adopted第37号/37th决议/resolution ，核准了/approved 第二
次/second探索/exploration 及/and和平peaceful利用/using外层空
间/outer space会议/conference 的/of各项/various建议/suggestions 。
On 10 December 1982 , the General Assembly adopted resolution 37 in
which it endorsed the recommendations of the Second United Nations
Conference on the Exploration and Peaceful Uses of Outer Space .
Note the many ways the English and Chinese differ. For example the order-
ing differs in major ways; the Chinese order of the noun phrase is “peaceful using
outer space conference of suggestions” while the English has “suggestions of the ...
conference on peaceful use of outer space”). And the order differs in minor ways
(the date is ordered differently). English requires thein many places that Chinese
doesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in
Chinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English,
which has the “-s” in “recommendations”), and so the Chinese must use the modi-
ﬁer各项/various to make it clear that there is not just one recommendation. English
capitalizes some words but not others. Encoder-decoder networks are very success-
ful at handling these sorts of complicated cases of sequence mappings.
We’ll begin in the next section by considering the linguistic background about
how languages vary, and the implications this variance has for the task of MT. Then
we’ll sketch out the standard algorithm, give details about things like input tokeniza-
tion and creating training corpora of parallel sentences, give some more low-level
details about the encoder-decoder network, and ﬁnally discuss how MT is evaluated,
introducing the simple chrF metric.
13.1 Language Divergences and Typology
There are about 7,000 languages in the world. Some aspects of human language
seem to be universal , holding true for every one of these languages, or are statistical universal
universals, holding true for most of these languages. Many universals arise from the
functional role of language as a communicative system by humans. Every language,
for example, seems to have words for referring to people, for talking about eating
and drinking, for being polite or not. There are also structural linguistic univer-
sals; for example, every language seems to have nouns and verbs (Chapter 17), has

--- Page 265 ---
13.1 • L ANGUAGE DIVERGENCES AND TYPOLOGY 265
ways to ask questions, or issue commands, has linguistic mechanisms for indicating
agreement or disagreement.
Yet languages also differ in many ways (as has been pointed out since ancient
times; see Fig. 13.1). Understanding what causes such translation divergencestranslation
divergence
(Dorr, 1994) can help us build better MT models. We often distinguish the idiosyn-
cratic and lexical differences that must be dealt with one by one (the word for “dog”
differs wildly from language to language), from systematic differences that we can
model in a general way (many languages put the verb before the grammatical ob-
ject; others put the verb after the grammatical object). The study of these systematic
cross-linguistic similarities and differences is called linguistic typology . This sec- typology
tion sketches some typological facts that impact machine translation; the interested
reader should also look into WALS, the World Atlas of Language Structures, which
gives many typological facts about languages (Dryer and Haspelmath, 2013).
Figure 13.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the
Kunsthistorisches Museum, Vienna.
13.1.1 Word Order Typology
As we hinted at in our example above comparing English and Japanese, languages
differ in the basic word order of verbs, subjects, and objects in simple declara-
tive clauses. German, French, English, and Mandarin, for example, are all SVO SVO
(Subject-Verb-Object ) languages, meaning that the verb tends to come between
the subject and object. Hindi and Japanese, by contrast, are SOV languages, mean- SOV
ing that the verb tends to come at the end of basic clauses, and Irish and Arabic are
VSO languages. Two languages that share their basic word order type often have VSO
other similarities. For example, VOlanguages generally have prepositions , whereas
OVlanguages generally have postpositions .
Let’s look in more detail at the example we saw above. In this SVO English
sentence, the verb wrote is followed by its object a letter and the prepositional phrase

--- Page 266 ---
266 CHAPTER 13 • M ACHINE TRANSLATION
to a friend , in which the preposition tois followed by its argument a friend . Arabic,
with a VSO order, also has the verb before the object and prepositions. By contrast,
in the Japanese example that follows, each of these orderings is reversed; the verb is
preceded by its arguments, and the postposition follows its argument.
(13.3) English: He wrote a letter to a friend
Japanese: tomodachi
friendni
totegami-o
letterkaita
wrote
Arabic: katabt
wroteris¯ala
letterli
to˙sadq
friend
Other kinds of ordering preferences vary idiosyncratically from language to lan-
guage. In some SVO languages (like English and Mandarin) adjectives tend to ap-
pear before nouns, while in others languages like Spanish and Modern Hebrew, ad-
jectives appear after the noun:
(13.4) Spanish bruja verde English green witch
(a) (b)
Figure 13.2 Examples of other word order differences: (a) In German, adverbs occur in
initial position that in English are more natural later, and tensed verbs occur in second posi-
tion. (b) In Mandarin, preposition phrases expressing goals often occur pre-verbally, unlike
in English.
Fig. 13.2 shows examples of other word order differences. All of these word
order differences between languages can cause problems for translation, requiring
the system to do huge structural reorderings as it generates the output.
13.1.2 Lexical Divergences
Of course we also need to translate the individual words from one language to an-
other. For any translation, the appropriate word can vary depending on the context.
The English source-language word bass, for example, can appear in Spanish as the
ﬁshlubina or the musical instrument bajo. German uses two distinct words for what
in English would be called a wall:Wand for walls inside a building, and Mauer for
walls outside a building. Where English uses the word brother for any male sib-
ling, Chinese and many other languages have distinct words for older brother and
younger brother (Mandarin gege anddidi, respectively). In all these cases, trans-
lating bass,wall, orbrother from English would require a kind of specialization,
disambiguating the different uses of a word. For this reason the ﬁelds of MT and
Word Sense Disambiguation (Appendix G) are closely linked.
Sometimes one language places more grammatical constraints on word choice
than another. We saw above that English marks nouns for whether they are singular
or plural. Mandarin doesn’t. Or French and Spanish, for example, mark grammat-
ical gender on adjectives, so an English translation into French requires specifying
adjective gender.
The way that languages differ in lexically dividing up conceptual space may be
more complex than this one-to-many translation problem, leading to many-to-many

--- Page 267 ---
13.1 • L ANGUAGE DIVERGENCES AND TYPOLOGY 267
mappings. For example, Fig. 13.3 summarizes some of the complexities discussed
by Hutchins and Somers (1992) in translating English leg, foot , and paw, to French.
For example, when legis used about an animal it’s translated as French jambe ; but
about the leg of a journey, as French etape ; if the leg is of a chair, we use French
pied.
Further, one language may have a lexical gap , where no word or phrase, short lexical gap
of an explanatory footnote, can express the exact meaning of a word in the other
language. For example, English does not have a word that corresponds neatly to
Mandarin xi`aoor Japanese oyak ¯ok¯o(in English one has to make do with awkward
phrases like ﬁlial piety orloving child , orgood son/daughter for both).
etapepattejambepied   paw        footlegJOURNEYANIMALHUMANCHAIRANIMALBIRDHUMAN
Figure 13.3 The complex overlap between English leg,foot, etc., and various French trans-
lations as discussed by Hutchins and Somers (1992).
Finally, languages differ systematically in how the conceptual properties of an
event are mapped onto speciﬁc words. Talmy (1985, 1991) noted that languages
can be characterized by whether direction of motion and manner of motion are
marked on the verb or on the “satellites”: particles, prepositional phrases, or ad-
verbial phrases. For example, a bottle ﬂoating out of a cave would be described in
English with the direction marked on the particle out, while in Spanish the direction
would be marked on the verb:
(13.5) English: The bottle ﬂoated out.
Spanish: La
Thebotella
bottlesali´o
exitedﬂotando .
ﬂoating.
Verb-framed languages mark the direction of motion on the verb (leaving the verb-framed
satellites to mark the manner of motion), like Spanish acercarse ‘approach’, al-
canzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the satellite-framed
direction of motion on the satellite (leaving the verb to mark the manner of motion),
like English crawl out ,ﬂoat off ,jump down ,run after . Languages like Japanese,
Tamil, and the many languages in the Romance, Semitic, and Mayan languages fam-
ilies, are verb-framed; Chinese as well as non-Romance Indo-European languages
like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991,
Slobin 1996).
13.1.3 Morphological Typology
Morphologically , languages are often characterized along two dimensions of vari-
ation. The ﬁrst is the number of morphemes per word, ranging from isolating isolating
languages like Vietnamese and Cantonese, in which each word generally has one
morpheme, to polysynthetic languages like Siberian Yupik (“Eskimo”), in which a polysynthetic
single word may have very many morphemes, corresponding to a whole sentence in
English. The second dimension is the degree to which morphemes are segmentable,
ranging from agglutinative languages like Turkish, in which morphemes have rel- agglutinative

--- Page 268 ---
268 CHAPTER 13 • M ACHINE TRANSLATION
atively clean boundaries, to fusion languages like Russian, in which a single afﬁx fusion
may conﬂate multiple morphemes, like -om in the word stolom (table- SG-INSTR -
DECL 1), which fuses the distinct morphological categories instrumental, singular,
and ﬁrst declension.
Translating between languages with rich morphology requires dealing with struc-
ture below the word level, and for this reason modern systems generally use subword
models like the wordpiece or BPE models of Section 13.2.1.
13.1.4 Referential density
Finally, languages vary along a typological dimension related to the things they tend
to omit. Some languages, like English, require that we use an explicit pronoun when
talking about a referent that is given in the discourse. In other languages, however,
we can sometimes omit pronouns altogether, as the following example from Spanish
shows1:
(13.6) [El jefe] idio con un libro. / 0 iMostr ´o su hallazgo a un descifrador ambulante.
[The boss] came upon a book. [He] showed his ﬁnd to a wandering decoder.
Languages that can omit pronouns are called pro-drop languages. Even among pro-drop
the pro-drop languages, there are marked differences in frequencies of omission.
Japanese and Chinese, for example, tend to omit far more than does Spanish. This
dimension of variation across languages is called the dimension of referential den-
sity. We say that languages that tend to use more pronouns are more referentiallyreferential
density
dense than those that use more zeros. Referentially sparse languages, like Chinese or
Japanese, that require the hearer to do more inferential work to recover antecedents
are also called cold languages. Languages that are more explicit and make it easier cold language
for the hearer are called hotlanguages. The terms hotandcold are borrowed from hot language
Marshall McLuhan’s 1964 distinction between hot media like movies, which ﬁll in
many details for the viewer, versus cold media like comics, which require the reader
to do more inferential work to ﬁll out the representation (Bickel, 2003).
Translating from languages with extensive pro-drop, like Chinese or Japanese, to
non-pro-drop languages like English can be difﬁcult since the model must somehow
identify each zero and recover who or what is being talked about in order to insert
the proper pronoun.
13.2 Machine Translation using Encoder-Decoder
The standard architecture for MT is the encoder-decoder transformer orsequence-
to-sequence model, an architecture we saw for RNNs in Chapter 8. We’ll see the
details of how to apply this architecture to transformers in Section 13.3, but ﬁrst let’s
talk about the overall task.
Most machine translation tasks make the simpliﬁcation that we can translate each
sentence independently, so we’ll just consider individual sentences for now. Given
a sentence in a source language, the MT task is then to generate a corresponding
sentence in a target language. For example, an MT system is given an English
sentence like
The green witch arrived
1Here we use the / 0-notation; we’ll introduce this and discuss this issue further in Chapter 23

--- Page 269 ---
13.2 • M ACHINE TRANSLATION USING ENCODER -DECODER 269
and must translate it into the Spanish sentence:
Lleg ´o la bruja verde
MT uses supervised machine learning: at training time the system is given a
large set of parallel sentences (each sentence in a source language matched with
a sentence in the target language), and learns to map source sentences into target
sentences. In practice, rather than using words (as in the example above), we split
the sentences into a sequence of subword tokens (tokens can be words, or subwords,
or individual characters). The systems are then trained to maximize the probability
of the sequence of tokens in the target language y1;:::;ymgiven the sequence of
tokens in the source language x1;:::;xn:
P(y1;:::; ymjx1;:::; xn) (13.7)
Rather than use the input tokens directly, the encoder-decoder architecture con-
sists of two components, an encoder and a decoder . The encoder takes the input
words x= [x1;:::; xn]and produces an intermediate context h. At decoding time, the
system takes hand, word by word, generates the output y:
h=encoder (x) (13.8)
yt+1=decoder (h;y1;:::; yt))8t2[1;:::; m] (13.9)
In the next two sections we’ll talk about subword tokenization, and then how to get
parallel corpora for training, and then we’ll introduce the details of the encoder-
decoder architecture.
13.2.1 Tokenization
Machine translation systems use a vocabulary that is ﬁxed in advance, and rather
than using space-separated words, this vocabulary is generated with subword to-
kenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared
vocabulary is used for the source and target languages, which makes it easy to copy
tokens (like names) from source to target. Using subword tokenization with tokens
shared between languages makes it natural to translate between languages like En-
glish or Hindi that use spaces to separate words, and languages like Chinese or Thai
that don’t.
We build the vocabulary by running a subword tokenization algorithm on a cor-
pus that contains both source and target language data.
Rather than the simple BPE algorithm from Fig. 2.13, modern systems often use
more powerful tokenization algorithms. Some systems (like BERT) use a variant of
BPE called the wordpiece algorithm, which instead of choosing the most frequent wordpiece
set of tokens to merge, chooses merges based on which one most increases the lan-
guage model probability of the tokenization. Wordpieces use a special symbol at the
beginning of each token; here’s a resulting tokenization from the Google MT system
(Wu et al., 2016):
words : Jet makers feud over seat width with big orders at stake
wordpieces :J et makers fe ud over seat width with big orders atstake
The wordpiece algorithm is given a training corpus and a desired vocabulary size
V , and proceeds as follows:
1. Initialize the wordpiece lexicon with characters (for example a subset of Uni-
code characters, collapsing all the remaining characters to a special unknown
character token).

--- Page 270 ---
270 CHAPTER 13 • M ACHINE TRANSLATION
2. Repeat until there are V wordpieces:
(a) Train an n-gram language model on the training corpus, using the current
set of wordpieces.
(b) Consider the set of possible new wordpieces made by concatenating two
wordpieces from the current lexicon. Choose the one new wordpiece that
most increases the language model probability of the training corpus.
Recall that with BPE we had to specify the number of merges to perform; in
wordpiece, by contrast, we specify the total vocabulary, which is a more intuitive
parameter. A vocabulary of 8K to 32K word pieces is commonly used.
An even more commonly used tokenization algorithm is (somewhat ambigu-
ously) called the unigram algorithm (Kudo, 2018) or sometimes the SentencePiece unigram
SentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-
fel et al., 2020). (Because unigram is the default tokenization algorithm used in a
library called SentencePiece that adds a useful wrapper around tokenization algo-
rithms (Kudo and Richardson, 2018b), authors often say they are using Sentence-
Piece tokenization but really mean they are using the unigram algorithm).
In unigram tokenization, instead of building up a vocabulary by merging tokens,
we start with a huge vocabulary of every individual unicode character plus all fre-
quent sequences of characters (including all space-separated words, for languages
with spaces), and iteratively remove some tokens to get to a desired ﬁnal vocabulary
size. The algorithm is complex (involving sufﬁx-trees for efﬁciently storing many
tokens, and the EM algorithm for iteratively assigning probabilities to tokens), so we
don’t give it here, but see Kudo (2018) and Kudo and Richardson (2018b). Roughly
speaking the algorithm proceeds iteratively by estimating the probability of each
token, tokenizing the input data using various tokenizations, then removing a per-
centage of tokens that don’t occur in high-probability tokenization, and then iterates
until the vocabulary has been reduced down to the desired number of tokens.
Why does unigram tokenization work better than BPE? BPE tends to create lots
of very small non-meaningful tokens (because BPE can only create larger words or
morphemes by merging characters one at a time), and it also tends to merge very
common tokens, like the sufﬁx ed, onto their neighbors. We can see from these
examples from Bostrom and Durrett (2020) that unigram tends to produce tokens
that are more semantically meaningful:
Original: corrupted Original: Completely preposterous suggestions
BPE: cor rupted BPE: Comple t ely prep ost erous suggest ions
Unigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s
13.2.2 Creating the Training data
Machine translation models are trained on a parallel corpus , sometimes called a parallel corpus
bitext , a text that appears in two (or more) languages. Large numbers of paral-
lel corpora are available. Some are governmental; the Europarl corpus (Koehn, Europarl
2005), extracted from the proceedings of the European Parliament, contains between
400,000 and 2 million sentences each from 21 European languages. The United Na-
tions Parallel Corpus contains on the order of 10 million sentences in the six ofﬁcial
languages of the United Nations (Arabic, Chinese, English, French, Russian, Span-
ish) Ziemski et al. (2016). Other parallel corpora have been made from movie and
TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from
general web text, like the ParaCrawl corpus of 223 million sentence pairs between
23 EU languages and English extracted from the CommonCrawl Ba ˜n´on et al. (2020).

--- Page 271 ---
13.2 • M ACHINE TRANSLATION USING ENCODER -DECODER 271
Sentence alignment
Standard training corpora for MT come as aligned pairs of sentences. When creat-
ing new corpora, for example for underresourced languages or new domains, these
sentence alignments must be created. Fig. 13.4 gives a sample hypothetical sentence
alignment.
F1: -Bonjour, dit le petit prince.F2: -Bonjour, dit le marchand de pilules perfectionnées qui apaisent la soif.F3: On en avale une par semaine et l'on n'éprouve plus le besoin de boire.F4: -C’est une grosse économie de temps, dit le marchand.F5: Les experts ont fait des calculs.F6: On épargne cinquante-trois minutes par semaine.F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes à dépenser, je marcherais tout doucement vers une fontaine..."E1: “Good morning," said the little prince.E2: “Good morning," said the merchant.E3: This was a merchant who sold pills that had been perfected to quench thirst.E4: You just swallow one pill a week and you won’t feel the need for anything to drink.E5: “They save a huge amount of time," said the merchant.E6: “Fifty−three minutes a week."E7: “If I had  fifty−three minutes to spend?" said the little prince to himself. E8: “I would take a stroll to a spring of fresh water”
Figure 13.4 A sample alignment between sentences in English and French, with sentences extracted from
Antoine de Saint-Exupery’s Le Petit Prince and a hypothetical translation. Sentence alignment takes sentences
e1;:::;en, and f1;:::;fnand ﬁnds minimal sets of sentences that are translations of each other, including single
sentence mappings like (e 1,f1), (e 4,f3), (e 5,f4), (e 6,f6) as well as 2-1 alignments (e 2/e3,f2), (e 7/e8,f7), and null
alignments (f 5).
Given two documents that are translations of each other, we generally need two
steps to produce sentence alignments:
• a cost function that takes a span of source sentences and a span of target sen-
tences and returns a score measuring how likely these spans are to be transla-
tions.
• an alignment algorithm that takes these scores to ﬁnd a good alignment be-
tween the documents.
To score the similarity of sentences across languages, we need to make use of
amultilingual embedding space , in which sentences from different languages are
in the same embedding space (Artetxe and Schwenk, 2019). Given such a space,
cosine similarity of such embeddings provides a natural scoring function (Schwenk,
2018). Thompson and Koehn (2019) give the following cost function between two
sentences or spans x,yfrom the source and target documents respectively:
c(x;y) =(1 cos(x;y))nSents (x)nSents (y)PS
s=11 cos(x;ys)+PS
s=11 cos(xs;y)(13.10)
where nSents ()gives the number of sentences (this biases the metric toward many
alignments of single sentences instead of aligning very large spans). The denom-
inator helps to normalize the similarities, and so x1;:::;xS;y1;:::;yS;are randomly
selected sentences sampled from the respective documents.
Usually dynamic programming is used as the alignment algorithm (Gale and
Church, 1993), in a simple extension of the minimum edit distance algorithm we
introduced in Chapter 2.
Finally, it’s helpful to do some corpus cleanup by removing noisy sentence pairs.
This can involve handwritten rules to remove low-precision pairs (for example re-
moving sentences that are too long, too short, have different URLs, or even pairs

--- Page 272 ---
272 CHAPTER 13 • M ACHINE TRANSLATION
that are too similar, suggesting that they were copies rather than translations). Or
pairs can be ranked by their multilingual embedding cosine score and low-scoring
pairs discarded.
13.3 Details of the Encoder-Decoder Model
EncoderThegreenllegówitcharrived<s>llególa
labruja
brujaverde
verde</s>Decodercross-attentiontransformerblocks
Figure 13.5 The encoder-decoder transformer architecture for machine translation. The encoder uses the
transformer blocks we saw in Chapter 8, while the decoder uses a more powerful block with an extra cross-
attention layer that can attend to all the encoder words. We’ll see this in more detail in the next section.
The standard architecture for MT is the encoder-decoder transformer. The encoder-
decoder architecture was introduced already for RNNs in Chapter 8, and the trans-
former version has the same idea. Fig. 13.5 shows the intuition of the architec-
ture at a high level. You’ll see that the encoder-decoder architecture is made up of
two transformers: an encoder , which is the same as the basic transformers from
Chapter 9, and a decoder , which is augmented with a special new layer called the
cross-attention layer. The encoder takes the source language input word tokens
X=x1;:::;xnand maps them to an output representation Henc=h1;:::;hn; via a
stack of encoder blocks.
The decoder is essentially a conditional language model that attends to the en-
coder representation and generates the target words one by one, at each timestep
conditioning on the source sentence and the previously generated target language
words to generate a token. Decoding can use any of the decoding methods discussed
in Chapter 9 like greedy, or temperature or nucleus sampling. But the most com-
mon decoding algorithm for MT is the beam search algorithm that we’ll introduce
in Section 13.4.
But the components of the architecture differ somewhat from the transformer
block we’ve seen. First, in order to attend to the source language, the transformer
blocks in the decoder have an extra cross-attention layer. Recall that the transformer
block of Chapter 9 consists of a self-attention layer that attends to the input from
the previous layer, followed by layer norm, a feed forward layer, and another layer
norm. The decoder transformer block includes an extra layer with a special kind
of attention, cross-attention (also sometimes called encoder-decoder attention or cross-attention
source attention ). Cross-attention has the same form as the multi-head attention
in a normal transformer block, except that while the queries as usual come from
the previous layer of the decoder, the keys and values come from the output of the
encoder .

--- Page 273 ---
13.3 • D ETAILS OF THE ENCODER -DECODER MODEL 273
Encoderx1x2x3xn…Decoderh3h2h1…hn
EncoderBlock 1Block 2Block Ky3y2y1…
DecoderBlock 1Block 2Block LUnembedding Matrixym
Multi-Head AttentionLayer NormalizeLayer Normalize++…Feedforward
Causal Multi-Head AttentionLayer NormalizeLayer Normalize+
+…FeedforwardLayer Normalize+Cross-Attention………………LanguageModeling HeadHenc
Figure 13.6 The transformer block for the encoder and the decoder. The ﬁnal output of the encoder Henc=
h1;:::;hnis the context used in the decoder. The decoder is a standard transformer except with one extra layer,
thecross-attention layer, which takes that encoder output Hencand uses it to form its KandVinputs.
That is, where in standard multi-head attention the input to each attention layer is
X, in cross attention the input is the the ﬁnal output of the encoder Henc=h1;:::;hn.
Hencis of shape [nd], each row representing one input token. To link the keys
and values from the encoder with the query from the prior layer of the decoder, we
multiple the encoder output Hencby the cross-attention layer’s key weights WKand
value weights WV. The query comes from the output from the prior decoder layer
Hdec[` 1], which is multiplied by the cross-attention layer’s query weights WQ:
Q=Hdec[` 1]WQ;K=HencWK;V=HencWV(13.11)
CrossAttention (Q;K;V) = softmaxQK|
pdk
V (13.12)
The cross attention thus allows the decoder to attend to each of the source language
words as projected into the entire encoder ﬁnal output representations. The other
attention layer in each decoder block, the multi-head attention layer, is the same
causal (left-to-right) attention that we saw in Chapter 9. The multi-head attention in
the encoder, however, is allowed to look ahead at the entire source language text, so
it is not masked.
To train an encoder-decoder model, we use the same self-supervision model we
used for training encoder-decoders RNNs in Chapter 8. The network is given the
source text and then starting with the separator token is trained autoregressively to
predict the next token using cross-entropy loss. Recall that cross-entropy loss for

--- Page 274 ---
274 CHAPTER 13 • M ACHINE TRANSLATION
language modeling is determined by the probability the model assigns to the correct
next word. So at time tthe CE loss is the negative log probability the model assigns
to the next word in the training sequence:
LCE(ˆyt;yt) = logˆyt[wt+1] (13.13)
As in that case, we use teacher forcing in the decoder. Recall that in teacher forc- teacher forcing
ing, at each time step in decoding we force the system to use the gold target token
from training as the next input xt+1, rather than allowing it to rely on the (possibly
erroneous) decoder output ˆ yt.
13.4 Decoding in MT: Beam Search
Recall the greedy decoding algorithm from Chapter 9: at each time step tin gen-
eration, the output ytis chosen by computing the probability for each word in the
vocabulary and then choosing the highest probability word (the argmax):
ˆwt=argmaxw2VP(wjw<t) (13.14)
A problem with greedy decoding is that what looks high probability at word tmight
turn out to have been the wrong choice once we get to word t+1. The beam search
algorithm maintains multiple choices until later when we can see which one is best.
In beam search we model decoding as searching the space of possible genera-
tions, represented as a search tree whose branches represent actions (generating a search tree
token), and nodes represent states (having generated a particular preﬁx). We search
for the best action sequence, i.e., the string with the highest probability.
An illustration of the problem
Fig. 13.7 shows a made-up example. The most probable sequence is ok ok EOS (its
probability is .4.71.0). But greedy search doesn’t ﬁnd it, incorrectly choosing
yesas the ﬁrst word since it has the highest local probability (0.5).
startokyesEOSokyesEOSokyesEOSEOSEOSEOSEOSt2t3p(t1|start)
t1p(t2| t1)p(t3| t1,t2)
.1.5.4.3.4.3.1.2.71.01.01.01.0
Figure 13.7 A search tree for generating the target string T=t1;t2;:::from vocabulary
V=fyes;ok;<s>g, showing the probability of generating each token from that state. Greedy
search chooses yesfollowed by yes, instead of the globally most probable sequence ok ok .
Recall from Chapter 17 that for part-of-speech tagging we used dynamic pro-
gramming search (the Viterbi algorithm) to address this problem. Unfortunately,

--- Page 275 ---
13.4 • D ECODING IN MT: B EAM SEARCH 275
dynamic programming is not applicable to generation problems with long-distance
dependencies between the output decisions. The only method guaranteed to ﬁnd the
best solution is exhaustive search: computing the probability of every one of the VT
possible sentences (for some length value T) which is obviously too slow.
The solution: beam search
Instead, MT systems generally decode using beam search , a heuristic search method beam search
ﬁrst proposed by Lowerre (1976). In beam search, instead of choosing the best token
to generate at each timestep, we keep kpossible tokens at each step. This ﬁxed-size
memory footprint kis called the beam width , on the metaphor of a ﬂashlight beam beam width
that can be parameterized to be wider or narrower.
Thus at the ﬁrst step of decoding, we compute a softmax over the entire vocab-
ulary, assigning a probability to each word. We then select the k-best options from
this softmax output. These initial koutputs are the search frontier and these kinitial
words are called hypotheses . A hypothesis is an output sequence, a translation-so-
far, together with its probability.
a…aardvark..arrived..the…zebrastart
t1a…aardvark..the..witch…zebraa…aardvark..green..witch…zebrat2hd1y1BOSy1y2
y2hd1hd2thetheBOShd2greengreeny3hd1hd2arrivedarrivedBOSy2
t3hd1hd2thetheBOSy2
hd1hd2thetheBOShd2witchwitchy3a…mage..the..witch…zebraarrived…aardvark..green..who…zebray3
y3
Figure 13.8 Beam search decoding with a beam width of k=2. At each time step, we choose the kbest
hypotheses, form the Vpossible extensions of each, score those kVhypotheses and choose the best k=2
to continue. At time 1, the frontier has the best 2 options from the initial decoder state: arrived andthe. We
extend each, compute the probability of all the hypotheses so far ( arrived the ,arrived aardvark ,the green ,the
witch ) and again chose the best 2 ( the green andthe witch ) to be the search frontier. The images on the arcs
schematically represent the decoders that must be run at each step to score the next words (for simplicity not
depicting cross-attention).
At subsequent steps, each of the kbest hypotheses is extended incrementally
by being passed to distinct decoders, which each generate a softmax over the entire

--- Page 276 ---
276 CHAPTER 13 • M ACHINE TRANSLATION
vocabulary to extend the hypothesis to every possible next token. Each of these kV
hypotheses is scored by P(yijx;y<i): the product of the probability of the current
word choice multiplied by the probability of the path that led to it. We then prune
thekVhypotheses down to the kbest hypotheses, so there are never more than k
hypotheses at the frontier of the search, and never more than kdecoders. Fig. 13.8
illustrates this with a beam width of 2 for the beginning of The green witch arrived .
This process continues until an EOS is generated indicating that a complete can-
didate output has been found. At this point, the completed hypothesis is removed
from the frontier and the size of the beam is reduced by one. The search continues
until the beam has been reduced to 0. The result will be khypotheses.
To score each node by its log probability, we use the chain rule of probability to
break down p(yjx)into the product of the probability of each word given its prior
context, which we can turn into a sum of logs (for an output string of length t):
score(y) = logP(yjx)
=log(P(y1jx)P(y2jy1;x)P(y3jy1;y2;x):::P(ytjy1;:::;yt 1;x))
=tX
i=1logP(yijy1;:::;yi 1;x) (13.15)
Thus at each step, to compute the probability of a partial sentence, we simply add the
log probability of the preﬁx sentence so far to the log probability of generating the
next token. Fig. 13.9 shows the scoring for the example sentence shown in Fig. 13.8,
using some simple made-up probabilities. Log probabilities are negative or 0, and
the max of two log probabilities is the one that is greater (closer to 0).
BOSarrivedthethewitchgreenwitchmage
whoy2y3log P(y1|x)y1log P(y2|y1,x)log P(y3|y2,y1,x)-.92-1.6-1.2-.69-2.3-.69-1.6
-2.3arrived-.11-.51witch-.36-.22EOS
-.51EOS-2.3at-1.61bylog P(y4|y3,y2,y1,x)log P(y5|y4,y3,y2,y1,x)arrivedcame-1.6
y4y5log P(arrived|x) log P(arrived witch|x)log P(the|x)log P(the green|x)log P(the witch|x) =-1.6log P (arrived the|x) log P (“the green witch arrived”|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x)+logP(arrived|the,green,witch,x)+log P(EOS|the,green,witch,arrived,x)= -2.3= -3.9= -1.6= -2.1=-.92-2.1-3.2
-4.4-2.2-2.5-3.7-2.7-3.8-2.7-4.8
Figure 13.9 Scoring for beam search decoding with a beam width of k=2. We maintain the log probability
of each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top
kpaths are extended to the next step.
Fig. 13.10 gives the algorithm. One problem with this version of the algorithm is
that the completed hypotheses may have different lengths. Because language mod-
els generally assign lower probabilities to longer strings, a naive algorithm would

--- Page 277 ---
13.4 • D ECODING IN MT: B EAM SEARCH 277
function BEAM DECODE (c,beam width )returns best paths
y0,h0 0
path ()
complete paths ()
state (c,y0,h0, path) ;initial state
frontier hstatei ;initial frontier
while frontier contains incomplete paths andbeamwidth>0
extended frontier hi
for each state2frontier do
y DECODE (state )
for each word i2Vocabulary do
successor NEWSTATE (state ,i,yi)
extended frontier ADDTOBEAM (successor ,extended frontier ,
beam width )
for each state inextended frontier do
ifstate is complete do
complete paths APPEND (complete paths ,state )
extended frontier REMOVE (extended frontier ,state )
beam width beam width - 1
frontier extended frontier
return completed paths
function NEWSTATE (state ,word ,word prob)returns new state
function ADDTOBEAM (state ,frontier ,width )returns updated frontier
ifLENGTH (frontier )<width then
frontier INSERT (state ,frontier )
else if SCORE (state )>SCORE (WORST OF(frontier ))
frontier REMOVE (WORST OF(frontier ))
frontier INSERT (state ,frontier )
return frontier
Figure 13.10 Beam search decoding.
choose shorter strings for y. (This is not an issue during the earlier steps of decod-
ing; since beam search is breadth-ﬁrst, all the hypotheses being compared had the
same length.) For this reason we often apply length normalization methods, like
dividing the logprob by the number of words:
score(y) =logP(yjx) =1
ttX
i=1logP(yijy1;:::;yi 1;x) (13.16)
For MT we generally use beam widths kbetween 5 and 10, giving us khypotheses at
the end. We can pass all kto the downstream application with their respective scores,
or if we just need a single translation we can pass the most probable hypothesis.
13.4.1 Minimum Bayes Risk Decoding
Minimum Bayes risk orMBR decoding is an alternative decoding algorithm thatminimum
Bayes risk
MBR

--- Page 278 ---
278 CHAPTER 13 • M ACHINE TRANSLATION
can work even better than beam search and also tends to be better than the other
decoding algorithms like temperature sampling introduced in Section 10.2.
The intuition of minimum Bayes risk is that instead of trying to choose the trans-
lation which is most probable, we choose the one that is likely have the least error.
For example, we might want our decoding algorithm to ﬁnd the translation which
has the highest score on some evaluation metric. For example in Section 13.6 we will
introduce metrics like chrF or BERTScore that measure the goodness-of-ﬁt between
a candidate translation and a set of reference human translations. A translation that
maximizes this score, especially with a hypothetically huge set of perfect human
translations is likely to be a good one (have minimum risk) even if it is not the most
probable translation by our particular probability estimator.
In practice, we don’t know the perfect set of translations for a given sentence. So
the standard simpliﬁcation used in MBR decoding algorithms is to instead choose
the candidate translation which is most similar (by some measure of goodness-of-
ﬁt) with some set of candidate translations. We’re essentially approximating the
enormous space of all possible translations Uwith a smaller set of possible candidate
translations Y.
Given this set of possible candidate translations Y, and some similarity or align-
ment function util, we choose the best translation ˆ yas the translation which is most
similar to all the other candidate translations:
ˆy=argmax
y2YX
c2Yutil(y;c) (13.17)
Various util functions can be used, like chrF or BERTscore or BLEU. We can get the
set of candidate translations by sampling using one of the basic sampling algorithms
of Section 10.2 like temperature sampling; good results can be obtained with as few
as 32 or 64 candidates.
Minimum Bayes risk decoding can also be used for other NLP tasks; indeed
it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne,
2000) before being applied to machine translation (Kumar and Byrne, 2004), and
has been shown to work well across many other generation tasks as well (e.g., sum-
marization, dialogue, and image captioning (Suzgun et al., 2023a)).
13.5 Translating in low-resource situations
For some languages, and especially for English, online resources are widely avail-
able. There are many large parallel corpora that contain translations between En-
glish and many languages. But the vast majority of the world’s languages do not
have large parallel training texts available. An important ongoing research question
is how to get good translation with lesser resourced languages. The resource prob-
lem can even be true for high resource languages when we need to translate into low
resource domains (for example in a particular genre that happens to have very little
bitext).
Here we brieﬂy introduce two commonly used approaches for dealing with this
data sparsity: backtranslation , which is a special case of the general statistical
technique called data augmentation , and multilingual models , and also discuss
some socio-technical issues.

--- Page 279 ---
13.5 • T RANSLATING IN LOW -RESOURCE SITUATIONS 279
13.5.1 Data Augmentation
Data augmentation is a statistical technique for dealing with insufﬁcient training
data, by adding new synthetic data that is generated from the current natural data.
The most common data augmentation technique for machine translation is called
backtranslation . Backtranslation relies on the intuition that while parallel corpora backtranslation
may be limited for particular languages or domains, we can often ﬁnd a large (or
at least larger) monolingual corpus, to add to the smaller parallel corpora that are
available. The algorithm makes use of monolingual corpora in the target language
by creating synthetic bitexts.
In backtranslation, our goal is to improve source-to-target MT, given a small
parallel text (a bitext) in the source/target languages, and some monolingual data in
the target language. We ﬁrst use the bitext to train a MT system in the reverse di-
rection: a target-to-source MT system . We then use it to translate the monolingual
target data to the source language. Now we can add this synthetic bitext (natural
target sentences, aligned with MT-produced source sentences) to our training data,
and retrain our source-to-target MT model. For example suppose we want to trans-
late from Navajo to English but only have a small Navajo-English bitext, although of
course we can ﬁnd lots of monolingual English data. We use the small bitext to build
an MT engine going the other way (from English to Navajo). Once we translate the
monolingual English text to Navajo, we can add this synthetic Navajo/English bitext
to our training data.
Backtranslation has various parameters. One is how we generate the backtrans-
lated data; we can run the decoder in greedy inference, or use beam search. Or
we can do sampling, like the temperature sampling algorithm we saw in Chapter 9.
Another parameter is the ratio of backtranslated data to natural bitext data; we can
choose to upsample the bitext data (include multiple copies of each sentence). In
general backtranslation works surprisingly well; one estimate suggests that a system
trained on backtranslated text gets about 2/3 of the gain as would training on the
same amount of natural bitext (Edunov et al., 2018).
13.5.2 Multilingual models
The models we’ve described so far are for bilingual translation: one source language,
one target language. It’s also possible to build a multilingual translator.
In a multilingual translator, we train the system by giving it parallel sentences
in many different pairs of languages. That means we need to tell the system which
language to translate from and to! We tell the system which language is which
by adding a special token lsto the encoder specifying the source language we’re
translating from, and a special token ltto the decoder telling it the target language
we’d like to translate into.
Thus we slightly update Eq. 13.9 above to add these tokens in Eq. 13.19:
h=encoder (x;ls) (13.18)
yi+1=decoder (h;lt;y1;:::; yi))8i2[1;:::; m] (13.19)
One advantage of a multilingual model is that they can improve the translation
of lower-resourced languages by drawing on information from a similar language
in the training data that happens to have more resources. Perhaps we don’t know
the meaning of a word in Galician, but the word appears in the similar and higher-
resourced language Spanish.

--- Page 280 ---
280 CHAPTER 13 • M ACHINE TRANSLATION
13.5.3 Sociotechnical issues
Many issues in dealing with low-resource languages go beyond the purely techni-
cal. One problem is that for low-resource languages, especially from low-income
countries, native speakers are often not involved as the curators for content selec-
tion, as the language technologists, or as the evaluators who measure performance
(8et al., 2020). Indeed, one well-known study that manually audited a large set of
parallel corpora and other major multilingual datasets found that for many of the
corpora, less than 50% of the sentences were of acceptable quality, with a lot of
data consisting of repeated sentences with web boilerplate or incorrect translations,
suggesting that native speakers may not have been sufﬁciently involved in the data
process (Kreutzer et al., 2022).
Other issues, like the tendency of many MT approaches to focus on the case
where one of the languages is English (Anastasopoulos and Neubig, 2020), have to
do with allocation of resources. Where most large multilingual systems were trained
on bitexts in which English was one of the two languages, recent huge corporate
systems like those of Fan et al. (2021) and Costa-juss `a et al. (2022) and datasets
like Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200
languages) and create bitexts between many more pairs of languages and not just
through English.
At the smaller end, 8et al. (2020) propose a participatory design process to
encourage content creators, curators, and language technologists who speak these
low-resourced languages to participate in developing MT algorithms. They provide
online groups, mentoring, and infrastructure, and report on a case study on devel-
oping MT algorithms for low-resource African languages. Among their conclusions
was to perform MT evaluation by post-editing rather than direct evaluation, since
having labelers edit an MT system and then measure the distance between the MT
output and its post-edited version both was simpler to train evaluators and makes it
easier to measure true errors in the MT output and not differences due to linguistic
variation (Bentivogli et al., 2018).
13.6 MT Evaluation
Translations are evaluated along two dimensions:
1.adequacy: how well the translation captures the exact meaning of the source adequacy
sentence. Sometimes called faithfulness orﬁdelity .
2.ﬂuency: how ﬂuent the translation is in the target language (is it grammatical, ﬂuency
clear, readable, natural).
Using humans to evaluate is most accurate, but automatic metrics are also used for
convenience.
13.6.1 Using Human Raters to Evaluate MT
The most accurate evaluations use human raters, such as online crowdworkers, to
evaluate each translation along the two dimensions. For example, along the dimen-
sion of ﬂuency , we can ask how intelligible, how clear, how readable, or how natural
the MT output (the target text) is. We can give the raters a scale, for example, from
1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate
each sentence or paragraph of the MT output.

--- Page 281 ---
13.6 • MT E VALUATION 281
We can do the same thing to judge the second dimension, adequacy , using raters
to assign scores on a scale. If we have bilingual raters, we can give them the source
sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale,
how much of the information in the source was preserved in the target. If we only
have monolingual raters but we have a good human translation of the source text, we
can give the monolingual raters the human reference translation and a target machine
translation and again rate how much information is preserved. An alternative is to
doranking : give the raters a pair of candidate translations, and ask them which one ranking
they prefer.
Training of human raters (who are often online crowdworkers) is essential; raters
without translation expertise ﬁnd it difﬁcult to separate ﬂuency and adequacy, and
so training includes examples carefully distinguishing these. Raters often disagree
(source sentences may be ambiguous, raters will have different world knowledge,
raters may apply scales differently). It is therefore common to remove outlier raters,
and (if we use a ﬁne-grained enough scale) normalizing raters by subtracting the
mean from their scores and dividing by the variance.
As discussed above, an alternative way of using human raters is to have them
post-edit translations, taking the MT output and changing it minimally until they
feel it represents a correct translation. The difference between their post-edited
translations and the original MT output can then be used as a measure of quality.
13.6.2 Automatic Evaluation
While humans produce the best evaluations of machine translation output, running a
human evaluation can be time consuming and expensive. For this reason automatic
metrics are often used as temporary proxies. Automatic metrics are less accurate
than human evaluation, but can help test potential system improvements, and even
be used as an automatic loss function for training. In this section we introduce two
families of such metrics, those based on character- or word-overlap and those based
on embedding similarity.
Automatic Evaluation by Character Overlap: chrF
The simplest and most robust metric for MT evaluation is called chrF , which stands chrF
forcharacter F-score (Popovi ´c, 2015). chrF (along with many other earlier related
metrics like BLEU, METEOR, TER, and others) is based on a simple intuition de-
rived from the pioneering work of Miller and Beebe-Center (1956): a good machine
translation will tend to contain characters and words that occur in a human trans-
lation of the same sentence. Consider a test set from a parallel corpus, in which
each source sentence has both a gold human target translation and a candidate MT
translation we’d like to evaluate. The chrF metric ranks each MT target sentence by
a function of the number of character n-gram overlaps with the human translation.
Given the hypothesis and the reference, chrF is given a parameter kindicating
the length of character n-grams to be considered, and computes the average of the
kprecisions (unigram precision, bigram, and so on) and the average of the krecalls
(unigram recall, bigram recall, etc.):
chrP percentage of character 1-grams, 2-grams, ..., k-grams in the hypothesis that
occur in the reference, averaged.
chrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that
occur in the hypothesis, averaged.
The metric then computes an F-score by combining chrP and chrR using a weighting

--- Page 282 ---
282 CHAPTER 13 • M ACHINE TRANSLATION
parameter b. It is common to set b=2, thus weighing recall twice as much as
precision:
chrFb= (1+b2)chrPchrR
b2chrP+chrR(13.20)
Forb=2, that would be:
chrF2 =5chrPchrR
4chrP+chrR
For example, consider two hypotheses that we’d like to score against the refer-
ence translation witness for the past . Here are the hypotheses along with chrF values
computed using parameters k=b=2 (in real examples, kwould be a higher number
like 6):
REF:witness for the past,
HYP1:witness of the past, chrF2,2 = .86
HYP2:past witness chrF2,2 = .62
Let’s see how we computed that chrF value for HYP1 (we’ll leave the compu-
tation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores
spaces, so we’ll remove them from both the reference and hypothesis:
REF:witnessforthepast, (18 unigrams, 17 bigrams)
HYP1:witnessofthepast, (17 unigrams, 16 bigrams)
Next let’s see how many unigrams and bigrams match between the reference and
hypothesis:
unigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams)
bigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams)
We use that to compute the unigram and bigram precisions and recalls:
unigram P: 17/17 = 1 unigram R: 17/18 = .944
bigram P: 13/16 = .813 bigram R: 13/17 = .765
Finally we average to get chrP and chrR, and compute the F-score:
chrP = (17=17+13=16)=2=:906
chrR = (17=18+13=17)=2=:855
chrF2,2 =5chrPchrR
4chrP +chrR=:86
chrF is simple, robust, and correlates very well with human judgments in many
languages (Kocmi et al., 2021).
Alternative overlap metric: BLEU
There are various alternative overlap metrics. For example, before the development
of chrF, it was common to use a word-based overlap metric called BLEU (for BiLin-
gual Evaluation Understudy), that is purely precision-based rather than combining
precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candi-
date translation sentences is a function of the n-gram word precision over all the
sentences combined with a brevity penalty computed over the corpus as a whole.
What do we mean by n-gram precision? Consider a corpus composed of a single
sentence. The unigram precision for this corpus is the percentage of unigram tokens

--- Page 283 ---
13.6 • MT E VALUATION 283
in the candidate translation that also occur in the reference translation, and ditto for
bigrams and so on, up to 4-grams. BLEU extends this unigram metric to the whole
corpus by computing the numerator as the sum over all sentences of the counts of all
the unigram types that also occur in the reference translation, and the denominator
is the total of the counts of all unigrams in all candidate sentences. We compute
this n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the
geometric mean. BLEU has many further complications, including a brevity penalty
for penalizing candidate translations that are too short, and it also requires the n-
gram counts be clipped in a particular way.
Because BLEU is a word-based metric, it is very sensitive to word tokenization,
making it impossible to compare different systems if they rely on different tokeniza-
tion standards, and doesn’t work as well in languages with complex morphology.
Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly
for translation into English. In such cases it’s important to use packages that enforce
standardization for tokenization like S ACRE BLEU (Post, 2018).
Statistical Signiﬁcance Testing for MT evals
Character or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly
used to compare two systems, with the goal of answering questions like: did the
new algorithm we just invented improve our MT system? To know if the difference
between the chrF scores of two MT systems is a signiﬁcant difference, we use the
paired bootstrap test, or the similar randomization test.
To get a conﬁdence interval on a single chrF score using the bootstrap test, recall
from Section 4.9 that we take our test set (or devset) and create thousands of pseudo-
testsets by repeatedly sampling with replacement from the original test set. We now
compute the chrF score of each of the pseudo-testsets. If we drop the top 2.5% and
bottom 2.5% of the scores, the remaining scores will give us the 95% conﬁdence
interval for the chrF score of our system.
To compare two MT systems A and B, we draw the same set of pseudo-testsets,
and compute the chrF scores for each of them. We then compute the percentage of
pseudo-test-sets in which A has a higher chrF score than B.
chrF: Limitations
While automatic character and word-overlap metrics like chrF or BLEU are useful,
they have important limitations. chrF is very local: a large phrase that is moved
around might barely change the chrF score at all, and chrF can’t evaluate cross-
sentence properties of a document like its discourse coherence (Chapter 24). chrF
and similar automatic metrics also do poorly at comparing very different kinds of
systems, such as comparing human-aided translation against machine translation, or
different machine translation architectures against each other (Callison-Burch et al.,
2006). Instead, automatic overlap metrics like chrF are most appropriate when eval-
uating changes to a single system.
13.6.3 Automatic Evaluation: Embedding-Based Methods
The chrF metric is based on measuring the exact character n-grams a human refer-
ence and candidate machine translation have in common. However, this criterion
is overly strict, since a good translation may use alternate words or paraphrases. A
solution ﬁrst pioneered in early metrics like METEOR (Banerjee and Lavie, 2005)
was to allow synonyms to match between the reference xand candidate ˜ x. More

--- Page 284 ---
284 CHAPTER 13 • M ACHINE TRANSLATION
recent metrics use BERT or other embeddings to implement this intuition.
For example, in some situations we might have datasets that have human as-
sessments of translation quality. Such datasets consists of tuples (x;˜x;r), where
x= (x1;:::; xn)is a reference translation, ˜ x= (˜x1;:::; ˜xm)is a candidate machine
translation, and r2Ris a human rating that expresses the quality of ˜ xwith respect
tox. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam
et al., 2020) train a predictor on the human-labeled datasets, for example by passing
xand ˜xthrough a version of BERT (trained with extra pretraining, and then ﬁnetuned
on the human-labeled sentences), followed by a linear layer that is trained to predict
r. The output of such models correlates highly with human labels.
In other cases, however, we don’t have such human-labeled datasets. In that
case we can measure the similarity of xand ˜xby the similarity of their embeddings.
The BERTS CORE algorithm (Zhang et al., 2020) shown in Fig. 13.11, for example,
passes the reference xand the candidate ˜ xthrough BERT, computing a BERT em-
bedding for each token xiand ˜xj. Each pair of tokens (xi;˜xj)is scored by its cosine
xi˜xj
jxijj˜xjj. Each token in xis matched to a token in ˜ xto compute recall, and each token in
˜xis matched to a token in xto compute precision (with each token greedily matched
to the most similar token in the corresponding sentence). BERTS CORE provides
precision and recall (and hence F 1):
RBERT=1
jxjX
xi2xmax
˜xj2˜xxi˜xjPBERT=1
j˜xjX
˜xj2˜xmax
xi2xxi˜xj (13.21)
Published as a conference paper at ICLR 2020
Referencethe weather is cold today
Candidateit is freezing today
CandidateContextualEmbeddingPairwise CosineSimilarityRBERT=(0.713 1.27)+(0.515 7.94)+...1.27+7.94+1.82+7.90+8.88
<latexit sha1_base64="OJyoKlmBAgUA0KDtUcsH/di5BlI=">AAACSHicbZDLattAFIaPnLRJ3JvTLrsZYgoJAqFxGqwsCqal0FVJQ5wELCNG41EyZHRh5ijECL1EnqAv002X2eUZsumipXRR6Mj2Ipf+MPDznXM4Z/64UNKg7187raXlR49XVtfaT54+e/6is/7y0OSl5mLIc5Xr45gZoWQmhihRieNCC5bGShzFZx+a+tG50Ebm2QFOCzFO2UkmE8kZWhR1ov2oClFcYPX+4/5BXZN3JEw049Wm7/XpdogyFYZQr9ffci3aoTsL1Pd23265oZrkaOqqaXAb5FIv6DXOdwMvCOqo0/U9fyby0NCF6Q52/15+BYC9qHMVTnJepiJDrpgxI+oXOK6YRsmVqNthaUTB+Bk7ESNrM2aPGVezIGryxpIJSXJtX4ZkRm9PVCw1ZprGtjNleGru1xr4v9qoxCQYVzIrShQZny9KSkUwJ02qZCK14Kim1jCupb2V8FNmc0SbfduGQO9/+aE57HnU9+gX2h18hrlW4TVswCZQ6MMAPsEeDIHDN7iBn/DL+e78cH47f+atLWcx8wruqNX6B8dUrVw=</latexit><latexit sha1_base64="RInTcZkWiVBnf/ncBstCvatCtG4=">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit sha1_base64="RInTcZkWiVBnf/ncBstCvatCtG4=">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit sha1_base64="fGWl4NCvlvtMu17rjLtk25oWpdc=">AAACSHicbZBLS+RAFIUrPT7bVzsu3RQ2ghIIqVbpuBgQRZiVqNgqdJpQqa5oYeVB1Y1ME/Lz3Lic3fwGNy6UwZ2VNgtfBwoO372Xe+uEmRQaXPef1fgxMTk1PTPbnJtfWFxqLf8812muGO+xVKbqMqSaS5HwHgiQ/DJTnMah5BfhzUFVv7jlSos0OYNRxgcxvUpEJBgFg4JWcBoUPvA/UOwfnp6VJf6F/UhRVmy4Tpds+SBirjFxOt1N26AdslOjrrO7vWn7cpiCLouqwa6QTRyvUznX9hzPK4NW23XcsfBXQ2rTRrWOg9Zff5iyPOYJMEm17hM3g0FBFQgmedn0c80zym7oFe8bm1BzzKAYB1HidUOGOEqVeQngMX0/UdBY61Ecms6YwrX+XKvgd7V+DpE3KESS5cAT9rYoyiWGFFep4qFQnIEcGUOZEuZWzK6pyRFM9k0TAvn85a/mvOMQ1yEnpL13VMcxg1bRGtpABHXRHvqNjlEPMXSHHtATerburUfrv/Xy1tqw6pkV9EGNxisxMKq0</latexit>1.27
7.941.827.908.88idf
weightsImportance Weighting(Optional)Maximum Similarityx<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>ˆx<latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit>
ReferenceFigure 1: Illustration of the computation of the recall metric RBERT. Given the reference xand
candidate ˆx, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy
matching in red, and include the optional idfimportance weighting.
We experiment with different models (Section 4), using the tokenizer provided with each model.
Given a tokenized reference sentence x=hx1,...,x ki, the embedding model generates a se-
quence of vectors hx1,...,xki. Similarly, the tokenized candidate ˆx=hˆx1,..., ˆxmiis mapped
tohˆx1,...,ˆxli. The main model we use is BERT, which tokenizes the input text into a sequence
of word pieces (Wu et al., 2016), where unknown words are split into several commonly observed
sequences of characters. The representation for each word piece is computed with a Transformer
encoder (Vaswani et al., 2017) by repeatedly applying self-attention and nonlinear transformations
in an alternating fashion. BERT embeddings have been shown to beneﬁt various NLP tasks (Devlin
et al., 2019; Liu, 2019; Huang et al., 2019; Yang et al., 2019a).
Similarity Measure The vector representation allows for a soft measure of similarity instead of
exact-string (Papineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosine
similarity of a reference token xiand a candidate token ˆxjisx>
iˆxj
kxikkˆxjk. We use pre-normalized
vectors, which reduces this calculation to the inner product x>
iˆxj. While this measure considers
tokens in isolation, the contextual embeddings contain information from the rest of the sentence.
BERTS CORE The complete score matches each token in xto a token in ˆxto compute recall,
and each token in ˆxto a token in xto compute precision. We use greedy matching to maximize
the matching similarity score,2where each token is matched to the most similar token in the other
sentence. We combine precision and recall to compute an F1 measure. For a reference xand
candidate ˆx, the recall, precision, and F1 scores are:
RBERT =1
|x|X
xi2xmax
ˆxj2ˆxx>
iˆxj,P BERT =1
|ˆx|X
ˆxj2ˆxmax
xi2xx>
iˆxj,F BERT =2PBERT ·RBERT
PBERT +RBERT.
Importance Weighting Previous work on similarity measures demonstrated that rare words can
be more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam
et al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment
with inverse document frequency ( idf) scores computed from the test corpus. Given Mreference
sentences {x(i)}M
i=1, the idfscore of a word-piece token wis
idf(w)= log1
MMX
i=1I[w2x(i)],
where I[·]is an indicator function. We do not use the full tf-idfmeasure because we process single
sentences, where the term frequency ( tf) is likely 1. For example, recall with idfweighting is
RBERT =P
xi2xidf(xi) max ˆxj2ˆxx>
iˆxjP
xi2xidf(xi).
Because we use reference sentences to compute idf, the idfscores remain the same for all systems
evaluated on a speciﬁc test set. We apply plus-one smoothing to handle unknown word pieces.
2We compare greedy matching with optimal assignment in Appendix C.
4
Figure 13.11 The computation of BERTS CORE recall from reference xand candidate ˆ x, from Figure 1 in
Zhang et al. (2020). This version shows an extended version of the metric in which tokens are also weighted by
their idf values.
13.7 Bias and Ethical Issues
Machine translation raises many of the same ethical issues that we’ve discussed in
earlier chapters. For example, consider MT systems translating from Hungarian
(which has the gender neutral pronoun ˝o) or Spanish (which often drops pronouns)
into English (in which pronouns are obligatory, and they have grammatical gender).
When translating a reference to a person described without speciﬁed gender, MT
systems often default to male gender (Schiebinger 2014, Prates et al. 2019). And
MT systems often assign gender according to culture stereotypes of the sort we saw
in Section 6.11. Fig. 13.12 shows examples from Prates et al. (2019), in which Hun-
garian gender-neutral ˝o is a nurse is translated with she, but gender-neutral ˝o is a
CEO is translated with he. Prates et al. (2019) ﬁnd that these stereotypes can’t com-
pletely be accounted for by gender bias in US labor statistics, because the biases are

--- Page 285 ---
13.8 • S UMMARY 285
ampliﬁed by MT systems, with pronouns being mapped to male or female gender
with a probability higher than if the mapping was based on actual labor employment
statistics.
Hungarian (gender neutral) source English MT output
˝o egy ´apol´o she is a nurse
˝o egy tud ´os he is a scientist
˝o egy m ´ern¨ok he is an engineer
˝o egy p ´ek he is a baker
˝o egy tan ´ar she is a teacher
˝o egy esk ¨uv˝oszervez ˝o she is a wedding organizer
˝o egy vez ´erigazgat ´o he is a CEO
Figure 13.12 When translating from gender-neutral languages like Hungarian into English,
current MT systems interpret people from traditionally male-dominated occupations as male,
and traditionally female-dominated occupations as female (Prates et al., 2019).
Similarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019)
shows that MT systems perform worse when they are asked to translate sentences
that describe people with non-stereotypical gender roles, like “The doctor asked the
nurse to help her in the operation”.
Many ethical questions in MT require further research. One open problem is
developing metrics for knowing what our systems don’t know. This is because MT
systems can be used in urgent situations where human translators may be unavailable
or delayed: in medical domains, to help translate when patients and doctors don’t
speak the same language, or in legal domains, to help judges or lawyers communi-
cate with witnesses or defendants. In order to ‘do no harm’, systems need ways to
assign conﬁdence values to candidate translations, so they can abstain from giving conﬁdence
incorrect translations that may cause harm.
13.8 Summary
Machine translation is one of the most widely used applications of NLP, and the
encoder-decoder model, ﬁrst developed for MT is a key tool that has applications
throughout NLP.
• Languages have divergences , both structural and lexical, that make translation
difﬁcult.
• The linguistic ﬁeld of typology investigates some of these differences; lan-
guages can be classiﬁed by their position along typological dimensions like
whether verbs precede their objects.
•Encoder-decoder networks (for transformers just as we saw in Chapter 8 for
RNNs) are composed of an encoder network that takes an input sequence
and creates a contextualized representation of it, the context . This context
representation is then passed to a decoder which generates a task-speciﬁc
output sequence.
•Cross-attention allows the transformer decoder to view information from all
the hidden states of the encoder.
• Machine translation models are trained on a parallel corpus , sometimes called
abitext , a text that appears in two (or more) languages.

--- Page 286 ---
286 CHAPTER 13 • M ACHINE TRANSLATION
•Backtranslation is a way of making use of monolingual corpora in the target
language by running a pilot MT engine backwards to create synthetic bitexts.
• MT is evaluated by measuring a translation’s adequacy (how well it captures
the meaning of the source sentence) and ﬂuency (how ﬂuent or natural it is
in the target language). Human evaluation is the gold standard, but automatic
evaluation metrics like chrF , which measure character n-gram overlap with
human translations, or more recent metrics based on embedding similarity,
are also commonly used.
Bibliographical and Historical Notes
MT was proposed seriously by the late 1940s, soon after the birth of the computer
(Weaver, 1949/1955). In 1954, the ﬁrst public demonstration of an MT system pro-
totype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The
next decade saw a great ﬂowering of ideas, preﬁguring most subsequent develop-
ments. But this work was ahead of its time—implementations were limited by, for
example, the fact that pending the development of disks there was no good way to
store dictionary information.
As high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus
on the need for better evaluation and more basic research in the new ﬁelds of for-
mal and computational linguistics. This consensus culminated in the famously crit-
ical ALPAC (Automatic Language Processing Advisory Committee) report of 1966
(Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT
in the US. As MT research lost academic respectability, the Association for Ma-
chine Translation and Computational Linguistics dropped MT from its name. Some
MT developers, however, persevered, and there were early MT systems like M ´et´eo,
which translated weather forecasts from English to French (Chandioux, 1976), and
industrial systems like Systran.
In the early years, the space of MT architectures spanned three general mod-
els. In direct translation , the system proceeds word-by-word through the source-
language text, translating each word incrementally. Direct translation uses a large
bilingual dictionary, each of whose entries is a small program with the job of trans-
lating one word. In transfer approaches, we ﬁrst parse the input text and then ap-
ply rules to transform the source-language parse into a target language parse. We
then generate the target language sentence from the parse tree. In interlingua ap-
proaches, we analyze the source language text into some abstract meaning repre-
sentation, called an interlingua . We then generate into the target language from
this interlingual representation. A common way to visualize these three early ap-
proaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows theVauquois
triangle
increasing depth of analysis required (on both the analysis and generation end) as
we move from the direct approach through transfer approaches to interlingual ap-
proaches. In addition, it shows the decreasing amount of transfer knowledge needed
as we move up the triangle, from huge amounts of transfer at the direct level (al-
most all knowledge is transfer knowledge for each word) through transfer (transfer
rules only for parse trees or thematic roles) through interlingua (no speciﬁc transfer
knowledge). We can view the encoder-decoder network as an interlingual approach,
with attention acting as an integration of direct and transfer, allowing words or their
representations to be directly accessed by the decoder.

--- Page 287 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 287
sourcetexttarget textDirect TranslationTransferInterlinguaSource Text:Semantic/SyntacticStructureTarget Text:Semantic/SyntacticStructuresource languageanalysissource languageanalysistarget language generation
Figure 13.13 The Vauquois (1968) triangle.
Statistical methods began to be applied around 1990, enabled ﬁrst by the devel-
opment of large bilingual corpora like the Hansard corpus of the proceedings of the
Canadian Parliament, which are kept in both French and English, and then by the
growth of the Web. Early on, a number of researchers showed that it was possible
to extract pairs of aligned sentences from bilingual corpora, using words or simple
cues like sentence length (Kay and R ¨oscheisen 1988, Gale and Church 1991, Gale
and Church 1993, Kay and R ¨oscheisen 1993).
At the same time, the IBM group, drawing directly on the noisy channel model
for speech recognition, proposed two related paradigms for statistical MT . These statistical MT
include the generative algorithms that became known as IBM Models 1 through IBM Models
5, implemented in the Candide system. The algorithms (except for the decoder) Candide
were published in full detail— encouraged by the US government who had par-
tially funded the work— which gave them a huge impact on the research community
(Brown et al. 1990, Brown et al. 1993).
The group also developed a discriminative approach, called MaxEnt (for maxi-
mum entropy, an alternative formulation of logistic regression), which allowed many
features to be combined discriminatively rather than generatively (Berger et al.,
1996), which was further developed by Och and Ney (2002).
By the turn of the century, most academic research on machine translation used
statistical MT, either in the generative or discriminative mode. An extended version
of the generative approach, called phrase-based translation was developed, basedphrase-based
translation
on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn
et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia).
Once automatic metrics like BLEU were developed (Papineni et al., 2002), the
discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM
MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics
like BLEU in a method known as Minimum Error Rate Training , orMERT (Och, MERT
2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits
like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) Moses
were widely used.
There were also approaches around the turn of the century that were based on
syntactic structure (Chapter 18). Models based on transduction grammars (alsotransduction
grammars
called synchronous grammars assign a parallel syntactic tree structure to a pair of
sentences in different languages, with the goal of translating the sentences by ap-
plying reordering operations on the trees. From a generative perspective, we can
view a transduction grammar as generating pairs of aligned sentences in two lan-
guages. Some of the most widely used models included the inversion transduction
grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005),inversion
transduction
grammar

--- Page 288 ---
288 CHAPTER 13 • M ACHINE TRANSLATION
Neural networks had been applied at various times to various aspects of machine
translation; for example Schwenk et al. (2006) showed how to use neural language
models to replace n-gram language models in a Spanish-English system based on
IBM Model 4. The modern neural encoder-decoder approach was pioneered by
Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder,
and was ﬁrst applied to MT by Bahdanau et al. (2015). The transformer encoder-
decoder was proposed by Vaswani et al. (2017) (see the History section of Chap-
ter 9).
Research on evaluation of machine translation began quite early. Miller and
Beebe-Center (1956) proposed a number of methods drawing on work in psycholin-
guistics. These included the use of cloze and Shannon tasks to measure intelligibility
as well as a metric of edit distance from a human translation, the intuition that un-
derlies all modern overlap-based automatic evaluation metrics. The ALPAC report
included an early evaluation study conducted by John Carroll that was extremely in-
ﬂuential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for
ﬁdelity and intelligibility, and had raters score them subjectively on 9-point scales.
Much early evaluation work focuses on automatic word-overlap metrics like BLEU
(Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate)
(Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR
(Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (Popovi ´c,
2015) came later. More recent evaluation work, echoing the ALPAC report, has
emphasized the importance of careful statistical methodology and the use of human
evaluation (Kocmi et al., 2021; Marie et al., 2021).
The early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al.
(2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions
to linguistic typology.
Exercises
13.1 Compute by hand the chrF2,2 score for HYP2 on page 282 (the answer should
round to .62).

--- Page 289 ---
CHAPTER
14Question Answering, Informa-
tion Retrieval, and Retrieval-
Augmented Generation
People need to know things. So pretty much as soon as there were computers we
were asking them questions. Systems in the early 1960s were answering questions
about baseball statistics and scientiﬁc facts. Even ﬁctional computers in the 1970s
like Deep Thought, invented by Douglas Adams in The Hitchhiker’s Guide to the
Galaxy , answered “the Ultimate Question Of Life, The Universe, and Everything”.1
And because so much knowledge is encoded in text, QA systems were performing
at human levels even before LLMs: IBM’s Watson system won the TV game-show
Jeopardy! in 2011, surpassing humans at answering questions like:
WILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL
2
Question answering systems are designed to ﬁll human information needs .
Since a lot of information is present in text form (on the web or in other data like
our email, or books), question answering is closely related to the task behind search
engines. Indeed, the distinction is becoming ever more fuzzy, as modern search
engines are integrated with large language models trained to do question answering.
Question answering systems often focus on a useful subset of information needs:
factoid questions , questions of fact or reasoning that can be answered with simple
facts expressed in short or medium-length texts, like the following:
(14.1) Where is the Louvre Museum located?
(14.2) Where does the energy in a nuclear explosion come from?
(14.3) How to get a script l in latex?
Modern NLP systems answer these questions using large language models, in
one of two ways. The ﬁrst is to make use of the method from Chapter 12: prompt
a pretrained and instruction-tuned LLM, an LLM that has been ﬁnetuned on ques-
tion/answer datasets with the question in the prompt. For example, we could prompt
a causal language model with a string like
Q: Where is the Louvre Museum located? A:
have it do conditional generation given this preﬁx, and take the response as the an-
swer. The idea is that language models have read a lot of facts in their pretraining
data, presumably including the location of the Louvre, and have encoded this infor-
mation in their parameters.
Simply prompting an LLM can be a useful approach to answer many factoid
questions. But it is not yet a complete solution for question answering.
1The answer was 42, but unfortunately the question was never revealed.
2The answer, of course, is ‘Who is Bram Stoker’, and the novel was Dracula .

--- Page 290 ---
290 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
The ﬁrst and main problem is that large language models often give the wrong
answer! Large language models hallucinate . A hallucination is a response that is hallucinate
not faithful to the facts of the world. That is, when asked questions, large language
models simply make up answers that sound reasonable. For example, Dahl et al.
(2024) found that when asked questions about the legal domain (like about particular
legal cases), large language models hallucinated from 69% to 88% of the time!
And it’s not always possible to tell when language models are hallucinating,
partly because LLMs aren’t well- calibrated . In a calibrated system, the conﬁdence calibrated
of a system in the correctness of its answer is highly correlated with the probability
of an answer being correct. So if a calibrated system is wrong, at least it might hedge
its answer or tell us to go check another source. But since language models are not
well-calibrated, they often give a very wrong answer with complete certainty (Zhou
et al., 2024).
A second problem is that simply prompting a large language model doesn’t allow
us to ask questions about proprietary data. A common use of question answering is
about data like our personal email or medical records. Or a company may have
internal documents that contain answers for customer service or internal use. Or
legal ﬁrms need to ask questions about legal discovery from proprietary documents.
Finally, static large language models also have problems with questions about
rapidly changing information (like questions about something that happened last
week) since LLMs won’t have up-to-date information from after their release data.
For this reason the most common way to do question-answering with LLMs is
retrieval-augmented generation orRAG , and that is the method we will focus on RAG
in this chapter. In RAG we use information retrieval (IR) techniques to retrieveinformation
retrieval
documents that are likely to have information that might help answer the question.
Then we use a large language model to generate an answer given these documents.
Basing our answers on retrieved documents can solve some of the problems with
using simple prompting to answer questions. First, it helps ensure that the answer is
grounded in facts from some curated dataset. And the system can give the user the
answer accompanied by the context of the passage or document the answer came
from. This information can help users have conﬁdence in the accuracy of the answer
(or help them spot when it is wrong!). And these retrieval techniques can be used on
any proprietary data we want, such as legal or medical data for those applications.
We’ll begin by introducing information retrieval, the task of choosing the most
relevant document from a document set given a user’s query expressing their infor-
mation need. We’ll see the classic method based on cosines of sparse tf-idf vectors,
a modern neural ‘dense’ retrievers based on instead representing queries and docu-
ments neurally with BERT or other language models. We then introduce retriever-
based question answering and the retrieval-augmented generation paradigm.
Finally, we’ll discuss various QA datasets. These are used for ﬁnetuning LLMs
in instruction tuning, as we saw in Chapter 12. And they are also used as bench-
marks, since question answering has an important function as a benchmark for mea-
suring the abilities of language models.
14.1 Information Retrieval
Information retrieval orIRis the name of the ﬁeld encompassing the retrieval of allinformation
retrieval
IR manner of media based on user information needs. The resulting IR system is often
called a search engine . Our goal in this section is to give a sufﬁcient overview of IR

--- Page 291 ---
14.1 • I NFORMATION RETRIEVAL 291
to see its application to question answering. Readers with more interest speciﬁcally
in information retrieval should see the Historical Notes section at the end of the
chapter and textbooks like Manning et al. (2008).
The IR task we consider is called ad hoc retrieval , in which a user poses a ad hoc retrieval
query to a retrieval system, which then returns an ordered set of documents from
some collection . Adocument refers to whatever unit of text the system indexes and document
retrieves (web pages, scientiﬁc papers, news articles, or even shorter passages like
paragraphs). A collection refers to a set of documents being used to satisfy user collection
requests. A term refers to a word in a collection, but it may also include phrases. term
Finally, a query represents a user’s information need expressed as a set of terms. query
The high-level architecture of an ad hoc retrieval engine is shown in Fig. 14.1.
DocumentDocumentDocumentDocumentDocumentDocumentQuery ProcessingIndexingSearchDocumentDocumentDocumentDocumentDocumentRanked DocumentsDocumentqueryInvertedIndexqueryvectordocument collection
Figure 14.1 The architecture of an ad hoc IR system.
The basic IR architecture uses the vector space model we introduced in Chap-
ter 6, in which we map queries and document to vectors based on unigram word
counts, and use the cosine similarity between the vectors to rank potential documents
(Salton, 1971). This is thus an example of the bag-of-words model introduced in
Chapter 4, since words are considered independently of their positions.
14.1.1 Term weighting and document scoring
Let’s look at the details of how the match between a document and query is scored.
We don’t use raw word counts in IR, instead computing a term weight for each term weight
document word. Two term weighting schemes are common: the tf-idf weighting
introduced in Chapter 6, and a slightly more powerful variant called BM25 . BM25
We’ll reintroduce tf-idf here so readers don’t need to look back at Chapter 6.
Tf-idf (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, the
term frequency tfand the inverse document frequency idf.
The term frequency tells us how frequent the word is; words that occur more
often in a document are likely to be informative about the document’s contents. We
usually use the log 10of the word frequency, rather than the raw count. The intuition
is that a word appearing 100 times in a document doesn’t make that word 100 times
more likely to be relevant to the meaning of the document. We also need to do
something special with counts of 0, since we can’t take the log of 0.3
tft;d=(
1+log10count (t;d) if count (t;d)>0
0 otherwise(14.4)
3We can also use this alternative formulation, which we have used in earlier editions: tf t;d=
log10(count (t;d)+1)

--- Page 292 ---
292 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
If we use log weighting, terms which occur 0 times in a document would have tf =0,
1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =
1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.
The document frequency dftof a term tis the number of documents it oc-
curs in. Terms that occur in only a few documents are useful for discriminating
those documents from the rest of the collection; terms that occur across the entire
collection aren’t as helpful. The inverse document frequency oridfterm weight
(Sparck Jones, 1972) is deﬁned as:
idft=log10N
dft(14.5)
where Nis the total number of documents in the collection, and df tis the number
of documents in which term toccurs. The fewer documents in which a term occurs,
the higher this weight; the lowest weight of 0 is assigned to terms that occur in every
document.
Here are some idf values for some words in the corpus of Shakespeare plays,
ranging from extremely informative words that occur in only one play like Romeo ,
to those that occur in a few like salad orFalstaff , to those that are very common like
foolor so common as to be completely non-discriminative since they occur in all 37
plays like good orsweet .4
Word df idf
Romeo 1 1.57
salad 2 1.27
Falstaff 4 0.967
forest 12 0.489
battle 21 0.246
wit 34 0.037
fool 36 0.012
good 37 0
sweet 37 0
Thetf-idf value for word tin document dis then the product of term frequency
tft;dand IDF:
tf-idf(t;d) =tft;didft (14.6)
14.1.2 Document Scoring
We score document dby the cosine of its vector dwith the query vector q:
score(q;d) =cos(q;d) =qd
jqjjdj(14.7)
Another way to think of the cosine computation is as the dot product of unit vectors;
we ﬁrst normalize both the query and document vector to unit vectors, by dividing
by their lengths, and then take the dot product:
score(q;d) =cos(q;d) =q
jqjd
jdj(14.8)
4Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

--- Page 293 ---
14.1 • I NFORMATION RETRIEVAL 293
We can spell out Eq. 14.8, using the tf-idf values and spelling out the dot product as
a sum of products:
score(q;d) =X
t2qtf-idf(t;q)qP
qi2qtf-idf2(qi;q)tf-idf(t;d)qP
di2dtf-idf2(di;d)(14.9)
Now let’s use (14.9) to walk through an example of a tiny query against a collec-
tion of 4 nano documents, computing tf-idf values and seeing the rank of the docu-
ments. We’ll assume all words in the following query and documents are downcased
and punctuation is removed:
Query :sweet love
Doc 1 :Sweet sweet nurse! Love?
Doc 2 :Sweet sorrow
Doc 3 :How sweet is love?
Doc 4 :Nurse!
Fig. 14.2 shows the computation of the tf-idf cosine between the query and Doc-
ument 1, and the query and Document 2. The cosine is the normalized dot product
of tf-idf values, so for the normalization we must need to compute the document
vector lengthsjqj,jd1j, andjd2jfor the query and the ﬁrst two documents using
Eq. 14.4, Eq. 14.5, Eq. 14.6, and Eq. 14.9 (computations for Documents 3 and 4 are
also needed but are left as an exercise for the reader). The dot product between the
vectors is the sum over dimensions of the product, for each dimension, of the values
of the two tf-idf vectors for that dimension. This product is only non-zero where
both the query and document have non-zero values, so for this example, in which
only sweet andlove have non-zero values in the query, the dot product will be the
sum of the products of those elements of each vector.
Document 1 has a higher cosine with the query (0.747) than Document 2 has
with the query (0.0779), and so the tf-idf cosine model would rank Document 1
above Document 2. This ranking is intuitive given the vector space model, since
Document 1 has both terms including two instances of sweet , while Document 2 is
missing one of the terms. We leave the computation for Documents 3 and 4 as an
exercise for the reader.
In practice, there are many variants and approximations to Eq. 14.9. For exam-
ple, we might choose to simplify processing by removing some terms. To see this,
let’s start by expanding the formula for tf-idf in Eq. 14.9 to explicitly mention the tf
and idf terms from (14.6):
score(q;d) =X
t2qtft;qidftqP
qi2qtf-idf2(qi;q)tft;didftqP
di2dtf-idf2(di;d)(14.10)
In one common variant of tf-idf cosine, for example, we drop the idf term for the
document. Eliminating the second copy of the idf term (since the identical term is
already computed for the query) turns out to sometimes result in better performance:
score(q;d) =X
t2qtft;qidftqP
qi2qtf-idf2(qi;q)tft;didftqP
di2dtf-idf2(di;d)(14.11)
Other variants of tf-idf eliminate various other terms.
A slightly more complex variant in the tf-idf family is the BM25 weighting BM25

--- Page 294 ---
294 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
Query
word cnt tf df idf tf-idf n’lized = tf-idf/jqj
sweet 1 1 3 0.125 0.125 0.383
nurse 0 0 2 0.301 0 0
love 1 1 2 0.301 0.301 0.924
how 0 0 1 0.602 0 0
sorrow 0 0 1 0.602 0 0
is 0 0 1 0.602 0 0
jqj=p
:1252+:3012=:326
Document 1 Document 2
word cnt tf tf-idf n’lized q cnt tf tf-idf n’lized q
sweet 2 1.301 0.163 0.357 0.137 1 1.000 0.125 0.203 0.0779
nurse 1 1.000 0.301 0.661 0 0 0 0 0 0
love 1 1.000 0.301 0.661 0.610 0 0 0 0 0
how 0 0 0 0 0 0 0 0 0 0
sorrow 0 0 0 0 0 1 1.000 0.602 0.979 0
is 0 0 0 0 0 0 0 0 0 0
jd1j=p
:1632+:3012+:3012=:456 jd2j=p
:1252+:6022=:615
Cosine:Pof column: 0.747 Cosine:Pof column: 0.0779
Figure 14.2 Computation of tf-idf cosine score between the query and nano-documents 1 (0.747) and 2
(0.0779), using Eq. 14.4, Eq. 14.5, Eq. 14.6 and Eq. 14.9.
scheme (sometimes called Okapi BM25 after the Okapi IR system in which it was
introduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that
adjust the balance between term frequency and IDF, and b, which controls the im-
portance of document length normalization. The BM25 score of a document dgiven
a query qis:
X
t2qIDFz}|{
logN
dftweighted tfz}|{
tft;d
k
1 b+b
jdj
jdavgj
+tft;d(14.12)
wherejdavgjis the length of the average document. When kis 0, BM25 reverts to
no use of term frequency, just a binary selection of terms in the query (plus idf).
A large kresults in raw term frequency (plus idf). branges from 1 (scaling by
document length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable
values are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of
the many minor variants of BM25.
Stop words In the past it was common to remove high-frequency words from both
the query and document before representing them. The list of such high-frequency
words to be removed is called a stop list . The intuition is that high-frequency terms stop list
(often function words like the,a,to) carry little semantic weight and may not help
with retrieval, and can also help shrink the inverted index ﬁles we describe below.
The downside of using a stop list is that it makes it difﬁcult to search for phrases
that contain words in the stop list. For example, common stop lists would reduce the
phrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists
is much less common, partly due to improved efﬁciency and partly because much
of their function is already handled by IDF weighting, which downweights function

--- Page 295 ---
14.1 • I NFORMATION RETRIEVAL 295
words that occur in every document. Nonetheless, stop word removal is occasionally
useful in various NLP tasks so is worth keeping in mind.
14.1.3 Inverted Index
In order to compute scores, we need to efﬁciently ﬁnd documents that contain words
in the query. (Any document that contains none of the query terms will have a score
of 0 and can be ignored.) The basic search problem in IR is thus to ﬁnd all documents
d2Cthat contain a term q2Q.
The data structure for this task is the inverted index , which we use for mak- inverted index
ing this search efﬁcient, and also conveniently storing useful information like the
document frequency and the count of each term in each document.
An inverted index, given a query term, gives a list of documents that contain the
term. It consists of two parts, a dictionary and the postings . The dictionary is a list postings
of terms (designed to be efﬁciently accessed), each pointing to a postings list for the
term. A postings list is the list of document IDs associated with each term, which
can also contain information like the term frequency or even the exact positions of
terms in the document. The dictionary can also store the document frequency for
each term. For example, a simple inverted index for our 4 sample documents above,
with each word containing its document frequency in fg, and a pointer to a postings
list that contains document IDs and term counts in [], might look like the following:
howf1g!3 [1]
isf1g!3 [1]
lovef2g!1 [1]!3 [1]
nursef2g!1 [1]!4 [1]
sorryf1g!2 [1]
sweetf3g!1 [2]!2 [1]!3 [1]
Given a list of terms in query, we can very efﬁciently get lists of all candidate
documents, together with the information necessary to compute the tf-idf scores we
need.
There are alternatives to the inverted index. For the question-answering domain
of ﬁnding Wikipedia pages to match a user query, Chen et al. (2017a) show that
indexing based on bigrams works better than unigrams, and use efﬁcient hashing
algorithms rather than the inverted index to make the search efﬁcient.
14.1.4 Evaluation of Information-Retrieval Systems
We measure the performance of ranked retrieval systems using the same precision
andrecall metrics we have been using. We make the assumption that each docu-
ment returned by the IR system is either relevant to our purposes or not relevant .
Precision is the fraction of the returned documents that are relevant, and recall is the
fraction of all relevant documents that are returned. More formally, let’s assume a
system returns Tranked documents in response to an information request, a subset
Rof these are relevant, a disjoint subset, N, are the remaining irrelevant documents,
andUdocuments in the collection as a whole are relevant to this request. Precision
and recall are then deﬁned as:
Precision =jRj
jTjRecall =jRj
jUj(14.13)
Unfortunately, these metrics don’t adequately measure the performance of a system
thatranks the documents it returns. If we are comparing the performance of two

--- Page 296 ---
296 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
ranked retrieval systems, we need a metric that prefers the one that ranks the relevant
documents higher. We need to adapt precision and recall to capture how well a
system does at putting relevant documents higher in the ranking.
Rank Judgment Precision Rank Recall Rank
1 R 1.0 .11
2 N .50 .11
3 R .66 .22
4 N .50 .22
5 R .60 .33
6 R .66 .44
7 N .57 .44
8 R .63 .55
9 N .55 .55
10 N .50 .55
11 R .55 .66
12 N .50 .66
13 N .46 .66
14 N .43 .66
15 R .47 .77
16 N .44 .77
17 N .44 .77
18 R .44 .88
19 N .42 .88
20 N .40 .88
21 N .38 .88
22 N .36 .88
23 N .35 .88
24 N .33 .88
25 R .36 1.0
Figure 14.3 Rank-speciﬁc precision and recall values calculated as we proceed down
through a set of ranked documents (assuming the collection has 9 relevant documents).
Let’s turn to an example. Assume the table in Fig. 14.3 gives rank-speciﬁc pre-
cision and recall values calculated as we proceed down through a set of ranked doc-
uments for a particular query; the precisions are the fraction of relevant documents
seen at a given rank, and recalls the fraction of relevant documents found at the same
rank. The recall measures in this example are based on this query having 9 relevant
documents in the collection as a whole.
Note that recall is non-decreasing; when a relevant document is encountered,
recall increases, and when a non-relevant document is found it remains unchanged.
Precision, on the other hand, jumps up and down, increasing when relevant doc-
uments are found, and decreasing otherwise. The most common way to visualize
precision and recall is to plot precision against recall in a precision-recall curve ,precision-recall
curve
like the one shown in Fig. 14.4 for the data in table 14.3.
Fig. 14.4 shows the values for a single query. But we’ll need to combine values
for all the queries, and in a way that lets us compare one system to another. One way
of doing this is to plot averaged precision values at 11 ﬁxed levels of recall (0 to 100,
in steps of 10). Since we’re not likely to have datapoints at these exact levels, we
useinterpolated precision values for the 11 recall values from the data points we dointerpolated
precision
have. We can accomplish this by choosing the maximum precision value achieved
at any level of recall at or above the one we’re calculating. In other words,
IntPrecision (r) =max
i>=rPrecision (i) (14.14)

--- Page 297 ---
14.1 • I NFORMATION RETRIEVAL 297
0.0 0.2 0.4 0.6 0.8 1.0
Recall0.00.20.40.60.81.0Precision
Figure 14.4 The precision recall curve for the data in table 14.3.
This interpolation scheme not only lets us average performance over a set of queries,
but also helps smooth over the irregular precision values in the original data. It is
designed to give systems the beneﬁt of the doubt by assigning the maximum preci-
sion value achieved at higher levels of recall from the one being measured. Fig. 14.5
and Fig. 14.6 show the resulting interpolated data points from our example.
Interpolated Precision Recall
1.0 0.0
1.0 .10
.66 .20
.66 .30
.66 .40
.63 .50
.55 .60
.47 .70
.44 .80
.36 .90
.36 1.0
Figure 14.5 Interpolated data points from Fig. 14.3.
Given curves such as that in Fig. 14.6 we can compare two systems or approaches
by comparing their curves. Clearly, curves that are higher in precision across all
recall values are preferred. However, these curves can also provide insight into the
overall behavior of a system. Systems that are higher in precision toward the left
may favor precision over recall, while systems that are more geared towards recall
will be higher at higher levels of recall (to the right).
A second way to evaluate ranked retrieval is mean average precision (MAP),mean average
precision
which provides a single metric that can be used to compare competing systems or
approaches. In this approach, we again descend through the ranked list of items,
but now we note the precision only at those points where a relevant item has been
encountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 14.3). For a single
query, we average these individual precision measurements over the return set (up
to some ﬁxed cutoff). More formally, if we assume that Rris the set of relevant

--- Page 298 ---
298 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
Interpolated Precision Recall Curve
00.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
RecallPrecision
Figure 14.6 An 11 point interpolated precision-recall curve. Precision at each of the 11
standard recall levels is interpolated for each query from the maximum at any higher level of
recall. The original measured precision recall points are also shown.
documents at or above r, then the average precision (AP) for a single query is
AP=1
jRrjX
d2RrPrecision r(d) (14.15)
where Precision r(d)is the precision measured at the rank at which document dwas
found. For an ensemble of queries Q, we then average over these averages, to get
our ﬁnal MAP measure:
MAP =1
jQjX
q2QAP(q) (14.16)
The MAP for the single query (hence = AP) in Fig. 14.3 is 0.6.
14.2 Information Retrieval with Dense Vectors
The classic tf-idf or BM25 algorithms for IR have long been known to have a con-
ceptual ﬂaw: they work only if there is exact overlap of words between the query
and document. In other words, the user posing a query (or asking a question) needs
to guess exactly what words the writer of the answer might have used, an issue called
thevocabulary mismatch problem (Furnas et al., 1987).
The solution to this problem is to use an approach that can handle synonymy:
instead of (sparse) word-count vectors, using (dense) embeddings. This idea was
ﬁrst proposed for retrieval in the last century under the name of Latent Semantic
Indexing approach (Deerwester et al., 1990), but is implemented in modern times
via encoders like BERT.
The most powerful approach is to present both the query and the document to a
single encoder, allowing the transformer self-attention to see all the tokens of both

--- Page 299 ---
14.2 • I NFORMATION RETRIEVAL WITH DENSE VECTORS 299
the query and the document, and thus building a representation that is sensitive to
the meanings of both query and document. Then a linear layer can be put on top of
the [CLS] token to predict a similarity score for the query/document tuple:
z=BERT (q;[SEP] ;d)[CLS]
score(q;d) =softmax (U(z)) (14.17)
This architecture is shown in Fig. 14.7a. Usually the retrieval step is not done on
an entire document. Instead documents are broken up into smaller passages, such
as non-overlapping ﬁxed-length chunks of say 100 tokens, and the retriever encodes
and retrieves these passages rather than entire documents. The query and document
have to be made to ﬁt in the BERT 512-token window, for example by truncating
the query to 64 tokens and truncating the document if necessary so that it, the query,
[CLS], and [SEP] ﬁt in 512 tokens. The BERT system together with the linear layer
Ucan then be ﬁne-tuned for the relevance task by gathering a tuning dataset of
relevant and non-relevant passages.
QueryDocument………………[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document………………•s(q,d)
(a) (b)
Figure 14.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-
resent self-attention: (a) Use a single encoder to jointly encode query and document and ﬁnetune to produce a
relevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring
(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the
query and document as the score. This is less compute-expensive, but not as accurate.
The problem with the full BERT architecture in Fig. 14.7a is the expense in
computation and time. With this architecture, every time we get a query, we have to
pass every single single document in our entire collection through a BERT encoder
jointly with the new query! This enormous use of resources is impractical for real
cases.
At the other end of the computational spectrum is a much more efﬁcient archi-
tecture, the bi-encoder . In this architecture we can encode the documents in the
collection only one time by using two separate encoder models, one to encode the
query and one to encode the document. We encode each document, and store all
the encoded document vectors in advance. When a query comes in, we encode just
this query and then use the dot product between the query vector and the precom-
puted document vectors as the score for each candidate document (Fig. 14.7b). For
example, if we used BERT, we would have two encoders BERT Qand BERT Dand

--- Page 300 ---
300 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
we could represent the query and document as the [CLS] token of the respective
encoders (Karpukhin et al., 2020):
zq=BERT Q(q)[CLS]
zd=BERT D(d)[CLS]
score(q;d) =zqzd (14.18)
The bi-encoder is much cheaper than a full query/document encoder, but is also
less accurate, since its relevance decision can’t take full advantage of all the possi-
ble meaning interactions between all the tokens in the query and the tokens in the
document.
There are numerous approaches that lie in between the full encoder and the bi-
encoder. One intermediate alternative is to use cheaper methods (like BM25) as the
ﬁrst pass relevance ranking for each document, take the top N ranked documents,
and use expensive methods like the full BERT scoring to rerank only the top N
documents rather than the whole set.
Another intermediate approach is the ColBERT approach of Khattab and Za- ColBERT
haria (2020) and Khattab et al. (2021), shown in Fig. 14.8. This method separately
encodes the query and document, but rather than encoding the entire query or doc-
ument into one vector, it separately encodes each of them into contextual represen-
tations for each token. These BERT representations of each document word can be
pre-stored for efﬁciency. The relevance score between a query qand a document dis
a sum of maximum similarity (MaxSim) operators between tokens in qand tokens
ind. Essentially, for each token in q, ColBERT ﬁnds the most contextually simi-
lar token in d, and then sums up these similarities. A relevant document will have
tokens that are contextually very similar to the query.
More formally, a question qis tokenized as [q1;:::; qn], prepended with a [CLS]
and a special [Q]token, truncated to N=32 tokens (or padded with [MASK] tokens if
it is shorter), and passed through BERT to get output vectors q= [q1;:::;qN]. The
passage dwith tokens [d1;:::; dm], is processed similarly, including a [CLS] and
special[D]token. A linear layer is applied on top of dandqto control the output
dimension, so as to keep the vectors small for storage efﬁciency, and vectors are
rescaled to unit length, producing the ﬁnal vector sequences Eq(length N) and Ed
(length m). The ColBERT scoring mechanism is:
score(q;d) =NX
i=1mmax
j=1EqiEdj(14.19)
While the interaction mechanism has no tunable parameters, the ColBERT ar-
chitecture still needs to be trained end-to-end to ﬁne-tune the BERT encoders and
train the linear layers (and the special [Q] and[D] embeddings) from scratch. It
is trained on triples hq;d+;d iof query q, positive document d+and negative doc-
ument d to produce a score for each document using (14.19), optimizing model
parameters using a cross-entropy loss.
All the supervised algorithms (like ColBERT or the full-interaction version of
the BERT algorithm applied for reranking) need training data in the form of queries
together with relevant and irrelevant passages or documents (positive and negative
examples). There are various semi-supervised ways to get labels; some datasets (like
MS MARCO Ranking, Section 14.3.2) contain gold positive examples. Negative
examples can be sampled randomly from the top-1000 results from some existing
IR system. If datasets don’t have labeled positive examples, iterative methods like

--- Page 301 ---
14.3 • A NSWERING QUESTIONS WITH RAG 301
QueryDocument………………s(q,d)MaxSimMaxSimMaxSim∑
normnormnormnormnormnorm
Figure 14.8 A sketch of the ColBERT algorithm at inference time. The query and docu-
ment are ﬁrst passed through separate BERT encoders. Similarity between query and doc-
ument is computed by summing a soft alignment between the contextual representations of
tokens in the query and the document. Training is end-to-end. (Various details aren’t de-
picted; for example the query is prepended by a [CLS] and[Q:] tokens, and the document
by[CLS] and[D:] tokens). Figure adapted from Khattab and Zaharia (2020).
relevance-guided supervision can be used (Khattab et al., 2021) which rely on the
fact that many datasets contain short answer strings. In this method, an existing IR
system is used to harvest examples that do contain short answer strings (the top few
are taken as positives) or don’t contain short answer strings (the top few are taken as
negatives), these are used to train a new retriever, and then the process is iterated.
Efﬁciency is an important issue, since every possible document must be ranked
for its similarity to the query. For sparse word-count vectors, the inverted index
allows this very efﬁciently. For dense vector algorithms ﬁnding the set of dense
document vectors that have the highest dot product with a dense query vector is
an instance of the problem of nearest neighbor search . Modern systems there-
fore make use of approximate nearest neighbor vector search algorithms like Faiss Faiss
(Johnson et al., 2017).
14.3 Answering Questions with RAG
The dominant paradigm for question answering is to answer a user’s question by ﬁrst
ﬁnding supportive text segments from the web or another other large collection of
documents, and then generating an answer based on the documents. The method of
generating based on retrieved documents is called retrieval-augmented generation
orRAG , and the two components are sometimes called the retriever and the reader
(Chen et al., 2017a). Fig. 14.9 sketches out this standard QA model.
In the ﬁrst stage of the 2-stage retrieve and read model in Fig. 14.9 we retrieve
relevant passages from a text collection, for example using the dense retrievers of the
previous section. In the second reader stage, we generate the answer via retrieval-

--- Page 302 ---
302 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
Q: When wasthe premiere ofThe Magic Flute?RelevantDocsA:  1791RetrieverIndexed Docs
querydocsLLMpromptReader/Generator
Figure 14.9 Retrieval-based question answering has two stages: retrieval , which returns relevant documents
from the collection, and reading , in which an LLM generates answers given the documents as a prompt.
augmented generation . In this method, we take a large pretrained language model,
give it the set of retrieved passages and other text as its prompt, and autoregressively
generate a new answer token by token.
14.3.1 Retrieval-Augmented Generation
The standard reader algorithm is to generate from a large language model, condi-
tioned on the retrieved passages. This method is known as retrieval-augmented
generation , orRAG .retrieval-
augmented
generation
RAG Recall that in simple conditional generation, we can cast the task of question
answering as word prediction by giving a language model a question and a token
likeA:suggesting that an answer should come next:
Q: Who wrote the book ``The Origin of Species"? A:
Then we generate autoregressively conditioned on this text.
More formally, recall that simple autoregressive language modeling computes
the probability of a string from the previous tokens:
p(x1;:::; xn) =nY
i=1p(xijx<i)
And simple conditional generation for question answering adds a prompt like Q:,
followed by a query q, andA:, all concatenated:
p(x1;:::; xn) =nY
i=1p([Q:] ;q;[A:] ;x<i)
The advantage of using a large language model is the enormous amount of
knowledge encoded in its parameters from the text it was pretrained on. But as
we mentioned at the start of the chapter, while this kind of simple prompted gener-
ation can work ﬁne for many simple factoid questions, it is not a general solution
for QA, because it leads to hallucination, is unable to show users textual evidence to
support the answer, and is unable to answer questions from proprietary data.
The idea of retrieval-augmented generation is to address these problems by con-
ditioning on the retrieved passages as part of the preﬁx, perhaps with some prompt
text like “Based on these texts, answer this question:”. Let’s suppose we have a
query q, and call the set of retrieved passages based on it R( q). For example, we
could have a prompt like:

--- Page 303 ---
14.3 • A NSWERING QUESTIONS WITH RAG 303
Schematic of a RAG Prompt
retrieved passage 1
retrieved passage 2
...
retrieved passage n
Based on these texts, answer this question: Q: Who wrote
the book ``The Origin of Species"? A:
Or more formally,
p(x1;:::; xn) =nY
i=1p(xijR(q) ; prompt ; [Q:] ;q;[A:] ;x<i)
As with the span-based extraction reader, successfully applying the retrieval-
augmented generation algorithm for QA requires a successful retriever, and often
a two-stage retrieval algorithm is used in which the retrieval is reranked. Some
complex questions may require multi-hop architectures, in which a query is used to multi-hop
retrieve documents, which are then appended to the original query for a second stage
of retrieval. Details of prompt engineering also have to be worked out, like deciding
whether to demarcate passages, for example with [SEP] tokens, and so on. Combi-
nations of private data and public data involving an externally hosted large language
model may lead to privacy concerns that need to be worked out (Arora et al., 2023).
Much research in this area also focuses on ways to more tightly integrated the re-
trieval and reader stages.
14.3.2 Question Answering Datasets
There are scores of question answering datasets, used both for instruction tuning and
for evaluation of the question answering abilities of language models.
We can distinguish the datasets along many dimensions, summarized nicely in
Rogers et al. (2023). One is the original purpose of the questions in the data, whether
they were natural information-seeking questions, or whether they were questions
designed for probing : evaluating or testing systems or humans.
On the natural side there are datasets like Natural Questions (KwiatkowskiNatural
Questions
et al., 2019), a set of anonymized English queries to the Google search engine and
their answers. The answers are created by annotators based on Wikipedia infor-
mation, and include a paragraph-length long answer and a short span answer. For
example the question “When are hops added to the brewing process?” has the short
answer the boiling process and a long answer which is an entire paragraph from the
Wikipedia page on Brewing .
A similar natural question set is the MS MARCO (Microsoft Machine Reading MS MARCO
Comprehension) collection of datasets, including 1 million real anonymized English
questions from Microsoft Bing query logs together with a human generated answer
and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval
ranking and question answering.

--- Page 304 ---
304 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
Although many datasets focus on English, natural information-seeking ques-
tion datasets exist in other languages. The DuReader dataset is a Chinese QA
resource based on search engine queries and community QA (He et al., 2018).
TyDi QA dataset contains 204K question-answer pairs from 11 typologically di- TyDi QA
verse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark
et al., 2020a). In the T YDIQA task, a system is given a question and the passages
from a Wikipedia article and must (a) select the passage containing the answer (or
NULL if no passage contains the answer), and (b) mark the minimal answer span (or
NULL).
On the probing side are datasets like MMLU (Massive Multitask Language Un- MMLU
derstanding), a commonly-used dataset of 15908 knowledge and reasoning ques-
tions in 57 areas including medicine, mathematics, computer science, law, and oth-
ers. MMLU questions are sourced from various exams for humans, such as the US
Graduate Record Exam, Medical Licensing Examination, and Advanced Placement
exams. So the questions don’t represent people’s information needs, but rather are
designed to test human knowledge for academic or licensing purposes. Fig. 14.10
shows some examples, with the correct answers in bold.
Some of the question datasets described above augment each question with pas-
sage(s) from which the answer can be extracted. These datasets were mainly created
for an earlier QA task called reading comprehension in which a model is givenreading
comprehension
a question and a document and is required to extract the answer from the given
document. We sometimes call the task of question answering given one or more
documents (for example via RAG), the open book QA task, while the task of an- open book
swering directly from the LM with no retrieval component at all is the closed book closed book
QA task.5Thus datasets like Natural Questions can be treated as open book if the
solver uses each question’s attached document, or closed book if the documents are
not used, while datasets like MMLU are solely closed book.
Another dimension of variation is the format of the answer: multiple-choice
versus freeform. And of course there are variations in prompting, like whether the
model is just the question (zero-shot) or also given demonstrations of answers to
similar questions (few-shot). MMLU offers both zero-shot and few-shot prompt
options.
14.4 Evaluating Question Answering
Three techniques are commonly employed to evaluate question-answering systems,
with the choice depending on the type of question and QA situation. For multiple
choice questions like in MMLU, we report exact match:
Exact match : The % of predicted answers that match the gold answer
exactly.
For questions with free text answers, like Natural Questions, we commonly evalu-
ated with token F1score to roughly measure the partial string overlap between the
answer and the reference answer:
F1score : The average token overlap between predicted and gold an-
swers. Treat the prediction and gold as a bag of tokens, and compute F 1
for each question, then return the average F 1over all questions.
5This repurposes the word for types of exams in which students are allowed to ‘open their books’ or
not.

--- Page 305 ---
14.4 • E VALUATING QUESTION ANSWERING 305
MMLU examples
College Computer Science
Any set of Boolean operators that is sufﬁcient to represent all Boolean ex-
pressions is said to be complete. Which of the following is NOT complete?
(A) AND, NOT
(B) NOT, OR
(C) AND, OR
(D) NAND
College Physics
The primary source of the Sun’s energy is a series of thermonuclear
reactions in which the energy produced is c2times the mass difference
between
(A) two hydrogen atoms and one helium atom
(B)four hydrogen atoms and one helium atom
(C) six hydrogen atoms and two helium atoms
(D) three helium atoms and one carbon atom
International Law
Which of the following is a treaty-based human rights mechanism?
(A)The UN Human Rights Committee
(B) The UN Human Rights Council
(C) The UN Universal Periodic Review
(D) The UN special mandates
Prehistory
Unlike most other early civilizations, Minoan culture shows little evidence
of
(A) trade.
(B) warfare.
(C) the development of a common religion.
(D)conspicuous consumption by elites.
Figure 14.10 Example problems from MMLU
Finally, in some situations QA systems give multiple ranked answers. In such cases
we evaluated using mean reciprocal rank , orMRR (V oorhees, 1999). MRR ismean
reciprocal rank
MRR designed for systems that return a short ranked list of answers or passages for each
test set question, which we can compare against the (human-labeled) correct answer.
First, each test set question is scored with the reciprocal of the rank of the ﬁrst
correct answer. For example if the system returned ﬁve answers to a question but
the ﬁrst three are wrong (so the highest-ranked correct answer is ranked fourth), the
reciprocal rank for that question is1
4. The score for questions that return no correct
answer is 0. The MRR of a system is the average of the scores for each question in
the test set. In some versions of MRR, questions with a score of zero are ignored
in this calculation. More formally, for a system returning ranked answers to each
question in a test set Q, (or in the alternate version, let Qbe the subset of test set

--- Page 306 ---
306 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
questions that have non-zero scores). MRR is then deﬁned as
MRR =1
jQjjQjX
i=11
rank i(14.20)
14.5 Summary
This chapter introduced the tasks of question answering andinformation retrieval .
•Question answering (QA) is the task of answering a user’s questions.
• We focus in this chapter on the task of retrieval-based question answering,
in which the user’s questions are intended to be answered by the material in
some set of documents (which might be the web).
•Information Retrieval (IR) is the task of returning documents to a user based
on their information need as expressed in a query . In ranked retrieval, the
documents are returned in ranked order.
• The match between a query and a document can be done by ﬁrst representing
each of them with a sparse vector that represents the frequencies of words,
weighted by tf-idf orBM25 . Then the similarity can be measured by cosine.
• Documents or queries can instead be represented by dense vectors, by encod-
ing the question and document with an encoder-only model like BERT, and in
that case computing similarity in embedding space.
• The inverted index is a storage mechanism that makes it very efﬁcient to ﬁnd
documents that have a particular word.
• Ranked retrieval is generally evaluated by mean average precision orinter-
polated precision .
• Question answering systems generally use the retriever /reader architecture.
In the retriever stage, an IR system is given a query and returns a set of
documents.
• The reader stage is implemented by retrieval-augmented generation , in
which a large language model is prompted with the query and a set of doc-
uments and then conditionally generates a novel answer.
• QA can be evaluated by exact match with a known answer if only a single
answer is given, with token F 1score for free text answers, or with mean re-
ciprocal rank if a ranked set of answers is given.
Bibliographical and Historical Notes
Question answering was one of the earliest NLP tasks, and early versions of the text-
based and knowledge-based paradigms were developed by the very early 1960s. The
text-based algorithms generally relied on simple parsing of the question and of the
sentences in the document, and then looking for matches. This approach was used
very early on (Phillips, 1960) but perhaps the most complete early system, and one
that strikingly preﬁgures modern relation-based systems, was the Protosynthex sys-
tem of Simmons et al. (1964). Given a question, Protosynthex ﬁrst formed a query

--- Page 307 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 307
from the content words in the question, and then retrieved candidate answer sen-
tences in the document, ranked by their frequency-weighted term overlap with the
question. The query and each retrieved sentence were then parsed with dependency
parsers, and the sentence whose structure best matches the question structure se-
lected. Thus the question What do worms eat? would match worms eat grass : both
have the subject worms as a dependent of eat, in the version of dependency grammar
used at the time, while birds eat worms hasbirds as the subject:
Whatdowormseat Wormseatgrass Birdseatworms
The alternative knowledge-based paradigm was implemented in the BASEBALL
system (Green et al., 1961). This system answered questions about baseball games
like “Where did the Red Sox play on July 7” by querying a structured database of
game information. The database was stored as a kind of attribute-value matrix with
values for attributes of each game:
Month = July
Place = Boston
Day = 7
Game Serial No. = 96
(Team = Red Sox, Score = 5)
(Team = Yankees, Score = 3)
Each question was constituency-parsed using the algorithm of Zellig Harris’s
TDAP project at the University of Pennsylvania, essentially a cascade of ﬁnite-state
transducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen
1999). Then in a content analysis phase each word or phrase was associated with a
program that computed parts of its meaning. Thus the phrase ‘Where’ had code to
assign the semantics Place = ? , with the result that the question “Where did the
Red Sox play on July 7” was assigned the meaning
Place = ?
Team = Red Sox
Month = July
Day = 7
The question is then matched against the database to return the answer. Simmons
(1965) summarizes other early QA systems.
Another important progenitor of the knowledge-based paradigm for question-
answering is work that used predicate calculus as the meaning representation lan-
guage. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to be LUNAR
a natural language interface to a database of chemical facts about lunar geology. It
could answer questions like Do any samples have greater than 13 percent aluminum
by parsing them into a logical form
(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16
(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))
By a couple decades later, drawing on new machine learning approaches in NLP,
Zelle and Mooney (1996) proposed to treat knowledge-based QA as a semantic pars-
ing task, by creating the Prolog-based GEOQUERY dataset of questions about US
geography. This model was extended by Zettlemoyer and Collins (2005) and 2007.

--- Page 308 ---
308 CHAPTER 14 • Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG
By a decade later, neural models were applied to semantic parsing (Dong and Lap-
ata 2016, Jia and Liang 2016), and then to knowledge-based question answering by
mapping text to SQL (Iyer et al., 2017).
Meanwhile, the information-retrieval paradigm for question answering was in-
ﬂuenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC
(Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed
for evaluating information-retrieval tasks and techniques (V oorhees and Harman,
2005). TREC added an inﬂuential QA track in 1999, which led to a wide variety of
factoid and non-factoid systems competing in annual evaluations.
At that same time, Hirschman et al. (1999) introduced the idea of using chil-
dren’s reading comprehension tests to evaluate machine text comprehension algo-
rithms. They acquired a corpus of 120 passages with 5 questions each designed for
3rd-6th grade children, built an answer extraction system, and measured how well
the answers given by their system corresponded to the answer key from the test’s
publisher. Their algorithm focused on word overlap as a feature; later algorithms
added named entity features and more complex similarity between the question and
the answer span (Riloff and Thelen 2000, Ng et al. 2000).
The DeepQA component of the Watson Jeopardy! system was a large and so-
phisticated feature-based system developed just before neural systems became com-
mon. It is described in a series of papers in volume 56 of the IBM Journal of Re-
search and Development, e.g., Ferrucci (2012).
Early neural reading comprehension systems drew on the insight common to
early systems that answer ﬁnding should focus on question-passage similarity. Many
of the architectural outlines of these neural systems were laid out in Hermann et al.
(2015a), Chen et al. (2017a), and Seo et al. (2017). These systems focused on
datasets like Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their succes-
sors, usually using separate IR algorithms as input to neural reading comprehension
systems. The paradigm of using dense retrieval with a span-based reader, often with
a single end-to-end architecture, is exempliﬁed by systems like Lee et al. (2019) or
Karpukhin et al. (2020). An important research area with dense retrieval for open-
domain QA is training data: using self-supervised methods to avoid having to label
positive and negative passages (Sachan et al., 2023).
Early work on large language models showed that they stored sufﬁcient knowl-
edge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al.,
2020; Radford et al., 2019; Roberts et al., 2020), at ﬁrst not competitively with
special-purpose question answerers, but then surpassing them. Retrieval-augmented
generation algorithms were ﬁrst introduced as a way to improve language modeling
(Khandelwal et al., 2019), but were quickly applied to question answering (Izacard
et al., 2022; Ram et al., 2023; Shi et al., 2023).
Exercises

--- Page 309 ---
CHAPTER
15Chatbots & Dialogue Systems
Les lois de la conversation sont en g ´en´eral de ne s’y appesantir sur aucun ob-
jet, mais de passer l ´eg`erement, sans effort et sans affectation, d’un sujet `a un
autre ; de savoir y parler de choses frivoles comme de choses s ´erieuses
[The rules of conversation are, in general, not to dwell on any one subject,
but to pass lightly from one to another without effort and without affectation;
to know how to speak about trivial topics as well as serious ones;]
The 18th C. Encyclopedia of Diderot, start of the entry on conversation
The literature of the fantastic abounds in inanimate objects magically endowed with
the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s story about
Frankenstein, we continually reinvent stories about creat-
ing something and then having a chat with it. Legend has
it that after ﬁnishing his sculpture Moses , Michelangelo
thought it so lifelike that he tapped it on the knee and
commanded it to speak. Perhaps this shouldn’t be sur-
prising. Language is the mark of humanity and sentience,
andconversation ordialogue is the most fundamental conversation
dialogue arena of language. It is the ﬁrst kind of language we
learn as children, and the kind we engage in constantly,
whether we are ordering lunch, buying train tickets, or
talking with our families, friends, or coworkers.
This chapter introduces the fundamental algorithms of programs that use con-
versation to interact with users. We often distinguish between two kinds of archi-
tectures. Task-oriented dialogue systems converse with users to accomplish ﬁxed dialogue system
tasks like controlling appliances or ﬁnding restaurants, relying on a data structure
called the frame , which represents the knowledge a system needs to acquire from frame
the user (like the time to set an alarm clock). Chatbots , by contrast, are designed chatbot
to mimic the longer and more unstructured conversations or ‘chats’ characteristic of
human-human interaction. Modern systems incorporate aspects of both; industrial
chatbots like ChatGPT can carry on longer unstructured conversations; industrial
digital assistants like Siri or Alexa are generally frame-based dialogue systems.
The fact that chatbots and dialogue systems are designed for human-computer
interaction has strong implications for their design and use. Many of these impli-
cations already became clear in one of the earliest chatbots, ELIZA (Weizenbaum,
1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch
of clinical psychology whose methods involve drawing the patient out by reﬂecting
patient’s statements back at them. Rogerian interactions are the rare type of conver-
sation in which, as Weizenbaum points out, one can “assume the pose of knowing
almost nothing of the real world”. If a patient says “I went for a long boat ride” and
the psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what

--- Page 310 ---
310 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
Men are all alike.
IN WHAT WAY
They’re always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
Figure 15.1 Sample ELIZA dialogue from Weizenbaum (1966).
a boat is, but rather assume she had some conversational goal.1
Weizenbaum made use of this property of Rogerian psychiatric conversations,
along with clever regular expressions, to allow ELIZA to interact in ways that seemed
deceptively human-like, as in the sample conversational fragment in Fig. 15.1.
As we foreshadowed in Chapter 2, ELIZA worked by simple rules roughly like:
(.*) YOU (.*) ME -> WHAT MAKES YOU THINK I \2 YOU
to transform a user sentence like “ You hate me ” into a system response like
WHAT MAKES YOU THINK I HATE YOU
Among Weizenbaum’s clever tricks are the linking of each ELIZA pattern/rule
to a keyword. Consider the following user sentence:
I know everybody laughed at me
Because it has the word “I”, this sentence could match the following rule whose
keyword is I:
I (.*) -> You say you \1
producing:
YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU
Weizenbaum points out, however, that a more powerful response would rely on
the keyword “everybody”, since someone using universals like everybody oralways
is probably thinking about a speciﬁc person or situation. So the ELIZA algorithm
prefers to respond using patterns associated more speciﬁc keywords like everybody :
WHO IN PARTICULAR ARE YOU THINKING OF?
If no keyword matches, the algorithm chooses a non-committal response like
“PLEASE GO ON ”, “THAT'S VERY INTERESTING ”, or “I SEE ”.
ELIZA illustrates a number of important issues with chatbots. First, people
became deeply emotionally involved and conducted very personal conversations,
even to the extent of asking Weizenbaum to leave the room while they were typ-
ing. Reeves and Nass (1996) show that people tend to assign human characteristics
to computers and interact with them in ways that are typical of human-human in-
teractions. They interpret an utterance in the way they would if it had spoken by a
human, (even though they are aware they are talking to a computer). This means that
chatbots can have signiﬁcant inﬂuences on people’s cognitive and emotional state.
A second related issue is privacy. When Weizenbaum suggested that he might
want to store the ELIZA conversations, people immediately pointed out that this
would violate people’s privacy. Modern chatbots in the home are likely to overhear
1This is due to the Gricean principle of relevance that we’ll discuss in the next section..

--- Page 311 ---
15.1 • P ROPERTIES OF HUMAN CONVERSATION 311
private information, even if they aren’t used for counseling as ELIZA was. Indeed,
if a chatbot is human-like, users are more likely to disclose private information, and
yet less likely to worry about the harm of this disclosure (Ischen et al., 2019).
Both of these issues (emotional engagement and privacy) mean we need to think
carefully about how we deploy chatbots and the people who are interacting with
them. Dialogue research that uses human participants often requires getting permis-
sion from the Institutional Review Board (IRB) of your institution.
In the next section we introduce some basic properties of human conversation.
We then turn in the rest of the chapter to the two basic paradigms for conversational
interaction: frame-based dialogue systems and chatbots.
15.1 Properties of Human Conversation
Conversation between humans is an intricate and complex joint activity. Before
we attempt to design a dialogue system to converse with humans, it is crucial to
understand something about how humans converse with each other. Consider some
of the phenomena that occur in the conversation between a human travel agent and
a human client excerpted in Fig. 15.2.
C1: . . . I need to travel in May.
A2: And, what day in May did you want to travel?
C3: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
A4: And you’re ﬂying into what city?
C5: Seattle.
A6: And what time would you like to leave Pittsburgh?
C7: Uh hmm I don’t think there’s many options for non-stop.
A8: Right. There’s three non-stops today.
C9: What are they?
A10: The ﬁrst one departs PGH at 10:00am arrives Seattle at 12:05 their time.
The second ﬂight departs PGH at 5:55pm, arrives Seattle at 8pm. And the
last ﬂight departs PGH at 8:15pm arrives Seattle at 10:28pm.
C11: OK I’ll take the 5ish ﬂight on the night before on the 11th.
A12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air
ﬂight 115.
C13: OK.
A14: And you said returning on May 15th?
C15: Uh, yeah, at the end of the day.
A16: OK. There’s #two non-stops . . . #
C17: #Act. . . actually #, what day of the week is the 15th?
A18: It’s a Friday.
C19: Uh hmm. I would consider staying there an extra day til Sunday.
A20: OK. . . OK. On Sunday I have . . .
Figure 15.2 Part of a phone conversation between a human travel agent (A) and human
client (C). The passages framed by # in A 16and C 17indicate overlaps in speech.
Turns
A dialogue is a sequence of turns (C1, A2, C3, and so on), each a single contribution turn
from one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,

--- Page 312 ---
312 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
then me, and so on). There are 20 turns in Fig. 15.2. A turn can consist of a sentence
(like C 1), although it might be as short as a single word (C 13) or as long as multiple
sentences (A 10).
Turn structure has important implications for spoken dialogue. A human has
to know when to stop talking; the client interrupts (in A 16and C 17), so a system
that was performing this role must know to stop talking (and that the user might be
making a correction). A system also has to know when to start talking. For example,
most of the time in conversation, speakers start their turns almost immediately after
the other speaker ﬁnishes, without a long pause, because people are can usually
predict when the other person is about to ﬁnish talking. Spoken dialogue systems
must also detect whether a user is done speaking, so they can process the utterance
and respond. This task—called endpointing orendpoint detection — can be quite endpointing
challenging because of noise and because people often pause in the middle of turns.
Speech Acts
A key insight into conversation—due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962)—is that each utterance in a
dialogue is a kind of action being performed by the speaker. These actions are com-
monly called speech acts ordialogue acts : here’s one taxonomy consisting of 4 speech acts
major classes (Bach and Harnish, 1979):
Constatives: committing the speaker to something’s being the case ( answering ,claiming ,
conﬁrming ,denying ,disagreeing ,stating )
Directives: attempts by the speaker to get the addressee to do something ( advising ,ask-
ing,forbidding ,inviting ,ordering ,requesting )
Commissives: committing the speaker to some future course of action ( promising, planning,
vowing, betting, opposing )
Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some so-
cial action ( apologizing ,greeting ,thanking ,accepting an acknowledgment )
A user asking a person or a dialogue system to do something (‘Turn up the mu-
sic’) is issuing a D IRECTIVE . Asking a question that requires an answer is also
a way of issuing a D IRECTIVE : in a sense when the system says (A 2) “what day
in May did you want to travel?” it’s as if the system is (very politely) command-
ing the user to answer. By contrast, a user stating a constraint (like C 1‘I need to
travel in May’) is issuing a C ONSTATIVE . A user thanking the system is issuing
an A CKNOWLEDGMENT . The speech act expresses an important component of the
intention of the speaker (or writer) in saying what they said.
Grounding
A dialogue is not just a series of independent speech acts, but rather a collective act
performed by the speaker and the hearer. Like all collective acts, it’s important for
the participants to establish what they both agree on, called the common groundcommon
ground
(Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Ground- grounding
ing means acknowledging that the hearer has understood the speaker (Clark, 1996).
(People need grounding for non-linguistic actions as well; the reason an elevator but-
ton lights up when it’s pressed is to acknowledge that the elevator has indeed been
called, essentially grounding your action of pushing the button (Norman, 1988).)
Humans constantly ground each other’s utterances. We can ground by explicitly
saying “OK”, as the agent does in A 8or A 10. Or we can ground by repeating what
the other person says; in utterance A 2the agent repeats “in May”, demonstrating her

--- Page 313 ---
15.1 • P ROPERTIES OF HUMAN CONVERSATION 313
understanding to the client. Or notice that when the client answers a question, the
agent begins the next question with “And”. The “And” implies that the new question
is ‘in addition’ to the old question, again indicating to the client that the agent has
successfully understood the answer to the last question.
Subdialogues and Dialogue Structure
Conversations have structure. Consider, for example, the local structure between
speech acts discussed in the ﬁeld of conversation analysis (Sacks et al., 1974).conversation
analysis
QUESTIONS set up an expectation for an ANSWER . PROPOSALS are followed by
ACCEPTANCE (orREJECTION ). C OMPLIMENTS (“Nice jacket!”) often give rise to
DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs are adjacency pair
composed of a ﬁrst pair part and a second pair part (Schegloff, 1968), and these
expectations can help systems decide what actions to take.
However, dialogue acts aren’t always followed immediately by their second pair
part. The two parts can be separated by a side sequence (Jefferson 1972) or sub- side sequence
dialogue . For example utterances C 17to A 20constitute a correction subdialogue subdialogue
(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):
C17: #Act. . . actually#, what day of the week is the 15th?
A18: It’s a Friday.
C19: Uh hmm. I would consider staying there an extra day til Sunday.
A20: OK. . . OK. On Sunday I have . . .
The question in C 17interrupts the prior discourse, in which the agent was looking
for a May 15 return ﬂight. The agent must answer the question and also realize that
‘’I would consider staying...til Sunday” means that the client would probably like to
change their plan, and now go back to ﬁnding return ﬂights, but for the 17th.
Another side sequence is the clariﬁcation question , which can form a subdia-
logue between a REQUEST and a RESPONSE . This is especially common in dialogue
systems where speech recognition errors causes the system to have to ask for clari-
ﬁcations or repetitions like the following:
User: What do you have going to UNKNOWN WORD on the 5th?
System: Let’s see, going where on the 5th?
User: Going to Hong Kong.
System: OK, here are some ﬂights...
In addition to side-sequences, questions often have presequences , like the fol- presequence
lowing example where a user starts with a question about the system’s capabilities
(“Can you make train reservations”) before making a request.
User: Can you make train reservations?
System: Yes I can.
User: Great, I’d like to reserve a seat on the 4pm train to New York.
Initiative
Sometimes a conversation is completely controlled by one participant. For exam-
ple a reporter interviewing a chef might ask questions, and the chef responds. We
say that the reporter in this case has the conversational initiative (Carbonell, 1970; initiative
Nickerson, 1976). In normal human-human dialogue, however, it’s more common
for initiative to shift back and forth between the participants, as they sometimes
answer questions, sometimes ask them, sometimes take the conversations in new di-
rections, sometimes not. You may ask me a question, and then I respond asking you

--- Page 314 ---
314 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
to clarify something you said, which leads the conversation in all sorts of ways. We
call such interactions mixed initiative (Carbonell, 1970).
Full mixed initiative, while the norm for human-human conversations, can be
difﬁcult for dialogue systems. The most primitive dialogue systems tend to use
system-initiative , where the system asks a question and the user can’t do anything
until they answer it, or user-initiative like simple search engines, where the user
speciﬁes a query and the system passively responds. Even modern large language
model-based dialogue systems, which come much closer to using full mixed initia-
tive, often don’t have completely natural initiative switching. Getting this right is an
important goal for modern systems.
Inference and Implicature
Inference is also important in dialogue understanding. Consider the client’s response
C2, repeated here:
A2: And, what day in May did you want to travel?
C3: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
Notice that the client does not in fact answer the agent’s question. The client
merely mentions a meeting at a certain time. What is it that licenses the agent to
infer that the client is mentioning this meeting so as to inform the agent of the travel
dates?
The speaker seems to expect the hearer to draw certain inferences; in other
words, the speaker is communicating more information than seems to be present
in the uttered words. This kind of example was pointed out by Grice (1975, 1978)
as part of his theory of conversational implicature .Implicature means a particu- implicature
lar class of licensed inferences. Grice proposed that what enables hearers to draw
these inferences is that conversation is guided by a set of maxims , general heuristics
that play a guiding role in the interpretation of conversational utterances. One such
maxim is the maxim of relevance which says that speakers attempt to be relevant, relevance
they don’t just utter random speech acts. When the client mentions a meeting on the
12th, the agent reasons ‘There must be some relevance for mentioning this meeting.
What could it be?’. The agent knows that one precondition for having a meeting
(at least before Web conferencing) is being at the place where the meeting is held,
and therefore that maybe the meeting is a reason for the travel, and if so, then since
people like to arrive the day before a meeting, the agent should infer that the ﬂight
should be on the 11th.
These subtle characteristics of human conversations ( turns ,speech acts ,ground-
ing,dialogue structure ,initiative , and implicature ) are among the reasons it is dif-
ﬁcult to build dialogue systems that can carry on natural conversations with humans.
Many of these challenges are active areas of dialogue systems research.
15.2 Frame-Based Dialogue Systems
Atask-based dialogue system has the goal of helping a user solve a speciﬁc task
like making a travel reservation or buying a product. Task-based dialogue systems
are based around frames , ﬁrst introduced in the early inﬂuential GUS system for frame
GUS travel planning (Bobrow et al., 1977). Frames are knowledge structures representing
the details of the user’s task speciﬁcation. Each frame consists of a collection of
slots , each of which can take a set of possible values . Together a set of frames is slot

--- Page 315 ---
15.2 • F RAME -BASED DIALOGUE SYSTEMS 315
sometimes called a domain ontology .
Here we’ll describe the most well-studied frame-based architecture, the dialogue-
state architecture, made up of the six components shown in Fig. 15.3. In the next
sections we’ll introduce four of them, after introducing the idea of frames (deferring
the speech recognition and synthesis components to Chapter 16).
Figure 15.3 Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016).
15.2.1 Frames and Slot Filling
The frame and its slots in a task-based dialogue system specify what the system
needs to know to perform its task. A hotel reservation system needs dates and loca-
tions. An alarm clock system needs a time. The system’s goal is to ﬁll the slots in
the frame with the ﬁllers the user intends, and then perform the relevant action for
the user (answering a question, or booking a ﬂight).
Fig. 15.4 shows a sample frame for booking air travel, with some sample ques-
tions used for ﬁlling slots. In the simplest frame-based systems (including most com-
mercial assistants until quite recently), these questions are pre-written templates, but
in more sophisticated systems, questions are generated on-the-ﬂy. The slot ﬁllers are
often constrained to a particular semantic type, like type CITY (taking on values like
San Francisco , orHong Kong ) or DATE ,AIRLINE , or TIME .
Slot Type Example Question
ORIGIN CITY city “From what city are you leaving?”
DESTINATION CITY city “Where are you going?”
DEPARTURE TIME time “When would you like to leave?”
DEPARTURE DATE date “What day would you like to leave?”
ARRIV AL TIME time “When do you want to arrive?”
ARRIV AL DATE date “What day would you like to arrive?”
Figure 15.4 A frame in a frame-based dialogue system, showing the type of each slot and
a sample question used to ﬁll the slot.

--- Page 316 ---
316 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
Many domains require multiple frames. Besides frames for car or hotel reser-
vations, we might need other frames for things like general route information (for
questions like Which airlines ﬂy from Boston to San Francisco? ), That means the
system must be able to disambiguate which slot of which frame a given input is
supposed to ﬁll.
The task of slot-ﬁlling is usually combined with two other tasks, to extract 3
things from each user utterance. The ﬁrst is domain classiﬁcation : is this user for
example talking about airlines, programming an alarm clock, or dealing with their
calendar? The second is user intent determination : what general task or goal is theintent
determination
user trying to accomplish? For example the task could be to Find a Movie, or Show
a Flight, or Remove a Calendar Appointment. Together, the domain classiﬁcation
and intent determination tasks decide which frame we are ﬁlling. Finally, we need
to do slot ﬁlling itself: extract the particular slots and ﬁllers that the user intends the slot ﬁlling
system to understand from their utterance with respect to their intent. From a user
utterance like this one:
Show me morning flights from Boston to San Francisco on Tuesday
a system might want to build a representation like:
DOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS
ORIGIN-CITY: Boston DEST-CITY: San Francisco
ORIGIN-DATE: Tuesday ORIGIN-TIME: morning
Similarly an utterance like this: should give an intent like this:
Wake me tomorrow at 6 DOMAIN: ALARM-CLOCK
INTENT: SET-ALARM
TIME: 2017-07-01 0600
The simplest dialogue systems use handwritten rules for slot-ﬁlling, like this
regular expression for recognizing the SET-ALARM intent:
wake me (up) | set (the|an) alarm | get me up
But most systems use supervised machine-learning: each sentence in a training
set is annotated with slots, domain, and intent, and a sequence model maps from
input words to slot ﬁllers, domain and intent. For example we’ll have pairs of sen-
tences that are labeled for domain ( AIRLINE ) and intent ( SHOWFLIGHT ), and are also
labeled with BIO representations for the slots and ﬁllers. (Recall from Chapter 17
that in BIO tagging we introduce a tag for the beginning (B) and inside (I) of each
slot label, and one for tokens outside (O) any slot label.)
O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O AIRLINE-SHOWFLIGHT
I want to fly to San Francisco on Monday afternoon please EOS
Fig. 15.5 shows a typical architecture for inference. The input words w1:::wn
are passed through a pretrained language model encoder, followed by a feedforward
layer and a softmax at each token position over possible BIO tags, with the output
a series of BIO tags s1:::sn. We generally combine the domain-classiﬁcation and
intent-extraction tasks with slot-ﬁlling by adding a domain concatenated with an
intent as the desired output for the ﬁnal EOS token.
Once the sequence labeler has tagged the user utterance, a ﬁller string can be ex-
tracted for each slot from the tags (e.g., “San Francisco”), and these word strings
can then be normalized to the correct form in the ontology (perhaps the airport

--- Page 317 ---
15.3 • D IALOGUE ACTS AND DIALOGUE STATE 317
SanFranciscoonMondayEncodingsClassiﬁer+softmaxB-DESI-DESOB-DTIME
…d+i
<EOS>Encoder 
Figure 15.5 Slot ﬁlling by passing input words through an encoder, and then using a linear
or feedforward layer followed by a softmax to generate a series of BIO tags. Here we also
show a ﬁnal state: a domain concatenated with an intent.
code ‘SFO’), for example with dictionaries that specify that SF, SFO, and San Fran-
cisco are synonyms. Often in industrial contexts, combinations of rules and machine
learning are used for each of these components.
We can make a very simple frame-based dialogue system by wrapping a small
amount of code around this slot extractor. Mainly we just need to ask the user
questions until all the slots are full, do a database query, then report back to the user,
using hand-built templates for generating sentences.
15.2.2 Evaluating Task-Based Dialogue
We evaluate task-based systems by computing the task error rate , ortask success task error rate
rate: the percentage of times the system booked the right plane ﬂight, or put the
right event on the calendar. A more ﬁne-grained, but less extrinsic metric is the slot
error rate , the percentage of slots ﬁlled with the correct values: slot error rate
Slot Error Rate for a Sentence =# of inserted/deleted/subsituted slots
# of total reference slots for sentence(15.1)
For example a system that extracted the slot structure below from this sentence:
(15.2) Make an appointment with Chris at 10:30 in Gates 104
Slot Filler
PERSON Chris
TIME 11:30 a.m.
ROOM Gates 104
has a slot error rate of 1/3, since the TIME is wrong. Instead of error rate, slot
precision, recall, and F-score can also be used. We can also measure efﬁciency
costs like the length of the dialogue in seconds or turns. efﬁciency costs
15.3 Dialogue Acts and Dialogue State
While the naive slot-extractor system described above can handle simple dialogues,
often we want more complex interactions. For example, we might want to conﬁrm
that we’ve understand the user, or ask them to repeat themselves. We can build a
more sophisticated system using dialogue acts anddialogue state .

--- Page 318 ---
318 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
15.3.1 Dialogue Acts
Dialogue acts are a generalization of speech acts that also represent grounding. The dialogue acts
set of acts can be general, or can be designed for particular dialogue tasks.
Tag Sys User Description
HELLO (a=x;b=y;:::)X X Open a dialogue and give info a=x;b=y;:::
INFORM (a=x;b=y;:::)X X Give info a=x;b=y;:::
REQUEST (a;b=x;:::)X X Request value for a given b=x;:::
REQALTS (a=x;:::) cX Request alternative with a=x;:::
CONFIRM (a=x;b=y;:::)X X Explicitly conﬁrm a=x;b=y;:::
CONFREQ (a=x;:::;d)Xc Implicitly conﬁrm a=x;:::and request value of d
SELECT (a=x;a=y)Xc Implicitly conﬁrm a=x;:::and request value of d
AFFIRM (a=x;b=y;:::)X X Afﬁrm and give further info a=x;b=y;:::
NEGATE (a=x) cX Negate and give corrected value a=x
DENY (a=x) cX Deny that a=x
BYE() X X Close a dialogue
Figure 15.6 Dialogue acts used by the HIS restaurant recommendation system of Young
et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and
user inputs, respectively.
Figure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7
shows these tags labeling a sample dialogue from the HIS system (Young et al.,
2010). This example also shows the content of each dialogue act, which are the slot
ﬁllers being communicated. So the user might INFORM the system that they want
Italian food near a museum, or CONFIRM with the system that the price is reasonable.
Utterance Dialogue act
U: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)
S: You are looking for a restaurant. What
type of food do you like?confreq(type = restaurant, food)
U: I’d like an Italian near the museum. inform(food = Italian, near=museum)
S: Roma is a nice Italian restaurant near
the museum.inform(name = "Roma", type = restaurant,
food = Italian, near = museum)
U: Is it reasonably priced? confirm(pricerange = moderate)
S: Yes, Roma is in the moderate price
range.affirm(name = "Roma", pricerange =
moderate)
U: What is the phone number? request(phone)
S: The number of Roma is 385456. inform(name = "Roma", phone = "385456")
U: Ok, thank you goodbye. bye()
Figure 15.7 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 15.6.
15.3.2 Dialogue State Tracking
The job of the dialogue-state tracker is to determine the current state of the frame
(the ﬁllers of each slot), and the user’s most recent dialogue act. The dialogue-state
is not just the slot-ﬁllers in the current sentence; it includes the entire state of the
frame at this point, summarizing all of the user’s constraints. Fig. 15.8 from Mrk ˇsi´c
et al. (2017) shows the dialogue state after each turn.
Dialogue act detection is done just like domain or intent classiﬁcation, by passing
the input sentence through an encoder and adding an act classiﬁer. Often passing in
the prior dialogue act as well can improve classiﬁcation. And since dialogue acts

--- Page 319 ---
15.3 • D IALOGUE ACTS AND DIALOGUE STATE 319
User: I’m looking for a cheaper restaurant
inform(price=cheap)
System: Sure. What kind - and where?
User: Thai food, somewhere downtown
inform(price=cheap, food=Thai, area=centre)
System: The House serves cheap Thai food
User: Where is it?
inform(price=cheap, food=Thai, area=centre); request(address)
System: The House is at 106 Regent Street
Figure 15.8 The output of the dialogue state tracker after each turn (Mrk ˇsi´c et al., 2017).
place some constraints on the slots and values, the tasks of dialogue-act detection and
slot-ﬁlling are often performed jointly. The state tracker can just take the output of
a slot-ﬁlling sequence-model (Section 15.2.1) after each sentence, or do something
more complicated like training a classiﬁer to decide if a value has been changed.
A special case: detecting correction acts. If a dialogue system misrecognizes
or misunderstands an utterance, users will repeat or reformulate the utterance. De-
tecting these user correction acts is quite important, especially for spoken lan-user correction
acts
guage. Ironically, corrections are actually harder to recognize than normal sentences
(Swerts et al., 2000), because users who are frustrated adjust their speech in a way
that is difﬁcult for speech recognizers (Goldberg et al., 2003). For example speak-
ers often use a prosodic style for corrections called hyperarticulation , in which thehyperarticula-
tion
utterance is louder or longer or exaggerated in pitch, such as I said BAL-TI-MORE,
not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Detecting acts
can be part of the general dialogue act detection classiﬁer, or can make use of spe-
cial features beyond the words, like those shown below (Levow 1998, Litman et al.
1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015).
features examples
semantic embedding similarity between correction and user’s prior utterance
phonetic phonetic overlap between candidate correction act and user’s prior utterance
(i.e. “WhatsApp” may be incorrectly recognized as “What’s up”)
prosodic hyperarticulation, increases in F0 range, pause duration, and word duration
ASR ASR conﬁdence, language model probability
15.3.3 Dialogue Policy: Which act to generate
In early commercial frame-based systems, the dialogue policy is simple: ask ques-
tions until all the slots are full, do a database query, then report back to the user. A
more sophisticated dialogue policy can help a system decide when to answer the dialogue policy
user’s questions, when to instead ask the user a clariﬁcation question, and so on. A
dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act
to generate, along with its arguments, is sometimes called content planning .content
planning
Let’s see how to do this for some important dialogue acts. Dialogue systems, es-
pecially speech systems, often misrecognize the users’ words or meaning. To ensure
system and user share a common ground, systems must conﬁrm understandings with
the user or reject utterances that the system don’t understand. A system might use
anexplicit conﬁrmation act to conﬁrm with the user, like Is that correct? below:explicit
conﬁrmation

--- Page 320 ---
320 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
U:I’d like to ﬂy from Denver Colorado to New York City on September
twenty ﬁrst in the morning on United Airlines
S:Let’s see then. I have you going from Denver Colorado to New York
on September twenty ﬁrst. Is that correct?
When using an implicit conﬁrmation act, a system instead grounds more im-implicit
conﬁrmation
plicitly, for example by repeating the system’s understanding as part of asking the
next question, as Shanghai is conﬁrmed in passing in this example:
U:I want to travel to to Shanghai
S:When do you want to travel to Shanghai?
There’s a tradeoff. Explicit conﬁrmation makes it easier for users to correct mis-
recognitions by just answering “no” to the conﬁrmation question. But explicit con-
ﬁrmation is time-consuming and awkward (Danieli and Gerbino 1995, Walker et al.
1998a). We also might want an act that expresses lack of understanding: rejection , rejection
for example with a prompt like I’m sorry, I didn’t understand that . To decide among
these acts, we can make use of the fact that ASR systems often compute their conﬁ-
dence in their transcription (often based on the log-likelihood the system assigns the
sentence). A system can thus choose to explicitly conﬁrm only low-conﬁdence sen-
tences. Or systems might have a four-tiered level of conﬁdence with three thresholds
a,b, and g:
<alow conﬁdence reject
aabove the threshold conﬁrm explicitly
bhigh conﬁdence conﬁrm implictly
gvery high conﬁdence don’t conﬁrm at all
15.3.4 Natural language generation: Sentence Realization
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french)
1 Au Midi is in Midtown and serves French food.
2 There is a French restaurant in Midtown called Au Midi.
Figure 15.9 Sample inputs to the sentence realization phase of NLG, showing the dialogue
act and attributes prespeciﬁed by the content planner, and two distinct potential output sen-
tences to be generated. From the restaurant recommendation system of Nayak et al. (2017).
Once a dialogue act has been chosen, we need to generate the text of the re-
sponse to the user. This part of the generation process is called sentence realiza-
tion. Fig. 15.9 shows a sample input/output for the sentence realization phase. Thesentence
realization
content planner has chosen the dialogue act RECOMMEND and some slots (name,
neighborhood, cuisine) and ﬁllers. The sentence realizer generates a sentence like
lines 1 or 2 (by training on examples of representation/sentence pairs from a corpus
of labeled dialogues). Because we won’t see every restaurant or attribute in every
possible wording, we can delexicalize : generalize the training examples by replac- delexicalize
ing speciﬁc slot value words in the training set with a generic placeholder token
representing the slot. Fig. 15.10 shows the sentences in Fig. 15.9 delexicalized.
We can map from frames to delexicalized sentences with an encoder decoder
model (Mrk ˇsi´c et al. 2017, inter alia), trained on hand-labeled dialogue corpora like
MultiWOZ (Budzianowski et al., 2018). The input to the encoder is a sequence of

--- Page 321 ---
15.4 • C HATBOTS 321
recommend(restaurant name= Au Midi, neighborhood = midtown,
cuisine = french)
1restaurant name is inneighborhood and serves cuisine food.
2 There is a cuisine restaurant in neighborhood calledrestaurant name .
Figure 15.10 Delexicalized sentences that can be used for generating many different relex-
icalized sentences. From the restaurant recommendation system of Nayak et al. (2017).
decentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODER
Figure 15.11 An encoder decoder sentence realizer mapping slots/ﬁllers to English.
tokens xtthat represent the dialogue act (e.g., RECOMMEND ) and its arguments (e.g.,
service:decent, cuisine:null ) (Nayak et al., 2017), as in Fig. 15.11.
The decoder outputs the delexicalized English sentence “ name has decent ser-
vice”, which we can then relexicalize , i.e. ﬁll back in correct slot values, resulting relexicalize
in “Au Midi has decent service”.
15.4 Chatbots
Chatbots are systems that can carry on extended conversations with the goal of chatbot
mimicking the unstructured conversations or ‘chats’ characteristic of informal human-
human interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY
(Colby et al., 1971) had theoretical goals like testing theories of psychological coun-
seling, for most of the last 50 years chatbots have been designed for entertainment.
That changed with the recent rise of neural chatbots like ChatGPT, which incor-
porate solutions to NLP tasks like question answering, writing tools, or machine
translation into a conversational interface. A conversation with ChatGPT is shown
in Fig. 15.12. In this section we describe neural chatbot architectures and datasets.
[TBD]
Figure 15.12 A conversation with ChatGPT.
15.4.1 Training chatbots
Data Chatbots are generally trained on a training set that includes standard large
language model training data of the type discussed in Section 10.3.2: versions of the
web from the Common Crawl, including news sites, Wikipedia, as well as books.
For training chatbots, it is common to additionally add lots of dialogue data.
This can include datasets created speciﬁcally for training chatbots by hiring
speakers of the language to have conversations, such as by having them take on
personas or talk about knowledge provided to them. For example the Topical-Chat
dataset has 11K crowdsourced conversations spanning 8 broad topics (Gopalakrish-
nan et al., 2019), the E MPATHETIC DIALOGUES includes 25K crowdsourced con-

--- Page 322 ---
322 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
versations grounded in a speciﬁc situation where a speaker was feeling a speciﬁc
emotion (Rashkin et al., 2019), and the SaFeRDialogues dataset (Ung et al., 2022)
has 8k dialogues demonstrating graceful responses to conversational feedback about
safety failures.
Such datasets are far too small to train a language model alone, and so it’s com-
mon to also pretrain on large datasets of pseudo-conversations drawn from Twitter
(Ritter et al., 2010a), Reddit (Roller et al., 2021), Weibo ( 微博), and other social
media platforms. To turn social media data into data that has the structure of a con-
versation, we can treat any post on the platform as the ﬁrst turn in a conversation,
and the sequence of comments/replies as subsequent turns in that conversation.
Datasets from the web can be enormously toxic, so it’s crucial to ﬁlter the di-
alogues ﬁrst. This can be done by using the same toxicity classiﬁers we describe
below in the ﬁne-tuning section.
Architecture For training chatbots, it’s most common to use the standard causal
language model architecture, in which the model predicts each word given all the
prior words, and the loss is the standard language modeling loss. Fig. 15.13 shows a
standard training setup; no different than language model training in Chapter 9. The
only difference is the data, which has the addition of signiﬁcant conversation and
pseudo-conversation data as described in the prior section. As usual, the left context
can include the entire prior conversation (or as much as ﬁts in the context window).
TransformerBlocksLM head
gotpromoted!<s>gotpromoted!<s>Next wordCongratsLM Loss…LM headLM headLM headLM headLM head
ICongrats!…LM headLM head!-log y!-log yCongrats-log y<s>-log y!-log ypromoted-log ygot………………
Figure 15.13 Training a causal (decoder-only) language model for a chatbot.
An alternative is to use the encoder-decoder architecture of Chapter 13. In this
case the entire conversation up to the last turn (as much as ﬁts in the context) is
presented to the encoder, and the decoder generates the next turn.
promotedgot!<s>Congrats!ENCODERDECODERI
Figure 15.14 An alternative: an encoder-decoder language model for a chatbot.
In practice, dialogue systems require additional customization beyond just pre-
training on dialogue data. In the next few sections we’ll discuss various stages of

--- Page 323 ---
15.4 • C HATBOTS 323
ﬁne-tuning that can be used for this customization.
15.4.2 Fine Tuning for Quality and Safety
It is a common practice for dialogue systems to use further labeled data for ﬁne-
tuning. One function of this ﬁne-tuning step is to improve the quality of the dialogue,
training the system to produce responses that are sensible and interesting. Another
function might be to improve safety, keeping a dialogue system from suggesting
harmful actions (like ﬁnancial fraud, medical harm, inciting hatred, or abusing the
user or other people).
In the simplest method for improving quality and safety, speakers of the lan-
guage are given an initial prompt and instructions to have high-quality, safe dia-
logues. They then interact with an initial dialogue system and their responses are
used to ﬁnetune the model, usually as part of the instruct tuning step we introduced
in Chapter 12. Thus a dialogue system learns to answer questions, follow other
instructions, and also carry on high-quality, safe dialogues, in a single multi-task
learning format.
While ﬁne-tuning on positive examples is helpful, it is generally insufﬁcient and
so it is common to add more discriminative data that speciﬁcally downweights low-
quality or harmful responses. The simplest paradigm for this is to train a model to
predict turn-level safety and quality values, by training on human-labeled ratings.
Such ratings might be collected by ﬁrst having speakers of the language carry on
dialogues with a system, and then a second set of people act as labelers to label
every system turn for its quality and safety, resulting in a binary label for quality and
safety for each turn.
Once a dataset has been created with these labels, a language model can be used
in a classiﬁcation task to label the quality and safety of a turn. For example in the
Lamda system (Cohen et al., 2022), a single language model is used in two phases,
roughly corresponding to generative and discriminative tasks: ﬁrst generating a re-
sponse, and then generating a label. In the generative phase, the model is given the
prior turn and a special RESPONSE token and generates the blue response turn. (In
training, the training loss is given only for the blue response):
“What’s up? RESPONSE Not much.
In a second, discriminative phase, the model is ﬁne-tuned to see an attribute
(SENSIBLE, INTERESTING, UNSAFE) and then to predict a 0 or 1 value, again
with training losses given only for the blue value.
What’s up? RESPONSE Not much. SENSIBLE 1
What’s up? RESPONSE Not much. INTERESTING 0
What’s up? RESPONSE Not much. UNSAFE 0
To use the system in inference, the model ﬁrst generates a response given the context,
and then it is given the attribute and asked to generate a rating. The result is a
generated turn along with a label. This label isn’t shown to the user but can be use
for ﬁltering, either at training time or at deployment time. For example, the system
can generate multiple potential responses, ﬁlter out any response that is unsafe, and
return to the user the highest ranking response.

--- Page 324 ---
324 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
15.4.3 Learning to perform retrieval as part of responding
Modern chatbots are also trained to do retrieval (calls to Google or Bing or other
search engines) as a component of generating a response. This is generally done
by having a sort of fake dialogue participant called something like Search Query
whose utterances are passed to a search engine. The system can be encourged to do
searches either by prompting or by ﬁne-tuning.
For example in the Sparrow chatbot (Glaese et al., 2022), the prompt includes
a few sample conversations that include two special users, Search Query , and
Search Results :
...[this is preceded by other parts of the prompt]
User: What color is the sky?
Sparrow: Blue.
User: Why?
Search Query: why is sky blue
Search Results: Page title: Sky Except for direct sunlight, most of the light in
the daytime sky is caused by scattering, which is dominated by
a small-particle limit called Rayleigh scattering. The scattering
due to molecule-sized particles (as in air) is greater in the di-
rections both toward and away from the source of light than it
is in directions perpendicular to the incident path. Scattering is
signiﬁcant for light at all visible wavelengths, but is stronger at
the shorter (bluer) end of the visible spectrum, meaning that the
scattered light is bluer than its source: the Sun. The remain-
ing direct sunlight, having lost some of its shorter-wavelength
components, appears slightly less blue.
Sparrow: Because of the Rayleigh scattering. It causes short waves of
blue light to be scattered more than the other colours in the spec-
trum.
User: Who was Raleigh?
Search Query: who was Rayleigh
... ...
From these prompts, the system learns to generate texts with Search Query
turns for fact-based questions from the user, and these are passed to a search engine
to generate the Search Results turns.
Alternatively, systems can be ﬁnetuned to to know when to use a search en-
gine. For example, labelers can interact with a system, fact check each of the re-
sponses, and whenever the system emits an incorrect response, perform the web
search queries that the system should have used to check its answer, and then the in-
teration is recorded and used for ﬁne-tuning. Or labelers can look at a transcript of a
language model carrying on a dialogue, and similarly mark every place where a fact
was wrong (or out-of-date) and write the set of search queries that would have been
appropriate. A system is then ﬁne-tuned to generate search query turns which
are again passed to a search engine to generate the search responses . The set
of pages or snippets returned by the search engine in the search response turn are
then treated as the context for generation, similarly to the retrieval-based question-
answering methods of Chapter 14.

--- Page 325 ---
15.5 • D IALOGUE SYSTEM DESIGN 325
15.4.4 RLHF
A more sophisticated family of methods uses reinforcement learning to learn to
match human preferences for generated turns. In this method, RLHF for Rein- RLHF
forcement Learning from Human Feedback, we give a system a dialogue context
and sample two possible turns from the language model. We then have humans la-
bel which of the two is better, creating a large dataset of sentence pairs with human
preferences. These pairs are used to train a dialogue policy, and reinforcement learn-
ing is used to train the language model to generate turns that have higher rewards
(Christiano et al., 2017; Ouyang et al., 2022). While using RLHF is the current state
of the art at the time of this writing, a number of alternatives have been recently
developed that don’t require reinforcement learning (Rafailov et al., 2023, e.g.,) and
so this aspect of the ﬁeld is changing very quickly.
15.4.5 Evaluating Chatbots
Chatbots are evaluated by humans, who assign a score. This can be the human who
talked to the chatbot ( participant evaluation ) or a third party who reads a transcript
of a human/chatbot conversation ( observer evaluation ). In the participant evalua-
tion of See et al. (2019), the human evaluator chats with the model for six turns and
rates the chatbot on 8 dimensions capturing conversational quality: avoiding repe-
tition, interestingness, making sense, ﬂuency, listening, inquisitiveness, humanness
and engagingness on Likert scales like these:
Engagingness How much did you enjoy talking to this user?
Not at allA littleSomewhatA lot
Making sense How often did this user say something which did NOT make sense?
Never made any sense Most responses didn’t make sense Some re-
sponses didn’t make sense Everything made perfect sense
Observer evaluations use third party annotators to look at the text of a complete
conversation. Sometimes we’re interested in having raters assign a score to each
system turn; for example (Artstein et al., 2009) have raters mark how coherent each
turn is. Often, however, we just want a single high-level score to know if system A
is better than system B. The acute-eval metric (Li et al., 2019a) is such an observer acute-eval
evaluation in which annotators look at two separate human-computer conversations
and choose the system which performed better on four metrics: engagingness, inter-
estingness, humanness, and knowledgability.
15.5 Dialogue System Design
Because of the important role of the user, the ﬁeld of dialogue systems is closely
linked with Human-Computer Interaction (HCI). This is especially true for task-
oriented dialogue and assistants, where the design of dialogue strategies, sometimes
called voice user interface design, generally follows user-centered design princi-voice user
interface
ples (Gould and Lewis, 1985):
1. Study the user and task: Understand the users and the task by interviewing
users, investigating similar systems, and studying related human-human dialogues.
2. Build simulations and prototypes: A crucial tool in building dialogue systems
is the Wizard-of-Oz system . In wizard systems, the users interact with what theyWizard-of-Oz
system

--- Page 326 ---
326 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
think is a program but is in fact a human “wizard” disguised by a software interface
(Gould et al. 1983, Good et al. 1984, Fraser and
Gilbert 1991). The name comes from the chil-
dren’s book The Wizard of Oz (Baum, 1900),
in which the wizard turned out to be a simu-
lation controlled by a man behind a curtain or
screen. A wizard system can be used to test out
an architecture before implementation; only the
interface software and databases need to be in
place. The wizard gets input from the user, uses
a database interface to run queries based on the
user utterance, and then outputs sentences, ei-
ther by typing them or speaking them.
Wizard-of-Oz systems are not a perfect
simulation, since the wizard doesn’t exactly
simulate the errors or limitations of a real sys-
tem; but wizard studies can still provide a useful ﬁrst idea of the domain issues.
3. Iteratively test the design on users: An iterative design cycle with embedded
user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich
et al. 1995, Landauer 1995). For example in a well-known incident, an early dia-
logue system required the user to press a key to interrupt the system (Stifelman et al.,
1993). But user testing showed users barged in (interrupted, talking over the sys- barged in
tem), which led to a redesign of the system to recognize overlapped speech. It’s also
important to incorporate value sensitive design , in which we carefully consider dur-value sensitive
design
ing the design process the beneﬁts, harms and possible stakeholders of the resulting
system (Friedman et al. 2017, Friedman and Hendry 2019).
15.5.1 Ethical Issues in Dialogue System Design
Ethical issues have been key to how we think about designing artiﬁcial agents since
well before we had dialogue systems. Mary Shelley (depicted below) centered her
novel Frankenstein around the problem of creating artiﬁcial agents without consider-
ing
ethical and humanistic concerns. One issue is the
safety of users. If users seek information from di-
alogue systems in safety-critical situations like ask-
ing medical advice, or in emergency situations, or
when indicating the intentions of self-harm, incorrect
advice can be dangerous and even life-threatening.
For example (Bickmore et al., 2018) gave participants
medical problems to pose to three commercial di-
alogue systems (Siri, Alexa, Google Assistant) and
asked them to determine an action to take based on
the system responses; many of the proposed actions,
if actually taken, would have led to harm or death.
A system can also harm users by verbally attacking them, or creating represen-
tational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes
that demean particular groups of people. Both abuse and stereotypes can cause psy-
chological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken Tay
ofﬂine 16 hours after it went live, when it began posting messages with racial slurs,

--- Page 327 ---
15.6 • S UMMARY 327
conspiracy theories, and personal attacks on its users. Tay had learned these biases
and actions from its training data, including from users who seemed to be purposely
teaching the system to repeat this kind of language (Neff and Nagy 2016). Hender-
son et al. (2017) examined dialogue datasets used to train corpus-based chatbots and
found toxic and abusive language, especially in social media corpora like Twitter
and Reddit, and indeed such language then appears in the text generated by lan-
guage models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which
can even amplify the bias from the training data (Dinan et al., 2020). Liu et al.
(2020) developed another method for investigating bias, testing how neural dialogue
systems responded to pairs of simulated user turns that are identical except for men-
tioning different genders or race. They found, for example, that simple changes like
using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more
offensively and with more negative sentiment.
Another important ethical issue is privacy . Already in the ﬁrst days of ELIZA,
Weizenbaum pointed out the privacy implications of people’s revelations to the chat-
bot. The ubiquity of in-home dialogue systems means they may often overhear
private information (Henderson et al., 2017). If a chatbot is human-like, users are
also more likely to disclose private information, and less likely to worry about the
harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained
on transcripts of human-human or human-machine conversation must anonymize
personally identiﬁable information.
Finally, chatbots raise important issues of gender equality in addition to textual
bias. Current chatbots are overwhelmingly given female names, likely perpetuating
the stereotype of a subservient female servant (Paolino, 2017). And when users
use sexually harassing language, most commercial chatbots evade or give positive
responses rather than responding in clear negative ways (Fessler, 2017).
These ethical issues are an important area of investigation, including ﬁnding
ways to mitigate problems of abuse and toxicity, like detecting and responding ap-
propriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020).
Value sensitive design, carefully considering possible harms in advance (Friedman
et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give
a number of suggestions for best practices in dialogue system design. For exam-
ple getting informed consent from participants, whether they are used for training,
or whether they are interacting with a deployed system is important. Because di-
alogue systems by deﬁnition involve human participants, researchers also work on
these issues with the Institutional Review Boards ( IRB) at their institutions, who IRB
help protect the safety of experimental subjects.
15.6 Summary
Chatbots anddialogue systems are crucial speech and language processing appli-
cations that are already widely used commercially.
• In human dialogue, speaking is a kind of action; these acts are referred to
as speech acts or dialogue acts. Speakers also attempt to achieve common
ground by acknowledging that they have understand each other. Conversation
also is characterized by turn structure and dialogue structure.
• Chatbots are conversational systems designed to mimic the appearance of in-
formal human conversation. Rule-based chatbots like ELIZA and its modern

--- Page 328 ---
328 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
descendants use rules to map user sentences into system responses. Corpus-
based chatbots mine logs of human conversation to learn to automatically map
user sentences into system responses.
• For task-based dialogue, most commercial dialogue systems use the GUS or
frame-based architecture, in which the designer speciﬁes frames consisting of
slots that the system must ﬁll by asking the user.
• The dialogue-state architecture augments the GUS frame-and-slot architec-
ture with richer representations and more sophisticated algorithms for keeping
track of user’s dialogue acts, policies for generating its own dialogue acts, and
a natural language component.
• Dialogue systems are a kind of human-computer interaction, and general HCI
principles apply in their design, including the role of the user, simulations such
as Wizard-of-Oz systems, and the importance of iterative design and testing
on real users.
Bibliographical and Historical Notes
The linguistic, philosophical, and psychological literature on dialogue is quite ex-
tensive. For example the idea that utterances in a conversation are a kind of action
being performed by the speaker was due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962) and his student John Searle.
Various sets of speech acts have been deﬁned over the years, and a rich linguistic
and philosophical literature developed, especially focused on explaining the use of
indirect speech acts. The idea of dialogue acts draws also from a number of other
sources, including the ideas of adjacency pairs, pre-sequences, and other aspects of
the interactional properties of human conversation developed in the ﬁeld of conver-
sation analysis (see Levinson (1983) for an introduction to the ﬁeld). This idea thatconversation
analysis
acts set up strong local dialogue expectations was also preﬁgured by Firth (1935, p.
70), in a famous quotation:
Most of the give-and-take of conversation in our everyday life is stereotyped
and very narrowly conditioned by our particular type of culture. It is a sort
of roughly prescribed social ritual, in which you generally say what the other
fellow expects you, one way or the other, to say.
Another important research thread modeled dialogue as a kind of collaborative
behavior, including the ideas of common ground (Clark and Marshall, 1981), ref-
erence as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention
(Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).
The earliest conversational systems were simple pattern-action chatbots like ELIZA
(Weizenbaum, 1966). ELIZA had a widespread inﬂuence on popular perceptions of
artiﬁcial intelligence, and brought up some of the ﬁrst ethical questions in natural
language processing —such as the issues of privacy we discussed above as well the
role of algorithms in decision-making— leading its creator Joseph Weizenbaum to
ﬁght for social responsibility in AI and computer science in general.
Computational-implemented theories of dialogue blossomed in the 1970. That
period saw the very inﬂuential GUS system (Bobrow et al., 1977), which in the late
1970s established the frame-based paradigm that became the dominant industrial
paradigm for dialogue systems for over 30 years.

--- Page 329 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 329
Another inﬂuential line of research from that decade focused on modeling the hi-
erarchical structure of dialogue. Grosz’s pioneering 1977b dissertation ﬁrst showed
that “task-oriented dialogues have a structure that closely parallels the structure of
the task being performed” (p. 27), leading to her work with Sidner and others show-
ing how to use similar notions of intention and plans to model discourse structure
and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the
role of intentional structure in dialogue.
Yet a third line, ﬁrst suggested by Bruce (1975), suggested that since speech acts
are actions, they should be planned like other actions, and drew on the AI planning
literature (Fikes and Nilsson, 1971). A system seeking to ﬁnd out some information
can come up with the plan of asking the interlocutor for the information. A system
hearing an utterance can interpret a speech act by running the planner “in reverse”,
using inference rules to infer from what the interlocutor said what the plan might
have been. Plan-based models of dialogue are referred to as BDI models because BDI
such planners model the beliefs ,desires , and intentions (BDI) of the system and in-
terlocutor. BDI models of dialogue were ﬁrst introduced by Allen, Cohen, Perrault,
and their colleagues in a number of inﬂuential papers showing how speech acts could
be generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980,
Allen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based
models of understanding as part of the task of interpreting stories.
In the 1990s, machine learning models that had ﬁrst been applied to natural
language processing began to be applied to dialogue tasks like slot ﬁlling (Miller
et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the
linguistic properties of dialogue acts and on machine-learning-based methods for
their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and
Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke
et al. 2000, Gravano et al. 2012. This work strongly informed the development
of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking
quickly became an important problem for task-oriented dialogue, and there has been
an inﬂuential annual evaluation of state-tracking algorithms (Williams et al., 2016).
The turn of the century saw a line of work on applying reinforcement learning
to dialogue, which ﬁrst came out of AT&T and Bell Laboratories with work on
MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along
with work on cue phrases, prosody, and rejection and conﬁrmation. Reinforcement
learning research turned quickly to the more sophisticated POMDP models (Roy
et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-
ﬁlling dialogue tasks. Neural reinforcement learning models have been used both for
chatbot systems, for example simulating dialogues between two dialogue systems,
rewarding good conversational properties like coherence and ease of answering (Li
et al., 2016a), and for task-oriented dialogue (Williams et al., 2017).
By around 2010 the GUS architecture ﬁnally began to be widely used commer-
cially in dialogue systems on phones like Apple’s SIRI (Bellegarda, 2013) and other
digital assistants.
The rise of the web gave rise to corpus-based chatbot architectures around the
turn of the century, ﬁrst using information retrieval models and then in the 2010s,
after the rise of deep learning, with sequence-to-sequence models.
[TBD: Modern history of neural chatbots]
Other important dialogue areas include the study of affect in dialogue (Rashkin
et al. 2019, Lin et al. 2019) and conversational interface design (Cohen et al. 2004,
Harris 2005, Pearl 2017, Deibel and Evanhoe 2021).

--- Page 330 ---
330 CHAPTER 15 • C HATBOTS & D IALOGUE SYSTEMS
Exercises
15.1 Write a ﬁnite-state automaton for a dialogue manager for checking your bank
balance and withdrawing money at an automated teller machine.
15.2 Adispreferred response is a response that has the potential to make a persondispreferred
response
uncomfortable or embarrassed in the conversational context; the most com-
mon example dispreferred responses is turning down a request. People signal
their discomfort with having to say no with surface cues (like the word well),
or via signiﬁcant silence. Try to notice the next time you or someone else
utters a dispreferred response, and write down the utterance. What are some
other cues in the response that a system might use to detect a dispreferred
response? Consider non-verbal cues like eye gaze and body gestures.
15.3 When asked a question to which they aren’t sure they know the answer, peo-
ple display their lack of conﬁdence by cues that resemble other dispreferred
responses. Try to notice some unsure answers to questions. What are some
of the cues? If you have trouble doing this, read Smith and Clark (1993) and
listen speciﬁcally for the cues they mention.
15.4 Implement a small air-travel help system based on text input. Your system
should get constraints from users about a particular ﬂight that they want to
take, expressed in natural language, and display possible ﬂights on a screen.
Make simplifying assumptions. You may build in a simple ﬂight database or
you may use a ﬂight information system on the Web as your backend.

--- Page 331 ---
CHAPTER
16Automatic Speech Recognition
and Text-to-Speech
I KNOW not whether
I see your meaning: if I do, it lies
Upon the wordy wavelets of your voice,
Dim as an evening shadow in a brook,
Thomas Lovell Beddoes, 1851
Understanding spoken language, or at least transcribing the words into writing, is
one of the earliest goals of computer language processing. In fact, speech processing
predates the computer by many decades!
The ﬁrst machine that recognized speech
was a toy from the 1920s. “Radio Rex”,
shown to the right, was a celluloid dog
that moved (by means of a spring) when
the spring was released by 500 Hz acous-
tic energy. Since 500 Hz is roughly the
ﬁrst formant of the vowel [eh] in “Rex”,
Rex seemed to come when he was called
(David, Jr. and Selfridge, 1962).
In modern times, we expect more of our automatic systems. The task of auto-
matic speech recognition (ASR ) is to map any waveform like this: ASR
to the appropriate string of words:
It's time for lunch!
Automatic transcription of speech by any speaker in any environment is still far from
solved, but ASR technology has matured to the point where it is now viable for many
practical tasks. Speech is a natural interface for communicating with smart home ap-
pliances, personal assistants, or cellphones, where keyboards are less convenient, in
telephony applications like call-routing (“Accounting, please”) or in sophisticated
dialogue applications (“I’d like to change the return date of my ﬂight”). ASR is also
useful for general transcription, for example for automatically generating captions
for audio or video text (transcribing movies or videos or live discussions). Transcrip-
tion is important in ﬁelds like law where dictation plays an important role. Finally,
ASR is important as part of augmentative communication (interaction between com-
puters and humans with some disability resulting in difﬁculties or inabilities in typ-
ing or audition). The blind Milton famously dictated Paradise Lost to his daughters,
and Henry James dictated his later novels after a repetitive stress injury.
What about the opposite problem, going from text to speech? This is a problem
with an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for

--- Page 332 ---
332 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton
consisting of a wooden box ﬁlled with gears, behind which sat a robot mannequin
who played chess by moving pieces with his mechanical arm. The Turk toured Eu-
rope and the Americas for decades, defeating Napoleon Bonaparte and even playing
Charles Babbage. The Mechanical Turk might have been one of the early successes
of artiﬁcial intelligence were it not for the fact that it was, alas, a hoax, powered by
a human chess player hidden inside the box.
What is less well known is that von Kempelen, an extraordinarily
proliﬁc inventor, also built between
1769 and 1790 what was deﬁnitely
not a hoax: the ﬁrst full-sentence
speech synthesizer, shown partially to
the right. His device consisted of a
bellows to simulate the lungs, a rub-
ber mouthpiece and a nose aperture, a
reed to simulate the vocal folds, var-
ious whistles for the fricatives, and a
small auxiliary bellows to provide the puff of air for plosives. By moving levers
with both hands to open and close apertures, and adjusting the ﬂexible leather “vo-
cal tract”, an operator could produce different consonants and vowels.
More than two centuries later, we no longer build our synthesizers out of wood
and leather, nor do we need human operators. The modern task of speech synthesis ,speech
synthesis
also called text-to-speech orTTS , is exactly the reverse of ASR; to map text: text-to-speech
TTSIt's time for lunch!
to an acoustic waveform:
Modern speech synthesis has a wide variety of applications. TTS is used in
conversational agents that conduct dialogues with people, plays a role in devices
that read out loud for the blind or in games, and can be used to speak for sufferers
of neurological disorders, such as the late astrophysicist Steven Hawking who, after
he lost the use of his voice because of ALS, spoke by manipulating a TTS system.
In the next sections we’ll show how to do ASR with encoder-decoders, intro-
duce the CTC loss functions, the standard word error rate evaluation metric, and
describe how acoustic features are extracted. We’ll then see how TTS can be mod-
eled with almost the same algorithm in reverse, and conclude with a brief mention
of other speech tasks.
16.1 The Automatic Speech Recognition Task
Before describing algorithms for ASR, let’s talk about how the task itself varies.
One dimension of variation is vocabulary size. Some ASR tasks can be solved with
extremely high accuracy, like those with a 2-word vocabulary ( yesversus no) or
an 11 word vocabulary like digit recognition (recognizing sequences of digits in-digit
recognition
cluding zero tonine plus oh). Open-ended tasks like transcribing videos or human
conversations, with large vocabularies of up to 60,000 words, are much harder.

--- Page 333 ---
16.1 • T HEAUTOMATIC SPEECH RECOGNITION TASK 333
A second dimension of variation is who the speaker is talking to. Humans speak-
ing to machines (either dictating or talking to a dialogue system) are easier to recog-
nize than humans speaking to humans. Read speech , in which humans are reading read speech
out loud, for example in audio books, is also relatively easy to recognize. Recog-
nizing the speech of two humans talking to each other in conversational speech ,conversational
speech
for example, for transcribing a business meeting, is the hardest. It seems that when
humans talk to machines, or read without an audience present, they simplify their
speech quite a bit, talking more slowly and more clearly.
A third dimension of variation is channel and noise. Speech is easier to recognize
if it’s recorded in a quiet room with head-mounted microphones than if it’s recorded
by a distant microphone on a noisy city street, or in a car with the window open.
A ﬁnal dimension of variation is accent or speaker-class characteristics. Speech
is easier to recognize if the speaker is speaking the same dialect or variety that the
system was trained on. Speech by speakers of regional or ethnic dialects, or speech
by children can be quite difﬁcult to recognize if the system is only trained on speak-
ers of standard dialects, or only adult speakers.
A number of publicly available corpora with human-created transcripts are used
to create ASR test and training sets to explore this variation; we mention a few of
them here since you will encounter them in the literature. LibriSpeech is a large LibriSpeech
open-source read-speech 16 kHz dataset with over 1000 hours of audio books from
the LibriV ox project, with transcripts aligned at the sentence level (Panayotov et al.,
2015). It is divided into an easier (“clean”) and a more difﬁcult portion (“other”)
with the clean portion of higher recording quality and with accents closer to US
English. This was done by running a speech recognizer (trained on read speech from
the Wall Street Journal) on all the audio, computing the WER for each speaker based
on the gold transcripts, and dividing the speakers roughly in half, with recordings
from lower-WER speakers called “clean” and recordings from higher-WER speakers
“other”.
TheSwitchboard corpus of prompted telephone conversations between strangers Switchboard
was collected in the early 1990s; it contains 2430 conversations averaging 6 min-
utes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey
et al., 1992). Switchboard has the singular advantage of an enormous amount of
auxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic
and prosodic labeling, and discourse and information structure. The CALLHOME CALLHOME
corpus was collected in the late 1990s and consists of 120 unscripted 30-minute
telephone conversations between native speakers of English who were usually close
friends or family (Canavan et al., 1997).
The Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is
a large corpus of naturally occurring everyday spoken interactions from all over the
United States, mostly face-to-face conversation, but also town-hall meetings, food
preparation, on-the-job talk, and classroom lectures. The corpus was anonymized by
removing personal names and other identifying information (replaced by pseudonyms
in the transcripts, and masked in the audio).
CORAAL is a collection of over 150 sociolinguistic interviews with African CORAAL
American speakers, with the goal of studying African American Language ( AAL ),
the many variations of language used in African American communities (Kendall
and Farrington, 2020). The interviews are anonymized with transcripts aligned at
the utterance level. The CHiME Challenge is a series of difﬁcult shared tasks with CHiME
corpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of
conversational speech in real home environments (speciﬁcally dinner parties). The

--- Page 334 ---
334 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
corpus contains recordings of twenty different dinner parties in real homes, each
with four participants, and in three locations (kitchen, dining area, living room),
recorded both with distant room microphones and with body-worn mikes. The
HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con- HKUST
versations between speakers of Mandarin across China, including transcripts of the
conversations, which are between either friends or strangers (Liu et al., 2006). The
AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken AISHELL-1
from various domains, read by different speakers mainly from northern China (Bu
et al., 2017).
Figure 16.1 shows the rough percentage of incorrect words (the word error rate ,
or WER, deﬁned on page 346) from state-of-the-art systems on some of these tasks.
Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is
around 2%; this is a solved task, although these numbers come from systems that re-
quire enormous computational resources. By contrast, the error rate for transcribing
conversations between humans is much higher; 5.8 to 11% for the Switchboard and
CALLHOME corpora. The error rate is higher yet again for speakers of varieties
like African American Vernacular English, and yet again for difﬁcult conversational
tasks like transcription of 4-speaker dinner party speech, which can have error rates
as high as 81.3%. Character error rates (CER) are also much lower for read Man-
darin speech than for natural conversation.
English Tasks WER %
LibriSpeech audiobooks 960hour clean 1.4
LibriSpeech audiobooks 960hour other 2.6
Switchboard telephone conversations between strangers 5.8
CALLHOME telephone conversations between family 11.0
Sociolinguistic interviews, CORAAL (AAL) 27.0
CHiMe5 dinner parties with body-worn microphones 47.9
CHiMe5 dinner parties with distant microphones 81.3
Chinese (Mandarin) Tasks CER %
AISHELL-1 Mandarin read speech corpus 6.7
HKUST Mandarin Chinese telephone conversations 23.5
Figure 16.1 Rough Word Error Rates (WER = % of words misrecognized) reported around
2020 for ASR on various American English recognition tasks, and character error rates (CER)
for two Chinese recognition tasks.
16.2 Feature Extraction for ASR: Log Mel Spectrum
The ﬁrst step in ASR is to transform the input waveform into a sequence of acoustic
feature vectors , each vector representing the information in a small time window feature vector
of the signal. Let’s see how to convert a raw waveﬁle to the most commonly used
features, sequences of log mel spectrum vectors. A speech signal processing course
is recommended for more details.
16.2.1 Sampling and Quantization
The input to a speech recognizer is a complex series of changes in air pressure.
These changes in air pressure obviously originate with the speaker and are caused

--- Page 335 ---
16.2 • F EATURE EXTRACTION FOR ASR: L OGMELSPECTRUM 335
by the speciﬁc way that air passes through the glottis and out the oral or nasal cav-
ities. We represent sound waves by plotting the change in air pressure over time.
One metaphor which sometimes helps in understanding these graphs is that of a ver-
tical plate blocking the air pressure waves (perhaps in a microphone in front of a
speaker’s mouth, or the eardrum in a hearer’s ear). The graph measures the amount
ofcompression orrarefaction (uncompression) of the air molecules at this plate.
Figure 16.2 shows a short segment of a waveform taken from the Switchboard corpus
of telephone speech of the vowel [iy] from someone saying “she just had a baby”.
Time (s)0 0.03875–0.016970.02283
0
Figure 16.2 A waveform of an instance of the vowel [iy] (the last vowel in the word “baby”). The y-axis
shows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time. Notice
that the wave repeats regularly.
The ﬁrst step in digitizing a sound wave like Fig. 16.2 is to convert the analog
representations (ﬁrst air pressure and then analog electric signals in a microphone)
into a digital signal. This analog-to-digital conversion has two steps: sampling and sampling
quantization . To sample a signal, we measure its amplitude at a particular time; the
sampling rate is the number of samples taken per second. To accurately measure a
wave, we must have at least two samples in each cycle: one measuring the positive
part of the wave and one measuring the negative part. More than two samples per
cycle increases the amplitude accuracy, but fewer than two samples causes the fre-
quency of the wave to be completely missed. Thus, the maximum frequency wave
that can be measured is one whose frequency is half the sample rate (since every
cycle needs two samples). This maximum frequency for a given sampling rate is
called the Nyquist frequency . Most information in human speech is in frequenciesNyquist
frequency
below 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-
plete accuracy. But telephone speech is ﬁltered by the switching network, and only
frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz
sampling rate is sufﬁcient for telephone-bandwidth speech like the Switchboard
corpus, while 16,000 Hz sampling is often used for microphone speech.
Although using higher sampling rates produces higher ASR accuracy, we can’t
combine different sampling rates for training and testing ASR systems. Thus if
we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must
downsample our training corpus to 8 KHz. Similarly, if we are training on mul-
tiple corpora and one of them includes telephone speech, we downsample all the
wideband corpora to 8Khz.
Amplitude measurements are stored as integers, either 8 bit (values from -128–
127) or 16 bit (values from -32768–32767). This process of representing real-valued
numbers as integers is called quantization ; all values that are closer together than quantization
the minimum granularity (the quantum size) are represented identically. We refer to
each sample at time index nin the digitized, quantized waveform as x[n].
Once data is quantized, it is stored in various formats. One parameter of these
formats is the sample rate and sample size discussed above; telephone speech is
often sampled at 8 kHz and stored as 8-bit samples, and microphone data is often
sampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of

--- Page 336 ---
336 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
channels . For stereo data or for two-party conversations, we can store both channels channel
in the same ﬁle or we can store them in separate ﬁles. A ﬁnal parameter is individual
sample storage—linearly or compressed. One common compression format used for
telephone speech is m-law (often written u-law but still pronounced mu-law). The
intuition of log compression algorithms like m-law is that human hearing is more
sensitive at small intensities than large ones; the log represents small values with
more faithfulness at the expense of more error on large values. The linear (unlogged)
values are generally referred to as linear PCM values (PCM stands for pulse code PCM
modulation, but never mind that). Here’s the equation for compressing a linear PCM
sample value xto 8-bit m-law, (where m=255 for 8 bits):
F(x) =sgn(x)log(1+mjxj)
log(1+m) 1x1 (16.1)
There are a number of standard ﬁle formats for storing the resulting digitized wave-
ﬁle, such as Microsoft’s .wav and Apple’s AIFF all of which have special headers;
simple headerless “raw” ﬁles are also used. For example, the .wav format is a sub-
set of Microsoft’s RIFF format for multimedia ﬁles; RIFF is a general format that
can represent a series of nested chunks of data and control information. Figure 16.3
shows a simple .wav ﬁle with a single data chunk together with its format chunk.
Figure 16.3 Microsoft waveﬁle header format, assuming simple ﬁle with one chunk. Fol-
lowing this 44-byte header would be the data chunk.
16.2.2 Windowing
From the digitized, quantized representation of the waveform, we need to extract
spectral features from a small window of speech that characterizes part of a par-
ticular phoneme. Inside this small window, we can roughly think of the signal as
stationary (that is, its statistical properties are constant within this region). (By stationary
contrast, in general, speech is a non-stationary signal, meaning that its statistical non-stationary
properties are not constant over time). We extract this roughly stationary portion of
speech by using a window which is non-zero inside a region and zero elsewhere, run-
ning this window across the speech signal and multiplying it by the input waveform
to produce a windowed waveform.
The speech extracted from each window is called a frame . The windowing is frame
characterized by three parameters: the window size orframe size of the window
(its width in milliseconds), the frame stride , (also called shift oroffset ) between stride
successive windows, and the shape of the window.
To extract the signal we multiply the value of the signal at time n,s[n]by the
value of the window at time n,w[n]:
y[n] =w[n]s[n] (16.2)
The window shape sketched in Fig. 16.4 is rectangular ; you can see the ex- rectangular
tracted windowed signal looks just like the original signal. The rectangular window,

--- Page 337 ---
16.2 • F EATURE EXTRACTION FOR ASR: L OGMELSPECTRUM 337
Shift10 msWindow25 msShift10 msWindow25 msWindow25 ms
Figure 16.4 Windowing, showing a 25 ms rectangular window with a 10ms stride.
however, abruptly cuts off the signal at its boundaries, which creates problems when
we do Fourier analysis. For this reason, for acoustic feature creation we more com-
monly use the Hamming window, which shrinks the values of the signal toward Hamming
zero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both;
the equations are as follows (assuming a window that is Lframes long):
rectangular w [n] =1 0nL 1
0 otherwise(16.3)
Hamming w [n] =
0:54 0:46cos (2pn
L)0nL 1
0 otherwise(16.4)
Time (s)00.0475896–0.50.49990Rectangular windowHamming window
Time (s)0.004559380.0256563–0.48260.49990
Time (s)0.004559380.0256563–0.50.49990
Figure 16.5 Windowing a sine wave with the rectangular or Hamming windows.
16.2.3 Discrete Fourier Transform
The next step is to extract spectral information for our windowed signal; we need to
know how much energy the signal contains at different frequency bands. The tool

--- Page 338 ---
338 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
for extracting spectral information for discrete frequency bands for a discrete-time
(sampled) signal is the discrete Fourier transform orDFT .Discrete
Fourier
transformDFT The input to the DFT is a windowed signal x[n]:::x[m], and the output, for each
ofNdiscrete frequency bands, is a complex number X[k]representing the magni-
tude and phase of that frequency component in the original signal. If we plot the
magnitude against the frequency, we can visualize the spectrum (see Appendix H
for more on spectra). For example, Fig. 16.6 shows a 25 ms Hamming-windowed
portion of a signal and its spectrum as computed by a DFT (with some additional
smoothing).
Time (s)0.0141752 0.039295–0.041210.04414
0
Frequency (Hz)0 8000Sound pressure level (dB /Hz)
–20020
(a) (b)
Figure 16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]
and (b) its spectrum computed by a DFT.
We do not introduce the mathematical details of the DFT here, except to note
that Fourier analysis relies on Euler’s formula , with jas the imaginary unit: Euler’s formula
ejq=cosq+jsinq (16.5)
As a brief reminder for those students who have already studied signal processing,
the DFT is deﬁned as follows:
X[k] =N 1X
n=0x[n]e j2p
Nkn(16.6)
A commonly used algorithm for computing the DFT is the fast Fourier transformfast Fourier
transform
orFFT . This implementation of the DFT is very efﬁcient but only works for values FFT
ofNthat are powers of 2.
16.2.4 Mel Filter Bank and Log
The results of the FFT tell us the energy at each frequency band. Human hearing,
however, is not equally sensitive at all frequency bands; it is less sensitive at higher
frequencies. This bias toward low frequencies helps human recognition, since in-
formation in low frequencies (like formants) is crucial for distinguishing vowels or
nasals, while information in high frequencies (like stop bursts or fricative noise) is
less crucial for successful recognition. Modeling this human perceptual property
improves speech recognition performance in the same way.
We implement this intuition by collecting energies, not equally at each frequency
band, but according to the melscale, an auditory frequency scale. A mel(Stevens mel
et al. 1937, Stevens and V olkmann 1940) is a unit of pitch. Pairs of sounds that are
perceptually equidistant in pitch are separated by an equal number of mels. The mel

--- Page 339 ---
16.3 • S PEECH RECOGNITION ARCHITECTURE 339
frequency mcan be computed from the raw acoustic frequency by a log transforma-
tion:
mel(f) =1127ln (1+f
700) (16.7)
We implement this intuition by creating a bank of ﬁlters that collect energy from
each frequency band, spread logarithmically so that we have very ﬁne resolution
at low frequencies, and less resolution at high frequencies. Figure 16.7 shows a
sample bank of triangular ﬁlters that implement this idea, that can be multiplied by
the spectrum to get a mel spectrum.
m1m2mM...mel spectrum0770000.51AmplitudeFrequency (Hz)8K
Figure 16.7 The mel ﬁlter bank (Davis and Mermelstein, 1980). Each triangular ﬁlter,
spaced logarithmically along the mel scale, collects energy from a given frequency range.
Finally, we take the log of each of the mel spectrum values. The human response
to signal level is logarithmic (like the human response to frequency). Humans are
less sensitive to slight differences in amplitude at high amplitudes than at low ampli-
tudes. In addition, using a log makes the feature estimates less sensitive to variations
in input such as power variations due to the speaker’s mouth moving closer or further
from the microphone.
16.3 Speech Recognition Architecture
The basic architecture for ASR is the encoder-decoder (implemented with either
RNNs or Transformers), exactly the same architecture introduced for MT in Chap-
ter 13. Generally we start from the log mel spectral features described in the previous
section, and map to letters, although it’s also possible to map to induced morpheme-
like chunks like wordpieces or BPE.
Fig. 16.8 sketches the standard encoder-decoder architecture, which is com-
monly referred to as the attention-based encoder decoder orAED , orlisten attend AED
and spell (LAS ) after the two papers which ﬁrst applied it to speech (Chorowskilisten attend
and spell
et al. 2014, Chan et al. 2016). The input is a sequence of tacoustic feature vectors
F=f1;f2;:::;ft, one vector per 10 ms frame. The output can be letters or word-
pieces; we’ll assume letters here. Thus the output sequence Y= (hSOSi;y1;:::;ymhEOSi),
assuming special start of sequence and end of sequence tokens hsosiandheosiand
each yiis a character; for English we might choose the set:
yi2fa;b;c;:::;z;0;:::;9;hspacei;hcommai;hperiodi;hapostrophei;hunkig
Of course the encoder-decoder architecture is particularly appropriate when in-
put and output sequences have stark length differences, as they do for speech, with
very long acoustic feature sequences mapping to much shorter sequences of letters

--- Page 340 ---
340 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
ENCODER…DECODER……ym
Feature ComputationSubsampling…Hftf180-dimensional log Mel spectrumper frameShorter sequence Xy1<s>iy2ity3t‘y4‘sy5s y6 ty7tiy8imy9mex1xn
Figure 16.8 Schematic architecture for an encoder-decoder speech recognizer.
or words. A single word might be 5 letters long but, supposing it lasts about 2
seconds, would take 200 acoustic frames (of 10ms each).
Because this length difference is so extreme for speech, encoder-decoder ar-
chitectures for speech need to have a special compression stage that shortens the
acoustic feature sequence before the encoder stage. (Alternatively, we can use a loss
function that is designed to deal well with compression, like the CTC loss function
we’ll introduce in the next section.)
The goal of the subsampling is to produce a shorter sequence X=x1;:::;xnthat
will be the input to the encoder. The simplest algorithm is a method sometimes
called low frame rate (Pundak and Sainath, 2016): for time iwe stack (concatenate) low frame rate
the acoustic feature vector fiwith the prior two vectors fi 1andfi 2to make a new
vector three times longer. Then we simply delete fi 1and fi 2. Thus instead of
(say) a 40-dimensional acoustic feature vector every 10 ms, we have a longer vector
(say 120-dimensional) every 30 ms, with a shorter sequence length n=t
3.1
After this compression stage, encoder-decoders for speech use the same archi-
tecture as for MT or other text, composed of either RNNs (LSTMs) or Transformers.
For inference, the probability of the output string Yis decomposed as:
p(y1;:::; yn) =nY
i=1p(yijy1;:::; yi 1;X) (16.8)
We can produce each letter of the output via greedy decoding:
ˆyi=argmax char2Alphabet P(charjy1:::yi 1;X) (16.9)
Alternatively we can use beam search as described in the next section. This is par-
ticularly relevant when we are adding a language model.
Adding a language model Since an encoder-decoder model is essentially a con-
ditional language model, encoder-decoders implicitly learn a language model for the
output domain of letters from their training data. However, the training data (speech
paired with text transcriptions) may not include sufﬁcient text to train a good lan-
guage model. After all, it’s easier to ﬁnd enormous amounts of pure text training
1There are also more complex alternatives for subsampling, like using a convolutional net that down-
samples with max pooling, or layers of pyramidal RNNs , RNNs where each successive layer has half
the number of RNNs as the previous layer.

--- Page 341 ---
16.4 • CTC 341
data than it is to ﬁnd text paired with speech. Thus we can can usually improve a
model at least slightly by incorporating a very large language model.
The simplest way to do this is to use beam search to get a ﬁnal beam of hy-
pothesized sentences; this beam is sometimes called an n-best list . We then use a n-best list
language model to rescore each hypothesis on the beam. The scoring is done by in- rescore
terpolating the score assigned by the language model with the encoder-decoder score
used to create the beam, with a weight ltuned on a held-out set. Also, since most
models prefer shorter sentences, ASR systems normally have some way of adding a
length factor. One way to do this is to normalize the probability by the number of
characters in the hypothesis jYjc. The following is thus a typical scoring function
(Chan et al., 2016):
score(YjX) =1
jYjclogP(YjX)+llogPLM(Y) (16.10)
16.3.1 Learning
Encoder-decoders for speech are trained with the normal cross-entropy loss gener-
ally used for conditional language models. At timestep iof decoding, the loss is the
log probability of the correct token (letter) yi:
LCE= logp(yijy1;:::; yi 1;X) (16.11)
The loss for the entire sentence is the sum of these losses:
LCE= mX
i=1logp(yijy1;:::; yi 1;X) (16.12)
This loss is then backpropagated through the entire end-to-end model to train the
entire encoder-decoder.
As we described in Chapter 13, we normally use teacher forcing, in which the
decoder history is forced to be the correct gold yirather than the predicted ˆ yi. It’s
also possible to use a mixture of the gold and decoder output, for example using
the gold output 90% of the time, but with probability .1 taking the decoder output
instead:
LCE= logp(yijy1;:::; ˆyi 1;X) (16.13)
16.4 CTC
We pointed out in the previous section that speech recognition has two particular
properties that make it very appropriate for the encoder-decoder architecture, where
the encoder produces an encoding of the input that the decoder uses attention to
explore. First, in speech we have a very long acoustic input sequence Xmapping to
a much shorter sequence of letters Y, and second, it’s hard to know exactly which
part of Xmaps to which part of Y.
In this section we brieﬂy introduce an alternative to encoder-decoder: an algo-
rithm and loss function called CTC , short for Connectionist Temporal Classiﬁca- CTC
tion(Graves et al., 2006), that deals with these problems in a very different way. The
intuition of CTC is to output a single character for every frame of the input, so that

--- Page 342 ---
342 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
the output is the same length as the input, and then to apply a collapsing function
that combines sequences of identical letters, resulting in a shorter sequence.
Let’s imagine inference on someone saying the word dinner , and let’s suppose
we had a function that chooses the most probable letter for each input spectral frame
representation xi. We’ll call the sequence of letters corresponding to each input
frame an alignment , because it tells us where in the acoustic signal each letter aligns alignment
to. Fig. 16.9 shows one such alignment, and what happens if we use a collapsing
function that just removes consecutive duplicate letters.
X (input)A (alignment)Y (output)dx1ix2ix3nx4nx5nx6nx7ex8rx9rx10rx11rx12rx13rx14dinerwavefile
Figure 16.9 A naive algorithm for collapsing an alignment between input and letters.
Well, that doesn’t work; our naive algorithm has transcribed the speech as diner ,
notdinner ! Collapsing doesn’t handle double letters. There’s also another problem
with our naive function; it doesn’t tell us what symbol to align with silence in the
input. We don’t want to be transcribing silence as random letters!
The CTC algorithm solves both problems by adding to the transcription alphabet
a special symbol for a blank , which we’ll represent as . The blank can be used in blank
the alignment whenever we don’t want to transcribe a letter. Blank can also be used
between letters; since our collapsing function collapses only consecutive duplicate
letters, it won’t collapse across . More formally, let’s deﬁne the mapping B:a!y
between an alignment aand an output y, which collapses all repeated letters and
then removes all blanks. Fig. 16.10 sketches this collapsing function B.
X (input)A (alignment)remove blanksdx1ix2x3nx4nx5x6nx7ex8rx9rx10rx11rx12x13x14dinernmerge duplicatesdinernY (output)dinern␣␣␣␣␣␣␣
Figure 16.10 The CTC collapsing function B, showing the space blank character ; re-
peated (consecutive) characters in an alignment Aare removed to form the output Y.
The CTC collapsing function is many-to-one; lots of different alignments map
to the same output string. For example, the alignment shown in Fig. 16.10 is not
the only alignment that results in the string dinner . Fig. 16.11 shows some other
alignments that would produce the same output.
It’s useful to think of the set of all alignments that might produce the same output
Y. We’ll use the inverse of our Bfunction, called B 1, and represent that set as
B 1(Y).

--- Page 343 ---
16.4 • CTC 343
dinnneeerrr␣␣ddinnnerr␣␣␣dddinnnerr␣i␣␣␣␣␣
Figure 16.11 Three other legitimate alignments producing the transcript dinner .
16.4.1 CTC Inference
Before we see how to compute PCTC(YjX)let’s ﬁrst see how CTC assigns a proba-
bility to one particular alignment ˆA=fˆa1;:::; ˆang. CTC makes a strong conditional
independence assumption: it assumes that, given the input X, the CTC model output
atat time tis independent of the output labels at any other time ai. Thus:
PCTC(AjX) =TY
t=1p(atjX) (16.14)
Thus to ﬁnd the best alignment ˆA=fˆa1;:::; ˆaTgwe can greedily choose the charac-
ter with the max probability at each time step t:
ˆat=argmax
c2Cpt(cjX) (16.15)
We then pass the resulting sequence Ato the CTC collapsing function Bto get the
output sequence Y.
Let’s talk about how this simple inference algorithm for ﬁnding the best align-
ment A would be implemented. Because we are making a decision at each time
point, we can treat CTC as a sequence-modeling task, where we output one letter
ˆytat time tcorresponding to each input token xt, eliminating the need for a full de-
coder. Fig. 16.12 sketches this architecture, where we take an encoder, produce a
hidden state htat each timestep, and decode by taking a softmax over the character
vocabulary at each time step.
ENCODER…yn
Feature ComputationSubsampling…ftf1 log Mel spectrumShorter inputsequence Xy1iy2iy3iy4tx1xnClassiﬁer+softmax…ty5……output lettersequence Y
Figure 16.12 Inference with CTC: using an encoder-only model, with decoding done by
simple softmaxes over the hidden state htat each output step.
Alas, there is a potential ﬂaw with the inference algorithm sketched in (Eq. 16.15)
and Fig. 16.11. The problem is that we chose the most likely alignment A, but the

--- Page 344 ---
344 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
most likely alignment may not correspond to the most likely ﬁnal collapsed output
string Y. That’s because there are many possible alignments that lead to the same
output string, and hence the most likely output string might not correspond to the
most probable alignment. For example, imagine the most probable alignment Afor
an input X= [x1x2x3]is the string [a b ] but the next two most probable alignments
are [bb] and [b b]. The output Y=[b b], summing over those two alignments,
might be more probable than Y=[a b].
For this reason, the most probable output sequence Yis the one that has, not
the single best CTC alignment, but the highest sum over the probability of all its
possible alignments:
PCTC(YjX) =X
A2B 1(Y)P(AjX)
=X
A2B 1(Y)TY
t=1p(atjht)
ˆY=argmax
YPCTC(YjX) (16.16)
Alas, summing over all alignments is very expensive (there are a lot of alignments),
so we approximate this sum by using a version of Viterbi beam search that cleverly
keeps in the beam the high-probability alignments that map to the same output string,
and sums those as an approximation of (Eq. 16.16). See Hannun (2017) for a clear
explanation of this extension of beam search for CTC.
Because of the strong conditional independence assumption mentioned earlier
(that the output at time tis independent of the output at time t 1, given the input),
CTC does not implicitly learn a language model over the data (unlike the attention-
based encoder-decoder architectures). It is therefore essential when using CTC to
interpolate a language model (and some sort of length factor L(Y)) using interpola-
tion weights that are trained on a devset:
score CTC(YjX) =logPCTC(YjX)+l1logPLM(Y)l2L(Y) (16.17)
16.4.2 CTC Training
To train a CTC-based ASR system, we use negative log-likelihood loss with a special
CTC loss function. Thus the loss for an entire dataset Dis the sum of the negative
log-likelihoods of the correct output Yfor each input X:
LCTC=X
(X;Y)2D logPCTC(YjX) (16.18)
To compute CTC loss function for a single input pair (X;Y), we need the probability
of the output Ygiven the input X. As we saw in Eq. 16.16, to compute the probability
of a given output Ywe need to sum over all the possible alignments that would
collapse to Y. In other words:
PCTC(YjX) =X
A2B 1(Y)TY
t=1p(atjht) (16.19)
Naively summing over all possible alignments is not feasible (there are too many
alignments). However, we can efﬁciently compute the sum by using dynamic pro-

--- Page 345 ---
16.4 • CTC 345
gramming to merge alignments, with a version of the forward-backward algo-
rithm also used to train HMMs (Appendix A) and CRFs. The original dynamic pro-
gramming algorithms for both training and inference are laid out in (Graves et al.,
2006); see (Hannun, 2017) for a detailed explanation of both.
16.4.3 Combining CTC and Encoder-Decoder
It’s also possible to combine the two architectures/loss functions we’ve described,
the cross-entropy loss from the encoder-decoder architecture, and the CTC loss.
Fig. 16.13 shows a sketch. For training, we can simply weight the two losses with a
ltuned on a devset:
L= llogPencdec(YjX) (1 l)logPctc(YjX) (16.20)
For inference, we can combine the two with the language model (or the length
penalty), again with learned weights:
ˆY=argmax
Y[llogPencdec(YjX) (1 l)logPCTC(YjX)+glogPLM(Y)](16.21)
ENCODER…DECODER…H<s>it‘s timx1xn……i   t   ’   s      t   i   m   e  …CTC LossEncoder-Decoder Loss
Figure 16.13 Combining the CTC and encoder-decoder loss functions.
16.4.4 Streaming Models: RNN-T for improving CTC
Because of the strong independence assumption in CTC (assuming that the output
at time tis independent of the output at time t 1), recognizers based on CTC
don’t achieve as high an accuracy as the attention-based encoder-decoder recog-
nizers. CTC recognizers have the advantage, however, that they can be used for
streaming . Streaming means recognizing words on-line rather than waiting until streaming
the end of the sentence to recognize them. Streaming is crucial for many applica-
tions, from commands to dictation, where we want to start recognition while the
user is still talking. Algorithms that use attention need to compute the hidden state
sequence over the entire input ﬁrst in order to provide the attention distribution con-
text, before the decoder can start decoding. By contrast, a CTC algorithm can input
letters from left to right immediately.
If we want to do streaming, we need a way to improve CTC recognition to re-
move the conditional independent assumption, enabling it to know about output his-
tory. The RNN-Transducer ( RNN-T ), shown in Fig. 16.14, is just such a model RNN-T
(Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC
acoustic model, and a separate language model component called the predictor that
conditions on the output token history. At each time step t, the CTC encoder outputs

--- Page 346 ---
346 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
a hidden state henc
tgiven the input x1:::xt. The language model predictor takes as in-
put the previous output token (not counting blanks), outputting a hidden state hpred
u.
The two are passed through another network whose output is then passed through a
softmax to predict the next character.
PRNN T(YjX) =X
A2B 1(Y)P(AjX)
=X
A2B 1(Y)TY
t=1p(atjht;y<ut)
ENCODERP ( yt,u | x[1..t] , y[1..u-1] )
xtPREDICTIONNETWORKyu-1JOINT NETWORKhencthpreduSOFTMAXzt,uDECODER
Figure 16.14 The RNN-T model computing the output token distribution at time tby inte-
grating the output of a CTC acoustic encoder and a separate ‘predictor’ language model.
16.5 ASR Evaluation: Word Error Rate
The standard evaluation metric for speech recognition systems is the word error word error
rate. The word error rate is based on how much the word string returned by the
recognizer (the hypothesized word string) differs from a reference transcription.
The ﬁrst step in computing word error is to compute the minimum edit distance in
words between the hypothesized and correct strings, giving us the minimum num-
ber of word substitutions , word insertions , and word deletions necessary to map
between the correct and hypothesized strings. The word error rate (WER) is then
deﬁned as follows (note that because the equation includes insertions, the error rate
can be greater than 100%):
Word Error Rate =100Insertions +Substitutions +Deletions
Total Words in Correct Transcript
Here is a sample alignment between a reference and a hypothesis utterance from alignment
the CallHome corpus, showing the counts used to compute the error rate:
REF: i *** ** UM the PHONE IS i LEFT THE portable **** PHONE UPSTAIRS last night
HYP: i GOT IT TO the ***** FULLEST i LOVE TO portable FORM OF STORES last night
Eval: I I S D S S S I S S
This utterance has six substitutions, three insertions, and one deletion:
Word Error Rate =1006+3+1
13=76:9%

--- Page 347 ---
16.5 • ASR E VALUATION : W ORD ERROR RATE 347
The standard method for computing word error rates is a free script called sclite ,
available from the National Institute of Standards and Technologies (NIST) (NIST,
2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sen-
tences and a matching set of hypothesis sentences. Besides performing alignments,
and computing word error rate, sclite performs a number of other useful tasks. For
example, for error analysis it gives useful information such as confusion matrices
showing which words are often misrecognized for others, and summarizes statistics
of words that are often inserted or deleted. sclite also gives error rates by speaker
(if sentences are labeled for speaker ID), as well as useful statistics like the sentence
error rate , the percentage of sentences with at least one word error.Sentence error
rate
Statistical signiﬁcance for ASR: MAPSSWE or MacNemar
As with other language processing algorithms, we need to know whether a particular
improvement in word error rate is signiﬁcant or not.
The standard statistical tests for determining if two word error rates are different
is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in
Gillick and Cox (1989).
The MAPSSWE test is a parametric test that looks at the difference between
the number of word errors the two systems produce, averaged across a number of
segments. The segments may be quite short or as long as an entire utterance; in
general, we want to have the largest number of (short) segments in order to justify
the normality assumption and to maximize power. The test requires that the errors
in one segment be statistically independent of the errors in another segment. Since
ASR systems tend to use trigram LMs, we can approximate this requirement by
deﬁning a segment as a region bounded on both sides by words that both recognizers
get correct (or by turn/utterance boundaries). Here’s an example from NIST (2007)
with four regions:
I II III IV
REF: |it was|the best|of|times it|was the worst|of times| |it was
| | | | | | | |
SYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was
| | | | | | | |
SYS B:|it was|the best| |times it|WON the TEST |of times| |it was
In region I, system A has two errors (a deletion and an insertion) and system B
has zero; in region III, system A has one error (a substitution) and system B has two.
Let’s deﬁne a sequence of variables Zrepresenting the difference between the errors
in the two systems as follows:
Ni
Athe number of errors made on segment iby system A
Ni
B the number of errors made on segment iby system B
Z Ni
A Ni
B;i=1;2;;nwhere nis the number of segments
In the example above, the sequence of Zvalues isf2; 1; 1;1g. Intuitively, if
the two systems are identical, we would expect the average difference, that is, the
average of the Zvalues, to be zero. If we call the true average of the differences
muz, we would thus like to know whether muz=0. Following closely the original
proposal and notation of Gillick and Cox (1989), we can estimate the true average
from our limited sample as ˆmz=Pn
i=1Zi=n. The estimate of the variance of the Zi’s
is
s2
z=1
n 1nX
i=1(Zi mz)2(16.22)

--- Page 348 ---
348 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
Let
W=ˆmz
sz=pn(16.23)
For a large enough n(>50),Wwill approximately have a normal distribution with
unit variance. The null hypothesis is H0:mz=0, and it can thus be rejected if
2P(Zjwj)0:05 (two-tailed) or P(Zjwj)0:05 (one-tailed), where Zis
standard normal and wis the realized value W; these probabilities can be looked up
in the standard tables of the normal distribution.
Earlier work sometimes used McNemar’s test for signiﬁcance, but McNemar’s McNemar’s test
is only applicable when the errors made by the system are independent, which is not
true in continuous speech recognition, where errors made on a word are extremely
dependent on errors made on neighboring words.
Could we improve on word error rate as a metric? It would be nice, for exam-
ple, to have something that didn’t give equal weight to every word, perhaps valuing
content words like Tuesday more than function words like aorof. While researchers
generally agree that this would be a good idea, it has proved difﬁcult to agree on
a metric that works in every application of ASR. For dialogue systems, however,
where the desired semantic output is more clear, a metric called slot error rate or
concept error rate has proved extremely useful; it is discussed in Chapter 15 on page
317.
16.6 TTS
The goal of text-to-speech (TTS) systems is to map from strings of letters to wave-
forms, a technology that’s important for a variety of applications from dialogue sys-
tems to games to education.
Like ASR systems, TTS systems are generally based on the encoder-decoder
architecture, either using LSTMs or Transformers. There is a general difference in
training. The default condition for ASR systems is to be speaker-independent: they
are trained on large corpora with thousands of hours of speech from many speakers
because they must generalize well to an unseen test speaker. By contrast, in TTS, it’s
less crucial to use multiple voices, and so basic TTS systems are speaker-dependent:
trained to have a consistent voice, on much less data, but all from one speaker. For
example, one commonly used public domain dataset, the LJ speech corpus, consists
of 24 hours of one speaker, Linda Johnson, reading audio books in the LibriV ox
project (Ito and Johnson, 2017), much smaller than standard ASR corpora which are
hundreds or thousands of hours.2
We generally break up the TTS task into two components. The ﬁrst component
is an encoder-decoder model for spectrogram prediction : it maps from strings of
letters to mel spectrographs: sequences of mel spectral values over time. Thus we
2There is also recent TTS research on the task of multi-speaker TTS, in which a system is trained on
speech from many speakers, and can switch between different voices.

--- Page 349 ---
16.6 • TTS 349
might map from this string:
It's time for lunch!
to the following mel spectrogram:
The second component maps from mel spectrograms to waveforms. Generating
waveforms from intermediate representations like spectrograms is called vocoding vocoding
and this second component is called a vocoder : vocoder
These standard encoder-decoder algorithms for TTS are still quite computation-
ally intensive, so a signiﬁcant focus of modern research is on ways to speed them
up.
16.6.1 TTS Preprocessing: Text normalization
Before either of these two steps, however, TTS systems require text normaliza-
tion preprocessing for handling non-standard words : numbers, monetary amounts,non-standard
words
dates, and other concepts that are verbalized differently than they are spelled. A TTS
system seeing a number like 151needs to know to verbalize it as one hundred ﬁfty
oneif it occurs as $151 but as one ﬁfty one if it occurs in the context 151 Chapulte-
pec Ave. . The number 1750 can be spoken in at least four different ways, depending
on the context:
seventeen fifty: (in“The European economy in 1750” )
one seven five zero: (in“The password is 1750” )
seventeen hundred and fifty: (in“1750 dollars” )
one thousand, seven hundred, and fifty: (in“1750 dollars” )
Often the verbalization of a non-standard word depends on its meaning (what
Taylor (2009) calls its semiotic class ). Fig. 16.15 lays out some English non-
standard word types.
Many classes have preferred realizations. A year is generally read as paired
digits (e.g., seventeen fifty for 1750). $3.2 billion must be read out with the
worddollars at the end, as three point two billion dollars . Some ab-
breviations like N.Y. are expanded (to New York ), while other acronyms like GPU
are pronounced as letter sequences. In languages with grammatical gender, normal-
ization may depend on morphological properties. In French, the phrase 1 mangue
(‘one mangue’) is normalized to une mangue , but 1 ananas (‘one pineapple’) is
normalized to un ananas . In German, Heinrich IV (‘Henry IV’) can be normalized
toHeinrich der Vierte ,Heinrich des Vierten ,Heinrich dem Vierten , or
HeinrichdenVierten depending on the grammatical case of the noun (Demberg,
2006).

--- Page 350 ---
350 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
semiotic class examples verbalization
abbreviations gov’t , N.Y., mph government
acronyms read as letters GPU , D.C., PC, UN, IBM G P U
cardinal numbers 12, 45, 1/2, 0.6 twelve
ordinal numbers May 7, 3rd, Bill Gates III seventh
numbers read as digits Room 101 one oh one
times 3.20, 11:45 eleven forty ﬁve
dates 28/02 (or in US, 2/28) February twenty eighth
years 1999 , 80s, 1900s, 2045 nineteen ninety nine
money $3.45 ,e250, $200K three dollars forty ﬁve
money in tr/m/billions $3.45 billion three point four ﬁve billion dollars
percentage 75% 3.4% seventy ﬁve percent
Figure 16.15 Some types of non-standard words in text normalization; see Sproat et al.
(2001) and (van Esch and Sproat, 2018) for many more.
Modern end-to-end TTS systems can learn to do some normalization themselves,
but TTS systems are only trained on a limited amount of data (like the 220,000 words
we mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate
normalization step is important.
Normalization can be done by rule or by an encoder-decoder model. Rule-based
normalization is done in two stages: tokenization and verbalization. In the tokeniza-
tion stage we hand-write rules to detect non-standard words. These can be regular
expressions, like the following for detecting years:
/(1[89][0-9][0-9])j(20[0-9][0-9]/
A second pass of rules express how to verbalize each semiotic class. Larger TTS
systems instead use more complex rule-systems, like the Kestral system of (Ebden
and Sproat, 2015), which ﬁrst classiﬁes and parses each input into a normal form
and then produces text using a verbalization grammar. Rules have the advantage
that they don’t require training data, and they can be designed for high precision, but
can be brittle, and require expert rule-writers so are hard to maintain.
The alternative model is to use encoder-decoder models, which have been shown
to work better than rules for such transduction tasks, but do require expert-labeled
training sets in which non-standard words have been replaced with the appropriate
verbalization; such training sets for some languages are available (Sproat and Gor-
man 2018, Zhang et al. 2019).
In the simplest encoder-decoder setting, we simply treat the problem like ma-
chine translation, training a system to map from:
They live at 224 Mission St.
to
They live at two twenty four Mission Street
While encoder-decoder algorithms are highly accurate, they occasionally pro-
duce errors that are egregious; for example normalizing 45 minutes asforty ﬁve mil-
limeters . To address this, more complex systems use mechanisms like lightweight
covering grammars , which enumerate a large set of possible verbalizations but
don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang
et al., 2019).
16.6.2 TTS: Spectrogram prediction
The exact same architecture we described for ASR—the encoder-decoder with attention–
can be used for the ﬁrst component of TTS. Here we’ll give a simpliﬁed overview

--- Page 351 ---
16.6 • TTS 351
of the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron Tacotron2
(Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., Wavenet
2016). Fig. 16.16 sketches out the entire architecture.
The encoder’s job is to take a sequence of letters and produce a hidden repre-
sentation representing the letter sequence, which is then used by the attention mech-
anism in the decoder. The Tacotron2 encoder ﬁrst maps every input grapheme to
a 512-dimensional character embedding. These are then passed through a stack
of 3 convolutional layers, each containing 512 ﬁlters with shape 5 1, i.e. each
ﬁlter spanning 5 characters, to model the larger letter context. The output of the
ﬁnal convolutional layer is passed through a biLSTM to produce the ﬁnal encoding.
It’s common to use a slightly higher quality (but slower) version of attention called
location-based attention , in which the computation of the avalues (Eq. 8.36 inlocation-based
attention
Chapter 8) makes use of the avalues from the prior time-state.
In the decoder, the predicted mel spectrum from the prior time slot is passed
through a small pre-net as a bottleneck. This prior output is then concatenated with
the encoder’s attention vector context and passed through 2 LSTM layers. The out-
put of this LSTM is used in two ways. First, it is passed through a linear layer, and
some output processing, to autoregressively predict one 80-dimensional log-mel ﬁl-
terbank vector frame (50 ms, with a 12.5 ms stride) at each step. Second, it is passed
through another linear layer to a sigmoid to make a “stop token prediction” decision
about whether to stop producing output.
While linear spectrograms discard phase information (and aretherefore lossy), algorithms such as Grifﬁn-Lim [14] are capable ofestimating this discarded information, which enables time-domainconversion via the inverse short-time Fourier transform. Mel spectro-grams discard even more information, presenting a challenging in-verse problem. However, in comparison to the linguistic and acousticfeatures used in WaveNet, the mel spectrogram is a simpler, lower-level acoustic representation of audio signals. It should thereforebe straightforward for a similar WaveNet model conditioned on melspectrograms to generate audio, essentially as a neural vocoder. In-deed, we will show that it is possible to generate high quality audiofrom mel spectrograms using a modiﬁed WaveNet architecture.2.2. Spectrogram Prediction NetworkAs in Tacotron, mel spectrograms are computed through a short-time Fourier transform (STFT) using a 50 ms frame size, 12.5 msframe hop, and a Hann window function. We experimented with a5 ms frame hop to match the frequency of the conditioning inputsin the original WaveNet, but the corresponding increase in temporalresolution resulted in signiﬁcantly more pronunciation issues.We transform the STFT magnitude to the mel scale using an 80channel mel ﬁlterbank spanning 125 Hz to 7.6 kHz, followed by logdynamic range compression. Prior to log compression, the ﬁlterbankoutput magnitudes are clipped to a minimum value of 0.01 in orderto limit dynamic range in the logarithmic domain.The network is composed of an encoder and a decoder with atten-tion. The encoder converts a character sequence into a hidden featurerepresentation which the decoder consumes to predict a spectrogram.Input characters are represented using a learned 512-dimensionalcharacter embedding, which are passed through a stack of 3 convolu-tional layers each containing 512 ﬁlters with shape5⇥1, i.e., whereeach ﬁlter spans 5 characters, followed by batch normalization [18]and ReLU activations. As in Tacotron, these convolutional layersmodel longer-term context (e.g.,N-grams) in the input charactersequence. The output of the ﬁnal convolutional layer is passed into asingle bi-directional [19] LSTM [20] layer containing 512 units (256in each direction) to generate the encoded features.The encoder output is consumed by an attention network whichsummarizes the full encoded sequence as a ﬁxed-length context vectorfor each decoder output step. We use the location-sensitive attentionfrom [21], which extends the additive attention mechanism [22] touse cumulative attention weights from previous decoder time stepsas an additional feature. This encourages the model to move forwardconsistently through the input, mitigating potential failure modeswhere some subsequences are repeated or ignored by the decoder.Attention probabilities are computed after projecting inputs and lo-cation features to 128-dimensional hidden representations. Locationfeatures are computed using 32 1-D convolution ﬁlters of length 31.The decoder is an autoregressive recurrent neural network whichpredicts a mel spectrogram from the encoded input sequence oneframe at a time. The prediction from the previous time step is ﬁrstpassed through a smallpre-netcontaining 2 fully connected layersof 256 hidden ReLU units. We found that the pre-net acting as aninformation bottleneck was essential for learning attention. The pre-net output and attention context vector are concatenated and passedthrough a stack of 2 uni-directional LSTM layers with 1024 units.The concatenation of the LSTM output and the attention contextvector is projected through a linear transform to predict the targetspectrogram frame. Finally, the predicted mel spectrogram is passedthrough a 5-layer convolutionalpost-netwhich predicts a residualto add to the prediction to improve the overall reconstruction. Each
'LEVEGXIV)QFIHHMRK0SGEXMSR7IRWMXMZI%XXIRXMSR'SRZ0E]IVW&MHMVIGXMSREP0781-RTYX8I\X0E]IV4VI2IX07810E]IVW0MRIEV4VSNIGXMSR0MRIEV4VSNIGXMSR7XST8SOIR'SRZ0E]IV4SWX2IX
0HO6SHFWURJUDP;EZI2IX1S0;EZIJSVQ7EQTPIWFig. 1. Block diagram of the Tacotron 2 system architecture.post-net layer is comprised of 512 ﬁlters with shape5⇥1with batchnormalization, followed bytanhactivations on all but the ﬁnal layer.We minimize the summed mean squared error (MSE) from beforeand after the post-net to aid convergence. We also experimentedwith a log-likelihood loss by modeling the output distribution witha Mixture Density Network [23,24] to avoid assuming a constantvariance over time, but found that these were more difﬁcult to trainand they did not lead to better sounding samples.In parallel to spectrogram frame prediction, the concatenation ofdecoder LSTM output and the attention context is projected downto a scalar and passed through a sigmoid activation to predict theprobability that the output sequence has completed. This “stop token”prediction is used during inference to allow the model to dynamicallydetermine when to terminate generation instead of always generatingfor a ﬁxed duration. Speciﬁcally, generation completes at the ﬁrstframe for which this probability exceeds a threshold of 0.5.The convolutional layers in the network are regularized usingdropout [25] with probability 0.5, and LSTM layers are regularizedusing zoneout [26] with probability 0.1. In order to introduce outputvariation at inference time, dropout with probability 0.5 is appliedonly to layers in the pre-net of the autoregressive decoder.In contrast to the original Tacotron, our model uses simpler build-ing blocks, using vanilla LSTM and convolutional layers in the en-coder and decoder instead of “CBHG” stacks and GRU recurrentlayers. We do not use a “reduction factor”, i.e., each decoder stepcorresponds to a single spectrogram frame.2.3. WaveNet VocoderWe use a modiﬁed version of the WaveNet architecture from [8] toinvert the mel spectrogram feature representation into time-domainwaveform samples. As in the original architecture, there are 30dilated convolution layers, grouped into 3 dilation cycles, i.e., thedilation rate of layer k (k=0...29) is2k(mod 10). To work withthe 12.5 ms frame hop of the spectrogram frames, only 2 upsamplinglayers are used in the conditioning stack instead of 3 layers.Instead of predicting discretized buckets with a softmax layer,we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10-component mixture of logistic distributions (MoL) to generate 16-bitsamples at 24 kHz. To compute the logistic mixture distribution, theWaveNet stack output is passed through a ReLU activation followedEncoderDecoderVocoder
Figure 16.16 The Tacotron2 architecture: An encoder-decoder maps from graphemes to
mel spectrograms, followed by a vocoder that maps to waveﬁles. Figure modiﬁed from Shen
et al. (2018).
The system is trained on gold log-mel ﬁlterbank features, using teacher forcing,
that is the decoder is fed the correct log-model spectral feature at each decoder step
instead of the predicted decoder output from the prior step.
16.6.3 TTS: Vocoding
The vocoder for Tacotron 2 is an adaptation of the WaveNet vocoder (van den Oord WaveNet
et al., 2016). Here we’ll give a somewhat simpliﬁed description of vocoding using
WaveNet.
Recall that the goal of the vocoding process here will be to invert a log mel spec-
trum representations back into a time-domain waveform representation. WaveNet is
an autoregressive network, like the language models we introduced in Chapter 8. It

--- Page 352 ---
352 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
takes spectrograms as input and produces audio output represented as sequences of
8-bit mu-law (page 336). The probability of a waveform , a sequence of 8-bit mu-
law values Y=y1;:::;yt, given an intermediate input mel spectrogram his computed
as:
p(Y) =tY
t=1P(ytjy1;:::;yt 1;h1;:::;ht) (16.24)
This probability distribution is modeled by a stack of special convolution layers,
which include a speciﬁc convolutional structure called dilated convolutions , and a
speciﬁc non-linearity function.
A dilated convolution is a subtype of causal convolutional layer. Causal or
masked convolutions look only at the past input, rather than the future; the pre-
diction of yt+1can only depend on y1;:::;yt, useful for autoregressive left-to-right
processing. In dilated convolutions , at each successive layer we apply the convolu-dilated
convolutions
tional ﬁlter over a span longer than its length by skipping input values. Thus at time
twith a dilation value of 1, a convolutional ﬁlter of length 2 would see input values
xtandxt 1. But a ﬁlter with a distillation value of 2 would skip an input, so would
see input values xtandxt 1. Fig. 16.17 shows the computation of the output at time
twith 4 dilated convolution layers with dilation values, 1, 2, 4, and 8.
Because models with causal convolutions do not have recurrent connections, they are typically faster
to train than RNNs, especially when applied to very long sequences. One of the problems of causal
convolutions is that they require many layers, or large ﬁlters to increase the receptive ﬁeld. For
example, in Fig. 2 the receptive ﬁeld is only 5 (= #layers + ﬁlter length - 1). In this paper we use
dilated convolutions to increase the receptive ﬁeld by orders of magnitude, without greatly increasing
computational cost.
A dilated convolution (also called `a trous , or convolution with holes) is a convolution where the
ﬁlter is applied over an area larger than its length by skipping input values with a certain step. It is
equivalent to a convolution with a larger ﬁlter derived from the original ﬁlter by dilating it with zeros,
but is signiﬁcantly more efﬁcient. A dilated convolution effectively allows the network to operate on
a coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but
here the output has the same size as the input. As a special case, dilated convolution with dilation
1yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1,2,4,
and8. Dilated convolutions have previously been used in various contexts, e.g. signal processing
(Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &
Koltun, 2016).
InputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8
Figure 3: Visualization of a stack of dilated causal convolutional layers.
Stacked dilated convolutions enable networks to have very large receptive ﬁelds with just a few lay-
ers, while preserving the input resolution throughout the network as well as computational efﬁciency.
In this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g.
1,2,4,..., 512,1,2,4,..., 512,1,2,4,..., 512.
The intuition behind this conﬁguration is two-fold. First, exponentially increasing the dilation factor
results in exponential receptive ﬁeld growth with depth (Yu & Koltun, 2016). For example each
1,2,4,..., 512block has receptive ﬁeld of size 1024 , and can be seen as a more efﬁcient and dis-
criminative (non-linear) counterpart of a 1⇥1024 convolution. Second, stacking these blocks further
increases the model capacity and the receptive ﬁeld size.
2.2 S OFTMAX DISTRIBUTIONS
One approach to modeling the conditional distributions p(xt|x1,...,x t 1)over the individual
audio samples would be to use a mixture model such as a mixture density network (Bishop, 1994)
or mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,
van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the
data is implicitly continuous (as is the case for image pixel intensities or audio sample values). One
of the reasons is that a categorical distribution is more ﬂexible and can more easily model arbitrary
distributions because it makes no assumptions about their shape.
Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a
softmax layer would need to output 65,536 probabilities per timestep to model all possible values.
To make this more tractable, we ﬁrst apply a µ-law companding transformation (ITU-T, 1988) to
the data, and then quantize it to 256 possible values:
f(xt) = sign( xt)ln (1 + µ|xt|)
ln (1 + µ),
3
Figure 16.17 Dilated convolutions, showing one dilation cycle size of 4, i.e., dilation values
of 1, 2, 4, 8. Figure from van den Oord et al. (2016).
The Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a
dilation cycle size of 6, meaning that the ﬁrst 6 layers have dilations of 1, 2, 4, 8, 16,
and 32. and the next 6 layers again have dilations of 1, 2, 4, 8, 16, and 32. Dilated
convolutions allow the vocoder to grow the receptive ﬁeld exponentially with depth.
WaveNet predicts mu-law audio samples. Recall from page 336 that this is a
standard compression for audio in which the values at each sampling timestep are
compressed into 8-bits. This means that we can predict the value of each sample
with a simple 256-way categorical classiﬁer. The output of the dilated convolutions
is thus passed through a softmax which makes this 256-way decision.
The spectrogram prediction encoder-decoder and the WaveNet vocoder are trained
separately. After the spectrogram predictor is trained, the spectrogram prediction
network is run in teacher-forcing mode, with each predicted spectral frame condi-
tioned on the encoded text input and the previous frame from the ground truth spec-
trogram. This sequence of ground truth-aligned spectral features and gold audio
output is then used to train the vocoder.
This has been only a high-level sketch of the TTS process. There are numer-
ous important details that the reader interested in going further with TTS may want

--- Page 353 ---
16.7 • O THER SPEECH TASKS 353
to look into. For example WaveNet uses a special kind of a gated activation func-
tion as its non-linearity, and contains residual and skip connections. In practice,
predicting 8-bit audio values doesn’t as work as well as 16-bit, for which a simple
softmax is insufﬁcient, so decoders use fancier ways as the last step of predicting
audio sample values, like mixtures of distributions. Finally, the WaveNet vocoder
as we have described it would be so slow as to be useless; many different kinds of
efﬁciency improvements are necessary in practice, for example by ﬁnding ways to
do non-autoregressive generation, avoiding the latency of having to wait to generate
each frame until the prior frame has been generated, and instead making predictions
in parallel. We encourage the interested reader to consult the original papers and
various version of the code.
16.6.4 TTS Evaluation
Speech synthesis systems are evaluated by human listeners. (The development of a
good automatic metric for synthesis evaluation, one that would eliminate the need
for expensive and time-consuming human listening experiments, remains an open
and exciting research topic.)
We evaluate the quality of synthesized utterances by playing a sentence to lis-
teners and ask them to give a mean opinion score (MOS ), a rating of how good MOS
the synthesized utterances are, usually on a scale from 1–5. We can then compare
systems by comparing their MOS scores on the same sentences (using, e.g., paired
t-tests to test for signiﬁcant differences).
If we are comparing exactly two systems (perhaps to see if a particular change
actually improved the system), we can use AB tests . In AB tests, we play the same AB tests
sentence synthesized by two different systems (an A and a B system). The human
listeners choose which of the two utterances they like better. We do this for say
50 sentences (presented in random order) and compare the number of sentences
preferred for each system.
16.7 Other Speech Tasks
While we have focused on speech recognition and TTS in this chapter, there are a
wide variety of speech-related tasks.
The task of wake word detection is to detect a word or short phrase, usually in wake word
order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant.
The goal with wake words is build the detection into small devices at the computing
edge, to maintain privacy by transmitting the least amount of user speech to a cloud-
based server. Thus wake word detectors need to be fast, small footprint software that
can ﬁt into embedded devices. Wake word detectors usually use the same frontend
feature extraction we saw for ASR, often followed by a whole-word classiﬁer.
Speaker diarization is the task of determining ‘who spoke when’ in a longspeaker
diarization
multi-speaker audio recording, marking the start and end of each speaker’s turns in
the interaction. This can be useful for transcribing meetings, classroom speech, or
medical interactions. Often diarization systems use voice activity detection (V AD) to
ﬁnd segments of continuous speech, extract speaker embedding vectors, and cluster
the vectors to group together segments likely from the same speaker. More recent
work is investigating end-to-end algorithms to map directly from input speech to a
sequence of speaker labels for each frame.

--- Page 354 ---
354 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
Speaker recognition , is the task of identifying a speaker. We generally distin-speaker
recognition
guish the subtasks of speaker veriﬁcation , where we make a binary decision (is
this speaker Xor not?), such as for security when accessing personal information
over the telephone, and speaker identiﬁcation , where we make a one of Ndecision
trying to match a speaker’s voice against a database of many speakers . These tasks
are related to language identiﬁcation , in which we are given a waveﬁle and mustlanguage
identiﬁcation
identify which language is being spoken; this is useful for example for automatically
directing callers to human operators that speak appropriate languages.
16.8 Summary
This chapter introduced the fundamental algorithms of automatic speech recognition
(ASR) and text-to-speech (TTS).
• The task of speech recognition (or speech-to-text) is to map acoustic wave-
forms to sequences of graphemes.
• The input to a speech recognizer is a series of acoustic waves. that are sam-
pled,quantized , and converted to a spectral representation like the log mel
spectrum .
• Two common paradigms for speech recognition are the encoder-decoder with
attention model, and models based on the CTC loss function . Attention-
based models have higher accuracies, but models based on CTC more easily
adapt to streaming : outputting graphemes online instead of waiting until the
acoustic input is complete.
• ASR is evaluated using the Word Error Rate; the edit distance between the
hypothesis and the gold transcription.
•TTS systems are also based on the encoder-decoder architecture. The en-
coder maps letters to an encoding, which is consumed by the decoder which
generates mel spectrogram output. A neural vocoder then reads the spectro-
gram and generates waveforms.
• TTS systems require a ﬁrst pass of text normalization to deal with numbers
and abbreviations and other non-standard words.
• TTS is evaluated by playing a sentence to human listeners and having them
give a mean opinion score (MOS) or by doing AB tests.
Bibliographical and Historical Notes
ASR A number of speech recognition systems were developed by the late 1940s
and early 1950s. An early Bell Labs system could recognize any of the 10 digits
from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent
stored patterns, one for each digit, each of which roughly represented the ﬁrst two
vowel formants in the digit. They achieved 97%–99% accuracy by choosing the pat-
tern that had the highest relative correlation coefﬁcient with the input. Fry (1959)
and Denes (1959) built a phoneme recognizer at University College, London, that
recognized four vowels and nine consonants based on a similar pattern-recognition
principle. Fry and Denes’s system was the ﬁrst to use phoneme transition probabili-
ties to constrain the recognizer.

--- Page 355 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 355
The late 1960s and early 1970s produced a number of important paradigm shifts.
First were a number of feature-extraction algorithms, including the efﬁcient fast
Fourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral pro-
cessing to speech (Oppenheim et al., 1968), and the development of LPC for speech
coding (Atal and Hanauer, 1971). Second were a number of ways of handling warp-
ing; stretching or shrinking the input signal to handle differences in speaking rate warping
and segment length when matching against stored patterns. The natural algorithm for
solving this problem was dynamic programming, and, as we saw in Appendix A, the
algorithm was reinvented multiple times to address this problem. The ﬁrst applica-
tion to speech processing was by Vintsyuk (1968), although his result was not picked
up by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and
Sakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this
dynamic programming idea with the LPC coefﬁcients that had previously been used
only for speech coding. The resulting system extracted LPC features from incoming
words and used dynamic programming to match them against stored LPC templates.
The non-probabilistic use of dynamic programming to match a template against in-
coming speech is called dynamic time warping .dynamic time
warping
The third innovation of this period was the rise of the HMM. Hidden Markov
models seem to have been applied to speech independently at two laboratories around
1972. One application arose from the work of statisticians, in particular Baum and
colleagues at the Institute for Defense Analyses in Princeton who applied HMMs
to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967).
James Baker learned of this work and applied the algorithm to speech processing
(Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek
and collaborators (drawing from their research in information-theoretical models
inﬂuenced by the work of Shannon (1948)) applied HMMs to speech at the IBM
Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was
the decoding algorithm; Baker’s DRAGON system used Viterbi (dynamic program-
ming) decoding, while the IBM system applied Jelinek’s stack decoding algorithm
(Jelinek, 1969). Baker then joined the IBM group for a brief time before founding
the speech-recognition company Dragon Systems.
The use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic
component, slowly spread through the speech community, becoming the dominant
paradigm by the 1990s. One cause was encouragement by ARPA, the Advanced
Research Projects Agency of the U.S. Department of Defense. ARPA started a
ﬁve-year program in 1971 to build 1000-word, constrained grammar, few speaker
speech understanding (Klatt, 1977), and funded four competing systems of which
Carnegie-Mellon University’s Harpy system (Lowerre, 1976), which used a simpli-
ﬁed version of Baker’s HMM-based DRAGON system was the best of the tested sys-
tems. ARPA (and then DARPA) funded a number of new speech research programs,
beginning with 1000-word speaker-independent read-speech tasks like “Resource
Management” (Price et al., 1988), recognition of sentences read from the Wall Street
Journal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of
actual news broadcasts, including quite difﬁcult passages such as on-the-street inter-
views) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey
et al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or
strangers). Each of the ARPA tasks involved an approximately annual bakeoff at bakeoff
which systems were evaluated against each other. The ARPA competitions resulted
in wide-scale borrowing of techniques among labs since it was easy to see which
ideas reduced errors the previous year, and the competitions were probably an im-

--- Page 356 ---
356 CHAPTER 16 • A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH
portant factor in the eventual spread of the HMM paradigm.
By around 1990 neural alternatives to the HMM/GMM architecture for ASR
arose, based on a number of earlier experiments with neural networks for phoneme
recognition and other speech tasks. Architectures included the time-delay neural
network ( TDNN )—the ﬁrst use of convolutional networks for speech— (Waibel
et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid hybrid
HMM/MLP architecture in which a feedforward neural network is trained as a pho-
netic classiﬁer whose outputs are used as probability estimates for an HMM-based
architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and
Bourlard 1995).
While the hybrid systems showed performance close to the standard HMM/GMM
models, the problem was speed: large hybrid models were too slow to train on the
CPUs of that era. For example, the largest hybrid system, a feedforward network,
was limited to a hidden layer of 4000 units, producing probabilities over only a few
dozen monophones. Yet training this model still required the research group to de-
sign special hardware boards to do vector processing (Morgan and Bourlard, 1995).
A later analytic study showed the performance of such simple feedforward MLPs
for ASR increases sharply with more than 1 hidden layer, even controlling for the
total number of parameters (Maas et al., 2017). But the computational resources of
the time were insufﬁcient for more layers.
Over the next two decades a combination of Moore’s law and the rise of GPUs
allowed deep neural networks with many layers. Performance was getting close to
traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mo-
hamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed
traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia).
Originally it seemed that unsupervised pretraining of the networks using a tech-
nique like deep belief networks was important, but by 2013, it was clear that for
hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data
and enough layers, although a few other components did improve performance: us-
ing log mel features instead of MFCCs, using dropout, and using rectiﬁed linear
units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).
Meanwhile early work had proposed the CTC loss function by 2006 (Graves
et al., 2006), and by 2012 the RNN-Transducer was deﬁned and applied to phone
recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog-
nition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015),
with advances such as specialized beam search (Hannun et al., 2014). (Our de-
scription of CTC in the chapter draws on Hannun (2017), which we encourage the
interested reader to follow).
The encoder-decoder architecture was applied to speech at about the same time
by two different groups, in the Listen Attend and Spell system of Chan et al. (2016)
and the attention-based encoder decoder architecture of Chorowski et al. (2014)
and Bahdanau et al. (2016). By 2018 Transformers were included in this encoder-
decoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Trans-
formers in encoder-architectures for ASR, TTS, and speech-to-speech translation.
Popular toolkits for speech processing include Kaldi (Povey et al., 2011) and Kaldi
ESPnet (Watanabe et al. 2018, Hayashi et al. 2020). ESPnet
TTS As we noted at the beginning of the chapter, speech synthesis is one of the
earliest ﬁelds of speech and language processing. The 18th century saw a number
of physical models of the articulation process, including the von Kempelen model
mentioned above, as well as the 1773 vowel model of Kratzenstein in Copenhagen

--- Page 357 ---
EXERCISES 357
using organ pipes.
The early 1950s saw the development of three early paradigms of waveform
synthesis: formant synthesis, articulatory synthesis, and concatenative synthesis.
Modern encoder-decoder systems are distant descendants of formant synthesiz-
ers. Formant synthesizers originally were inspired by attempts to mimic human
speech by generating artiﬁcial spectrograms. The Haskins Laboratories Pattern
Playback Machine generated a sound wave by painting spectrogram patterns on a
moving transparent belt and using reﬂectance to ﬁlter the harmonics of a wave-
form (Cooper et al., 1951); other very early formant synthesizers include those of
Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant
synthesizers were the Klatt formant synthesizer and its successor systems, includ-
ing the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital
Equipment Corporation’s DECtalk (Klatt, 1982). See Klatt (1975) for details.
A second early paradigm, concatenative synthesis, seems to have been ﬁrst pro-
posed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of
magnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) pro-
posed a theoretical model based on diphones, including a database with multiple
copies of each diphone with differing prosody, each labeled with prosodic features
including F0, stress, and duration, and the use of join costs based on F0 and formant
distance between neighboring units. But such diphone synthesis models were not
actually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The
1980s and 1990s saw the invention of unit selection synthesis , based on larger units
of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al.
1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000).
A third paradigm, articulatory synthesizers attempt to synthesize speech by
modeling the physics of the vocal tract as an open tube. Representative models
include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt
(1975) and Flanagan (1972) for more details.
Most early TTS systems used phonemes as input; development of the text anal-
ysis components of TTS came somewhat later, drawing on NLP. Indeed the ﬁrst
true text-to-speech system seems to have been the system of Umeda and Teranishi
(Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a
parser that assigned prosodic boundaries, as well as accent and stress.
Exercises
16.1 Analyze each of the errors in the incorrectly recognized transcription of “um
the phone is I left the. . . ” on page 346. For each one, give your best guess as
to whether you think it is caused by a problem in signal processing, pronun-
ciation modeling, lexicon size, language model, or pruning in the decoding
search.

--- Page 359 ---
Part III
ANNOTATING LINGUISTIC
STRUCTURE
In the ﬁnal part of the book we discuss the task of detecting linguistic structure.
In the early history of NLP these structures were an intermediate step toward deeper
language processing. In modern NLP, we don’t generally make explicit use of parse
or other structures inside the neural language models we introduced in Part I, or
directly in applications like those we discussed in Part II.
Instead linguistic structure plays a number of new roles. One important role is for
interpretability : to provide a useful interpretive lens on neural networks. Knowing
that a particular layer or neuron may be computing something related to a particular
kind of structure can help us break open the ‘black box’ and understand what the
components of our language models are doing.
A second important role for linguistic structure is as a practical tool for social
scientiﬁc studies of text: knowing which adjective modiﬁes which noun, or whether
a particular implicit metaphor is being used, can be important for measuring attitudes
toward groups or individuals. Detailed semantic structure can be helpful, for exam-
ple in ﬁnding particular clauses that have particular meanings in legal contracts.
Word sense labels can help keep any corpus study from measuring facts about the
wrong word sense. Relation structures can be used to help build knowledge bases
from text.
Finally, computation of linguistic structure is an important tool for answering
questions about language itself, a research area called computational linguistics
that is sometimes distinguished from natural language processing. To answer lin-
guistic questions about how language changes over time or across individuals we’ll
need to be able, for example, to parse entire documents from different time periods.
To understand how certain linguistic structures are learned or processed by people,
it’s necessary to be able to automatically label structures for arbitrary text.
In our study of linguistic structure, we begin with one of the oldest tasks in
computational linguistics: the extraction of syntactic structure, and give two sets of
algorithms for parsing : extracting syntactic structure, including constituency pars-
ing and dependency parsing. We then introduce a variety of structures related to
meaning, including semantic roles, word senses, entity relations, and events. We

--- Page 360 ---
360
conclude with linguistic structures that tend to be related to discourse and meaning
over larger texts, including coreference and discourse coherence. In each case we’ll
give algorithms for automatically annotating the relevant structure.

--- Page 362 ---
362 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
CHAPTER
17Sequence Labeling for Parts of
Speech and Named Entities
To each word a warbling note
A Midsummer Night’s Dream , V .I
Dionysius Thrax of Alexandria ( c.100 B.C.), or perhaps someone else (it was a long
time ago), wrote a grammatical sketch of Greek (a “ techn ¯e”) that summarized the
linguistic knowledge of his day. This work is the source of an astonishing proportion
of modern linguistic vocabulary, including the words syntax ,diphthong ,clitic , and
analogy . Also included are a description of eight parts of speech : noun, verb, parts of speech
pronoun, preposition, adverb, conjunction, participle, and article. Although earlier
scholars (including Aristotle as well as the Stoics) had their own lists of parts of
speech, it was Thrax’s set of eight that became the basis for descriptions of European
languages for the next 2000 years. (All the way to the Schoolhouse Rock educational
television shows of our childhood, which had songs about 8 parts of speech, like the
late great Bob Dorough’s Conjunction Junction .) The durability of parts of speech
through two millennia speaks to their centrality in models of human language.
Proper names are another important and anciently studied linguistic category.
While parts of speech are generally assigned to individual words or morphemes, a
proper name is often an entire multiword phrase, like the name “Marie Curie”, the
location “New York City”, or the organization “Stanford University”. We’ll use the
term named entity for, roughly speaking, anything that can be referred to with a named entity
proper name: a person, a location, an organization, although as we’ll see the term is
commonly extended to include things that aren’t entities per se.
Parts of speech (also known as POS ) and named entities are useful clues to POS
sentence structure and meaning. Knowing whether a word is a noun or a verb tells us
about likely neighboring words (nouns in English are preceded by determiners and
adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to
nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named
entity like Washington is a name of a person, a place, or a university is important to
many natural language processing tasks like question answering, stance detection,
or information extraction.
In this chapter we’ll introduce the task of part-of-speech tagging , taking a se-
quence of words and assigning each word a part of speech like NOUN orVERB , and
the task of named entity recognition (NER ), assigning words or phrases tags like
PERSON ,LOCATION , or ORGANIZATION .
Such tasks in which we assign, to each word xiin an input word sequence, a
label yi, so that the output sequence Yhas the same length as the input sequence X
are called sequence labeling tasks. We’ll introduce classic sequence labeling algo-sequence
labeling
rithms, one generative— the Hidden Markov Model (HMM)—and one discriminative—
the Conditional Random Field (CRF). In following chapters we’ll introduce modern
sequence labelers based on RNNs and Transformers.

--- Page 363 ---
17.1 • (M OSTLY ) ENGLISH WORD CLASSES 363
17.1 (Mostly) English Word Classes
Until now we have been using part-of-speech terms like noun andverb rather freely.
In this section we give more complete deﬁnitions. While word classes do have
semantic tendencies—adjectives, for example, often describe properties and nouns
people — parts of speech are deﬁned instead based on their grammatical relationship
with neighboring words or the morphological properties about their afﬁxes.
Tag Description ExampleOpen ClassADJ Adjective: noun modiﬁers describing properties red,young ,awesome
ADV Adverb: verb modiﬁers of time, place, manner very,slowly ,home ,yesterday
NOUN words for persons, places, things, etc. algorithm ,cat,mango ,beauty
VERB words for actions and processes draw ,provide ,go
PROPN Proper noun: name of a person, organization, place, etc.. Regina ,IBM,Colorado
INTJ Interjection: exclamation, greeting, yes/no response, etc. oh,um,yes,helloClosed Class WordsADP Adposition (Preposition/Postposition): marks a noun’s
spacial, temporal, or other relationin, on, by, under
AUX Auxiliary: helping verb marking tense, aspect, mood, etc., can, may, should, are
CCONJ Coordinating Conjunction: joins two phrases/clauses and,or,but
DET Determiner: marks noun phrase properties a, an, the, this
NUM Numeral one, two, 2026, 11:00, hundred
PART Particle: a function word that must be associated with an-
other word’s, not, (inﬁnitive) to
PRON Pronoun: a shorthand for referring to an entity or event she, who, I, others
SCONJ Subordinating Conjunction: joins a main clause with a
subordinate clause such as a sentential complementwhether ,becauseOtherPUNCT Punctuation˙, , ()
SYM Symbols like $ or emoji $, %
X Other asdf, qwfg
Figure 17.1 The 17 parts of speech in the Universal Dependencies tagset (de Marneffe et al., 2021). Features
can be added to make ﬁner-grained distinctions (with properties like number, case, deﬁniteness, and so on).
Parts of speech fall into two broad categories: closed class andopen class . closed class
open class Closed classes are those with relatively ﬁxed membership, such as prepositions—
new prepositions are rarely coined. By contrast, nouns and verbs are open classes—
new nouns and verbs like iPhone orto fax are continually being created or borrowed.
Closed class words are generally function words likeof,it,and, oryou, which tend function word
to be very short, occur frequently, and often have structuring uses in grammar.
Four major open classes occur in the languages of the world: nouns (including
proper nouns), verbs ,adjectives , and adverbs , as well as the smaller open class of
interjections . English has all ﬁve, although not every language does.
Nouns are words for people, places, or things, but include others as well. Com- noun
mon nouns include concrete terms like catandmango , abstractions like algorithm common noun
andbeauty , and verb-like terms like pacing as in His pacing to and fro became quite
annoying . Nouns in English can occur with determiners ( a goat, this bandwidth )
take possessives ( IBM’s annual revenue ), and may occur in the plural ( goats, abaci ).
Many languages, including English, divide common nouns into count nouns and count noun
mass nouns . Count nouns can occur in the singular and plural ( goat/goats, rela- mass noun
tionship/relationships ) and can be counted ( one goat, two goats ). Mass nouns are
used when something is conceptualized as a homogeneous group. So snow, salt , and
communism are not counted (i.e., *two snows or*two communisms ).Proper nouns , proper noun
likeRegina ,Colorado , and IBM, are names of speciﬁc persons or entities.

--- Page 364 ---
364 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
Verbs refer to actions and processes, including main verbs like draw ,provide , verb
andgo. English verbs have inﬂections (non-third-person-singular ( eat), third-person-
singular ( eats), progressive ( eating ), past participle ( eaten )). While many scholars
believe that all human languages have the categories of noun and verb, others have
argued that some languages, such as Riau Indonesian and Tongan, don’t even make
this distinction (Broschart 1997; Evans 2000; Gil 2000) .
Adjectives often describe properties or qualities of nouns, like color ( white , adjective
black ), age ( old,young ), and value ( good ,bad), but there are languages without
adjectives. In Korean, for example, the words corresponding to English adjectives
act as a subclass of verbs, so what is in English an adjective “beautiful” acts in
Korean like a verb meaning “to be beautiful”.
Adverbs are a hodge-podge. All the italicized words in this example are adverbs: adverb
Actually , I ran home extremely quickly yesterday
Adverbs generally modify something (often verbs, hence the name “adverb”, but
also other adverbs and entire verb phrases). Directional adverbs orlocative ad- locative
verbs (home ,here,downhill ) specify the direction or location of some action; degree degree
adverbs (extremely ,very,somewhat ) specify the extent of some action, process, or
property; manner adverbs (slowly ,slinkily ,delicately ) describe the manner of some manner
action or process; and temporal adverbs describe the time that some action or event temporal
took place ( yesterday ,Monday ).
Interjections (oh, hey, alas, uh, um ) are a smaller open class that also includes interjection
greetings ( hello, goodbye ) and question responses ( yes, no, uh-huh ).
English adpositions occur before nouns, hence are called prepositions . They can preposition
indicate spatial or temporal relations, whether literal ( on it,before then ,by the house )
or metaphorical ( on time ,with gusto ,beside herself ), and relations like marking the
agent in Hamlet was written by Shakespeare .
Aparticle resembles a preposition or an adverb and is used in combination with particle
a verb. Particles often have extended meanings that aren’t quite the same as the
prepositions they resemble, as in the particle over inshe turned the paper over . A
verb and a particle acting as a single unit is called a phrasal verb . The meaning phrasal verb
of phrasal verbs is often non-compositional —not predictable from the individual
meanings of the verb and the particle. Thus, turn down means ‘reject’, rule out
‘eliminate’, and go on ‘continue’.
Determiners likethisandthat(this chapter ,that page ) can mark the start of an determiner
English noun phrase. Articles likea,an, and the, are a type of determiner that mark article
discourse properties of the noun and are quite frequent; theis the most common
word in written English, with aandanright behind.
Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc- conjunction
tions like and,or, and butjoin two elements of equal status. Subordinating conjunc-
tions are used when one of the elements has some embedded status. For example,
the subordinating conjunction thatin“I thought that you might like some milk” links
the main clause I thought with the subordinate clause you might like some milk . This
clause is called subordinate because this entire clause is the “content” of the main
verb thought . Subordinating conjunctions like thatwhich link a verb to its argument
in this way are also called complementizers . complementizer
Pronouns act as a shorthand for referring to an entity or event. Personal pro- pronoun
nouns refer to persons or entities ( you,she,I,it,me, etc.). Possessive pronouns are
forms of personal pronouns that indicate either actual possession or more often just
an abstract relation between the person and some object ( my, your, his, her, its, one’s,
our, their ).Wh-pronouns (what, who, whom, whoever ) are used in certain question wh

--- Page 365 ---
17.2 • P ART-OF-SPEECH TAGGING 365
forms, or act as complementizers ( Frida, who married Diego. . . ).
Auxiliary verbs mark semantic features of a main verb such as its tense, whether auxiliary
it is completed (aspect), whether it is negated (polarity), and whether an action is
necessary, possible, suggested, or desired (mood). English auxiliaries include the
copula verb be, the two verbs doandhave , forms, as well as modal verbs used to copula
modal mark the mood associated with the event depicted by the main verb: canindicates
ability or possibility, may permission or possibility, must necessity.
An English-speciﬁc tagset, the Penn Treebank tagset (Marcus et al., 1993), shown
in Fig. 17.2, has been used to label many syntactically annotated corpora like the
Penn Treebank corpora, so it is worth knowing about.
Tag Description Example Tag Description Example Tag Description Example
CC coord. conj. and, but, or NNP proper noun, sing. IBM TO inﬁnitive to to
CD cardinal number one, two NNPS proper noun, plu. Carolinas UH interjection ah, oops
DT determiner a, the NNS noun, plural llamas VB verb base eat
EX existential ‘there’ there PDT predeterminer all, both VBD verb past tense ate
FW foreign word mea culpa POS possessive ending ’s VBG verb gerund eating
IN preposition/
subordin-conjof, in, by PRP personal pronoun I, you, he VBN verb past partici-
pleeaten
JJ adjective yellow PRP$ possess. pronoun your VBP verb non-3sg-pr eat
JJR comparative adj bigger RB adverb quickly VBZ verb 3sg pres eats
JJS superlative adj wildest RBR comparative adv faster WDT wh-determ. which, that
LS list item marker 1, 2, One RBS superlatv. adv fastest WP wh-pronoun what, who
MD modal can, should RP particle up, off WP$ wh-possess. whose
NN sing or mass noun llama SYM symbol +,%,& WRB wh-adverb how, where
Figure 17.2 Penn Treebank core 36 part-of-speech tags.
Below we show some examples with each word tagged according to both the UD
(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense
and participles on verbs, and has a special tag for the existential there construction in
English. Note that since London Journal of Medicine is a proper noun, both tagsets
mark its component nouns as PROPN/NNP, including journal andmedicine , which
might otherwise be labeled as common nouns (NOUN/NN).
(17.1) There/ PRON /EXare/ VERB /VBP 70/NUM /CDchildren/ NOUN /NNS
there/ ADV/RB./PUNC /.
(17.2) Preliminary/ ADJ/JJﬁndings/ NOUN /NNS were/ AUX/VBD
reported/ VERB /VBN in/ADP/INtoday/ NOUN /NN’s/PART /POS
London/ PROPN /NNP Journal/ PROPN /NNP of/ADP/INMedicine/ PROPN /NNP
17.2 Part-of-Speech Tagging
Part-of-speech tagging is the process of assigning a part-of-speech to each word inpart-of-speech
tagging
a text. The input is a sequence x1;x2;:::;xnof (tokenized) words and a tagset, and
the output is a sequence y1;y2;:::;ynof tags, each output yicorresponding exactly to
one input xi, as shown in the intuition in Fig. 17.3.
Tagging is a disambiguation task; words are ambiguous —have more than one ambiguous
possible part-of-speech—and the goal is to ﬁnd the correct tag for the situation.
For example, book can be a verb ( book that ﬂight ) or a noun ( hand me that book ).
That can be a determiner ( Does that ﬂight serve dinner ) or a complementizer ( I

--- Page 366 ---
366 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
willNOUNAUXVERBDETNOUNJanetbackthebillPart of Speech Taggerx1x2x3x4x5y1y2y3y4y5
Figure 17.3 The task of part-of-speech tagging: mapping from input words x1;x2;:::;xnto
output POS tags y1;y2;:::;yn.
thought that your ﬂight was earlier ). The goal of POS-tagging is to resolve theseambiguity
resolution
ambiguities, choosing the proper tag for the context.
The accuracy of part-of-speech tagging algorithms (the percentage of test set accuracy
tags that match human gold labels) is extremely high. One study found accuracies
over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu
and Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter
the algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also
about the human performance on this task, at least for English (Manning, 2011).
Types: WSJ Brown
Unambiguous (1 tag) 44,432 ( 86% ) 45,799 ( 85% )
Ambiguous (2+ tags) 7,025 ( 14% ) 8,050 ( 15% )
Tokens :
Unambiguous (1 tag) 577,421 ( 45% ) 384,349 ( 33% )
Ambiguous (2+ tags) 711,780 ( 55% ) 786,646 ( 67% )
Figure 17.4 Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).
We’ll introduce algorithms for the task in the next few sections, but ﬁrst let’s
explore the task. Exactly how hard is it? Fig. 17.4 shows that most word types
(85-86%) are unambiguous ( Janet is always NNP, hesitantly is always RB). But the
ambiguous words, though accounting for only 14-15% of the vocabulary, are very
common, and 55-67% of word tokens in running text are ambiguous. Particularly
ambiguous common words include that,back,down ,putandset; here are some
examples of the 6 different parts of speech for the word back:
earnings growth took a back/JJ seat
a small building in the back/NN
a clear majority of senators back/VBP the bill
Dave began to back/VB toward the door
enable the country to buy back/RP debt
I was twenty-one back/RB then
Nonetheless, many words are easy to disambiguate, because their different tags
aren’t equally likely. For example, acan be a determiner or the letter a, but the
determiner sense is much more likely.
This idea suggests a useful baseline : given an ambiguous word, choose the tag
which is most frequent in the training corpus. This is a key concept:
Most Frequent Class Baseline: Always compare a classiﬁer against a baseline at
least as good as the most frequent class baseline (assigning each token to the class
it occurred in most often in the training set).

--- Page 367 ---
17.3 • N AMED ENTITIES AND NAMED ENTITY TAGGING 367
The most-frequent-tag baseline has an accuracy of about 92%1. The baseline
thus differs from the state-of-the-art and human ceiling (97%) by only 5%.
17.3 Named Entities and Named Entity Tagging
Part of speech tagging can tell us that words like Janet ,Stanford University , and
Colorado are all proper nouns; being a proper noun is a grammatical property of
these words. But viewed from a semantic perspective, these proper nouns refer to
different kinds of entities: Janet is a person, Stanford University is an organization,
and Colorado is a location.
Here we re-introduce the concept of a named entity , which was also introduced named entity
in Section 11.5 for readers who haven’t yet read Chapter 11.
Anamed entity is, roughly speaking, anything that can be referred to with a named entity
proper name: a person, a location, an organization. The task of named entity recog-
nition (NER ) is to ﬁnd spans of text that constitute proper names and tag the type ofnamed entity
recognition
NER the entity. Four entity tags are most common: PER (person), LOC (location), ORG
(organization), or GPE (geo-political entity). However, the term named entity is
commonly extended to include things that aren’t entities per se, including dates,
times, and other kinds of temporal expressions, and even numerical expressions like
prices. Here’s an example of the output of an NER tagger:
Citing high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it
has increased fares by [ MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ ORG American Airlines ], a
unit of [ ORG AMR Corp.] , immediately matched the move, spokesman
[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,
said the increase took effect [ TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [ LOC Chicago]
to [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .
The text contains 13 mentions of named entities including 5 organizations, 4 loca-
tions, 2 times, 1 person, and 1 mention of money. Figure 17.5 shows typical generic
named entity types. Many applications will also need to use speciﬁc entity types like
proteins, genes, commercial products, or works of art.
Type Tag Sample Categories Example sentences
People PER people, characters Turing is a giant of computer science.
Organization ORG companies, sports teams The IPCC warned about the cyclone.
Location LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon .
Geo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.
Figure 17.5 A list of generic named entity types with the kinds of entities they refer to.
Named entity tagging is a useful ﬁrst step in lots of natural language processing
tasks. In sentiment analysis we might want to know a consumer’s sentiment toward a
particular entity. Entities are a useful ﬁrst stage in question answering, or for linking
text to information in structured knowledge sources like Wikipedia. And named
entity tagging is also central to tasks involving building semantic representations,
like extracting events and the relationship between participants.
1In English, on the WSJ corpus, tested on sections 22-24.

--- Page 368 ---
368 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
Unlike part-of-speech tagging, where there is no segmentation problem since
each word gets one tag, the task of named entity recognition is to ﬁnd and label
spans of text, and is difﬁcult partly because of the ambiguity of segmentation; we
need to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,
most words in a text will not be named entities. Another difﬁculty is caused by type
ambiguity. The mention JFKcan refer to a person, the airport in New York, or any
number of schools, bridges, and streets around the United States. Some examples of
this kind of cross-type confusion are given in Figure 17.6.
[PER Washington] was born into slavery on the farm of James Burroughs.
[ORG Washington] went up 2 games to 1 in the four-game series.
Blair arrived in [ LOC Washington] for what may well be his last state visit.
In June, [ GPE Washington] passed a primary seatbelt law.
Figure 17.6 Examples of type ambiguities in the use of the name Washington .
The standard approach to sequence labeling for a span-recognition problem like
NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us
to treat NER like a word-by-word sequence labeling task, via tags that capture both
the boundary and the named entity type. Consider the following sentence:
[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines
Holding ] , said the fare applies to the [ LOC Chicago ] route.
Figure 17.7 shows the same excerpt represented with BIO tagging, as well as BIO
variants called IOtagging and BIOES tagging. In BIO tagging we label any token
thatbegins a span of interest with the label B, tokens that occur inside a span are
tagged with an I, and any tokens outside of any span of interest are labeled O. While
there is only one Otag, we’ll have distinct Band Itags for each named entity class.
The number of tags is thus 2 n+1 tags, where nis the number of entity types. BIO
tagging can represent exactly the same information as the bracketed notation, but has
the advantage that we can represent the task in the same simple sequence modeling
way as part-of-speech tagging: assigning a single label yito each input word xi:
Words IO Label BIO Label BIOES Label
Jane I-PER B-PER B-PER
Villanueva I-PER I-PER E-PER
of O O O
United I-ORG B-ORG B-ORG
Airlines I-ORG I-ORG I-ORG
Holding I-ORG I-ORG E-ORG
discussed O O O
the O O O
Chicago I-LOC B-LOC S-LOC
route O O O
. O O O
Figure 17.7 NER as a sequence model, showing IO, BIO, and BIOES taggings.
We’ve also shown two variant tagging schemes: IO tagging, which loses some
information by eliminating the B tag, and BIOES tagging, which adds an end tag
Efor the end of a span, and a span tag Sfor a span consisting of only one word.
A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each
token in a text with tags that indicate the presence (or absence) of particular kinds
of named entities.

--- Page 369 ---
17.4 • HMM P ART-OF-SPEECH TAGGING 369
17.4 HMM Part-of-Speech Tagging
In this section we introduce our ﬁrst sequence labeling algorithm, the Hidden Markov
Model, and show how to apply it to part-of-speech tagging. Recall that a sequence
labeler is a model whose job is to assign a label to each unit in a sequence, thus
mapping a sequence of observations to a sequence of labels of the same length.
The HMM is a classic model that introduces many of the key concepts of sequence
modeling that we will see again in more modern models.
An HMM is a probabilistic sequence model: given a sequence of units (words,
letters, morphemes, sentences, whatever), it computes a probability distribution over
possible sequences of labels and chooses the best label sequence.
17.4.1 Markov Chains
The HMM is based on augmenting the Markov chain. A Markov chain is a model Markov chain
that tells us something about the probabilities of sequences of random variables,
states , each of which can take on values from some set. These sets can be words, or
tags, or symbols representing anything, for example the weather. A Markov chain
makes a very strong assumption that if we want to predict the future in the sequence,
all that matters is the current state. All the states before the current state have no im-
pact on the future except via the current state. It’s as if to predict tomorrow’s weather
you could examine today’s weather but you weren’t allowed to look at yesterday’s
weather.
WARM3HOT1COLD2.8.6.1.1.3.6.1.1.3
charminguniformlyare.1.4.5.5.5.2.6.2
(a) (b)
Figure 17.8 A Markov chain for weather (a) and one for words (b), showing states and
transitions. A start distribution pis required; setting p= [0:1;0:7;0:2]for (a) would mean a
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.
More formally, consider a sequence of state variables q1;q2;:::;qi. A Markov
model embodies the Markov assumption on the probabilities of this sequence: thatMarkov
assumption
when predicting the future, the past doesn’t matter, only the present.
Markov Assumption: P(qi=ajq1:::qi 1) =P(qi=ajqi 1) (17.3)
Figure 17.8a shows a Markov chain for assigning a probability to a sequence of
weather events, for which the vocabulary consists of HOT,COLD , and WARM . The
states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. The transitions are probabilities: the values of arcs leaving a given
state must sum to 1. Figure 17.8b shows a Markov chain for assigning a probabil-
ity to a sequence of words w1:::wt. This Markov chain should be familiar; in fact,
it represents a bigram language model, with each edge expressing the probability
p(wijwj)! Given the two models in Fig. 17.8, we can assign a probability to any
sequence from our vocabulary.

--- Page 370 ---
370 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
Formally, a Markov chain is speciﬁed by the following components:
Q=q1q2:::qN a set of Nstates
A=a11a12:::aN1:::aNN atransition probability matrix A, each ai jrepresent-
ing the probability of moving from state ito state j, s.t.Pn
j=1ai j=18i
p=p1;p2;:::;pN aninitial probability distribution over states. piis the
probability that the Markov chain will start in state i.
Some states jmay have pj=0, meaning that they cannot
be initial states. Also,Pn
i=1pi=1
Before you go on, use the sample probabilities in Fig. 17.8a (with p= [0:1;0:7;0:2])
to compute the probability of each of the following sequences:
(17.4) hot hot hot hot
(17.5) cold hot cold hot
What does the difference in these probabilities tell you about a real-world weather
fact encoded in Fig. 17.8a?
17.4.2 The Hidden Markov Model
A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden : we don’t observe them directly. For example we don’t normally observe hidden
part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
word sequence. We call the tags hidden because they are not observed.
Ahidden Markov model (HMM ) allows us to talk about both observed eventshidden Markov
model
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. An HMM is speciﬁed by
the following components:
Q=q1q2:::qN a set of Nstates
A=a11:::ai j:::aNNatransition probability matrix A, each ai jrepresenting the probability
of moving from state ito state j, s.t.PN
j=1ai j=18i
B=bi(ot) a sequence of observation likelihoods , also called emission probabili-
ties, each expressing the probability of an observation ot(drawn from a
vocabulary V=v1;v2;:::;vV) being generated from a state qi
p=p1;p2;:::;pN aninitial probability distribution over states. piis the probability that
the Markov chain will start in state i. Some states jmay have pj=0,
meaning that they cannot be initial states. Also,Pn
i=1pi=1
The HMM is given as input O=o1o2:::oT: a sequence of Tobservations , each
one drawn from the vocabulary V.
A ﬁrst-order hidden Markov model instantiates two simplifying assumptions.
First, as with a ﬁrst-order Markov chain, the probability of a particular state depends
only on the previous state:
Markov Assumption: P(qijq1;:::;qi 1) =P(qijqi 1) (17.6)
Second, the probability of an output observation oidepends only on the state that
produced the observation qiand not on any other states or any other observations:
Output Independence: P(oijq1;:::qi;:::; qT;o1;:::; oi;:::; oT) =P(oijqi)(17.7)

--- Page 371 ---
17.4 • HMM P ART-OF-SPEECH TAGGING 371
17.4.3 The components of an HMM tagger
An HMM has two components, the AandBprobabilities, both estimated by counting
on a tagged training corpus. (For this example we’ll use the tagged WSJ corpus.)
TheAmatrix contains the tag transition probabilities P(tijti 1)which represent
the probability of a tag occurring given the previous tag. For example, modal verbs
likewillare very likely to be followed by a verb in the base form, a VB, like race, so
we expect this probability to be high. We compute the maximum likelihood estimate
of this transition probability by counting, out of the times we see the ﬁrst tag in a
labeled corpus, how often the ﬁrst tag is followed by the second:
P(tijti 1) =C(ti 1;ti)
C(ti 1)(17.8)
In the WSJ corpus, for example, MD occurs 13124 times of which it is followed
by VB 10471, for an MLE estimate of
P(V BjMD) =C(MD;V B)
C(MD)=10471
13124=:80 (17.9)
TheBemission probabilities, P(wijti), represent the probability, given a tag (say
MD), that it will be associated with a given word (say will). The MLE of the emis-
sion probability is
P(wijti) =C(ti;wi)
C(ti)(17.10)
Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will4046
times:
P(willjMD) =C(MD;will)
C(MD)=4046
13124=:31 (17.11)
We saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood
term is not asking “which is the most likely tag for the word will?” That would be
the posterior P(MDjwill). Instead, P(willjMD)answers the slightly counterintuitive
question “If we were going to generate a MD, how likely is it that this modal would
bewill?”
NN3VB1MD2a22
a11a12a21a13a33a32a23a31P("aardvark" | NN)...P(“will” | NN)...P("the" | NN)...P(“back” | NN)...P("zebra" | NN)B3P("aardvark" | VB)...P(“will” | VB)...P("the" | VB)...P(“back” | VB)...P("zebra" | VB)B1P("aardvark" | MD)...P(“will” | MD)...P("the" | MD)...P(“back” | MD)...P("zebra" | MD)B2
Figure 17.9 An illustration of the two parts of an HMM representation: the Atransition
probabilities used to compute the prior probability, and the Bobservation likelihoods that are
associated with each state, one likelihood for each possible observation word.

--- Page 372 ---
372 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
The Atransition probabilities, and Bobservation likelihoods of the HMM are
illustrated in Fig. 17.9 for three states in an HMM part-of-speech tagger; the full
tagger would have one state for each tag.
17.4.4 HMM tagging as decoding
For any model, such as an HMM, that contains hidden variables, the task of deter-
mining the hidden variables sequence corresponding to the sequence of observations
is called decoding . More formally, decoding
Decoding : Given as input an HMM l= (A;B)and a sequence of ob-
servations O=o1;o2;:::;oT, ﬁnd the most probable sequence of states
Q=q1q2q3:::qT.
For part-of-speech tagging, the goal of HMM decoding is to choose the tag
sequence t1:::tnthat is most probable given the observation sequence of nwords
w1:::wn:
ˆt1:n=argmax
t1:::tnP(t1:::tnjw1:::wn) (17.12)
The way we’ll do this in the HMM is to use Bayes’ rule to instead compute:
ˆt1:n=argmax
t1:::tnP(w1:::wnjt1:::tn)P(t1:::tn)
P(w1:::wn)(17.13)
Furthermore, we simplify Eq. 17.13 by dropping the denominator P(wn
1):
ˆt1:n=argmax
t1:::tnP(w1:::wnjt1:::tn)P(t1:::tn) (17.14)
HMM taggers make two further simplifying assumptions. The ﬁrst (output in-
dependence, from Eq. 17.7) is that the probability of a word appearing depends only
on its own tag and is independent of neighboring words and tags:
P(w1:::wnjt1:::tn)nY
i=1P(wijti) (17.15)
The second assumption (the Markov assumption, Eq. 17.6) is that the probability of
a tag is dependent only on the previous tag, rather than the entire tag sequence;
P(t1:::tn)nY
i=1P(tijti 1) (17.16)
Plugging the simplifying assumptions from Eq. 17.15 and Eq. 17.16 into Eq. 17.14
results in the following equation for the most probable tag sequence from a bigram
tagger:
ˆt1:n=argmax
t1:::tnP(t1:::tnjw1:::wn)argmax
t1:::tnnY
i=1emissionz}|{
P(wijti)transitionz}|{
P(tijti 1) (17.17)
The two parts of Eq. 17.17 correspond neatly to the Bemission probability andA
transition probability that we just deﬁned above!

--- Page 373 ---
17.4 • HMM P ART-OF-SPEECH TAGGING 373
function VITERBI (observations of len T,state-graph of len N)returns best-path ,path-prob
create a path probability matrix viterbi[N,T]
foreach state sfrom 1toNdo ; initialization step
viterbi [s,1] psbs(o1)
backpointer [s,1] 0
foreach time step tfrom 2toTdo ; recursion step
foreach state sfrom 1toNdo
viterbi [s,t] Nmax
s0=1viterbi [s0;t 1]as0;sbs(ot)
backpointer [s,t] Nargmax
s0=1viterbi [s0;t 1]as0;sbs(ot)
bestpathprob Nmax
s=1viterbi [s;T] ; termination step
bestpathpointer Nargmax
s=1viterbi [s;T] ; termination step
bestpath the path starting at state bestpathpointer , that follows backpointer[] to states back in time
return bestpath ,bestpathprob
Figure 17.10 Viterbi algorithm for ﬁnding the optimal sequence of tags. Given an observation sequence and
an HMM l= (A;B), the algorithm returns the state path through the HMM that assigns maximum likelihood
to the observation sequence.
17.4.5 The Viterbi Algorithm
The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 17.10.Viterbi
algorithm
As an instance of dynamic programming , Viterbi resembles the dynamic program-
ming minimum edit distance algorithm of Chapter 2.
The Viterbi algorithm ﬁrst sets up a probability matrix or lattice , with one col-
umn for each observation otand one row for each state in the state graph. Each col-
umn thus has a cell for each state qiin the single combined automaton. Figure 17.11
shows an intuition of this lattice for the sentence Janet will back the bill .
Each cell of the lattice, vt(j), represents the probability that the HMM is in state
jafter seeing the ﬁrst tobservations and passing through the most probable state
sequence q1;:::;qt 1, given the HMM l. The value of each cell vt(j)is computed
by recursively taking the most probable path that could lead us to this cell. Formally,
each cell expresses the probability
vt(j) = max
q1;:::;qt 1P(q1:::qt 1;o1;o2:::ot;qt=jjl) (17.18)
We represent the most probable path by taking the maximum over all possible
previous state sequences max
q1;:::;qt 1. Like other dynamic programming algorithms,
Viterbi ﬁlls each cell recursively. Given that we had already computed the probabil-
ity of being in every state at time t 1, we compute the Viterbi probability by taking
the most probable of the extensions of the paths that lead to the current cell. For a
given state qjat time t, the value vt(j)is computed as
vt(j) =Nmax
i=1vt 1(i)ai jbj(ot) (17.19)
The three factors that are multiplied in Eq. 17.19 for extending the previous paths to
compute the Viterbi probability at time tare

--- Page 374 ---
374 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
JJNNPNNPNNPMDMDMDMDVBVBJJJJJJNNNNRBRBRBRBDTDTDTDT
NNPJanetwillbackthebillNNVBMDNNVBJJRB
NNPDTNNVB
Figure 17.11 A sketch of the lattice for Janet will back the bill , showing the possible tags
(qi) for each word and highlighting the path corresponding to the correct tag sequence through
the hidden states. States (parts of speech) which have a zero probability of generating a
particular word according to the Bmatrix (such as the probability that a determiner DT will
be realized as Janet ) are greyed out.
vt 1(i)theprevious Viterbi path probability from the previous time step
ai j thetransition probability from previous state qito current state qj
bj(ot) thestate observation likelihood of the observation symbol otgiven
the current state j
17.4.6 Working through an example
Let’s tag the sentence Janet will back the bill ; the goal is the correct series of tags
(see also Fig. 17.11):
(17.20) Janet/NNP will/MD back/VB the/DT bill/NN
NNP MD VB JJ NN RB DT
<s> 0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026
NNP 0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025
MD 0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041
VB 0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231
JJ 0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036
NN 0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068
RB 0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479
DT 0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017
Figure 17.12 TheAtransition probabilities P(tijti 1)computed from the WSJ corpus with-
out smoothing. Rows are labeled with the conditioning event; thus P(V BjMD)is 0.7968.
<s>is the start token.
Let the HMM be deﬁned by the two tables in Fig. 17.12 and Fig. 17.13. Fig-
ure 17.12 lists the ai jprobabilities for transitioning between the hidden states (part-
of-speech tags). Figure 17.13 expresses the bi(ot)probabilities, the observation
likelihoods of words given tags. This table is (slightly simpliﬁed) from counts in the
WSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts

--- Page 375 ---
17.4 • HMM P ART-OF-SPEECH TAGGING 375
Janet will back the bill
NNP 0.000032 0 0 0.000048 0
MD 0 0.308431 0 0 0
VB 0 0.000028 0.000672 0 0.000028
JJ 0 0 0.000340 0 0
NN 0 0.000200 0.000223 0 0.002337
RB 0 0 0.010446 0 0
DT 0 0 0 0.506099 0
Figure 17.13 Observation likelihoods Bcomputed from the WSJ corpus without smooth-
ing, simpliﬁed slightly.
of speech, and the word thecan appear as a determiner or as an NNP (in titles like
“Somewhere Over the Rainbow” all words are tagged as NNP).
πP(NNP|start) = .28* P(MD|MD)= 0*  P(MD|NNP).000009*.01  = .9e-8 v1(2)=.0006 x 0 = 0v1(1) = .28* .000032 = .000009
tMDq2q1
o1Janetbillwillo2o3backVBJJv1(3)=.0031 x 0 = 0v1(4)= .045*0=0
o4  *  P(MD|VB) = 0 * P(MD|JJ)= 0P(VB|start) = .0031P(JJ |start) =.045
backtraceq3q4
theNNq5RBq6DTq7
v2(2) =max * .308 =2.772e-8v2(5)=max * .0002 = .0000000001
v2(3)=max * .000028 =     2.5e-13v3(6)=max * .0104v3(5)=max * .000223v3(4)=max * .00034v3(3)=max * .00067v1(5)v1(6)v1(7)
v2(1)v2(4)v2(6)v2(7)
backtrace* P(RB|NN)* P(NN|NN)
startstartstartstartstarto5NNPP(MD|start) = .0006
Figure 17.14 The ﬁrst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps
the probability of the best path so far and a pointer to the previous cell along that path. We have only ﬁlled out
columns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the
reader. After the cells are ﬁlled in, backtracing from the endstate, we should be able to reconstruct the correct
state sequence NNP MD VB DT NN.
Figure 17.14 shows a ﬂeshed-out version of the sketch we saw in Fig. 17.11,
the Viterbi lattice for computing the best hidden state sequence for the observation
sequence Janet will back the bill .
There are N=5 state columns. We begin in column 1 (for the word Janet ) by
setting the Viterbi value in each cell to the product of the ptransition probability (the
start probability for that state i, which we get from the <s>entry of Fig. 17.12), and

--- Page 376 ---
376 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
the observation likelihood of the word Janet given the tag for that cell. Most of the
cells in the column are zero since the word Janet cannot be any of those tags. The
reader should ﬁnd this in Fig. 17.14.
Next, each cell in the willcolumn gets updated. For each state, we compute the
value viterbi [s;t]by taking the maximum over the extensions of all the paths from
the previous column that lead to the current cell according to Eq. 17.19. We have
shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 val-
ues from the previous column, multiplied by the appropriate transition probability;
as it happens in this case, most of them are zero from the previous column. The re-
maining value is multiplied by the relevant observation probability, and the (trivial)
max is taken. In this case the ﬁnal value, 2.772e-8, comes from the NNP state at the
previous column. The reader should ﬁll in the rest of the lattice in Fig. 17.14 and
backtrace to see whether or not the Viterbi algorithm returns the gold state sequence
NNP MD VB DT NN.
17.5 Conditional Random Fields (CRFs)
While the HMM is a useful and powerful model, it turns out that HMMs need a
number of augmentations to achieve high accuracy. For example, in POS tagging
as in other tasks, we often run into unknown words : proper names and acronymsunknown
words
are created very often, and even new common nouns and verbs enter the language
at a surprising rate. It would be great to have ways to add arbitrary features to
help with this, perhaps based on capitalization or morphology (words starting with
capital letters are likely to be proper nouns, words ending with -edtend to be past
tense (VBD or VBN), etc.) Or knowing the previous or following words might be a
useful feature (if the previous word is the, the current tag is unlikely to be a verb).
Although we could try to hack the HMM to ﬁnd ways to incorporate some of
these, in general it’s hard for generative models like HMMs to add arbitrary features
directly into the model in a clean way. We’ve already seen a model for combining
arbitrary features in a principled way: log-linear models like the logistic regression
model of Chapter 5! But logistic regression isn’t a sequence model; it assigns a class
to a single observation.
Luckily, there is a discriminative sequence model based on log-linear models:
theconditional random ﬁeld (CRF ). We’ll describe here the linear chain CRF , CRF
the version of the CRF most commonly used for language processing, and the one
whose conditioning closely matches the HMM.
Assuming we have a sequence of input words X=x1:::xnand want to compute
a sequence of output tags Y=y1:::yn. In an HMM to compute the best tag sequence
that maximizes P(YjX)we rely on Bayes’ rule and the likelihood P(XjY):
ˆY=argmax
Yp(YjX)
=argmax
Yp(XjY)p(Y)
=argmax
YY
ip(xijyi)Y
ip(yijyi 1) (17.21)
In a CRF, by contrast, we compute the posterior p(YjX)directly, training the CRF

--- Page 377 ---
17.5 • C ONDITIONAL RANDOM FIELDS (CRF S)377
to discriminate among the possible tag sequences:
ˆY=argmax
Y2YP(YjX) (17.22)
However, the CRF does not compute a probability for each tag at each time step. In-
stead, at each time step the CRF computes log-linear functions over a set of relevant
features, and these local features are aggregated and normalized to produce a global
probability for the whole sequence.
Let’s introduce the CRF more formally, again using XandYas the input and
output sequences. A CRF is a log-linear model that assigns a probability to an
entire output (tag) sequence Y, out of all possible sequences Y, given the entire input
(word) sequence X. We can think of a CRF as like a giant sequential version of
the multinomial logistic regression algorithm we saw for text categorization. Recall
that we introduced the feature function fin regular multinomial logistic regression
for text categorization as a function of a tuple: the input text xand a single class y
(page 86). In a CRF, we’re dealing with a sequence, so the function Fmaps an entire
input sequence Xand an entire output sequence Yto a feature vector. Let’s assume
we have Kfeatures, with a weight wkfor each feature Fk:
p(YjX) =exp KX
k=1wkFk(X;Y)!
X
Y02Yexp KX
k=1wkFk(X;Y0)! (17.23)
It’s common to also describe the same equation by pulling out the denominator into
a function Z(X):
p(YjX) =1
Z(X)exp KX
k=1wkFk(X;Y)!
(17.24)
Z(X) =X
Y02Yexp KX
k=1wkFk(X;Y0)!
(17.25)
We’ll call these Kfunctions Fk(X;Y)global features , since each one is a property
of the entire input sequence Xand output sequence Y. We compute them by decom-
posing into a sum of local features for each position iinY:
Fk(X;Y) =nX
i=1fk(yi 1;yi;X;i) (17.26)
Each of these local features fkin a linear-chain CRF is allowed to make use of the
current output token yi, the previous output token yi 1, the entire input string X(or
any subpart of it), and the current position i. This constraint to only depend on
the current and previous output tokens yiandyi 1are what characterizes a linear
chain CRF . As we will see, this limitation makes it possible to use versions of thelinear chain
CRF
efﬁcient Viterbi and Forward-Backwards algorithms from the HMM. A general CRF,
by contrast, allows a feature to make use of any output token, and are thus necessary
for tasks in which the decision depend on distant output tokens, like yi 4. General
CRFs require more complex inference, and are less commonly used for language
processing.

--- Page 378 ---
378 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
17.5.1 Features in a CRF POS Tagger
Let’s look at some of these features in detail, since the reason to use a discriminative
sequence model is that it’s easier to incorporate a lot of features.2
Again, in a linear-chain CRF, each local feature fkat position ican depend on
any information from: (yi 1;yi;X;i). So some legal features representing common
situations might be the following:
1fxi=the;yi=DETg
1fyi=PROPN, xi+1=Street ,yi 1=NUMg
1fyi=VERB, yi 1=AUXg
For simplicity, we’ll assume all CRF features take on the value 1 or 0. Above, we
explicitly use the notation 1fxgto mean “1 if xis true, and 0 otherwise”. From now
on, we’ll leave off the 1when we deﬁne features, but you can assume each feature
has it there implicitly.
Although the idea of what features to use is done by the system designer by hand,
the speciﬁc features are automatically populated by using feature templates as wefeature
templates
brieﬂy mentioned in Chapter 5. Here are some templates that only use information
from (yi 1;yi;X;i):
hyi;xii;hyi;yi 1i;hyi;xi 1;xi+2i
These templates automatically populate the set of features from every instance in
the training and test set. Thus for our example Janet/NNP will/MD back/VB the/DT
bill/NN , when xiis the word back, the following features would be generated and
have the value 1 (we’ve assigned them arbitrary feature numbers):
f3743:yi= VB and xi= back
f156:yi= VB and yi 1= MD
f99732 :yi= VB and xi 1= will and xi+2= bill
It’s also important to have features that help with unknown words. One of the
most important is word shape features, which represent the abstract letter pattern word shape
of the word by mapping lower-case letters to ‘x’, upper-case to ‘X’, numbers to
’d’, and retaining punctuation. Thus for example I.M.F. would map to X.X.X. and
DC10-30 would map to XXdd-dd. A second class of shorter word shape features is
also used. In these features consecutive character types are removed, so words in all
caps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to
Xd-d but I.M.F would still map to X.X.X. Preﬁx and sufﬁx features are also useful.
In summary, here are some sample feature templates that help with unknown words:
xicontains a particular preﬁx (perhaps from all preﬁxes of length 2)
xicontains a particular sufﬁx (perhaps from all sufﬁxes of length 2)
xi’s word shape
xi’s short word shape
For example the word well-dressed might generate the following non-zero val-
ued feature values:
2Because in HMMs all computation is based on the two probabilities P(tagjtag)andP(wordjtag), if
we want to include some source of knowledge into the tagging process, we must ﬁnd a way to encode
the knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of
complicated conditioning which gets harder and harder as we have more and more such features.

--- Page 379 ---
17.5 • C ONDITIONAL RANDOM FIELDS (CRF S)379
preﬁx( xi) =w
preﬁx( xi) =we
sufﬁx( xi) =ed
sufﬁx( xi) =d
word-shape( xi) =xxxx-xxxxxxx
short-word-shape( xi) =x-x
The known-word templates are computed for every word seen in the training
set; the unknown word features can also be computed for all words in training, or
only on training words whose frequency is below some threshold. The result of the
known-word templates and word-signature features is a very large set of features.
Generally a feature cutoff is used in which features are thrown out if they have count
<5 in the training set.
Remember that in a CRF we don’t learn weights for each of these local features
fk. Instead, we ﬁrst sum the values of each local feature (for example feature f3743)
over the entire sentence, to create each global feature (for example F3743). It is those
global features that will then be multiplied by weight w3743. Thus for training and
inference there is always a ﬁxed set of Kfeatures with Kweights, even though the
length of each sentence is different.
17.5.2 Features for CRF Named Entity Recognizers
A CRF for NER makes use of very similar features to a POS tagger, as shown in
Figure 17.15.
identity of wi, identity of neighboring words
embeddings for wi, embeddings for neighboring words
part of speech of wi, part of speech of neighboring words
presence of wiin agazetteer
wicontains a particular preﬁx (from all preﬁxes of length 4)
wicontains a particular sufﬁx (from all sufﬁxes of length 4)
word shape of wi, word shape of neighboring words
short word shape of wi, short word shape of neighboring words
gazetteer features
Figure 17.15 Typical features for a feature-based NER system.
One feature that is especially useful for locations is a gazetteer , a list of place gazetteer
names, often providing millions of entries for locations with detailed geographical
and political information.3This can be implemented as a binary feature indicating a
phrase appears in the list. Other related resources like name-lists , for example from
the United States Census Bureau4, can be used, as can other entity dictionaries like
lists of corporations or products, although they may not be as helpful as a gazetteer
(Mikheev et al., 1999).
The sample named entity token L’Occitane would generate the following non-
zero valued feature values (assuming that L’Occitane is neither in the gazetteer nor
the census).
3www.geonames.org
4www.census.gov

--- Page 380 ---
380 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
preﬁx( xi) =L sufﬁx( xi) =tane
preﬁx( xi) =L' sufﬁx( xi) =ane
preﬁx( xi) =L'O sufﬁx( xi) =ne
preﬁx( xi) =L'Oc sufﬁx( xi) =e
word-shape( xi) =X'Xxxxxxxx short-word-shape( xi) =X'Xx
Figure 17.16 illustrates the result of adding part-of-speech tags and some shape
information to our earlier example.
Words POS Short shape Gazetteer BIO Label
Jane NNP Xx 0 B-PER
Villanueva NNP Xx 1 I-PER
of IN x 0 O
United NNP Xx 0 B-ORG
Airlines NNP Xx 0 I-ORG
Holding NNP Xx 0 I-ORG
discussed VBD x 0 O
the DT x 0 O
Chicago NNP Xx 1 B-LOC
route NN x 0 O
. . . 0 O
Figure 17.16 Some NER features for a sample sentence, assuming that Chicago and Vil-
lanueva are listed as locations in a gazetteer. We assume features only take on the values 0 or
1, so the ﬁrst POS feature, for example, would be represented as 1fPOS=NNPg.
17.5.3 Inference and Training for CRFs
How do we ﬁnd the best tag sequence ˆYfor a given input X? We start with Eq. 17.22:
ˆY=argmax
Y2YP(YjX)
=argmax
Y2Y1
Z(X)exp KX
k=1wkFk(X;Y)!
(17.27)
=argmax
Y2Yexp KX
k=1wknX
i=1fk(yi 1;yi;X;i)!
(17.28)
=argmax
Y2YKX
k=1wknX
i=1fk(yi 1;yi;X;i) (17.29)
=argmax
Y2YnX
i=1KX
k=1wkfk(yi 1;yi;X;i) (17.30)
We can ignore the exp function and the denominator Z(X), as we do above, because
exp doesn’t change the argmax, and the denominator Z(X)is constant for a given
observation sequence X.
How should we decode to ﬁnd this optimal tag sequence ˆ y? Just as with HMMs,
we’ll turn to the Viterbi algorithm, which works because, like the HMM, the linear-
chain CRF depends at each timestep on only one previous output token yi 1.
Concretely, this involves ﬁlling an NTarray with the appropriate values, main-
taining backpointers as we proceed. As with HMM Viterbi, when the table is ﬁlled,
we simply follow pointers back from the maximum value in the ﬁnal column to
retrieve the desired set of labels.

--- Page 381 ---
17.6 • E VALUATION OF NAMED ENTITY RECOGNITION 381
The requisite changes from HMM Viterbi have to do only with how we ﬁll each
cell. Recall from Eq. 17.19 that the recursive step of the Viterbi equation computes
the Viterbi value of time tfor state jas
vt(j) =Nmax
i=1vt 1(i)ai jbj(ot); 1jN;1<tT (17.31)
which is the HMM implementation of
vt(j) =Nmax
i=1vt 1(i)P(sjjsi)P(otjsj)1jN;1<tT (17.32)
The CRF requires only a slight change to this latter formula, replacing the aandb
prior and likelihood probabilities with the CRF features:
vt(j) =Nmax
i=1vt 1(i)KX
k=1wkfk(yt 1;yt;X;t)1jN;1<tT(17.33)
Learning in CRFs relies on the same supervised learning algorithms we presented
for logistic regression. Given a sequence of observations, feature functions, and cor-
responding outputs, we use stochastic gradient descent to train the weights to maxi-
mize the log-likelihood of the training corpus. The local nature of linear-chain CRFs
means that the forward-backward algorithm introduced for HMMs in Appendix A
can be extended to a CRF version that will efﬁciently compute the necessary deriva-
tives. As with logistic regression, L1 or L2 regularization is important.
17.6 Evaluation of Named Entity Recognition
Part-of-speech taggers are evaluated by the standard metric of accuracy . Named
entity recognizers are evaluated by recall ,precision , and F1measure . Recall that
recall is the ratio of the number of correctly labeled responses to the total that should
have been labeled; precision is the ratio of the number of correctly labeled responses
to the total labeled; and F-measure is the harmonic mean of the two.
To know if the difference between the F 1scores of two NER systems is a signif-
icant difference, we use the paired bootstrap test, or the similar randomization test
(Section 4.9).
For named entity tagging, the entity rather than the word is the unit of response.
Thus in the example in Fig. 17.16, the two entities Jane Villanueva andUnited Air-
lines Holding and the non-entity discussed would each count as a single response.
The fact that named entity tagging has a segmentation component which is not
present in tasks like text categorization or part-of-speech tagging causes some prob-
lems with evaluation. For example, a system that labeled Jane but not Jane Vil-
lanueva as a person would cause two errors, a false positive for O and a false nega-
tive for I-PER. In addition, using entities as the unit of response but words as the unit
of training means that there is a mismatch between the training and test conditions.
17.7 Further Details
In this section we summarize a few remaining details of the data and models for
part-of-speech tagging and NER, beginning with data. Since the algorithms we have

--- Page 382 ---
382 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
presented are supervised, having labeled data is essential for training and testing. A
wide variety of datasets exist for part-of-speech tagging and/or NER. The Universal
Dependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in
over a hundred languages, as do the Penn Treebanks in English, Chinese, and Arabic.
OntoNotes has corpora labeled for named entities in English, Chinese, and Arabic
(Hovy et al., 2006). Named entity tagged corpora are also available in particular
domains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al.,
2019).
17.7.1 Rule-based Methods
While machine learned (neural or CRF) sequence models are the norm in academic
research, commercial approaches to NER are often based on pragmatic combina-
tions of lists and rules, with some smaller amount of supervised machine learning
(Chiticariu et al., 2013). For example in the IBM System T architecture, a user
speciﬁes declarative constraints for tagging tasks in a formal query language that
includes regular expressions, dictionaries, semantic constraints, and other operators,
which the system compiles into an efﬁcient extractor (Chiticariu et al., 2018).
One common approach is to make repeated rule-based passes over a text, starting
with rules with very high precision but low recall, and, in subsequent stages, using
machine learning methods that take the output of the ﬁrst pass into account (an
approach ﬁrst worked out for coreference (Lee et al., 2017a)):
1. First, use high-precision rules to tag unambiguous entity mentions.
2. Then, search for substring matches of the previously detected names.
3. Use application-speciﬁc name lists to ﬁnd likely domain-speciﬁc mentions.
4. Finally, apply supervised sequence labeling techniques that use tags from pre-
vious stages as additional features.
Rule-based methods were also the earliest methods for part-of-speech tagging.
Rule-based taggers like the English Constraint Grammar system (Karlsson et al.
1995, V outilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:
(1) a morphological analyzer with tens of thousands of word stem entries returns all
parts of speech for a word, then (2) a large set of thousands of constraints are applied
to the input sentence to rule out parts of speech inconsistent with the context.
17.7.2 POS Tagging for Morphologically Rich Languages
Augmentations to tagging algorithms become necessary when dealing with lan-
guages with rich morphology like Czech, Hungarian and Turkish.
These productive word-formation processes result in a large vocabulary for these
languages: a 250,000 word token corpus of Hungarian has more than twice as many
word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while
a 10 million word token corpus of Turkish contains four times as many word types
as a similarly sized English corpus (Hakkani-T ¨ur et al., 2002). Large vocabular-
ies mean many unknown words, and these unknown words cause signiﬁcant per-
formance degradations in a wide variety of languages (including Czech, Slovene,
Estonian, and Romanian) (Haji ˇc, 2000).
Highly inﬂectional languages also have much more information than English
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). Because this information is important for tasks like pars-
ing and coreference resolution, part-of-speech taggers for morphologically rich lan-

--- Page 383 ---
17.8 • S UMMARY 383
guages need to label words with case and gender information. Tagsets for morpho-
logically rich languages are therefore sequences of morphological tags rather than a
single primitive tag. Here’s a Turkish example, in which the word izinhas three pos-
sible morphological/part-of-speech tags and meanings (Hakkani-T ¨ur et al., 2002):
1. Yerdeki izintemizlenmesi gerek. iz + Noun+A3sg+Pnon+Gen
The trace on the ﬂoor should be cleaned.
2.¨Uzerinde parmak izinkalmis ¸. iz + Noun+A3sg+P2sg+Nom
Your ﬁnger print is left on (it).
3. Ic ¸eri girmek ic ¸in izinalman gerekiyor. izin + Noun+A3sg+Pnon+Nom
You need permission to enter.
Using a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-
of-speech tag greatly increases the number of parts of speech, and so tagsets can
be 4 to 10 times larger than the 50–100 tags we have seen for English. With such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. The
role of the tagger is then to disambiguate among these tags. This method also helps
with unknown words since morphological parsers can accept unknown stems and
still segment the afﬁxes properly.
17.8 Summary
This chapter introduced parts of speech andnamed entities , and the tasks of part-
of-speech tagging andnamed entity recognition :
• Languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words , and open-class words like
nouns ,verbs ,adjectives . Various part-of-speech tagsets exist, of between 40
and 200 tags.
•Part-of-speech tagging is the process of assigning a part-of-speech label to
each of a sequence of words.
•Named entities are words for proper nouns referring mainly to people, places,
and organizations, but extended to many other types that aren’t strictly entities
or even proper nouns.
• Two common approaches to sequence modeling are a generative approach,
HMM tagging, and a discriminative approach, CRF tagging. We will see a
neural approach in following chapters.
• The probabilities in HMM taggers are estimated by maximum likelihood es-
timation on tag-labeled training corpora. The Viterbi algorithm is used for
decoding , ﬁnding the most likely tag sequence
•Conditional Random Fields orCRF taggers train a log-linear model that can
choose the best tag sequence given an observation sequence, based on features
that condition on the output tag, the prior output tag, the entire input sequence,
and the current timestep. They use the Viterbi algorithm for inference, to
choose the best sequence of tags, and a version of the Forward-Backward
algorithm (see Appendix A) for training,

--- Page 384 ---
384 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
Bibliographical and Historical Notes
What is probably the earliest part-of-speech tagger was part of the parser in Zellig
Harris’s Transformations and Discourse Analysis Project (TDAP), implemented be-
tween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),
although earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quences and the relative frequency of tags for a word preﬁgures modern algorithms.
The parser was implemented essentially as a cascade of ﬁnite-state transducers; see
Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.
The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had
three components: a lexicon, a morphological analyzer, and a context disambigua-
tor. The small 1500-word lexicon listed only function words and other irregular
words. The morphological analyzer used inﬂectional and derivational sufﬁxes to as-
sign part-of-speech classes. These were run over words to produce candidate parts
of speech which were then disambiguated by a set of 500 context rules by relying on
surrounding islands of unambiguous words. For example, one rule said that between
an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-
ADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used
the same architecture as Klein and Simmons (1963), with a bigger dictionary and
more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis
and Ku ˇcera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the
Brown corpus was then tagged by hand. All these early algorithms were based on
a two-stage architecture in which a dictionary was ﬁrst used to assign each word a
set of potential parts of speech, and then lists of handwritten disambiguation rules
winnowed the set down to a single part of speech per word.
Probabilities were used in tagging by Stolz et al. (1965) and a complete proba-
bilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The
Lancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown cor-
pus, was tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Mar-
shall 1987; Garside 1987), a probabilistic algorithm that approximated a simpliﬁed
HMM tagger. The algorithm used tag bigram probabilities, but instead of storing the
word likelihood of each tag, the algorithm marked tags either as rare (P(tagjword)<
:01)infrequent (P(tagjword)<:10) or normally frequent (P(tagjword)>:10).
DeRose (1988) developed a quasi-HMM algorithm, including the use of dy-
namic programming, although computing P(tjw)P(w)instead of P(wjt)P(w). The
same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the
ﬁrst implemented HMM tagger, described correctly in Church (1989), although
Church (1988) also described the computation incorrectly as P(tjw)P(w)instead
ofP(wjt)P(w). Church (p.c.) explained that he had simpliﬁed for pedagogical pur-
poses because using the probability P(tjw)made the idea seem more understandable
as “storing a lexicon in an almost standard form”.
Later taggers explicitly introduced the use of the hidden Markov model (Kupiec
1992; Weischedel et al. 1993; Sch ¨utze and Singer 1994). Merialdo (1994) showed
that fully unsupervised EM didn’t work well for the tagging task and that reliance
on hand-labeled data was important. Charniak et al. (1993) showed the importance
of the most frequent tag baseline; the 92.3% number we give above was from Abney
et al. (1999). See Brants (2000) for HMM tagger implementation details, includ-
ing the extension to trigram contexts, and the use of sophisticated unknown word
features; its performance is still close to state of the art taggers.

--- Page 385 ---
EXERCISES 385
Log-linear models for POS tagging were introduced by Ratnaparkhi (1996),
who introduced a system called MXPOST which implemented a maximum entropy
Markov model (MEMM), a slightly simpler version of a CRF. Around the same
time, sequence labelers were applied to the task of named entity tagging, ﬁrst with
HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once
CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-
Callum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005).
Neural approaches to NER mainly follow from the pioneering results of Collobert
et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word
and character-based embeddings as input followed shortly and became a standard
neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.
2016) followed by the more recent use of Transformers and BERT.
The idea of using letter sufﬁxes for unknown words is quite old; the early Klein
and Simmons (1963) system checked all ﬁnal letter sufﬁxes of lengths 1-5. The un-
known word features described on page 378 come mainly from Ratnaparkhi (1996),
with augmentations from Toutanova et al. (2003) and Manning (2011).
State of the art POS taggers use neural algorithms, either bidirectional RNNs or
Transformers like BERT; see Chapter 8 to Chapter 11. HMM (Brants 2000; Thede
and Harper 1999) and CRF tagger accuracies are likely just a tad lower.
Manning (2011) investigates the remaining 2.7% of errors in a high-performing
tagger (Toutanova et al., 2003). He suggests that a third or half of these remaining
errors are due to errors or inconsistencies in the training data, a third might be solv-
able with richer linguistic models, and for the remainder the task is underspeciﬁed
or unclear.
Supervised tagging relies heavily on in-domain training data hand-labeled by
experts. Ways to relax this assumption include unsupervised algorithms for cluster-
ing words into part-of-speech-like classes, summarized in Christodoulopoulos et al.
(2010), and ways to combine labeled and unlabeled data, for example by co-training
(Clark et al. 2003; Søgaard 2010).
See Householder (1995) for historical notes on parts of speech, and Sampson
(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.
Exercises
17.1 Find one tagging error in each of the following sentences that are tagged with
the Penn Treebank tagset:
1. I/PRP need/VBP a/DT ﬂight/NN from/IN Atlanta/NN
2. Does/VBZ this/DT ﬂight/NN serve/VB dinner/NNS
3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP
4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN ﬂights/NNS
17.2 Use the Penn Treebank tagset to tag each word in the following sentences
from Damon Runyon’s short stories. You may ignore punctuation. Some of
these are quite difﬁcult; do your best.
1. It is a nice night.
2. This crap game is over a garage in Fifty-second Street. . .
3. . . . Nobody ever takes the newspapers she sells . . .
4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a
mournful voice.

--- Page 386 ---
386 CHAPTER 17 • S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES
5. . . . I am sitting in Mindy’s restaurant putting on the geﬁllte ﬁsh, which is
a dish I am very fond of, . . .
6. When a guy and a doll get to taking peeks back and forth at each other,
why there you are indeed.
17.3 Now compare your tags from the previous exercise with one or two friend’s
answers. On which words did you disagree the most? Why?
17.4 Implement the “most likely tag” baseline. Find a POS-tagged training set,
and use it to compute for each word the tag that maximizes p(tjw). You will
need to implement a simple tokenizer to deal with sentence boundaries. Start
by assuming that all unknown words are NN and compute your error rate on
known and unknown words. Now write at least ﬁve rules to do a better job of
tagging unknown words, and show the difference in error rates.
17.5 Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus.
First split the corpus into a training set and test set. From the labeled training
set, train the transition and observation probabilities of the HMM tagger di-
rectly on the hand-tagged data. Then implement the Viterbi algorithm so you
can decode a test sentence. Now run your algorithm on the test set. Report its
error rate and compare its performance to the most frequent tag baseline.
17.6 Do an error analysis of your tagger. Build a confusion matrix and investigate
the most frequent errors. Propose some features for improving the perfor-
mance of your tagger on these errors.
17.7 Develop a set of regular expressions to recognize the character shape features
described on page 378.
17.8 The BIO and other labeling schemes given in this chapter aren’t the only
possible one. For example, the Btag can be reserved only for those situations
where an ambiguity exists between adjacent entities. Propose a new set of
BIOtags for use with your NER system. Experiment with it and compare its
performance with the schemes presented in this chapter.
17.9 Names of works of art (books, movies, video games, etc.) are quite different
from the kinds of named entities we’ve discussed in this chapter. Collect a
list of names of works of art from a particular category from a Web-based
source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list
and give examples of ways that the names in it are likely to be problematic for
the techniques described in this chapter.
17.10 Develop an NER system speciﬁc to the category of names that you collected
in the last exercise. Evaluate your system on a collection of text likely to
contain instances of these named entities.

--- Page 387 ---
CHAPTER
18Context-Free Grammars and
Constituency Parsing
Because the Night by Bruce Springsteen and Patti Smith
The Fire Next Time by James Baldwin
If on a winter’s night a traveler by Italo Calvino
Love Actually by Richard Curtis
Suddenly Last Summer by Tennessee Williams
A Scanner Darkly by Philip K. Dick
Six titles that are not constituents, from Geoffrey K. Pullum on
Language Log (who was pointing out their incredible rarity).
One morning I shot an elephant in my pajamas.
How he got into my pajamas I don’t know.
Groucho Marx, Animal Crackers , 1930
The study of grammar has an ancient pedigree. The grammar of Sanskrit was
described by the Indian grammarian P ¯an.ini sometime between the 7th and 4th cen-
turies BCE, in his famous treatise the As .t.¯adhy ¯ay¯ı (‘8 books’). And our word syntax syntax
comes from the Greek s´yntaxis , meaning “setting out together or arrangement”, and
refers to the way words are arranged together. We have seen syntactic notions in pre-
vious chapters like the use of part-of-speech categories (Chapter 17). In this chapter
and the next one we introduce formal models for capturing more sophisticated no-
tions of grammatical structure and algorithms for parsing these structures.
Our focus in this chapter is context-free grammars and the CKY algorithm
for parsing them. Context-free grammars are the backbone of many formal mod-
els of the syntax of natural language (and, for that matter, of computer languages).
Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse
trees (whether for context-free grammars or for the dependency or CCG formalisms
we introduce in following chapters) can be used in applications such as grammar
checking : sentence that cannot be parsed may have grammatical errors (or at least
be hard to read). Parse trees can be an intermediate stage of representation for for-
mal semantic analysis . And parsers and the grammatical structure they assign a
sentence are a useful text analysis tool for text data science applications that require
modeling the relationship of elements in sentences.
In this chapter we introduce context-free grammars, give a small sample gram-
mar of English, introduce more formal deﬁnitions of context-free grammars and
grammar normal form, and talk about treebanks : corpora that have been anno-
tated with syntactic structure. We then discuss parse ambiguity and the problems
it presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger
(CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic program-
ming approach to syntactic parsing. The CKY algorithm returns an efﬁcient repre-
sentation of the set of parse trees for a sentence, but doesn’t tell us which parse tree
is the right one. For that, we need to augment CKY with scores for each possible
constituent. We’ll see how to do this with neural span-based parsers. Finally, we’ll
introduce the standard set of metrics for evaluating parser accuracy.

--- Page 388 ---
388 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
18.1 Constituency
Syntactic constituency is the idea that groups of words can behave as single units,
or constituents. Part of developing a grammar involves building an inventory of the
constituents in the language. How do words group together in English? Consider
thenoun phrase , a sequence of words surrounding at least one noun. Here are some noun phrase
examples of noun phrases (thanks to Damon Runyon):
Harry the Horse a high-class spot such as Mindy’s
the Broadway coppers the reason he comes into the Hot Box
they three parties from Brooklyn
What evidence do we have that these words group together (or “form constituents”)?
One piece of evidence is that they can all appear in similar syntactic environments,
for example, before a verb.
three parties from Brooklyn arrive . . .
a high-class spot such as Mindy’s attracts . . .
the Broadway coppers love. . .
they sit
But while the whole noun phrase can occur before a verb, this is not true of each
of the individual words that make up a noun phrase. The following are not grammat-
ical sentences of English (recall that we use an asterisk (*) to mark fragments that
are not grammatical English sentences):
*from arrive . . .*asattracts . . .
*the is. . . *spot sat. . .
Thus, to correctly describe facts about the ordering of these words in English, we
must be able to say things like “ Noun Phrases can occur before verbs ”. Let’s now
see how to do this in a more formal way!
18.2 Context-Free Grammars
A widely used formal system for modeling constituent structure in natural lan-
guage is the context-free grammar , orCFG . Context-free grammars are also called CFG
phrase-structure grammars , and the formalism is equivalent to Backus-Naur form ,
orBNF . The idea of basing a grammar on constituent structure dates back to the psy-
chologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and,
independently, Backus (1959).
A context-free grammar consists of a set of rules orproductions , each of which rules
expresses the ways that symbols of the language can be grouped and ordered to-
gether, and a lexicon of words and symbols. For example, the following productions lexicon
express that an NP(ornoun phrase ) can be composed of either a ProperNoun or NP
a determiner ( Det) followed by a Nominal ; aNominal in turn can consist of one or

--- Page 389 ---
18.2 • C ONTEXT -FREE GRAMMARS 389
more Noun s.1
NP!Det Nominal
NP!ProperNoun
Nominal!NounjNominal Noun
Context-free rules can be hierarchically embedded, so we can combine the previous
rules with others, like the following, that express facts about the lexicon:
Det!a
Det!the
Noun!ﬂight
The symbols that are used in a CFG are divided into two classes. The symbols
that correspond to words in the language (“the”, “nightclub”) are called terminal terminal
symbols; the lexicon is the set of rules that introduce these terminal symbols. The
symbols that express abstractions over these terminals are called non-terminals . In non-terminal
each context-free rule, the item to the right of the arrow ( !) is an ordered list of one
or more terminals and non-terminals; to the left of the arrow is a single non-terminal
symbol expressing some cluster or generalization. The non-terminal associated with
each word in the lexicon is its lexical category, or part of speech.
A CFG can be thought of in two ways: as a device for generating sentences
and as a device for assigning a structure to a given sentence. Viewing a CFG as a
generator, we can read the !arrow as “rewrite the symbol on the left with the string
of symbols on the right”.
So starting from the symbol: NP
we can use our ﬁrst rule to rewrite NPas: Det Nominal
and then rewrite Nominal as: Noun
and ﬁnally rewrite these parts-of-speech as: a ﬂight
We say the string a ﬂight can be derived from the non-terminal NP. Thus, a CFG
can be used to generate a set of strings. This sequence of rule expansions is called a
derivation of the string of words. It is common to represent a derivation by a parse derivation
tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree parse tree
representation of this derivation.
NP
Nom
Noun
ﬂightDet
a
Figure 18.1 A parse tree for “a ﬂight”.
In the parse tree shown in Fig. 18.1, we can say that the node NPdominates dominates
all the nodes in the tree ( Det,Nom ,Noun ,a,ﬂight ). We can say further that it
immediately dominates the nodes DetandNom .
The formal language deﬁned by a CFG is the set of strings that are derivable
from the designated start symbol . Each grammar must have one designated start start symbol
1When talking about these rules we can pronounce the rightarrow !as “goes to”, and so we might
read the ﬁrst rule above as “NP goes to Det Nominal”.

--- Page 390 ---
390 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
symbol, which is often called S. Since context-free grammars are often used to deﬁne
sentences, Sis usually interpreted as the “sentence” node, and the set of strings that
are derivable from Sis the set of sentences in some simpliﬁed version of English.
Let’s add a few additional rules to our inventory. The following rule expresses
the fact that a sentence can consist of a noun phrase followed by a verb phrase : verb phrase
S!NP VP I prefer a morning ﬂight
A verb phrase in English consists of a verb followed by assorted other things;
for example, one kind of verb phrase consists of a verb followed by a noun phrase:
VP!Verb NP prefer a morning ﬂight
Or the verb may be followed by a noun phrase and a prepositional phrase:
VP!Verb NP PP leave Boston in the morning
Or the verb phrase may have a verb followed by a prepositional phrase alone:
VP!Verb PP leaving on Thursday
A prepositional phrase generally has a preposition followed by a noun phrase.
For example, a common type of prepositional phrase in the ATIS corpus is used to
indicate location or direction:
PP!Preposition NP from Los Angeles
TheNPinside a PPneed not be a location; PPs are often used with times and
dates, and with other nouns as well; they can be arbitrarily complex. Here are ten
examples from the ATIS corpus:
to Seattle on these ﬂights
in Minneapolis about the ground transportation in Chicago
on Wednesday of the round trip ﬂight on United Airlines
in the evening of the AP ﬁfty seven ﬂight
on the ninth of July with a stopover in Nashville
Figure 18.2 gives a sample lexicon, and Fig. 18.3 summarizes the grammar rules
we’ve seen so far, which we’ll call L0. Note that we can use the or-symbol jto
indicate that a non-terminal has alternate possible expansions.
Noun!ﬂightsjﬂightjbreezejtripjmorning
Verb!isjpreferjlikejneedjwantjﬂyjdo
Adjective!cheapestjnon-stopjﬁrstjlatest
jotherjdirect
Pronoun!mejIjyoujit
Proper-Noun!AlaskajBaltimorejLos Angeles
jChicagojUnitedjAmerican
Determiner!thejajanjthisjthesejthat
Preposition!fromjtojonjnearjin
Conjunction!andjorjbut
Figure 18.2 The lexicon for L0.
We can use this grammar to generate sentences of this “ATIS-language”. We
start with S, expand it to NP VP , then choose a random expansion of NP(let’s say, to

--- Page 391 ---
18.2 • C ONTEXT -FREE GRAMMARS 391
Grammar Rules Examples
S!NP VP I + want a morning ﬂight
NP!Pronoun I
jProper-Noun Los Angeles
jDet Nominal a + ﬂight
Nominal!Nominal Noun morning + ﬂight
jNoun ﬂights
VP!Verb do
jVerb NP want + a ﬂight
jVerb NP PP leave + Boston + in the morning
jVerb PP leaving + on Thursday
PP!Preposition NP from + Los Angeles
Figure 18.3 The grammar for L0, with example phrases for each rule.
S
VP
NP
Nom
Noun
ﬂightNom
Noun
morningDet
aVerb
preferNP
Pro
I
Figure 18.4 The parse tree for “I prefer a morning ﬂight” according to grammar L0.
I), and a random expansion of VP(let’s say, to Verb NP ), and so on until we generate
the string I prefer a morning ﬂight . Figure 18.4 shows a parse tree that represents a
complete derivation of I prefer a morning ﬂight .
We can also represent a parse tree in a more compact format called bracketed
notation ; here is the bracketed representation of the parse tree of Fig. 18.4:bracketed
notation
(18.1) [ S[NP[ProI]] [VP[Vprefer] [ NP[Deta] [Nom [Nmorning] [ Nom [Nﬂight]]]]]]
A CFG like that of L0deﬁnes a formal language. Sentences (strings of words)
that can be derived by a grammar are in the formal language deﬁned by that gram-
mar, and are called grammatical sentences. Sentences that cannot be derived by grammatical
a given formal grammar are not in the language deﬁned by that grammar and are
referred to as ungrammatical . This hard line between “in” and “out” characterizes ungrammatical
all formal languages but is only a very simpliﬁed model of how natural languages
really work. This is because determining whether a given sentence is part of a given
natural language (say, English) often depends on the context. In linguistics, the use
of formal languages to model natural languages is called generative grammar sincegenerative
grammar
the language is deﬁned by the set of possible sentences “generated” by the grammar.
(Note that this is a different sense of the word ‘generate’ than when we talk about

--- Page 392 ---
392 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
language models generating text.)
18.2.1 Formal Deﬁnition of Context-Free Grammar
We conclude this section with a quick, formal description of a context-free gram-
mar and the language it generates. A context-free grammar Gis deﬁned by four
parameters: N;S;R;S(technically it is a “4-tuple”).
Na set of non-terminal symbols (orvariables )
Sa set of terminal symbols (disjoint from N)
Ra set of rules or productions, each of the form A!b,
where Ais a non-terminal,
bis a string of symbols from the inﬁnite set of strings (S[N)
Sa designated start symbol and a member of N
For the remainder of the book we adhere to the following conventions when dis-
cussing the formal properties of context-free grammars (as opposed to explaining
particular facts about English or other languages).
Capital letters like A,B, and S Non-terminals
S The start symbol
Lower-case Greek letters like a,b, and g Strings drawn from (S[N)
Lower-case Roman letters like u,v, and w Strings of terminals
A language is deﬁned through the concept of derivation. One string derives an-
other one if it can be rewritten as the second one by some series of rule applications.
More formally, following Hopcroft and Ullman (1979),
ifA!bis a production of Randaandgare any strings in the set
(S[N), then we say that aAgdirectly derives abg , oraAg)abg . directly derives
Derivation is then a generalization of direct derivation:
Leta1;a2;:::; ambe strings in (S[N);m1, such that
a1)a2;a2)a3;:::;am 1)am
We say that a1derives am, ora1)am. derives
We can then formally deﬁne the language LGgenerated by a grammar Gas the
set of strings composed of terminal symbols that can be derived from the designated
start symbol S.
LG=fwjwis inSandS)wg
The problem of mapping from a string of words to its parse tree is called syn-
tactic parsing , as we’ll see in Section 18.6.syntactic
parsing
18.3 Treebanks
A corpus in which every sentence is annotated with a parse tree is called a treebank . treebank

--- Page 393 ---
18.3 • T REEBANKS 393
Treebanks play an important role in parsing as well as in linguistic investigations of
syntactic phenomena.
Treebanks are generally made by running a parser over each sentence and then
having the resulting parse hand-corrected by human linguists. Figure 18.5 shows
sentences from the Penn Treebank project, which includes various treebanks in Penn Treebank
English, Arabic, and Chinese. The Penn Treebank part-of-speech tagset was deﬁned
in Chapter 17, but we’ll see minor formatting differences across treebanks. The use
of LISP-style parenthesized notation for trees is extremely common and resembles
the bracketed notation we saw earlier in (18.1). For those who are not familiar with
it we show a standard node-and-line tree representation in Fig. 18.6.
((S
(NP-SBJ (DT That)
(JJ cold) (, ,)
(JJ empty) (NN sky) )
(VP (VBD was)
(ADJP-PRD (JJ full)
(PP (IN of)
(NP (NN fire)
(CC and)
(NN light) ))))
(. .) ))((S
(NP-SBJ The/DT flight/NN )
(VP should/MD
(VP arrive/VB
(PP-TMP at/IN
(NP eleven/CD a.m/RB ))
(NP-TMP tomorrow/NN )))))
(a) (b)
Figure 18.5 Parses from the LDC Treebank3 for (a) Brown and (b) ATIS sentences.
S
.
.VP
ADJP-PRD
PP
NP
NN
lightCC
andNN
ﬁreIN
ofJJ
fullVBD
wasNP-SBJ
NN
skyJJ
empty,
,JJ
coldDT
That
Figure 18.6 The tree corresponding to the Brown corpus sentence in the previous ﬁgure.
The sentences in a treebank implicitly constitute a grammar of the language. For
example, from the parsed sentences in Fig. 18.5 we can extract the CFG rules shown
in Fig. 18.7 (with rule sufﬁxes ( -SBJ ) stripped for simplicity). The grammar used
to parse the Penn Treebank is very ﬂat, resulting in very many rules. For example,

--- Page 394 ---
394 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
Grammar Lexicon
S!NP VP . DT!thejthat
S!NP VP JJ!coldjemptyjfull
NP!DT NN NN!skyjﬁrejlightjﬂightjtomorrow
NP!NN CC NN CC!and
NP!DT JJ , JJ NN IN!ofjat
NP!NN CD!eleven
VP!MD VP RB!a.m.
VP!VBD ADJP VB!arrive
VP!MD VP VBD!wasjsaid
VP!VB PP NP MD!shouldjwould
ADJP!JJ PP
PP!IN NP
PP!IN NP RB
Figure 18.7 CFG grammar rules and lexicon from the treebank sentences in Fig. 18.5.
among the approximately 4,500 different rules for expanding VPs are separate rules
for PP sequences of any length and every possible arrangement of verb arguments:
VP!VBD PP
VP!VBD PP PP
VP!VBD PP PP PP
VP!VBD PP PP PP PP
VP!VB ADVP PP
VP!VB PP ADVP
VP!ADVP VB PP
18.4 Grammar Equivalence and Normal Form
A formal language is deﬁned as a (possibly inﬁnite) set of strings of words. This sug-
gests that we could ask if two grammars are equivalent by asking if they generate the
same set of strings. In fact, it is possible to have two distinct context-free grammars
generate the same language. We say that two grammars are strongly equivalent ifstrongly
equivalent
they generate the same set of strings andif they assign the same phrase structure
to each sentence (allowing merely for renaming of the non-terminal symbols). Two
grammars are weakly equivalent if they generate the same set of strings but do notweakly
equivalent
assign the same phrase structure to each sentence.
It is sometimes useful to have a normal form for grammars, in which each of normal form
the productions takes a particular form. For example, a context-free grammar is in
Chomsky normal form (CNF) (Chomsky, 1963) if it is -free and if in additionChomsky
normal form
each production is either of the form A!B C orA!a. That is, the right-hand side
of each rule either has two non-terminal symbols or one terminal symbol. Chomsky
normal form grammars are binary branching , that is they have binary trees (downbinary
branching
to the prelexical nodes). We make use of this binary branching property in the CKY
parsing algorithm in Section 18.6.
Any context-free grammar can be converted into a weakly equivalent Chomsky
normal form grammar. For example, a rule of the form
A!B C D
can be converted into the following two CNF rules (Exercise 18.1 asks the reader to

--- Page 395 ---
18.5 • A MBIGUITY 395
Grammar Lexicon
S!NP VP Det!thatjthisjtheja
S!Aux NP VP Noun!bookjﬂightjmealjmoney
S!VP Verb!bookjincludejprefer
NP!Pronoun Pronoun!Ijshejme
NP!Proper-Noun Proper-Noun!HoustonjNWA
NP!Det Nominal Aux!does
Nominal!Noun Preposition!fromjtojonjnearjthrough
Nominal!Nominal Noun
Nominal!Nominal PP
VP!Verb
VP!Verb NP
VP!Verb NP PP
VP!Verb PP
VP!VP PP
PP!Preposition NP
Figure 18.8 TheL1miniature English grammar and lexicon.
formulate the complete algorithm):
A!B X
X!C D
Sometimes using binary branching can actually produce smaller grammars. For
example, the sentences that might be characterized as
VP -> VBD NP PP*
are represented in the Penn Treebank by this series of rules:
VP!VBD NP PP
VP!VBD NP PP PP
VP!VBD NP PP PP PP
VP!VBD NP PP PP PP PP
...
but could also be generated by the following two-rule grammar:
VP!VBD NP PP
VP!VP PP
The generation of a symbol A with a potentially inﬁnite sequence of symbols B with
a rule of the form A!A Bis known as Chomsky-adjunction .Chomsky-
adjunction
18.5 Ambiguity
Ambiguity is the most serious problem faced by syntactic parsers. Chapter 17 intro-
duced the notions of part-of-speech ambiguity andpart-of-speech disambigua-
tion. Here, we introduce a new kind of ambiguity, called structural ambiguity ,structural
ambiguity
illustrated with a new toy grammar L1, shown in Figure 18.8, which adds a few
rules to the L0grammar.
Structural ambiguity occurs when the grammar can assign more than one parse
to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal

--- Page 396 ---
396 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
S
VP
NP
Nominal
PP
in my pajamasNominal
Noun
elephantDet
anVerb
shotNP
Pronoun
IS
VP
PP
in my pajamasVP
NP
Nominal
Noun
elephantDet
anVerb
shotNP
Pronoun
I
Figure 18.9 Two parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous
reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which
Captain Spaulding did the shooting in his pajamas.
Crackers is ambiguous because the phrase in my pajamas can be part of the NP
headed by elephant or a part of the verb phrase headed by shot. Figure 18.9 illus-
trates these two analyses of Marx’s line using rules from L1.
Structural ambiguity, appropriately enough, comes in many forms. Two common
kinds of ambiguity are attachment ambiguity andcoordination ambiguity . A
sentence has an attachment ambiguity if a particular constituent can be attached toattachment
ambiguity
the parse tree at more than one place. The Groucho Marx sentence is an example
ofPP-attachment ambiguity : the preposition phrase can be attached either as partPP-attachment
ambiguity
of the NP or as part of the VP. Various kinds of adverbial phrases are also subject
to this kind of ambiguity. For instance, in the following example the gerundive-VP
ﬂying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower
or it can be an adjunct modifying the VP headed by saw:
(18.2) We saw the Eiffel Tower ﬂying to Paris.
Incoordination ambiguity phrases can be conjoined by a conjunction like and.coordination
ambiguity
For example, the phrase old men and women can be bracketed as [old [men and
women]] , referring to old men andold women , or as [old men] and [women] , in
which case it is only the men who are old. These ambiguities combine in complex
ways in real sentences, like the following news sentence from the Brown corpus:
(18.3) President Kennedy today pushed aside other White House business to
devote all his time and attention to working on the Berlin crisis address he
will deliver tomorrow night to the American people over nationwide
television and radio.
This sentence has a number of ambiguities, although since they are semantically
unreasonable, it requires a careful reading to see them. The last noun phrase could be
parsed [nationwide [television and radio]] or[[nationwide television] and radio] .
The direct object of pushed aside should be other White House business but could
also be the bizarre phrase [other White House business to devote all his time and
attention to working] (i.e., a structure like Kennedy afﬁrmed [his intention to propose
a new budget to address the deﬁcit] ). Then the phrase on the Berlin crisis address he

--- Page 397 ---
18.6 • CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 397
will deliver tomorrow night to the American people could be an adjunct modifying
the verb pushed . APPlikeover nationwide television and radio could be attached
to any of the higher VPs orNPs (e.g., it could modify people ornight ).
The fact that there are many grammatically correct but semantically unreason-
able parses for naturally occurring sentences is an irksome problem that affects all
parsers. Fortunately, the CKY algorithm below is designed to efﬁciently handle
structural ambiguities. And as we’ll see in the following section, we can augment
CKY with neural methods to choose a single correct parse by syntactic disambigua-
tion.syntactic
disambiguation
18.6 CKY Parsing: A Dynamic Programming Approach
Dynamic programming provides a powerful framework for addressing the prob-
lems caused by ambiguity in grammars. Recall that a dynamic programming ap-
proach systematically ﬁlls in a table of solutions to subproblems. The complete
table has the solution to all the subproblems needed to solve the problem as a whole.
In the case of syntactic parsing, these subproblems represent parse trees for all the
constituents detected in the input.
The dynamic programming advantage arises from the context-free nature of our
grammar rules—once a constituent has been discovered in a segment of the input we
can record its presence and make it available for use in any subsequent derivation
that might require it. This provides both time and storage efﬁciencies since subtrees
can be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-
Younger (CKY) algorithm, the most widely used dynamic-programming based ap-
proach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach,
and dynamic programming methods are often referred to as chart parsing methods. chart parsing
18.6.1 Conversion to Chomsky Normal Form
The CKY algorithm requires grammars to ﬁrst be in Chomsky Normal Form (CNF).
Recall from Section 18.4 that grammars in CNF are restricted to rules of the form
A!B CorA!w. That is, the right-hand side of each rule must expand either to
two non-terminals or to a single terminal. Restricting a grammar to CNF does not
lead to any loss in expressiveness, since any context-free grammar can be converted
into a corresponding CNF grammar that accepts exactly the same set of strings as
the original grammar.
Let’s start with the process of converting a generic CFG into one represented in
CNF. Assuming we’re dealing with an -free grammar, there are three situations we
need to address in any generic grammar: rules that mix terminals with non-terminals
on the right-hand side, rules that have a single non-terminal on the right-hand side,
and rules in which the length of the right-hand side is greater than 2.
The remedy for rules that mix terminals and non-terminals is to simply introduce
a new dummy non-terminal that covers only the original terminal. For example, a
rule for an inﬁnitive verb phrase such as INF-VP!to VP would be replaced by the
two rules INF-VP!TO VP andTO!to.
Rules with a single non-terminal on the right are called unit productions . WeUnit
productions
can eliminate unit productions by rewriting the right-hand side of the original rules
with the right-hand side of all the non-unit production rules that they ultimately lead
to. More formally, if A)Bby a chain of one or more unit productions and B!g

--- Page 398 ---
398 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
is a non-unit production in our grammar, then we add A!gfor each such rule in
the grammar and discard all the intervening unit productions. As we demonstrate
with our toy grammar, this can lead to a substantial ﬂattening of the grammar and a
consequent promotion of terminals to fairly high levels in the resulting trees.
Rules with right-hand sides longer than 2 are normalized through the introduc-
tion of new non-terminals that spread the longer sequences over several new rules.
Formally, if we have a rule like
A!B Cg
we replace the leftmost pair of non-terminals with a new non-terminal and introduce
a new production, resulting in the following new rules:
A!X1g
X1!B C
In the case of longer right-hand sides, we simply iterate this process until the of-
fending rule has been replaced by rules of length 2. The choice of replacing the
leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results
in binary rules would sufﬁce.
In our current grammar, the rule S!Aux NP VP would be replaced by the two
rules S!X1 VP andX1!Aux NP .
The entire conversion process can be summarized as follows:
1. Copy all conforming rules to the new grammar unchanged.
2. Convert terminals within rules to dummy non-terminals.
3. Convert unit productions.
4. Make all rules binary and add them to new grammar.
Figure 18.10 shows the results of applying this entire conversion procedure to
theL1grammar introduced earlier on page 395. Note that this ﬁgure doesn’t show
the original lexical rules; since these original lexical rules are already in CNF, they
all carry over unchanged to the new grammar. Figure 18.10 does, however, show
the various places where the process of eliminating unit productions has, in effect,
created new lexical rules. For example, all the original verbs have been promoted to
both VPs and to Ss in the converted grammar.
18.6.2 CKY Recognition
With our grammar now in CNF, each non-terminal node above the part-of-speech
level in a parse tree will have exactly two daughters. A two-dimensional matrix can
be used to encode the structure of an entire tree. For a sentence of length n, we will
work with the upper-triangular portion of an (n+1)(n+1)matrix. Each cell [i;j]
in this matrix contains the set of non-terminals that represent all the constituents that
span positions ithrough jof the input. Since our indexing scheme begins with 0, it’s
natural to think of the indexes as pointing at the gaps between the input words (as in
0Book 1that 2ﬂight 3). These gaps are often called fenceposts , on the metaphor of fenceposts
the posts between segments of fencing. It follows then that the cell that represents
the entire input resides in position [0;n]in the matrix.
Since each non-terminal entry in our table has two daughters in the parse, it fol-
lows that for each constituent represented by an entry [i;j], there must be a position
in the input, k, where it can be split into two parts such that i<k<j. Given such

--- Page 399 ---
18.6 • CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 399
L1Grammar L1in CNF
S!NP VP S!NP VP
S!Aux NP VP S!X1 VP
X1!Aux NP
S!VP S!bookjincludejprefer
S!Verb NP
S!X2 PP
S!Verb PP
S!VP PP
NP!Pronoun NP!Ijshejme
NP!Proper-Noun NP!TWAjHouston
NP!Det Nominal NP!Det Nominal
Nominal!Noun Nominal!bookjﬂightjmealjmoney
Nominal!Nominal Noun Nominal!Nominal Noun
Nominal!Nominal PP Nominal!Nominal PP
VP!Verb VP!bookjincludejprefer
VP!Verb NP VP!Verb NP
VP!Verb NP PP VP!X2 PP
X2!Verb NP
VP!Verb PP VP!Verb PP
VP!VP PP VP!VP PP
PP!Preposition NP PP!Preposition NP
Figure 18.10 L1Grammar and its conversion to CNF. Note that although they aren’t shown
here, all the original lexical entries from L1carry over unchanged as well.
a position k, the ﬁrst constituent [i;k]must lie to the left of entry [i;j]somewhere
along row i, and the second entry [k;j]must lie beneath it, along column j.
To make this more concrete, consider the following example with its completed
parse matrix, shown in Fig. 18.11.
(18.4) Book the ﬂight through Houston.
The superdiagonal row in the matrix contains the parts of speech for each word in
the input. The subsequent diagonals above that superdiagonal contain constituents
that cover all the spans of increasing length in the input.
Given this setup, CKY recognition consists of ﬁlling the parse table in the right
way. To do this, we’ll proceed in a bottom-up fashion so that at the point where we
are ﬁlling any cell [i;j], the cells containing the parts that could contribute to this
entry (i.e., the cells to the left and the cells below) have already been ﬁlled. The
algorithm given in Fig. 18.12 ﬁlls the upper-triangular matrix a column at a time
working from left to right, with each column ﬁlled from bottom to top, as the right
side of Fig. 18.11 illustrates. This scheme guarantees that at each point in time we
have all the information we need (to the left, since all the columns to the left have
already been ﬁlled, and below since we’re ﬁlling bottom to top). It also mirrors on-
line processing, since ﬁlling the columns from left to right corresponds to processing
each word one at a time.
The outermost loop of the algorithm given in Fig. 18.12 iterates over the columns,
and the second loop iterates over the rows, from the bottom up. The purpose of the
innermost loop is to range over all the places where a substring spanning itojin
the input might be split in two. As kranges over the places where the string can be
split, the pairs of cells we consider move, in lockstep, to the right along row iand
down along column j. Figure 18.13 illustrates the general case of ﬁlling cell [i;j].

--- Page 400 ---
400 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2S,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]
Figure 18.11 Completed parse table for Book the ﬂight through Houston.
function CKY-P ARSE (words, grammar )returns table
forj from 1toLENGTH (words )do
for all fAjA!words [j]2grammar g
table [j 1;j] table [j 1;j][A
fori from j 2down to 0do
fork i+1toj 1do
for all fAjA!BC2grammar andB2table[i;k]andC2table[k;j]g
table [i,j] table [i,j][A
Figure 18.12 The CKY algorithm.
At each such split, the algorithm considers whether the contents of the two cells can
be combined in a way that is sanctioned by a rule in the grammar. If such a rule
exists, the non-terminal on its left-hand side is entered into the table.
Figure 18.14 shows how the ﬁve cells of column 5 of the table are ﬁlled after the
word Houston is read. The arrows point out the two spans that are being used to add
an entry to the table. Note that the action in cell [0;5]indicates the presence of three
alternative parses for this input, one where the PPmodiﬁes the ﬂight , one where
it modiﬁes the booking, and one that captures the second argument in the original
VP!Verb NP PP rule, now captured indirectly with the VP!X2 PP rule.
18.6.3 CKY Parsing
The algorithm given in Fig. 18.12 is a recognizer, not a parser. That is, it can tell
us whether a valid parse exists for a given sentence based on whether or not if ﬁnds
anSin cell [0;n], but it can’t provide the derivation, which is the actual job for a
parser. To turn it into a parser capable of returning all possible parses for a given
input, we can make two simple changes to the algorithm: the ﬁrst change is to
augment the entries in the table so that each non-terminal is paired with pointers to
the table entries from which it was derived (more or less as shown in Fig. 18.14), the
second change is to permit multiple versions of the same non-terminal to be entered
into the table (again as shown in Fig. 18.14). With these changes, the completed
table contains all the possible parses for a given input. Returning an arbitrary single

--- Page 401 ---
18.6 • CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 401
...
...[0,n]
[i,i+1][i,i+2][i,j-2][i,j-1][i+1,j][i+2,j]
[j-1,j][j-2,j][i,j]...[0,1]
[n-1, n]
Figure 18.13 All the ways to ﬁll the [ i,j]th cell in the CKY table.
parse consists of choosing an Sfrom cell [0;n]and then recursively retrieving its
component constituents from the table. Of course, instead of returning every parse
for a sentence, we usually want just the best parse; we’ll see how to do that in the
next section.
18.6.4 CKY in Practice
Finally, we should note that while the restriction to CNF does not pose a problem
theoretically, it does pose some non-trivial problems in practice. The returned CNF
trees may not be consistent with the original grammar built by the grammar devel-
opers, and will complicate any syntax-driven approach to semantic analysis.
One approach to getting around these problems is to keep enough information
around to transform our trees back to the original grammar as a post-processing step
of the parse. This is trivial in the case of the transformation used for rules with length
greater than 2. Simply deleting the new dummy non-terminals and promoting their
daughters restores the original tree.
In the case of unit productions, it turns out to be more convenient to alter the ba-
sic CKY algorithm to handle them directly than it is to store the information needed
to recover the correct trees. Exercise 18.3 asks you to make this change. Many of
the probabilistic parsers presented in Appendix C use the CKY algorithm altered in

--- Page 402 ---
402 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNominal,NounNominalPrepNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]
Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]
Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]S2, VPS3S1,VP, X2
Figure 18.14 Filling the cells of column 5 after reading the word Houston .

--- Page 403 ---
18.7 • S PAN-BASED NEURAL CONSTITUENCY PARSING 403
just this manner.
18.7 Span-Based Neural Constituency Parsing
While the CKY parsing algorithm we’ve seen so far does great at enumerating all
the possible parse trees for a sentence, it has a large problem: it doesn’t tell us which
parse is the correct one! That is, it doesn’t disambiguate among the possible parses.
To solve the disambiguation problem we’ll use a simple neural extension of the
CKY algorithm. The intuition of such parsing algorithms (often called span-based
constituency parsing , orneural CKY ), is to train a neural classiﬁer to assign a
score to each constituent, and then use a modiﬁed version of CKY to combine these
constituent scores to ﬁnd the best-scoring parse tree.
Here we’ll describe a version of the algorithm from Kitaev et al. (2019). This
parser learns to map a span of words to a constituent, and, like CKY , hierarchically
combines larger and larger spans to build the parse-tree bottom-up. But unlike clas-
sic CKY , this parser doesn’t use the hand-written grammar to constrain what con-
stituents can be combined, instead just relying on the learned neural representations
of spans to encode likely combinations.
18.7.1 Computing Scores for a Span
Let’s begin by considering just the constituent (we’ll call it a span ) that lies between span
fencepost positions iand jwith non-terminal symbol label l. We’ll build a system
to assign a score s(i;j;l)to this constituent span.
ENCODER[START]BooktheflightthroughHouston[END]map to subwordsmap back to words013245MLPi=1hj-hij=3NPCompute score for spanRepresent spanCKY for computing best parse
postprocessing layers
Figure 18.15 A simpliﬁed outline of computing the span score for the span the ﬂight with
the label NP.
Fig. 18.15 sketches the architecture. The input word tokens are embedded by

--- Page 404 ---
404 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
passing them through a pretrained language model like BERT. Because BERT oper-
ates on the level of subword (wordpiece) tokens rather than words, we’ll ﬁrst need to
convert the BERT outputs to word representations. One standard way of doing this
is to simply use the ﬁrst subword unit as the representation for the entire word; us-
ing the last subword unit, or the sum of all the subword units are also common. The
embeddings can then be passed through some postprocessing layers; Kitaev et al.
(2019), for example, use 8 Transformer layers.
The resulting word encoder outputs ytare then used to compute a span score.
First, we must map the word encodings (indexed by word positions) to span encod-
ings (indexed by fenceposts). We do this by representing each fencepost with two
separate values; the intuition is that a span endpoint to the right of a word represents
different information than a span endpoint to the left of a word. We convert each
word output ytinto a (leftward-pointing) value for spans ending at this fencepost,  yt, and a (rightward-pointing) value  !ytfor spans beginning at this fencepost, by
splitting ytinto two halves. Each span then stretches from one double-vector fence-
post to another, as in the following representation of the ﬂight , which is span (1;3):
START 0 Book the ﬂight through
y0  !y0  y1y1  !y1  y2 y2  !y2  y3y3  !y3  y4 y4  !y4  y5:::
0 1 2 3 4
span(1,3)
A traditional way to represent a span, developed originally for RNN-based models
(Wang and Chang, 2016), but extended also to Transformers, is to take the differ-
ence between the embeddings of its start and end, i.e., representing span (i;j)by
subtracting the embedding of ifrom the embedding of j. Here we represent a span
by concatenating the difference of each of its fencepost components:
v(i;j) = [  !yj   !yi;   yj+1    yi+1] (18.5)
The span vector vis then passed through an MLP span classiﬁer, with two fully-
connected layers and one ReLU activation function, whose output dimensionality is
the number of possible non-terminal labels:
s(i;j;) =W2ReLU (LayerNorm (W1v(i;j))) (18.6)
The MLP then outputs a score for each possible non-terminal.
18.7.2 Integrating Span Scores into a Parse
Now we have a score for each labeled constituent span s(i;j;l). But we need a score
for an entire parse tree. Formally a tree Tis represented as a set of jTjsuch labeled
spans, with the tthspan starting at position itand ending at position jt, with label lt:
T=f(it;jt;lt):t=1;:::;jTjg (18.7)
Thus once we have a score for each span, the parser can compute a score for the
whole tree s(T)simply by summing over the scores of its constituent spans:
s(T) =X
(i;j;l)2Ts(i;j;l) (18.8)

--- Page 405 ---
18.8 • E VALUATING PARSERS 405
And we can choose the ﬁnal parse tree as the tree with the maximum score:
ˆT=argmax
Ts(T) (18.9)
The simplest method to produce the most likely parse is to greedily choose the
highest scoring label for each span. This greedy method is not guaranteed to produce
a tree, since the best label for a span might not ﬁt into a complete tree. In practice,
however, the greedy method tends to ﬁnd trees; in their experiments Gaddy et al.
(2018) ﬁnds that 95% of predicted bracketings form valid trees.
Nonetheless it is more common to use a variant of the CKY algorithm to ﬁnd the
full parse. The variant deﬁned in Gaddy et al. (2018) works as follows. Let’s deﬁne
sbest(i;j)as the score of the best subtree spanning (i;j). For spans of length one, we
choose the best label:
sbest(i;i+1) =max
ls(i;i+1;l) (18.10)
For other spans (i;j), the recursion is:
sbest(i;j) = max
ls(i;j;l)
+max
k[sbest(i;k)+sbest(k;j)] (18.11)
Note that the parser is using the max label for span (i;j)+ the max labels for spans
(i;k)and(k;j)without worrying about whether those decisions make sense given a
grammar. The role of the grammar in classical parsing is to help constrain possible
combinations of constituents (NPs like to be followed by VPs). By contrast, the
neural model seems to learn these kinds of contextual constraints during its mapping
from spans to non-terminals.
For more details on span-based parsing, including the margin-based training al-
gorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and
Kitaev et al. (2019).
18.8 Evaluating Parsers
The standard tool for evaluating parsers that assign a single parse tree to a sentence
is the PARSEV AL metrics (Black et al., 1991). The PARSEV AL metric measures PARSEV AL
how much the constituents in the hypothesis parse tree look like the constituents in a
hand-labeled, reference parse. PARSEV AL thus requires a human-labeled reference
(or “gold standard”) parse tree for each sentence in the test set; we generally draw
these reference parses from a treebank like the Penn Treebank.
A constituent in a hypothesis parse Chof a sentence sis labeled correct if there
is a constituent in the reference parse Crwith the same starting point, ending point,
and non-terminal symbol. We can then measure the precision and recall just as for
tasks we’ve seen already like named entity tagging:
labeled recall: =# of correct constituents in hypothesis parse of s
# of total constituents in reference parse of s
labeled precision: =# of correct constituents in hypothesis parse of s
# of total constituents in hypothesis parse of s

--- Page 406 ---
406 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
S(dumped)
VP(dumped)
PP(into)
NP(bin)
NN(bin)
binDT(a)
aP
intoNP(sacks)
NNS(sacks)
sacksVBD(dumped)
dumpedNP(workers)
NNS(workers)
workers
Figure 18.16 A lexicalized tree from Collins (1999).
As usual, we often report a combination of the two, F 1:
F1=2PR
P+R(18.12)
We additionally use a new metric, crossing brackets, for each sentence s:
cross-brackets: the number of constituents for which the reference parse has a
bracketing such as ((A B) C) but the hypothesis parse has a bracketing such
as (A (B C)).
For comparing parsers that use different grammars, the PARSEV AL metric in-
cludes a canonicalization algorithm for removing information likely to be grammar-
speciﬁc (auxiliaries, pre-inﬁnitival “to”, etc.) and for computing a simpliﬁed score
(Black et al., 1991). The canonical implementation of the PARSEV AL metrics is
called evalb (Sekine and Collins, 1997). evalb
18.9 Heads and Head-Finding
Syntactic constituents can be associated with a lexical head ;Nis the head of an NP,
Vis the head of a VP. This idea of a head for each constituent dates back to Bloom-
ﬁeld 1914, and is central to the dependency grammars and dependency parsing we’ll
introduce in Chapter 19. Indeed, heads can be used as a way to map between con-
stituency and dependency parses. Heads are also important in probabilistic pars-
ing (Appendix C) and in constituent-based grammar formalisms like Head-Driven
Phrase Structure Grammar (Pollard and Sag, 1994)..
In one simple model of lexical heads, each context-free rule is associated with
a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is
grammatically the most important. Heads are passed up the parse tree; thus, each
non-terminal in a parse tree is annotated with a single word, which is its lexical head.
Figure 18.16 shows an example of such a tree from Collins (1999), in which each
non-terminal is annotated with its head.
For the generation of such a tree, each CFG rule must be augmented to identify
one right-side constituent to be the head child. The headword for a node is then set to
the headword of its head child. Choosing these head children is simple for textbook
examples ( NNis the head of NP) but is complicated and indeed controversial for

--- Page 407 ---
18.10 • S UMMARY 407
most phrases. (Should the complementizer toor the verb be the head of an inﬁnite
verb phrase?) Modern linguistic theories of syntax generally include a component
that deﬁnes heads (see, e.g., (Pollard and Sag, 1994)).
An alternative approach to ﬁnding a head is used in most practical computational
systems. Instead of specifying head rules in the grammar itself, heads are identiﬁed
dynamically in the context of trees for speciﬁc sentences. In other words, once
a sentence is parsed, the resulting tree is walked to decorate each node with the
appropriate head. Most current systems rely on a simple set of handwritten rules,
such as a practical one for Penn Treebank grammars given in Collins (1999) but
developed originally by Magerman (1995). For example, the rule for ﬁnding the
head of an NPis as follows (Collins, 1999, p. 238):
• If the last word is tagged POS, return last-word.
• Else search from right to left for the ﬁrst child which is an NN, NNP, NNPS, NX, POS,
or JJR.
• Else search from left to right for the ﬁrst child which is an NP.
• Else search from right to left for the ﬁrst child which is a $, ADJP, or PRN.
• Else search from right to left for the ﬁrst child which is a CD.
• Else search from right to left for the ﬁrst child which is a JJ, JJS, RB or QP.
• Else return the last word
Selected other rules from this set are shown in Fig. 18.17. For example, for VP
rules of the form VP!Y1Yn, the algorithm would start from the left of Y1
Ynlooking for the ﬁrst Yiof type TO; if no TOs are found, it would search for the
ﬁrstYiof type VBD; if no VBDs are found, it would search for a VBN, and so on.
See Collins (1999) for more details.
Parent Direction Priority List
ADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS
SBAR RB
ADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
PRN Left
PRT Right RP
QP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
S Left TO IN VP S SBAR ADJP UCP NP
SBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG
VP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP
Figure 18.17 Some head rules from Collins (1999). The head rules are also called a head percolation table .
18.10 Summary
This chapter introduced constituency parsing. Here’s a summary of the main points:
• In many languages, groups of consecutive words act as a group or a con-
stituent , which can be modeled by context-free grammars (which are also
known as phrase-structure grammars ).
• A context-free grammar consists of a set of rules orproductions , expressed
over a set of non-terminal symbols and a set of terminal symbols. Formally,
a particular context-free language is the set of strings that can be derived
from a particular context-free grammar .

--- Page 408 ---
408 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
•Structural ambiguity is a signiﬁcant problem for parsers. Common sources
of structural ambiguity include PP-attachment andcoordination ambiguity .
•Dynamic programming parsing algorithms, such as CKY , use a table of
partial parses to efﬁciently parse ambiguous sentences.
•CKY restricts the form of the grammar to Chomsky normal form (CNF).
• The basic CKY algorithm compactly represents all possible parses of the sen-
tence but doesn’t choose a single best parse.
• Choosing a single parse from all possible parses ( disambiguation ) can be
done by neural constituency parsers .
• Span-based neural constituency parses train a neural classiﬁer to assign a score
to each constituent, and then use a modiﬁed version of CKY to combine these
constituent scores to ﬁnd the best-scoring parse tree.
• Parsers are evaluated with three metrics: labeled recall ,labeled precision ,
andcross-brackets .
•Partial parsing andchunking are methods for identifying shallow syntac-
tic constituents in a text. They are solved by sequence models trained on
syntactically-annotated data.
Bibliographical and Historical Notes
According to Percival (1976), the idea of breaking up a sentence into a hierarchy of
constituents appeared in the V¨olkerpsychologie of the groundbreaking psychologist
Wilhelm Wundt (Wundt, 1900):
...den sprachlichen Ausdruck f ¨ur die willk ¨urliche Gliederung einer Ge-
sammtvorstellung in ihre in logische Beziehung zueinander gesetzten
Bestandteile
[the linguistic expression for the arbitrary division of a total idea
into its constituent parts placed in logical relations to one another]
Wundt’s idea of constituency was taken up into linguistics by Leonard Bloom-
ﬁeld in his early book An Introduction to the Study of Language (Bloomﬁeld, 1914).
By the time of his later book, Language (Bloomﬁeld, 1933), what was then called
“immediate-constituent analysis” was a well-established method of syntactic study
in the United States. By contrast, traditional European grammar, dating from the
Classical period, deﬁned relations between words rather than constituents, and Eu-
ropean syntacticians retained this emphasis on such dependency grammars, the sub-
ject of Chapter 19. (And indeed, both dependency and constituency grammars have
been in vogue in computational linguistics at different times).
American Structuralism saw a number of speciﬁc deﬁnitions of the immediate
constituent, couched in terms of their search for a “discovery procedure”: a method-
ological algorithm for describing the syntax of a language. In general, these attempt
to capture the intuition that “The primary criterion of the immediate constituent
is the degree in which combinations behave as simple units” (Bazell, 1952/1966, p.
284). The most well known of the speciﬁc deﬁnitions is Harris’ idea of distributional
similarity to individual units, with the substitutability test. Essentially, the method
proceeded by breaking up a construction into constituents by attempting to substitute
simple structures for possible constituents—if a substitution of a simple form, say,

--- Page 409 ---
EXERCISES 409
man, was substitutable in a construction for a more complex set (like intense young
man), then the form intense young man was probably a constituent. Harris’s test was
the beginning of the intuition that a constituent is a kind of equivalence class.
The context-free grammar was a formalization of this idea of hierarchical
constituency deﬁned in Chomsky (1956) and further expanded upon (and argued
against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky’s
initial work, the context-free grammar was reinvented by Backus (1959) and inde-
pendently by Naur et al. (1960) in their descriptions of the ALGOL programming
language; Backus (1996) noted that he was inﬂuenced by the productions of Emil
Post and that Naur’s work was independent of his (Backus’) own. After this early
work, a great number of computational models of natural language processing were
based on context-free grammars because of the early development of efﬁcient pars-
ing algorithms.
Dynamic programming parsing has a history of independent discovery. Ac-
cording to the late Martin Kay (personal communication), a dynamic programming
parser containing the roots of the CKY algorithm was ﬁrst implemented by John
Cocke in 1960. Later work extended and formalized the algorithm, as well as prov-
ing its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related well-
formed substring table (WFST ) seems to have been independently proposed by WFST
Kuno (1965) as a data structure that stores the results of all previous computations
in the course of the parse. Based on a generalization of Cocke’s work, a similar
data structure had been independently described in Kay (1967) (and Kay 1973). The
top-down application of dynamic programming to parsing was described in Earley’s
Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence
of the WFST and the Earley algorithm. Norvig (1991) shows that the efﬁciency of-
fered by dynamic programming can be captured in any language with a memoization
function (such as in LISP) simply by wrapping the memoization operation around a
simple top-down parser.
The earliest disambiguation algorithms for parsing were based on probabilistic
context-free grammars , ﬁrst worked out by Booth (1969) and Salomaa (1969); seeprobabilistic
context-free
grammarsAppendix C for more history. Neural methods were ﬁrst applied to parsing at around
the same time as statistical parsing methods were developed (Henderson, 1994). In
the earliest work neural networks were used to estimate some of the probabilities for
statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)
. The next decades saw a wide variety of neural parsing algorithms, including re-
cursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models
(Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans
(Cross and Huang, 2016). For more on the span-based self-attention approach we
describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein
(2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural
dependency parsing.
The classic reference for parsing algorithms is Aho and Ullman (1972); although
the focus of that book is on computer languages, most of the algorithms have been
applied to natural language.
Exercises
18.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.

--- Page 410 ---
410 CHAPTER 18 • C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING
Apply your program to the L1grammar.
18.2 Implement the CKY algorithm and test it with your converted L1grammar.
18.3 Rewrite the CKY algorithm given in Fig. 18.12 on page 400 so that it can
accept grammars that contain unit productions.
18.4 Discuss how to augment a parser to deal with input that may be incorrect, for
example, containing spelling errors or mistakes arising from automatic speech
recognition.
18.5 Implement the PARSEV AL metrics described in Section 18.8. Next, use a
parser and a treebank, compare your metrics against a standard implementa-
tion. Analyze the errors in your approach.

--- Page 411 ---
CHAPTER
19Dependency Parsing
Tout mot qui fait partie d’une phrase... Entre lui et ses voisins, l’esprit aperc ¸oit
des connexions, dont l’ensemble forme la charpente de la phrase.
[Between each word in a sentence and its neighbors, the mind perceives con-
nections . These connections together form the scaffolding of the sentence.]
Lucien Tesni `ere. 1959. ´El´ements de syntaxe structurale, A.1.§4
The focus of the last chapter was on context-free grammars and constituent-
based representations. Here we present another important family of grammar for-
malisms called dependency grammars . In dependency formalisms, phrasal con-dependency
grammars
stituents and phrase-structure rules do not play a direct role. Instead, the syntactic
structure of a sentence is described solely in terms of directed binary grammatical
relations between the words , as in the following dependency parse:
Iprefer themorning ﬂight through Denvernsubjobj
det
compoundnmod
caseroot
(19.1)
Relations among the words are illustrated above the sentence with directed, labeled
arcs from heads todependents . We call this a typed dependency structure becausetyped
dependency
the labels are drawn from a ﬁxed inventory of grammatical relations. A root node
explicitly marks the root of the tree, the head of the entire structure.
Figure 19.1 on the next page shows the dependency analysis from (19.1) but vi-
sualized as a tree, alongside its corresponding phrase-structure analysis of the kind
given in the prior chapter. Note the absence of nodes corresponding to phrasal con-
stituents or lexical categories in the dependency parse; the internal structure of the
dependency parse consists solely of directed relations between words. These head-
dependent relationships directly encode important information that is often buried in
the more complex phrase-structure parses. For example, the arguments to the verb
prefer are directly linked to it in the dependency structure, while their connection
to the main verb is more distant in the phrase-structure tree. Similarly, morning
andDenver , modiﬁers of ﬂight , are linked to it directly in the dependency structure.
This fact that the head-dependent relations are a good proxy for the semantic rela-
tionship between predicates and their arguments is an important reason why depen-
dency grammars are currently more common than constituency grammars in natural
language processing.
Another major advantage of dependency grammars is their ability to deal with
languages that have a relatively free word order . For example, word order in Czech free word order
can be much more ﬂexible than in English; a grammatical object might occur before
or after a location adverbial . A phrase-structure grammar would need a separate rule

--- Page 412 ---
412 CHAPTER 19 • D EPENDENCY PARSING
prefer
ﬂight
Denver
throughmorning theIS
VP
NP
Nom
PP
NP
Pro
DenverP
throughNom
Noun
ﬂightNom
Noun
morningDet
theVerb
preferNP
Pro
I
Figure 19.1 Dependency and constituent analyses for I prefer the morning ﬂight through Denver.
for each possible place in the parse tree where such an adverbial phrase could occur.
A dependency-based approach can have just one link type representing this particu-
lar adverbial relation; dependency grammar approaches can thus abstract away a bit
more from word order information.
In the following sections, we’ll give an inventory of relations used in dependency
parsing, discuss two families of parsing algorithms (transition-based, and graph-
based), and discuss evaluation.
19.1 Dependency Relations
The traditional linguistic notion of grammatical relation provides the basis for thegrammatical
relation
binary relations that comprise these dependency structures. The arguments to these
relations consist of a head and a dependent . The head plays the role of the central head
dependent organizing word, and the dependent as a kind of modiﬁer. The head-dependent rela-
tionship is made explicit by directly linking heads to the words that are immediately
dependent on them.
In addition to specifying the head-dependent pairs, dependency grammars allow
us to classify the kinds of grammatical relations, or grammatical function that thegrammatical
function
dependent plays with respect to its head. These include familiar notions such as
subject ,direct object andindirect object . In English these notions strongly corre-
late with, but by no means determine, both position in a sentence and constituent
type and are therefore somewhat redundant with the kind of information found in
phrase-structure trees. However, in languages with more ﬂexible word order, the
information encoded directly in these grammatical relations is critical since phrase-
based constituent syntax provides little help.
Linguists have developed taxonomies of relations that go well beyond the famil-
iar notions of subject and object. While there is considerable variation from theory

--- Page 413 ---
19.1 • D EPENDENCY RELATIONS 413
Clausal Argument Relations Description
NSUBJ Nominal subject
OBJ Direct object
IOBJ Indirect object
CCOMP Clausal complement
Nominal Modiﬁer Relations Description
NMOD Nominal modiﬁer
AMOD Adjectival modiﬁer
APPOS Appositional modiﬁer
DET Determiner
CASE Prepositions, postpositions and other case markers
Other Notable Relations Description
CONJ Conjunct
CC Coordinating conjunction
Figure 19.2 Some of the Universal Dependency relations (de Marneffe et al., 2021).
to theory, there is enough commonality that cross-linguistic standards have been
developed. The Universal Dependencies (UD) project (de Marneffe et al., 2021),Universal
Dependencies
an open community effort to annotate dependencies and other aspects of grammar
across more than 100 languages, provides an inventory of 37 dependency relations.
Fig. 19.2 shows a subset of the UD relations and Fig. 19.3 provides some examples.
The motivation for all of the relations in the Universal Dependency scheme is
beyond the scope of this chapter, but the core set of frequently used relations can be
broken into two sets: clausal relations that describe syntactic roles with respect to a
predicate (often a verb), and modiﬁer relations that categorize the ways that words
can modify their heads.
Consider, for example, the following sentence:
United canceled themorning ﬂights toHoustonnsubjobj
det
compoundnmod
caseroot
(19.2)
Here the clausal relations NSUBJ and OBJidentify the subject and direct object of
the predicate cancel , while the NMOD ,DET, and CASE relations denote modiﬁers of
the nouns ﬂights andHouston .
19.1.1 Dependency Formalisms
A dependency structure can be represented as a directed graph G= (V;A), consisting
of a set of vertices V, and a set of ordered pairs of vertices A, which we’ll call arcs.
For the most part we will assume that the set of vertices, V, corresponds exactly
to the set of words in a given sentence. However, they might also correspond to
punctuation, or when dealing with morphologically complex languages the set of
vertices might consist of stems and afﬁxes. The set of arcs, A, captures the head-
dependent and grammatical function relationships between the elements in V.
Different grammatical theories or formalisms may place further constraints on
these dependency structures. Among the more frequent restrictions are that the struc-
tures must be connected, have a designated root node, and be acyclic or planar. Of
most relevance to the parsing approaches discussed in this chapter is the common,

--- Page 414 ---
414 CHAPTER 19 • D EPENDENCY PARSING
Relation Examples with head anddependent
NSUBJ United canceled the ﬂight.
OBJ United diverted theﬂight to Reno.
Webooked her the ﬁrst ﬂight to Miami.
IOBJ Webooked herthe ﬂight to Miami.
COMPOUND We took the morning ﬂight .
NMOD ﬂight toHouston .
AMOD Book the cheapest ﬂight .
APPOS United , aunit of UAL, matched the fares.
DET The ﬂight was canceled.
Which ﬂight was delayed?
CONJ Weﬂewto Denver and drove to Steamboat.
CC We ﬂew to Denver anddrove to Steamboat.
CASE Book the ﬂight through Houston .
Figure 19.3 Examples of some Universal Dependency relations.
computationally-motivated, restriction to rooted trees. That is, a dependency treedependency
tree
is a directed graph that satisﬁes the following constraints:
1. There is a single designated root node that has no incoming arcs.
2. With the exception of the root node, each vertex has exactly one incoming arc.
3. There is a unique path from the root node to each vertex in V.
Taken together, these constraints ensure that each word has a single head, that the
dependency structure is connected, and that there is a single root node from which
one can follow a unique directed path to each of the words in the sentence.
19.1.2 Projectivity
The notion of projectivity imposes an additional constraint that is derived from the
order of the words in the input. An arc from a head to a dependent is said to be
projective if there is a path from the head to every word that lies between the head projective
and the dependent in the sentence. A dependency tree is then said to be projective if
all the arcs that make it up are projective. All the dependency trees we’ve seen thus
far have been projective. There are, however, many valid constructions which lead
to non-projective trees, particularly in languages with relatively ﬂexible word order.
Consider the following example.
JetBlue canceled our ﬂight this morning which was already latensubjobjobl
detacl:relcl
det nsubjcop
advroot
(19.3)
In this example, the arc from ﬂight to its modiﬁer lateis non-projective since there
is no path from ﬂight to the intervening words thisandmorning . As we can see from
this diagram, projectivity (and non-projectivity) can be detected in the way we’ve
been drawing our trees. A dependency tree is projective if it can be drawn with
no crossing edges. Here there is no way to link ﬂight to its dependent latewithout
crossing the arc that links morning to its head.

--- Page 415 ---
19.1 • D EPENDENCY RELATIONS 415
Our concern with projectivity arises from two related issues. First, the most
widely used English dependency treebanks were automatically derived from phrase-
structure treebanks through the use of head-ﬁnding rules. The trees generated in such
a fashion will always be projective, and hence will be incorrect when non-projective
examples like this one are encountered.
Second, there are computational limitations to the most widely used families of
parsing algorithms. The transition-based approaches discussed in Section 19.2 can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. This limitation is one of the motivations for
the more ﬂexible graph-based parsing approach described in Section 19.3.
19.1.3 Dependency Treebanks
Treebanks play a critical role in the development and evaluation of dependency
parsers. They are used for training parsers, they act as the gold labels for evaluating
parsers, and they also provide useful information for corpus linguistics studies.
Dependency treebanks are created by having human annotators directly generate
dependency structures for a given corpus, or by hand-correcting the output of an
automatic parser. A few early treebanks were also based on using a deterministic
process to translate existing constituent-based treebanks into dependency trees.
The largest open community project for building dependency trees is the Univer-
sal Dependencies project at https://universaldependencies.org/ introduced
above, which currently has almost 200 dependency treebanks in more than 100 lan-
guages (de Marneffe et al., 2021). Here are a few UD examples showing dependency
trees for sentences in Spanish, Basque, and Mandarin Chinese:
VERB ADP DET NOUN ADP DET NUM PUNCT
Subiremos a el tren a las cinco .
we-will-board on the train at the ﬁve .obl
detcase
detobl:tmod
casepunct
[Spanish] Subiremos al tren a las cinco. “We will be boarding the train at ﬁve.” (19.4)
NOUN NOUN VERB AUX PUNCT
Ekaitzak itsasontzia hondoratu du .
storm (Erg.) ship (Abs.) sunk has .nsubj
obj auxpunct
[Basque] Ekaitzak itsasontzia hondoratu du. “The storm has sunk the ship.” (19.5)

--- Page 416 ---
416 CHAPTER 19 • D EPENDENCY PARSING
ADV PRON NOUN ADV VERB VERB NOUN
但我昨天 才 收 到信
but I yesterday only-then receive arrive letter .adv
nsubj
obj:tmod
advmod compound:vvobj
[Chinese]但我昨天才收到信“But I didn’t receive the letter until yesterday” (19.6)
19.2 Transition-Based Dependency Parsing
Our ﬁrst approach to dependency parsing is called transition-based parsing. This transition-based
architecture draws on shift-reduce parsing , a paradigm originally developed for
analyzing programming languages (Aho and Ullman, 1972). In transition-based
parsing we’ll have a stack on which we build the parse, a buffer of tokens to be
parsed, and a parser which takes actions on the parse via a predictor called an oracle ,
as illustrated in Fig. 19.4.
wnw1w2s2...s1snParserInput buﬀerStackOracleLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w2
Figure 19.4 Basic transition-based parser. The parser examines the top two elements of the
stack and selects an action by consulting an oracle that examines the current conﬁguration.
The parser walks through the sentence left-to-right, successively shifting items
from the buffer onto the stack. At each time point we examine the top two elements
on the stack, and the oracle makes a decision about what transition to apply to build
the parse. The possible transitions correspond to the intuitive actions one might take
in creating a dependency tree by examining the words in a single pass over the input
from left to right (Covington, 2001):
• Assign the current word as the head of some previously seen word,
• Assign some previously seen word as the head of the current word,
• Postpone dealing with the current word, storing it for later processing.
We’ll formalize this intuition with the following three transition operators that
will operate on the top two elements of the stack:
•LEFT ARC: Assert a head-dependent relation between the word at the top of
the stack and the second word; remove the second word from the stack.
•RIGHT ARC: Assert a head-dependent relation between the second word on
the stack and the word at the top; remove the top word from the stack;

--- Page 417 ---
19.2 • T RANSITION -BASED DEPENDENCY PARSING 417
•SHIFT : Remove the word from the front of the input buffer and push it onto
the stack.
We’ll sometimes call operations like LEFT ARCand RIGHT ARCreduce operations,
based on a metaphor from shift-reduce parsing, in which reducing means combin-
ing elements on the stack. There are some preconditions for using operators. The
LEFT ARCoperator cannot be applied when ROOT is the second element of the stack
(since by deﬁnition the ROOT node cannot have any incoming arcs). And both the
LEFT ARCand RIGHT ARCoperators require two elements to be on the stack to be
applied.
This particular set of operators implements what is known as the arc standard arc standard
approach to transition-based parsing (Covington 2001, Nivre 2003). In arc standard
parsing the transition operators only assert relations between elements at the top of
the stack, and once an element has been assigned its head it is removed from the
stack and is not available for further processing. As we’ll see, there are alterna-
tive transition systems which demonstrate different parsing behaviors, but the arc
standard approach is quite effective and is simple to implement.
The speciﬁcation of a transition-based parser is quite simple, based on repre-
senting the current state of the parse as a conﬁguration : the stack, an input buffer conﬁguration
of words or tokens, and a set of relations representing a dependency tree. Parsing
means making a sequence of transitions through the space of possible conﬁgura-
tions. We start with an initial conﬁguration in which the stack contains the ROOT
node, the buffer has the tokens in the sentence, and an empty set of relations repre-
sents the parse. In the ﬁnal goal state, the stack and the word list should be empty,
and the set of relations will represent the ﬁnal parse. Fig. 19.5 gives the algorithm.
function DEPENDENCY PARSE (words )returns dependency tree
state f[root], [ words ], []g; initial conﬁguration
while state not ﬁnal
t ORACLE (state ) ; choose a transition operator to apply
state APPLY (t,state ) ; apply it, creating a new state
return state
Figure 19.5 A generic transition-based dependency parser
At each step, the parser consults an oracle (we’ll come back to this shortly) that
provides the correct transition operator to use given the current conﬁguration. It then
applies that operator to the current conﬁguration, producing a new conﬁguration.
The process ends when all the words in the sentence have been consumed and the
ROOT node is the only element remaining on the stack.
The efﬁciency of transition-based parsers should be apparent from the algorithm.
The complexity is linear in the length of the sentence since it is based on a single
left to right pass through the words in the sentence. (Each word must ﬁrst be shifted
onto the stack and then later reduced.)
Note that unlike the dynamic programming and search-based approaches dis-
cussed in Chapter 18, this approach is a straightforward greedy algorithm—the or-
acle provides a single choice at each step and the parser proceeds with that choice,
no other options are explored, no backtracking is employed, and a single parse is
returned in the end.
Figure 19.6 illustrates the operation of the parser with the sequence of transitions

--- Page 418 ---
418 CHAPTER 19 • D EPENDENCY PARSING
leading to a parse for the following example.
Book methemorning ﬂightiobjobj
det
compoundroot
(19.7)
Let’s consider the state of the conﬁguration at Step 2, after the word mehas been
pushed onto the stack.
Stack Word List Relations
[root, book, me] [the, morning, ﬂight]
The correct operator to apply here is RIGHT ARCwhich assigns book as the head of
meand pops mefrom the stack resulting in the following conﬁguration.
Stack Word List Relations
[root, book] [the, morning, ﬂight] (book!me)
After several subsequent applications of the SHIFT operator, the conﬁguration in
Step 6 looks like the following:
Stack Word List Relations
[root, book, the, morning, ﬂight] [] (book!me)
Here, all the remaining words have been passed onto the stack and all that is left
to do is to apply the appropriate reduce operators. In the current conﬁguration, we
employ the LEFT ARCoperator resulting in the following state.
Stack Word List Relations
[root, book, the, ﬂight] [] (book!me)
(morning ﬂight)
At this point, the parse for this sentence consists of the following structure.
Book methemorning ﬂightiobj compound
(19.8)
There are several important things to note when examining sequences such as
the one in Figure 19.6. First, the sequence given is not the only one that might lead
to a reasonable parse. In general, there may be more than one path that leads to the
same result, and due to ambiguity, there may be other transition sequences that lead
to different equally valid parses.
Second, we are assuming that the oracle always provides the correct operator
at each point in the parse—an assumption that is unlikely to be true in practice.
As a result, given the greedy nature of this algorithm, incorrect choices will lead to
incorrect parses since the parser has no opportunity to go back and pursue alternative
choices. Section 19.2.4 will introduce several techniques that allow transition-based
approaches to explore the search space more fully.

--- Page 419 ---
19.2 • T RANSITION -BASED DEPENDENCY PARSING 419
Step Stack Word List Action Relation Added
0 [root] [book, me, the, morning, ﬂight] SHIFT
1 [root, book] [me, the, morning, ﬂight] SHIFT
2 [root, book, me] [the, morning, ﬂight] RIGHT ARC (book!me)
3 [root, book] [the, morning, ﬂight] SHIFT
4 [root, book, the] [morning, ﬂight] SHIFT
5 [root, book, the, morning] [ﬂight] SHIFT
6 [root, book, the, morning, ﬂight] [] LEFT ARC (morning ﬂight)
7 [root, book, the, ﬂight] [] LEFT ARC (the ﬂight)
8 [root, book, ﬂight] [] RIGHT ARC (book!ﬂight)
9 [root, book] [] RIGHT ARC (root!book)
10 [root] [] Done
Figure 19.6 Trace of a transition-based parse.
Finally, for simplicity, we have illustrated this example without the labels on
the dependency relations. To produce labeled trees, we can parameterize the LEFT -
ARCand RIGHT ARCoperators with dependency labels, as in LEFT ARC(NSUBJ ) or
RIGHT ARC(OBJ). This is equivalent to expanding the set of transition operators from
our original set of three to a set that includes LEFT ARCand RIGHT ARCoperators for
each relation in the set of dependency relations being used, plus an additional one
for the SHIFT operator. This, of course, makes the job of the oracle more difﬁcult
since it now has a much larger set of operators from which to choose.
19.2.1 Creating an Oracle
The oracle for greedily selecting the appropriate transition is trained by supervised
machine learning. As with all supervised machine learning methods, we will need
training data: conﬁgurations annotated with the correct transition to take. We can
draw these from dependency trees. And we need to extract features of the con-
ﬁguration. We’ll introduce neural classiﬁers that represent the conﬁguration via
embeddings, as well as classic systems that use hand-designed features.
Generating Training Data
The oracle from the algorithm in Fig. 19.5 takes as input a conﬁguration and returns a
transition operator. Therefore, to train a classiﬁer, we will need conﬁgurations paired
with transition operators (i.e., LEFT ARC,RIGHT ARC, or SHIFT ). Unfortunately,
treebanks pair entire sentences with their corresponding trees, not conﬁgurations
with transitions.
To generate the required training data, we employ the oracle-based parsing algo-
rithm in a clever way. We supply our oracle with the training sentences to be parsed
along with their corresponding reference parses from the treebank. To produce train-
ing instances, we then simulate the operation of the parser by running the algorithm
and relying on a new training oracle to give us correct transition operators for each training oracle
successive conﬁguration.
To see how this works, let’s ﬁrst review the operation of our parser. It begins with
a default initial conﬁguration where the stack contains the ROOT , the input list is just
the list of words, and the set of relations is empty. The LEFT ARCand RIGHT ARC
operators each add relations between the words at the top of the stack to the set of
relations being accumulated for a given sentence. Since we have a gold-standard
reference parse for each training sentence, we know which dependency relations are
valid for a given sentence. Therefore, we can use the reference parse to guide the

--- Page 420 ---
420 CHAPTER 19 • D EPENDENCY PARSING
Step Stack Word List Predicted Action
0 [root] [book, the, ﬂight, through, houston] SHIFT
1 [root, book] [the, ﬂight, through, houston] SHIFT
2 [root, book, the] [ﬂight, through, houston] SHIFT
3 [root, book, the, ﬂight] [through, houston] LEFT ARC
4 [root, book, ﬂight] [through, houston] SHIFT
5 [root, book, ﬂight, through] [houston] SHIFT
6 [root, book, ﬂight, through, houston] [] LEFT ARC
7 [root, book, ﬂight, houston ] [] RIGHT ARC
8 [root, book, ﬂight] [] RIGHT ARC
9 [root, book] [] RIGHT ARC
10 [root] [] Done
Figure 19.7 Generating training items consisting of conﬁguration/predicted action pairs by simulating a parse
with a given reference parse.
selection of operators as the parser steps through a sequence of conﬁgurations.
To be more precise, given a reference parse and a conﬁguration, the training
oracle proceeds as follows:
• Choose LEFT ARCif it produces a correct head-dependent relation given the
reference parse and the current conﬁguration,
• Otherwise, choose RIGHT ARCif (1) it produces a correct head-dependent re-
lation given the reference parse and (2) all of the dependents of the word at
the top of the stack have already been assigned,
• Otherwise, choose SHIFT .
The restriction on selecting the RIGHT ARCoperator is needed to ensure that a
word is not popped from the stack, and thus lost to further processing, before all its
dependents have been assigned to it.
More formally, during training the oracle has access to the following:
• A current conﬁguration with a stack Sand a set of dependency relations Rc
• A reference parse consisting of a set of vertices Vand a set of dependency
relations Rp
Given this information, the oracle chooses transitions as follows:
LEFT ARC(r):if(S1r S2)2Rp
RIGHT ARC(r):if(S2r S1)2Rpand8r0;w s:t:(S1r0w)2Rpthen(S1r0w)2Rc
SHIFT :otherwise
Let’s walk through the processing of the following example as shown in Fig. 19.7.
Book theﬂight through Houstonobj
detnmod
caseroot
(19.9)
At Step 1, LEFT ARCis not applicable in the initial conﬁguration since it asserts
a relation, (root book), not in the reference answer; RIGHT ARCdoes assert a
relation contained in the ﬁnal answer (root !book), however book has not been
attached to any of its dependents yet, so we have to defer, leaving SHIFT as the only

--- Page 421 ---
19.2 • T RANSITION -BASED DEPENDENCY PARSING 421
possible action. The same conditions hold in the next two steps. In step 3, LEFT ARC
is selected to link theto its head.
Now consider the situation in Step 4.
Stack Word buffer Relations
[root, book, ﬂight] [through, Houston] (the ﬂight)
Here, we might be tempted to add a dependency relation between book andﬂight ,
which is present in the reference parse. But doing so now would prevent the later
attachment of Houston since ﬂight would have been removed from the stack. For-
tunately, the precondition on choosing RIGHT ARCprevents this choice and we’re
again left with SHIFT as the only viable option. The remaining choices complete the
set of operators needed for this example.
To recap, we derive appropriate training instances consisting of conﬁguration-
transition pairs from a treebank by simulating the operation of a parser in the con-
text of a reference dependency tree. We can deterministically record correct parser
actions at each step as we progress through each training example, thereby creating
the training set we require.
19.2.2 A feature-based classiﬁer
We’ll now introduce two classiﬁers for choosing transitions, here a classic feature-
based algorithm and in the next section a neural classiﬁer using embedding features.
Featured-based classiﬁers generally use the same features we’ve seen with part-
of-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the
head, and the dependency relation to the head. Other features may be relevant for
some languages, for example morphosyntactic features like case marking on subjects
or objects. The features are extracted from the training conﬁgurations , which consist
of the stack, the buffer and the current set of relations. Most useful are features
referencing the top levels of the stack, the words near the front of the buffer, and the
dependency relations already associated with any of those elements.
We’ll use a feature template as we did for sentiment analysis and part-of-speechfeature
template
tagging. Feature templates allow us to automatically generate large numbers of spe-
ciﬁc features from a training set. For example, consider the following feature tem-
plates that are based on single positions in a conﬁguration.
hs1:w;opi;hs2:w;opihs1:t;opi;hs2:t;opi
hb1:w;opi;hb1:t;opihs1:wt;opi (19.10)
Here features are denoted as location:property , where s= stack, b= the word
buffer, w= word forms, t= part-of-speech, and op= operator. Thus the feature for
the word form at the top of the stack would be s1:w, the part of speech tag at the
front of the buffer b1:t, and the concatenated feature s1:wtrepresents the word form
concatenated with the part of speech of the word at the top of the stack. Consider
applying these templates to the following intermediate conﬁguration derived from a
training oracle for (19.2).
Stack Word buffer Relations
[root, canceled, ﬂights] [to Houston] (canceled!United)
(ﬂights!morning)
(ﬂights!the)

--- Page 422 ---
422 CHAPTER 19 • D EPENDENCY PARSING
The correct transition here is SHIFT (you should convince yourself of this before
proceeding). The application of our set of feature templates to this conﬁguration
would result in the following set of instantiated features.
hs1:w=ﬂights;op=shifti (19.11)
hs2:w=canceled;op=shifti
hs1:t=NNS;op=shifti
hs2:t=VBD;op=shifti
hb1:w=to;op=shifti
hb1:t=TO;op=shifti
hs1:wt=ﬂightsNNS;op=shifti
Given that the left and right arc transitions operate on the top two elements of the
stack, features that combine properties from these positions are even more useful.
For example, a feature like s1:ts2:tconcatenates the part of speech tag of the word
at the top of the stack with the tag of the word beneath it.
hs1:ts2:t=NNSVBD;op=shifti (19.12)
Given the training data and features, any classiﬁer, like multinomial logistic re-
gression or support vector machines, can be used.
19.2.3 A neural classiﬁer
The oracle can also be implemented by a neural classiﬁer. A standard architecture
is simply to pass the sentence through an encoder, then take the presentation of the
top 2 words on the stack and the ﬁrst word of the buffer, concatenate them, and
present to a feedforward network that predicts the transition to take (Kiperwasser
and Goldberg, 2016; Kulmizev et al., 2019). Fig. 19.8 sketches this model. Learning
can be done with cross-entropy loss.
w…s2...s1Input buﬀerStackLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w2ENCODERw1w2w3w4w5w6Parser OracleSoftmaxFFNws1s2e(w)e(s1)e(s2)
Figure 19.8 Neural classiﬁer for the oracle for the transition-based parser. The parser takes
the top 2 words on the stack and the ﬁrst word of the buffer, represents them by their encodings
(from running the whole sentence through the encoder), concatenates the embeddings and
passes through a softmax to choose a parser action (transition).

--- Page 423 ---
19.2 • T RANSITION -BASED DEPENDENCY PARSING 423
19.2.4 Advanced Methods in Transition-Based Parsing
The basic transition-based approach can be elaborated in a number of ways to im-
prove performance by addressing some of the most obvious ﬂaws in the approach.
Alternative Transition Systems
The arc-standard transition system described above is only one of many possible sys-
tems. A frequently used alternative is the arc eager transition system. The arc eager arc eager
approach gets its name from its ability to assert rightward relations much sooner
than in the arc standard approach. To see this, let’s revisit the arc standard trace of
Example 19.9, repeated here.
Book theﬂight through Houstonobj
detnmod
caseroot
Consider the dependency relation between book andﬂight in this analysis. As
is shown in Fig. 19.7, an arc-standard approach would assert this relation at Step 8,
despite the fact that book andﬂight ﬁrst come together on the stack much earlier at
Step 4. The reason this relation can’t be captured at this point is due to the presence
of the postnominal modiﬁer through Houston . In an arc-standard approach, depen-
dents are removed from the stack as soon as they are assigned their heads. If ﬂight
had been assigned book as its head in Step 4, it would no longer be available to serve
as the head of Houston .
While this delay doesn’t cause any issues in this example, in general the longer
a word has to wait to get assigned its head the more opportunities there are for
something to go awry. The arc-eager system addresses this issue by allowing words
to be attached to their heads as early as possible, before all the subsequent words
dependent on them have been seen. This is accomplished through minor changes to
theLEFT ARCand RIGHT ARCoperators and the addition of a new REDUCE operator.
•LEFT ARC: Assert a head-dependent relation between the word at the front of
the input buffer and the word at the top of the stack; pop the stack.
•RIGHT ARC: Assert a head-dependent relation between the word on the top of
the stack and the word at the front of the input buffer; shift the word at the
front of the input buffer to the stack.
•SHIFT : Remove the word from the front of the input buffer and push it onto
the stack.
•REDUCE : Pop the stack.
The LEFT ARCand RIGHT ARCoperators are applied to the top of the stack and
the front of the input buffer, instead of the top two elements of the stack as in the
arc-standard approach. The RIGHT ARCoperator now moves the dependent to the
stack from the buffer rather than removing it, thus making it available to serve as the
head of following words. The new REDUCE operator removes the top element from
the stack. Together these changes permit a word to be eagerly assigned its head and
still allow it to serve as the head for later dependents. The trace shown in Fig. 19.9
illustrates the new decision sequence for this example.
In addition to demonstrating the arc-eager transition system, this example demon-
strates the power and ﬂexibility of the overall transition-based approach. We were
able to swap in a new transition system without having to make any changes to the

--- Page 424 ---
424 CHAPTER 19 • D EPENDENCY PARSING
Step Stack Word List Action Relation Added
0 [root] [book, the, ﬂight, through, houston] RIGHT ARC (root!book)
1 [root, book] [the, ﬂight, through, houston] SHIFT
2 [root, book, the] [ﬂight, through, houston] LEFT ARC (the ﬂight)
3 [root, book] [ﬂight, through, houston] RIGHT ARC (book!ﬂight)
4 [root, book, ﬂight] [through, houston] SHIFT
5 [root, book, ﬂight, through] [houston] LEFT ARC (through houston)
6 [root, book, ﬂight] [houston] RIGHT ARC (ﬂight!houston)
7 [root, book, ﬂight, houston] [] REDUCE
8 [root, book, ﬂight] [] REDUCE
9 [root, book] [] REDUCE
10 [root] [] Done
Figure 19.9 A processing trace of Book the ﬂight through Houston using the arc-eager transition operators.
underlying parsing algorithm. This ﬂexibility has led to the development of a di-
verse set of transition systems that address different aspects of syntax and semantics
including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the
generation of non-projective dependency structures (Nivre, 2009), assigning seman-
tic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages
(Bhat et al., 2017).
Beam Search
The computational efﬁciency of the transition-based approach discussed earlier de-
rives from the fact that it makes a single pass through the sentence, greedily making
decisions without considering alternatives. Of course, this is also a weakness – once
a decision has been made it can not be undone, even in the face of overwhelming
evidence arriving later in a sentence. We can use beam search to explore alterna- beam search
tive decision sequences. Recall from Chapter 9 that beam search uses a breadth-ﬁrst
search strategy with a heuristic ﬁlter that prunes the search frontier to stay within a
ﬁxed-size beam width . beam width
In applying beam search to transition-based parsing, we’ll elaborate on the al-
gorithm given in Fig. 19.5. Instead of choosing the single best transition operator
at each iteration, we’ll apply all applicable operators to each state on an agenda and
then score the resulting conﬁgurations. We then add each of these new conﬁgura-
tions to the frontier, subject to the constraint that there has to be room within the
beam. As long as the size of the agenda is within the speciﬁed beam width, we can
add new conﬁgurations to the agenda. Once the agenda reaches the limit, we only
add new conﬁgurations that are better than the worst conﬁguration on the agenda
(removing the worst element so that we stay within the limit). Finally, to insure that
we retrieve the best possible state on the agenda, the while loop continues as long as
there are non-ﬁnal states on the agenda.
The beam search approach requires a more elaborate notion of scoring than we
used with the greedy algorithm. There, we assumed that the oracle would be a
supervised classiﬁer that chose the best transition operator based on features of the
current conﬁguration. This choice can be viewed as assigning a score to all the
possible transitions and picking the best one.
ˆT(c) =argmaxScore (t;c)
With beam search we are now searching through the space of decision sequences,
so it makes sense to base the score for a conﬁguration on its entire history. So we
can deﬁne the score for a new conﬁguration as the score of its predecessor plus the

--- Page 425 ---
19.3 • G RAPH -BASED DEPENDENCY PARSING 425
score of the operator used to produce it.
ConﬁgScore (c0) = 0:0
ConﬁgScore (ci) = ConﬁgScore (ci 1)+Score (ti;ci 1)
This score is used both in ﬁltering the agenda and in selecting the ﬁnal answer. The
new beam search version of transition-based parsing is given in Fig. 19.10.
function DEPENDENCY BEAM PARSE (words ,width )returns dependency tree
state f[root], [ words ], [], 0.0 g;initial conﬁguration
agenda hstatei ;initial agenda
while agenda contains non-ﬁnal states
newagenda hi
for each state2agenda do
for all ftjt2VALID OPERATORS (state )gdo
child APPLY (t,state )
newagenda ADDTOBEAM (child ,newagenda ,width )
agenda newagenda
return BESTOF(agenda )
function ADDTOBEAM (state ,agenda ,width )returns updated agenda
ifLENGTH (agenda )<width then
agenda INSERT (state ,agenda )
else if SCORE (state )>SCORE (WORST OF(agenda ))
agenda REMOVE (WORST OF(agenda ))
agenda INSERT (state ,agenda )
return agenda
Figure 19.10 Beam search applied to transition-based dependency parsing.
19.3 Graph-Based Dependency Parsing
Graph-based methods are the second important family of dependency parsing algo-
rithms. Graph-based parsers are more accurate than transition-based parsers, espe-
cially on long sentences; transition-based methods have trouble when the heads are
very far from the dependents (McDonald and Nivre, 2011). Graph-based methods
avoid this difﬁculty by scoring entire trees, rather than relying on greedy local de-
cisions. Furthermore, unlike transition-based approaches, graph-based parsers can
produce non-projective trees. Although projectivity is not a signiﬁcant issue for
English, it is deﬁnitely a problem for many of the world’s languages.
Graph-based dependency parsers search through the space of possible trees for a
given sentence for a tree (or trees) that maximize some score. These methods encode
the search space as directed graphs and employ methods drawn from graph theory
to search the space for optimal solutions. More formally, given a sentence Swe’re
looking for the best dependency tree in Gs, the space of all possible trees for that
sentence, that maximizes some score.
ˆT(S) =argmax
t2GSScore (t;S)

--- Page 426 ---
426 CHAPTER 19 • D EPENDENCY PARSING
We’ll make the simplifying assumption that this score can be edge-factored , edge-factored
meaning that the overall score for a tree is the sum of the scores of each of the scores
of the edges that comprise the tree.
Score (t;S) =X
e2tScore (e)
Graph-based algorithms have to solve two problems: (1) assigning a score to
each edge, and (2) ﬁnding the best parse tree given the scores of all potential edges.
In the next few sections we’ll introduce solutions to these two problems, beginning
with the second problem of ﬁnding trees, and then giving a feature-based and a
neural algorithm for solving the ﬁrst problem of assigning scores.
19.3.1 Parsing via ﬁnding the maximum spanning tree
In graph-based parsing, given a sentence Swe start by creating a graph Gwhich is a
fully-connected, weighted, directed graph where the vertices are the input words and
the directed edges represent all possible head-dependent assignments. We’ll include
an additional ROOT node with outgoing edges directed at all of the other vertices.
The weights of each edge in Greﬂect the score for each possible head-dependent
relation assigned by some scoring algorithm.
It turns out that ﬁnding the best dependency parse for Sis equivalent to ﬁnding
themaximum spanning tree over G. A spanning tree over a graph Gis a subsetmaximum
spanning tree
ofGthat is a tree and covers all the vertices in G; a spanning tree over Gthat starts
from the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree
with the highest score. Thus a maximum spanning tree of Gemanating from the
ROOT is the optimal dependency parse for the sentence.
A directed graph for the example Book that ﬂight is shown in Fig. 19.11, with the
maximum spanning tree corresponding to the desired parse shown in blue. For ease
of exposition, we’ll describe here the algorithm for unlabeled dependency parsing.
rootBookthatﬂight1244568757
Figure 19.11 Initial rooted, directed graph for Book that ﬂight .
Before describing the algorithm it’s useful to consider two intuitions about di-
rected graphs and their spanning trees. The ﬁrst intuition begins with the fact that
every vertex in a spanning tree has exactly one incoming edge. It follows from this
that every connected component of a spanning tree (i.e., every set of vertices that
are linked to each other by paths over edges) will also have one incoming edge.
The second intuition is that the absolute values of the edge scores are not critical
to determining its maximum spanning tree. Instead, it is the relative weights of the
edges entering each vertex that matters. If we were to subtract a constant amount
from each edge entering a given vertex it would have no impact on the choice of

--- Page 427 ---
19.3 • G RAPH -BASED DEPENDENCY PARSING 427
the maximum spanning tree since every possible spanning tree would decrease by
exactly the same amount.
The ﬁrst step of the algorithm itself is quite straightforward. For each vertex
in the graph, an incoming edge (representing a possible head assignment) with the
highest score is chosen. If the resulting set of edges produces a spanning tree then
we’re done. More formally, given the original fully-connected graph G= (V;E), a
subgraph T= (V;F)is a spanning tree if it has no cycles and each vertex (other than
the root) has exactly one edge entering it. If the greedy selection process produces
such a tree then it is the best possible one.
Unfortunately, this approach doesn’t always lead to a tree since the set of edges
selected may contain cycles. Fortunately, in yet another case of multiple discovery,
there is a straightforward way to eliminate cycles generated during the greedy se-
lection phase. Chu and Liu (1965) and Edmonds (1967) independently developed
an approach that begins with greedy selection and follows with an elegant recursive
cleanup phase that eliminates cycles.
The cleanup phase begins by adjusting all the weights in the graph by subtracting
the score of the maximum edge entering each vertex from the score of all the edges
entering that vertex. This is where the intuitions mentioned earlier come into play.
We have scaled the values of the edges so that the weights of the edges in the cycle
have no bearing on the weight of anyof the possible spanning trees. Subtracting the
value of the edge with maximum weight from each edge entering a vertex results
in a weight of zero for all of the edges selected during the greedy selection phase,
including all of the edges involved in the cycle .
Having adjusted the weights, the algorithm creates a new graph by selecting a
cycle and collapsing it into a single new node. Edges that enter or leave the cycle
are altered so that they now enter or leave the newly collapsed node. Edges that do
not touch the cycle are included and edges within the cycle are dropped.
Now, if we knew the maximum spanning tree of this new graph, we would have
what we need to eliminate the cycle. The edge of the maximum spanning tree di-
rected towards the vertex representing the collapsed cycle tells us which edge to
delete in order to eliminate the cycle. How do we ﬁnd the maximum spanning tree
of this new graph? We recursively apply the algorithm to the new graph. This will
either result in a spanning tree or a graph with a cycle. The recursions can continue
as long as cycles are encountered. When each recursion completes we expand the
collapsed vertex, restoring all the vertices and edges from the cycle with the excep-
tion of the single edge to be deleted .
Putting all this together, the maximum spanning tree algorithm consists of greedy
edge selection, re-scoring of edge costs and a recursive cleanup phase when needed.
The full algorithm is shown in Fig. 19.12.
Fig. 19.13 steps through the algorithm with our Book that ﬂight example. The
ﬁrst row of the ﬁgure illustrates greedy edge selection with the edges chosen shown
in blue (corresponding to the set Fin the algorithm). This results in a cycle between
thatandﬂight . The scaled weights using the maximum value entering each node are
shown in the graph to the right.
Collapsing the cycle between that andﬂight to a single node (labelled tf) and
recursing with the newly scaled costs is shown in the second row. The greedy selec-
tion step in this recursion yields a spanning tree that links roottobook , as well as an
edge that links book to the contracted node. Expanding the contracted node, we can
see that this edge corresponds to the edge from book toﬂight in the original graph.
This in turn tells us which edge to drop to eliminate the cycle.

--- Page 428 ---
428 CHAPTER 19 • D EPENDENCY PARSING
function MAXSPANNING TREE(G=(V ,E) ,root,score )returns spanning tree
F []
T’ []
score’ []
for each v2Vdo
bestInEdge argmaxe=(u;v)2Escore[e]
F F[bestInEdge
for each e=(u,v)2Edo
score’[e] score[e] score[bestInEdge]
ifT=(V ,F) is a spanning tree then return it
else
C a cycle in F
G’ CONTRACT (G,C)
T’ MAXSPANNING TREE(G’,root,score’ )
T EXPAND (T’,C)
return T
function CONTRACT (G,C)returns contracted graph
function EXPAND (T,C)returns expanded graph
Figure 19.12 The Chu-Liu Edmonds algorithm for ﬁnding a maximum spanning tree in a
weighted directed graph.
On arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)
time, where mis the number of edges and nis the number of nodes. Since this par-
ticular application of the algorithm begins by constructing a fully connected graph
m=n2yielding a running time of O(n3). Gabow et al. (1986) present a more efﬁ-
cient implementation with a running time of O(m+nlogn ).
19.3.2 A feature-based algorithm for assigning scores
Recall that given a sentence, S, and a candidate tree, T, edge-factored parsing models
make the simpliﬁcation that the score for the tree is the sum of the scores of the edges
that comprise the tree:
score(S;T) =X
e2Tscore(S;e)
In a feature-based algorithm we compute the edge score as a weighted sum of fea-
tures extracted from it:
score(S;e) =NX
i=1wifi(S;e)
Or more succinctly.
score(S;e) = wf
Given this formulation, we need to identify relevant features and train the weights.
The features (and feature combinations) used to train edge-factored models mir-
ror those used in training transition-based parsers, such as

--- Page 429 ---
19.3 • G RAPH -BASED DEPENDENCY PARSING 429
rootBooktf
rootBookthatﬂight0-3-4
-7-1-6-2rootBook12that7ﬂight8-4-30-2-6-1-700
rootBook0tf-10-3-4
-7-1-6-2rootBook12that7ﬂight81244568757
Deleted from cycle
Figure 19.13 Chu-Liu-Edmonds graph-based example for Book that ﬂight
• Wordforms, lemmas, and parts of speech of the headword and its dependent.
• Corresponding features from the contexts before, after and between the words.
• Word embeddings.
• The dependency relation itself.
• The direction of the relation (to the right or left).
• The distance from the head to the dependent.
Given a set of features, our next problem is to learn a set of weights correspond-
ing to each. Unlike many of the learning problems discussed in earlier chapters,
here we are not training a model to associate training items with class labels, or
parser actions. Instead, we seek to train a model that assigns higher scores to cor-
rect trees than to incorrect ones. An effective framework for problems like this is to
useinference-based learning combined with the perceptron learning rule. In thisinference-based
learning
framework, we parse a sentence (i.e, perform inference) from the training set using
some initially random set of initial weights. If the resulting parse matches the cor-
responding tree in the training data, we do nothing to the weights. Otherwise, we
ﬁnd those features in the incorrect parse that are notpresent in the reference parse
and we lower their weights by a small amount based on the learning rate. We do this
incrementally for each sentence in our training data until the weights converge.

--- Page 430 ---
430 CHAPTER 19 • D EPENDENCY PARSING
19.3.3 A neural algorithm for assigning scores
State-of-the-art graph-based multilingual parsers are based on neural networks. In-
stead of extracting hand-designed features to represent each edge between words wi
andwj, these parsers run the sentence through an encoder, and then pass the encoded
representation of the two words wiandwjthrough a network that estimates a score
for the edge i!j.
bookthatﬂightr1score(h1head, h3dep)Biaﬃneb
ENCODERUh1 headFFNheadFFNheadFFNdepFFNdeph1 depFFNheadFFNdeph2 headh2 deph3 headh3 depWr2r3∑+
Figure 19.14 Computing scores for a single edge (book !ﬂight) in the biafﬁne parser of
Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net-
works to turn the encoder output for each word into a head and dependent representation for
the word. The biafﬁne function turns the head embedding of the head and the dependent
embedding of the dependent into a score for the dependency edge.
Here we’ll sketch the biafﬁne algorithm of Dozat and Manning (2017) and Dozat
et al. (2017) shown in Fig. 19.14, drawing on the work of Gr ¨unewald et al. (2021)
who tested many versions of the algorithm via their STEPS system. The algorithm
ﬁrst runs the sentence X=x1;:::;xnthrough an encoder to produce a contextual
embedding representation for each token R=r1;:::;rn. The embedding for each
token is now passed through two separate feedforward networks, one to produce a
representation of this token as a head, and one to produce a representation of this
token as a dependent:
hhead
i=FFNhead(ri) (19.13)
hdep
i=FFNdep(ri) (19.14)
Now to assign a score to the directed edge i!j, (wiis the head and wjis the depen-
dent), we feed the head representation of i,hhead
i, and the dependent representation
ofj,hdep
j, into a biafﬁne scoring function:
Score (i!j) = Biaff(hhead
i;hdep
j) (19.15)
Biaff(x;y) = x|Uy+W(xy)+b (19.16)

--- Page 431 ---
19.4 • E VALUATION 431
where U,W, and bare weights learned by the model. The idea of using a biafﬁne
function is to allow the system to learn multiplicative interactions between the vec-
torsxandy.
If we pass Score (i!j)through a softmax, we end up with a probability distri-
bution, for each token j, over potential heads i(all other tokens in the sentence):
p(i!j) =softmax ([Score (k!j);8k6=j;1kn]) (19.17)
This probability can then be passed to the maximum spanning tree algorithm of
Section 19.3.1 to ﬁnd the best tree.
This p(i!j)classiﬁer is trained by optimizing the cross-entropy loss.
Note that the algorithm as we’ve described it is unlabeled. To make this into
a labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two
classiﬁers. The ﬁrst classiﬁer, the edge-scorer , the one we described above, assigns
a probability p(i!j)to each word wiandwj. Then the Maximum Spanning Tree
algorithm is run to get a single best dependency parse tree for the second. We then
apply a second classiﬁer, the label-scorer , whose job is to ﬁnd the maximum prob-
ability label for each edge in this parse. This second classiﬁer has the same form
as (19.15-19.17), but instead of being trained to predict with binary softmax the
probability of an edge existing between two words, it is trained with a softmax over
dependency labels to predict the dependency label between the words.
19.4 Evaluation
As with phrase structure-based parsing, the evaluation of dependency parsers pro-
ceeds by measuring how well they work on a test set. An obvious metric would be
exact match (EM)—how many sentences are parsed correctly. This metric is quite
pessimistic, with most sentences being marked wrong. Such measures are not ﬁne-
grained enough to guide the development process. Our metrics need to be sensitive
enough to tell if actual improvements are being made.
For these reasons, the most common method for evaluating dependency parsers
are labeled and unlabeled attachment accuracy. Labeled attachment refers to the
proper assignment of a word to its head along with the correct dependency relation.
Unlabeled attachment simply looks at the correctness of the assigned head, ignor-
ing the dependency relation. Given a system output and a corresponding reference
parse, accuracy is simply the percentage of words in an input that are assigned the
correct head with the correct relation. These metrics are usually referred to as the
labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we
can make use of a label accuracy score (LS), the percentage of tokens with correct
labels, ignoring where the relations are coming from.
As an example, consider the reference parse and system parse for the following
example shown in Fig. 19.15.
(19.18) Book me the ﬂight through Houston.
The system correctly ﬁnds 4 of the 6 dependency relations present in the reference
parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found
by the system holds between book andﬂight , which are in a head-dependent relation
in the reference parse; the system therefore achieves a UAS of 5/6.
Beyond attachment scores, we may also be interested in how well a system is
performing on a particular kind of dependency relation, for example NSUBJ , across

--- Page 432 ---
432 CHAPTER 19 • D EPENDENCY PARSING
Book methe ﬂight through Houston
(a) Referenceiobjobj
detnmod
caseroot
Book methe ﬂight through Houston
(b) Systemxcomp
nsubj
detnmod
caseroot
Figure 19.15 Reference and system parses for Book me the ﬂight through Houston , resulting in an LAS of
2/3 and an UAS of 5/6.
a development corpus. Here we can make use of the notions of precision and recall
introduced in Chapter 17, measuring the percentage of relations labeled NSUBJ by
the system that were correct (precision), and the percentage of the NSUBJ relations
present in the development set that were in fact discovered by the system (recall).
We can employ a confusion matrix to keep track of how often each dependency type
was confused for another.
19.5 Summary
This chapter has introduced the concept of dependency grammars and dependency
parsing. Here’s a summary of the main points that we covered:
• In dependency-based approaches to syntax, the structure of a sentence is de-
scribed in terms of a set of binary relations that hold between the words in a
sentence. Larger notions of constituency are not directly encoded in depen-
dency analyses.
• The relations in a dependency structure capture the head-dependent relation-
ship among the words in a sentence.
• Dependency-based analysis provides information directly useful in further
language processing tasks including information extraction, semantic parsing
and question answering.
• Transition-based parsing systems employ a greedy stack-based algorithm to
create dependency structures.
• Graph-based methods for creating dependency structures are based on the use
of maximum spanning tree methods from graph theory.
• Both transition-based and graph-based approaches are developed using super-
vised machine learning techniques.
• Treebanks provide the data needed to train these systems. Dependency tree-
banks can be created directly by human annotators or via automatic transfor-
mation from phrase-structure treebanks.
• Evaluation of dependency parsers is based on labeled and unlabeled accuracy
scores as measured against withheld development and test corpora.

--- Page 433 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 433
Bibliographical and Historical Notes
The dependency-based approach to grammar is much older than the relatively recent
phrase-structure or constituency grammars, which date only to the 20th century. De-
pendency grammar dates back to the Indian grammarian P ¯an.ini sometime between
the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions.
Contemporary theories of dependency grammar all draw heavily on the 20th cen-
tury work of Tesni `ere (1959).
Automatic parsing using dependency grammars was ﬁrst introduced into compu-
tational linguistics by early work on machine translation at the RAND Corporation
led by David Hays. This work on dependency parsing closely paralleled work on
constituent parsing and made explicit use of grammars to guide the parsing process.
After this early period, computational work on dependency parsing remained inter-
mittent over the following decades. Notable implementations of dependency parsers
for English during this period include Link Grammar (Sleator and Temperley, 1993),
Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).
Dependency parsing saw a major resurgence in the late 1990’s with the appear-
ance of large dependency-based treebanks and the associated advent of data driven
approaches described in this chapter. Eisner (1996) developed an efﬁcient dynamic
programming approach to dependency parsing based on bilexical grammars derived
from the Penn Treebank. Covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. Yamada and Mat-
sumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce
paradigm and the use of supervised machine learning in the form of support vector
machines to dependency parsing.
Transition-based parsing is based on the shift-reduce parsing algorithm orig-
inally developed for analyzing programming languages (Aho and Ullman, 1972).
Shift-reduce parsing also makes use of a context-free grammar. Input tokens are
successively shifted onto the stack and the top two elements of the stack are matched
against the right-hand side of the rules in the grammar; when a match is found the
matched elements are replaced on the stack (reduced) by the non-terminal from the
left-hand side of the rule being matched. In transition-based dependency parsing
we skip the grammar, and alter the reduce operation to add a dependency relation
between a word and its head.
Nivre (2003) deﬁned the modern, deterministic, transition-based approach to
dependency parsing. Subsequent work by Nivre and his colleagues formalized and
analyzed the performance of numerous transition systems, training methods, and
methods for dealing with non-projective language (Nivre and Scholz 2004, Nivre
2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-
proach was pioneered by Chen and Manning (2014) and extended by Kiperwasser
and Goldberg (2016); Kulmizev et al. (2019).
The graph-based maximum spanning tree approach to dependency parsing was
introduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classiﬁer
was introduced by (Kiperwasser and Goldberg, 2016).
The long-running Prague Dependency Treebank project (Haji ˇc, 1998) is the most
signiﬁcant effort to directly annotate a corpus with multiple layers of morphological,
syntactic and semantic information. PDT 3.0 contains over 1.5 M tokens (Bej ˇcek
et al., 2013).
Universal Dependencies (UD) (de Marneffe et al., 2021) is an open community

--- Page 434 ---
434 CHAPTER 19 • D EPENDENCY PARSING
project to create a framework for dependency treebank annotation, with nearly 200
treebanks in over 100 languages. The UD annotation scheme evolved out of several
distinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marn-
effe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-of-speech
tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets
(Zeman, 2008).
The Conference on Natural Language Learning (CoNLL) has conducted an in-
ﬂuential series of shared tasks related to dependency parsing over the years (Buch-
holz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Haji ˇc et al. 2009).
More recent evaluations have focused on parser robustness with respect to morpho-
logically rich languages (Seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (Petrov and McDonald, 2012).
Choi et al. (2015) presents a performance analysis of 10 dependency parsers across
a range of metrics, as well as DEPEND ABLE, a robust parser evaluation tool.
Exercises

--- Page 435 ---
CHAPTER
20Information Extraction:
Relations, Events, and Time
Time will explain.
Jane Austen, Persuasion
Imagine that you are an analyst with an investment ﬁrm that tracks airline stocks.
You’re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. His-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? You will need to know at least the name of the airline, the nature of
the proposed fare hike, the dates of the announcement, and possibly the response of
other airlines. Fortunately, these can be all found in news articles like this one:
Citing high fuel prices, United Airlines said Friday it has increased fares
by $6 per round trip on ﬂights to some cities also served by lower-
cost carriers. American Airlines, a unit of AMR Corp., immediately
matched the move, spokesman Tim Wagner said. United, a unit of UAL
Corp., said the increase took effect Thursday and applies to most routes
where it competes against discount carriers, such as Chicago to Dallas
and Denver to San Francisco.
This chapter presents techniques for extracting limited kinds of semantic con-
tent from text. This process of information extraction (IE) turns the unstructuredinformation
extraction
information embedded in texts into structured data, for example for populating a
relational database to enable further processing.
We begin with the task of relation extraction : ﬁnding and classifying semanticrelation
extraction
relations among entities mentioned in a text, like child-of (X is the child-of Y), or
part-whole or geospatial relations. Relation extraction has close links to populat-
ing a relational database, and knowledge graphs , datasets of structured relationalknowledge
graphs
knowledge, are a useful way for search engines to present information to users.
Next, we discuss event extraction , the task of ﬁnding events in which these en-event
extraction
tities participate, like, in our sample text, the fare increases by United andAmerican
and the reporting events said andcite. Events are also situated in time , occurring at
a particular date or time, and events can be related temporally, happening before or
after or simultaneously with each other. We’ll need to recognize temporal expres-
sions like Friday ,Thursday ortwo days from now and times such as 3:30 P .M. , and
normalize them onto speciﬁc calendar dates or times. We’ll need to link Friday to
the time of United’s announcement, Thursday to the previous day’s fare increase,
and we’ll need to produce a timeline in which United’s announcement follows the
fare increase and American’s announcement follows both of those events.
The related task of template ﬁlling is to ﬁnd recurring stereotypical events or template ﬁlling
situations in documents and ﬁll in the template slots. These slot-ﬁllers may consist
of text segments extracted directly from the text, or concepts like times, amounts, or
ontology entities that have been inferred through additional processing. Our airline

--- Page 436 ---
436 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
ARTIFACTGENERALAFFILIATIONORGAFFILIATIONPART-WHOLEPERSON-SOCIALPHYSICALLocatedNearBusinessFamilyLasting PersonalCitizen-Resident-Ethnicity-ReligionOrg-Location-OriginFounderEmploymentMembershipOwnershipStudent-AlumInvestorUser-Owner-Inventor-ManufacturerGeographicalSubsidiary
Sports-Affiliation
Figure 20.1 The 17 relations used in the ACE relation extraction task.
text presents such a stereotypical situation since airlines often raise fares and then
wait to see if competitors follow along. Here we can identify United as a lead air-
line that initially raised its fares, $6 as the amount, Thursday as the increase date,
andAmerican as an airline that followed along, leading to a ﬁlled template like the
following:
FARE-RAISE ATTEMPT :2
6664LEAD AIRLINE : U NITED AIRLINES
AMOUNT : $6
EFFECTIVE DATE: 2006-10-26
FOLLOWER : A MERICAN AIRLINES3
7775
20.1 Relation Extraction
Let’s assume that we have detected the named entities in our sample text (perhaps
using the techniques of Chapter 17), and would like to discern the relationships that
exist among the detected entities:
Citing high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it
has increased fares by [ MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ ORG American Airlines ], a
unit of [ ORG AMR Corp.] , immediately matched the move, spokesman
[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,
said the increase took effect [ TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [ LOC Chicago]
to [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .
The text tells us, for example, that Tim Wagner is a spokesman for American
Airlines , that United is a unit of UAL Corp. , and that American is a unit of AMR .
These binary relations are instances of more generic relations such as part-of or
employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations
used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample
relations. We might also extract more domain-speciﬁc relation such as the notion of
an airline route. For example from this text we can conclude that United has routes
to Chicago, Dallas, Denver, and San Francisco.

--- Page 437 ---
20.1 • R ELATION EXTRACTION 437
Relations Types Examples
Physical-Located PER-GPE Hewas in Tennessee
Part-Whole-Subsidiary ORG-ORG XYZ , the parent company of ABC
Person-Social-Family PER-PER Yoko ’s husband John
Org-AFF-Founder PER-ORG Steve Jobs , co-founder of Apple ...
Figure 20.2 Semantic relations with examples and the named entity types they involve.
Sets of relations have been deﬁned for many other domains as well. For example
UMLS, the Uniﬁed Medical Language System from the US National Library of
Medicine has a network that deﬁnes 134 broad subject categories, entity types, and
54 relations between the entities, such as the following:
Entity Relation Entity
Injury disrupts Physiological Function
Bodily Location location-of Biologic Function
Anatomical Structure part-of Organism
Pharmacologic Substance causes Pathological Function
Pharmacologic Substance treats Pathologic Function
Given a medical sentence like this one:
(20.1) Doppler echocardiography can be used to diagnose left anterior descending
artery stenosis in patients with type 2 diabetes
We could thus extract the UMLS relation:
Echocardiography, Doppler Diagnoses Acquired stenosis
Wikipedia also offers a large supply of relations, drawn from infoboxes , struc- infoboxes
tured tables associated with certain Wikipedia articles. For example, the Wikipedia
infobox for Stanford includes structured facts like state = "California" or
president = "Marc Tessier-Lavigne" . These facts can be turned into rela-
tions like president-of orlocated-in . or into relations in a metalanguage called RDF RDF
(Resource Description Framework). An RDF triple is a tuple of entity-relation- RDF triple
entity, called a subject-predicate-object expression. Here’s a sample RDF triple:
subject predicate object
Golden Gate Park location San Francisco
For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-
rived from Wikipedia containing over 2 billion RDF triples. Another dataset from
Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrande ˇci´c Freebase
and Kr ¨otzsch, 2014), has relations between people and their nationality, or locations,
and other locations they are contained in.
WordNet or other ontologies offer useful ontological relations that express hier-
archical relations between words or concepts. For example WordNet has the is-aor is-a
hypernym relation between classes, hypernym
Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...
WordNet also has Instance-of relation between individuals and classes, so that for
example San Francisco is in the Instance-of relation with city. Extracting these
relations is an important step in extending or building ontologies.
Finally, there are large datasets that contain sentences hand-labeled with their
relations, designed for training and testing relation extractors. The TACRED dataset
(Zhang et al., 2017) contains 106,264 examples of relation triples about particular
people or organizations, labeled in sentences from news and web text drawn from the

--- Page 438 ---
438 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
annual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains
41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse),
plus a no relation tag; examples are shown in Fig. 20.3. About 80% of all examples
are annotated as no relation; having sufﬁcient negative data is important for training
supervised classiﬁers.
Example Entity Types & Label
Carey will succeed Cathleen P. Black, who held the position for 15
years and will take on a new role as chairwoman of Hearst Maga-
zines, the company said.PERSON /TITLE
Relation: per:title
Irene Morgan Kirkaldy, who was born and reared in Baltimore, lived
on Long Island and ran a child-care center in Queens with her second
husband, Stanley Kirkaldy.PERSON /CITY
Relation: per:city ofbirth
Baldwin declined further comment, and said JetBlue chief executive
Dave Barger was unavailable.Types: PERSON /TITLE
Relation: norelation
Figure 20.3 Example sentences and labels from the TACRED dataset (Zhang et al., 2017).
A standard dataset was also produced for the SemEval 2010 Task 8, detecting
relations between nominals (Hendrickx et al., 2009). The dataset has 10,717 exam-
ples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed
relations like product-producer ( afactory manufactures suits) orcomponent-whole
(myapartment has a large kitchen ).
20.2 Relation Extraction Algorithms
There are ﬁve main classes of algorithms for relation extraction: handwritten pat-
terns ,supervised machine learning ,semi-supervised (viabootstrapping ordis-
tant supervision ), and unsupervised . We’ll introduce each of these in the next
sections.
20.2.1 Using Patterns to Extract Relations
The earliest and still common algorithm for relation extraction is lexico-syntactic
patterns, ﬁrst developed by Hearst (1992a), and therefore often called Hearst pat-
terns . Consider the following sentence: Hearst patterns
Agar is a substance prepared from a mixture of red algae, such as Ge-
lidium, for laboratory or industrial use.
Hearst points out that most human readers will not know what Gelidium is, but that
they can readily infer that it is a kind of (a hyponym of)red algae , whatever that is.
She suggests that the following lexico-syntactic pattern
NP0such as NP 1f;NP2:::;(andjor)NPig;i1 (20.2)
implies the following semantics
8NPi;i1;hyponym (NPi;NP0) (20.3)
allowing us to infer
hyponym (Gelidium;red algae ) (20.4)

--- Page 439 ---
20.2 • R ELATION EXTRACTION ALGORITHMS 439
NPf, NPg*f,g(andjor) other NP H temples, treasuries, and other important civic buildings
NPHsuch asfNP,g*f(orjand)gNP red algae such as Gelidium
such NP HasfNP,g*f(orjand)gNP such authors as Herrick, Goldsmith, and Shakespeare
NPHf,gincludingfNP,g*f(orjand)gNP common-law countries, including Canada and England
NPHf,gespeciallyfNPg*f(orjand)gNP European countries, especially France, England, and Spain
Figure 20.4 Hand-built lexico-syntactic patterns for ﬁnding hypernyms, using fgto mark optionality (Hearst
1992a, Hearst 1998).
Figure 20.4 shows ﬁve patterns Hearst (1992a, 1998) suggested for inferring
the hyponym relation; we’ve shown NPHas the parent/hyponym. Modern versions
of the pattern-based approach extend it by adding named entity constraints. For
example if our goal is to answer questions about “Who holds what ofﬁce in which
organization?”, we can use patterns like the following:
PER, POSITION of ORG:
George Marshall, Secretary of State of the United States
PER (namedjappointedjchosejetc.) PER Prep? POSITION
Truman appointed Marshall Secretary of State
PER [be]? (namedjappointedjetc.) Prep? ORG POSITION
George Marshall was named US Secretary of State
Hand-built patterns have the advantage of high-precision and they can be tailored
to speciﬁc domains. On the other hand, they are often low-recall, and it’s a lot of
work to create them for all possible patterns.
20.2.2 Relation Extraction via Supervised Learning
Supervised machine learning approaches to relation extraction follow a scheme that
should be familiar by now. A ﬁxed set of relations and entities is chosen, a training
corpus is hand-annotated with the relations and entities, and the annotated texts are
then used to train classiﬁers to annotate an unseen test set.
The most straightforward approach, illustrated in Fig. 20.5 is: (1) Find pairs of
named entities (usually in the same sentence). (2): Apply a relation-classiﬁcation
on each pair. The classiﬁer can use any supervised technique (logistic regression,
RNN, Transformer, random forest, etc.).
An optional intermediate ﬁltering classiﬁer can be used to speed up the process-
ing by making a binary decision on whether a given pair of named entities are related
(by any relation). It’s trained on positive examples extracted directly from all rela-
tions in the annotated corpus, and negative examples generated from within-sentence
entity pairs that are not annotated with a relation.
Feature-based supervised relation classiﬁers. Let’s consider sample features for
a feature-based classiﬁer (like logistic regression or random forests), classifying the
relationship between American Airlines (Mention 1, or M1) and Tim Wagner (Men-
tion 2, M2) from this sentence:
(20.5) American Airlines , a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
These include word features (as embeddings, or 1-hot, stemmed or not):
• The headwords of M1 and M2 and their concatenation
Airlines Wagner Airlines-Wagner

--- Page 440 ---
440 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
function FINDRELATIONS (words )returns relations
relations nil
entities FINDENTITIES (words )
forall entity pairs he1,e2iinentities do
ifRELATED ?(e1,e2)
relations relations +CLASSIFY RELATION (e1,e2)
Figure 20.5 Finding and classifying the relations among entities in a text.
• Bag-of-words and bigrams in M1 and M2
American, Airlines, Tim, Wagner, American Airlines, Tim Wagner
• Words or bigrams in particular positions
M2: -1 spokesman
M2: +1 said
• Bag of words or bigrams between M1 and M2:
a, AMR, of, immediately, matched, move, spokesman, the, unit
Named entity features:
• Named-entity types and their concatenation
(M1: ORG, M2: PER, M1M2: ORG-PER)
• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)
M1: NAME [it or he would be PRONOUN]
M2: NAME [the company would be NOMINAL]
• Number of entities between the arguments (in this case 1, for AMR)
Syntactic structure is a useful signal, often represented as the dependency or
constituency syntactic path traversed through the tree between the entities.
• Constituent paths between M1 and M2
NP"NP"S"S#NP
• Dependency-tree paths
Airlines sub jmatched comp said!sub jWagner
Neural supervised relation classiﬁers Neural models for relation extraction sim-
ilarly treat the task as supervised classiﬁcation. Let’s consider a typical system ap-
plied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In
TACRED we are given a sentence and two spans within it: a subject, which is a
person or organization, and an object, which is any other entity. The task is to assign
a relation from the 42 TAC relations, or no relation.
A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a
pretrained encoder like BERT and adds a linear layer on top of the sentence repre-
sentation (for example the BERT [CLS] token), a linear layer that is ﬁnetuned as a
1-of-N classiﬁer to assign one of the 43 labels. The input to the BERT encoder is
partially de-lexiﬁed; the subject and object entities are replaced in the input by their
NER tags. This helps keep the system from overﬁtting to the individual lexical items
(Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it
helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi
et al., 2020) that don’t have two sequences separated by a [SEP] token, but instead
form the input from a single long sequence of sentences.
In general, if the test set is similar enough to the training set, and if there is
enough hand-labeled data, supervised relation extraction systems can get high ac-

--- Page 441 ---
20.2 • R ELATION EXTRACTION ALGORITHMS 441
ENCODER[CLS][SUBJ_PERSON]wasbornin[OBJ_LOC],MichiganLinearClassiﬁerp(relation|SUBJ,OBJ)
Figure 20.6 Relation extraction as a linear layer on top of an encoder (in this case BERT),
with the subject and object entities replaced in the input by their NER tags (Zhang et al. 2017,
Joshi et al. 2020).
curacies. But labeling a large training set is extremely expensive and supervised
models are brittle: they don’t generalize well to different text genres. For this rea-
son, much research in relation extraction has focused on the semi-supervised and
unsupervised approaches we turn to next.
20.2.3 Semisupervised Relation Extraction via Bootstrapping
Supervised machine learning assumes that we have lots of labeled data. Unfortu-
nately, this is expensive. But suppose we just have a few high-precision seed pat-
terns , like those in Section 20.2.1, or perhaps a few seed tuples . That’s enough seed patterns
seed tuples to bootstrap a classiﬁer! Bootstrapping proceeds by taking the entities in the seed
bootstrapping pair, and then ﬁnding sentences (on the web, or whatever dataset we are using) that
contain both entities. From all such sentences, we extract and generalize the context
around the entities to learn new patterns. Fig. 20.7 sketches a basic algorithm.
function BOOTSTRAP (Relation R )returns new relation tuples
tuples Gather a set of seed tuples that have relation R
iterate
sentences ﬁnd sentences that contain entities in tuples
patterns generalize the context between and around entities in sentences
newpairs usepatterns to identify more tuples
newpairs newpairs with high conﬁdence
tuples tuples +newpairs
return tuples
Figure 20.7 Bootstrapping from seed entity pairs to learn relations.
Suppose, for example, that we need to create a list of airline/hub pairs, and we
know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover
new patterns by ﬁnding other mentions of this relation in our corpus. We search
for the terms Ryanair ,Charleroi andhubin some proximity. Perhaps we ﬁnd the
following set of sentences:
(20.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all
weekend ﬂights out of the airport.
(20.7) All ﬂights in and out of Ryanair’s hub at Charleroi airport were grounded on
Friday...
(20.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000
passengers had already been affected.

--- Page 442 ---
442 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
From these results, we can use the context of words between the entity mentions,
the words before mention one, the word after mention two, and the named entity
types of the two mentions, and perhaps other features, to extract general patterns
such as the following:
/ [ORG], which uses [LOC] as a hub /
/ [ORG]'s hub at [LOC] /
/ [LOC], a main hub for [ORG] /
These new patterns can then be used to search for additional tuples.
Bootstrapping systems also assign conﬁdence values to new tuples to avoid se-conﬁdence
values
mantic drift . In semantic drift, an erroneous pattern leads to the introduction of semantic drift
erroneous tuples, which, in turn, lead to the creation of problematic patterns and the
meaning of the extracted relations ‘drifts’. Consider the following example:
(20.9) Sydney has a ferry hub at Circular Quay.
If accepted as a positive example, this expression could lead to the incorrect in-
troduction of the tuple hSydney;CircularQuayi. Patterns based on this tuple could
propagate further errors into the database.
Conﬁdence values for patterns are based on balancing two factors: the pattern’s
performance with respect to the current set of tuples and the pattern’s productivity
in terms of the number of matches it produces in the document collection. More
formally, given a document collection D, a current set of tuples T, and a proposed
pattern p, we need to track two factors:
•hits(p): the set of tuples in Tthatpmatches while looking in D
•ﬁnds (p): The total set of tuples that pﬁnds in D
The following equation balances these considerations (Riloff and Jones, 1999).
ConfRlogF(p) =jhits(p)j
jﬁnds(p)jlog(jﬁnds(p)j) (20.10)
This metric is generally normalized to produce a probability.
We can assess the conﬁdence in a proposed new tuple by combining the evidence
supporting it from all the patterns P0that match that tuple in D(Agichtein and Gra-
vano, 2000). One way to combine such evidence is the noisy-or technique. Assume noisy-or
that a given tuple is supported by a subset of the patterns in P, each with its own
conﬁdence assessed as above. In the noisy-or model, we make two basic assump-
tions. First, that for a proposed tuple to be false, allof its supporting patterns must
have been in error, and second, that the sources of their individual failures are all
independent. If we loosely treat our conﬁdence measures as probabilities, then the
probability of any individual pattern pfailing is 1 Conf(p); the probability of all
of the supporting patterns for a tuple being wrong is the product of their individual
failure probabilities, leaving us with the following equation for our conﬁdence in a
new tuple.
Conf(t) =1 Y
p2P0(1 Conf(p)) (20.11)
Setting conservative conﬁdence thresholds for the acceptance of new patterns
and tuples during the bootstrapping process helps prevent the system from drifting
away from the targeted relation.

--- Page 443 ---
20.2 • R ELATION EXTRACTION ALGORITHMS 443
20.2.4 Distant Supervision for Relation Extraction
Although hand-labeling text with relation labels is expensive to produce, there are
ways to ﬁnd indirect sources of training data. The distant supervision methoddistant
supervision
(Mintz et al., 2009) combines the advantages of bootstrapping with supervised learn-
ing. Instead of just a handful of seeds, distant supervision uses a large database to
acquire a huge number of seed examples, creates lots of noisy pattern features from
all these examples and then combines them in a supervised classiﬁer.
For example suppose we are trying to learn the place-of-birth relationship be-
tween people and their birth cities. In the seed-based approach, we might have only
5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase
have tens of thousands of examples of many relations; including over 100,000 ex-
amples of place-of-birth , (<Edwin Hubble, Marshfield> ,<Albert Einstein,
Ulm> , etc.,). The next step is to run named entity taggers on large amounts of text—
Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences
that have two named entities that match the tuple, like the following:
...Hubble was born in Marshﬁeld...
...Einstein, born (1879), Ulm...
...Hubble’s birthplace in Marshﬁeld...
Training instances can now be extracted from this data, one training instance
for each identical tuple <relation, entity1, entity2> . Thus there will be one
training instance for each of:
<born-in, Edwin Hubble, Marshfield>
<born-in, Albert Einstein, Ulm>
<born-year, Albert Einstein, 1879>
and so on.
We can then apply feature-based or neural classiﬁcation. For feature-based
classiﬁcation, we can use standard supervised relation extraction features like the
named entity labels of the two mentions, the words and dependency paths in be-
tween the mentions, and neighboring words. Each tuple will have features col-
lected from many training instances; the feature vector for a single training instance
like (<born-in,Albert Einstein, Ulm> will have lexical and syntactic features
from many different sentences that mention Einstein and Ulm.
Because distant supervision has very large training sets, it is also able to use very
rich features that are conjunctions of these individual features. So we will extract
thousands of patterns that conjoin the entity types with the intervening words or
dependency paths like these:
PER was born in LOC
PER, born (XXXX), LOC
PER’s birthplace in LOC
To return to our running example, for this sentence:
(20.12) American Airlines , a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
we would learn rich conjunction features like this one:
M1 = ORG & M2 = PER & nextword=“said”& path= NP"NP"S"S#NP
The result is a supervised classiﬁer that has a huge rich set of features to use
in detecting relations. Since not every test sentence will have one of the training

--- Page 444 ---
444 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
relations, the classiﬁer will also need to be able to label an example as no-relation .
This label is trained by randomly selecting entity pairs that do not appear in any
Freebase relation, extracting features for them, and building a feature vector for
each such tuple. The ﬁnal algorithm is sketched in Fig. 20.8.
function DISTANT SUPERVISION (Database D, Text T )returns relation classiﬁer C
foreach relation R
foreach tuple ( e1,e2 ) of entities with relation RinD
sentences Sentences in Tthat contain e1ande2
f Frequent features in sentences
observations observations + new training tuple ( e1, e2, f, R )
C Train supervised classiﬁer on observations
return C
Figure 20.8 The distant supervision algorithm for relation extraction. A neural classiﬁer
would skip the feature set f.
Distant supervision shares advantages with each of the methods we’ve exam-
ined. Like supervised classiﬁcation, distant supervision uses a classiﬁer with lots
of features, and supervised by detailed hand-created knowledge. Like pattern-based
classiﬁers, it can make use of high-precision evidence for the relation between en-
tities. Indeed, distance supervision systems learn patterns just like the hand-built
patterns of early relation extractors. For example the is-aorhypernym extraction
system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as
distant supervision, and then learned new patterns from large amounts of text. Their
system induced exactly the original 5 template patterns of Hearst (1992a), but also
70,000 additional patterns including these four:
NPHlike NP Many hormones like leptin...
NPHcalled NP ...using a markup language called XHTML
NP is a NP H Ruby is a programming language...
NP, a NP H IBM, a company with a long...
This ability to use a large number of features simultaneously means that, un-
like the iterative expansion of patterns in seed-based systems, there’s no semantic
drift. Like unsupervised classiﬁcation, it doesn’t use a labeled training corpus of
texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very
large amounts of unlabeled data. Distant supervision also has the advantage that it
can create training tuples to be used with neural classiﬁers, where features are not
required.
The main problem with distant supervision is that it tends to produce low-precision
results, and so current research focuses on ways to improve precision. Furthermore,
distant supervision can only help in extracting relations for which a large enough
database already exists. To extract new relations without datasets, or relations for
new domains, purely unsupervised methods must be used.
20.2.5 Unsupervised Relation Extraction
The goal of unsupervised relation extraction is to extract relations from the web
when we have no labeled training data, and not even any list of relations. This task
is often called open information extraction orOpen IE . In Open IE, the relationsopen
information
extraction

--- Page 445 ---
20.2 • R ELATION EXTRACTION ALGORITHMS 445
are simply strings of words (usually beginning with a verb).
For example, the ReVerb system (Fader et al., 2011) extracts a relation from a
sentence sin 4 steps:
1. Run a part-of-speech tagger and entity chunker over s
2. For each verb in s, ﬁnd the longest sequence of words wthat start with a verb
and satisfy syntactic and lexical constraints, merging adjacent matches.
3. For each phrase w, ﬁnd the nearest noun phrase xto the left which is not a
relative pronoun, wh-word or existential “there”. Find the nearest noun phrase
yto the right.
4. Assign conﬁdence cto the relation r= (x;w;y)using a conﬁdence classiﬁer
and return it.
A relation is only accepted if it meets syntactic and lexical constraints. The
syntactic constraints ensure that it is a verb-initial sequence that might also include
nouns (relations that begin with light verbs like make ,have , ordooften express the
core of the relation with a noun, like have a hub in ):
VjVPjVW*P
V = verb particle? adv?
W = (nounjadjjadvjpronjdet )
P = (prepjparticlejinﬁnitive “to”)
The lexical constraints are based on a dictionary Dthat is used to prune very rare,
long relation strings. The intuition is to eliminate candidate relations that don’t oc-
cur with sufﬁcient number of distinct argument types and so are likely to be bad
examples. The system ﬁrst runs the above relation extraction algorithm ofﬂine on
500 million web sentences and extracts a list of all the relations that occur after nor-
malizing them (removing inﬂection, auxiliary verbs, adjectives, and adverbs). Each
relation ris added to the dictionary if it occurs with at least 20 different arguments.
Fader et al. (2011) used a dictionary of 1.7 million normalized relations.
Finally, a conﬁdence value is computed for each relation using a logistic re-
gression classiﬁer. The classiﬁer is trained by taking 1000 random web sentences,
running the extractor, and hand labeling each extracted relation as correct or incor-
rect. A conﬁdence classiﬁer is then trained on this hand-labeled data, using features
of the relation and the surrounding words. Fig. 20.9 shows some sample features
used in the classiﬁcation.
(x,r,y) covers all words in s
the last preposition in risfor
the last preposition in rison
len(s)10
there is a coordinating conjunction to the left of rins
rmatches a lone V in the syntactic constraints
there is preposition to the left of xins
there is an NP to the right of yins
Figure 20.9 Features for the classiﬁer that assigns conﬁdence to relations extracted by the
Open Information Extraction system REVERB (Fader et al., 2011).
For example the following sentence:
(20.13) United has a hub in Chicago, which is the headquarters of United
Continental Holdings.

--- Page 446 ---
446 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
has the relation phrases has a hub in andis the headquarters of (it also has hasand
is, but longer phrases are preferred). Step 3 ﬁnds United to the left and Chicago to
the right of has a hub in , and skips over which to ﬁnd Chicago to the left of is the
headquarters of . The ﬁnal output is:
r1: <United, has a hub in, Chicago>
r2: <Chicago, is the headquarters of, United Continental Holdings>
The great advantage of unsupervised relation extraction is its ability to handle
a huge number of relations without having to specify them in advance. The dis-
advantage is the need to map all the strings into some canonical form for adding
to databases or knowledge graphs. Current methods focus heavily on relations ex-
pressed with verbs, and so will miss many relations that are expressed nominally.
20.2.6 Evaluation of Relation Extraction
Supervised relation extraction systems are evaluated by using test sets with human-
annotated, gold-standard relations and computing precision, recall, and F-measure.
Labeled precision and recall require the system to classify the relation correctly,
whereas unlabeled methods simply measure a system’s ability to detect entities that
are related.
Semi-supervised andunsupervised methods are much more difﬁcult to evalu-
ate, since they extract totally new relations from the web or a large text. Because
these methods use very large amounts of text, it is generally not possible to run them
solely on a small labeled test set, and as a result it’s not possible to pre-annotate a
gold set of correct instances of relations.
For these methods it’s possible to approximate (only) precision by drawing a
random sample of relations from the output, and having a human check the accuracy
of each of these relations. Usually this approach focuses on the tuples to be extracted
from a body of text rather than on the relation mentions ; systems need not detect
every mention of a relation to be scored correctly. Instead, the evaluation is based
on the set of tuples occupying the database when the system is ﬁnished. That is,
we want to know if the system can discover that Ryanair has a hub at Charleroi; we
don’t really care how many times it discovers it. The estimated precision ˆPis then
ˆP=# of correctly extracted relation tuples in the sample
total # of extracted relation tuples in the sample.(20.14)
Another approach that gives us a little bit of information about recall is to com-
pute precision at different levels of recall. Assuming that our system is able to
rank the relations it produces (by probability, or conﬁdence) we can separately com-
pute precision for the top 1000 new relations, the top 10,000 new relations, the top
100,000, and so on. In each case we take a random sample of that set. This will
show us how the precision curve behaves as we extract more and more tuples. But
there is no way to directly evaluate recall.
20.3 Extracting Events
The task of event extraction is to identify mentions of events in texts. For theevent
extraction
purposes of this task, an event mention is any expression denoting an event or state
that can be assigned to a particular point, or interval, in time. The following markup
of the sample text on page 435 shows all the events in this text.

--- Page 447 ---
20.4 • R EPRESENTING TIME 447
[EVENT Citing] high fuel prices, United Airlines [ EVENT said] Fri-
day it has [ EVENT increased] fares by $6 per round trip on ﬂights to
some cities also served by lower-cost carriers. American Airlines, a unit
of AMR Corp., immediately [ EVENT matched] [ EVENT the move],
spokesman Tim Wagner [ EVENT said]. United, a unit of UAL Corp.,
[EVENT said] [ EVENT the increase] took effect Thursday and [ EVENT
applies] to most routes where it [ EVENT competes] against discount
carriers, such as Chicago to Dallas and Denver to San Francisco.
In English, most event mentions correspond to verbs, and most verbs introduce
events. However, as we can see from our example, this is not always the case. Events
can be introduced by noun phrases, as in the move andthe increase , and some verbs
fail to introduce events, as in the phrasal verb took effect , which refers to when the
event began rather than to the event itself. Similarly, light verbs such as make ,take, light verbs
andhave often fail to denote events. A light verb is a verb that has very little meaning
itself, and the associated event is instead expressed by its direct object noun. In light
verb examples like took a ﬂight , it’s the word ﬂight that deﬁnes the event; these light
verbs just provide a syntactic structure for the noun’s arguments.
Various versions of the event extraction task exist, depending on the goal. For
example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract
events and aspects like their aspectual and temporal properties. Events are to be
classiﬁed as actions, states, reporting events (say, report, tell, explain ), perceptionreporting
events
events, and so on. The aspect, tense, and modality of each event also needs to be
extracted. Thus for example the various said events in the sample text would be
annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).
Event extraction is generally modeled via supervised learning, detecting events
via IOB sequence models and assigning event classes and attributes with multi-class
classiﬁers. The input can be neural models starting from encoders; or classic feature-
based models using features like those in Fig. 20.10.
Feature Explanation
Character afﬁxes Character-level preﬁxes and sufﬁxes of target word
Nominalization sufﬁx Character-level sufﬁxes for nominalizations (e.g., -tion )
Part of speech Part of speech of the target word
Light verb Binary feature indicating that the target is governed by a light verb
Subject syntactic category Syntactic category of the subject of the sentence
Morphological stem Stemmed version of the target word
Verb root Root form of the verb basis for a nominalization
WordNet hypernyms Hypernym set for the target
Figure 20.10 Features commonly used in classic feature-based approaches to event detection.
20.4 Representing Time
Let’s begin by introducing the basics of temporal logic and how human languages temporal logic
convey temporal information. The most straightforward theory of time holds that it
ﬂows inexorably forward and that events are associated with either points or inter-
vals in time, as on a timeline. We can order distinct events by situating them on the
timeline; one event precedes another if the ﬂow of time leads from the ﬁrst event

--- Page 448 ---
448 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
to the second. Accompanying these notions in most theories is the idea of the cur-
rent moment in time. Combining this notion with the idea of a temporal ordering
relationship yields the familiar notions of past, present, and future.
Various kinds of temporal representation systems can be used to talk about tem-
poral ordering relationship. One of the most commonly used in computational mod-
eling is the interval algebra of Allen (1984). Allen models all events and time interval algebra
expressions as intervals there is no representation for points (although intervals can
be very short). In order to deal with intervals without points, he identiﬁes 13 primi-
tive relations that can hold between these temporal intervals. Fig. 20.11 shows these
13Allen relations . Allen relations
BABA
BA
AAB
BAB
Time A  before BB after  AA overlaps BB overlaps' AA meets BB meets' AA equals B(B equals A)A starts BB starts' AA finishes BB finishes' A
BA during BB during' AA
Figure 20.11 The 13 temporal relations from Allen (1984).
20.4.1 Reichenbach’s reference point
The relation between simple verb tenses and points in time is by no means straight-
forward. The present tense can be used to refer to a future event, as in this example:
(20.15) Ok, we ﬂy from San Francisco to Boston at 10.
Or consider the following examples:
(20.16) Flight 1902 arrived late.
(20.17) Flight 1902 had arrived late.
Although both refer to events in the past, representing them in the same way seems
wrong. The second example seems to have another unnamed event lurking in the
background (e.g., Flight 1902 had already arrived late when something else hap-
pened).

--- Page 449 ---
20.4 • R EPRESENTING TIME 449
To account for this phenomena, Reichenbach (1947) introduced the notion of
areference point . In our simple temporal scheme, the current moment in time is reference point
equated with the time of the utterance and is used as a reference point for when
the event occurred (before, at, or after). In Reichenbach’s approach, the notion of
the reference point is separated from the utterance time and the event time. The
following examples illustrate the basics of this approach:
(20.18) When Mary’s ﬂight departed, I ate lunch.
(20.19) When Mary’s ﬂight departed, I had eaten lunch.
In both of these examples, the eating event has happened in the past, that is, prior
to the utterance. However, the verb tense in the ﬁrst example indicates that the eating
event began when the ﬂight departed, while the second example indicates that the
eating was accomplished prior to the ﬂight’s departure. Therefore, in Reichenbach’s
terms the departure event speciﬁes the reference point. These facts can be accom-
modated by additional constraints relating the eating anddeparture events. In the
ﬁrst example, the reference point precedes the eating event, and in the second exam-
ple, the eating precedes the reference point. Figure 20.12 illustrates Reichenbach’s
approach with the primary English tenses. Exercise 20.4 asks you to represent these
examples in FOL.
Past PerfectSimple PastPresent Perfect
Simple FutureFuture PerfectPresentEE
EER
RUR,EUR,U
U,R,EU,RU
Figure 20.12 Reichenbach’s approach applied to various English tenses. In these diagrams,
time ﬂows from left to right, Edenotes the time of the event, Rdenotes the reference time,
andUdenotes the time of the utterance.
Languages have many other ways to convey temporal information besides tense.
Most useful for our purposes will be temporal expressions like in the morning or
6:45 orafterwards .
(20.20) I’d like to go at 6:45 in the morning.
(20.21) Somewhere around noon, please.
(20.22) I want to take the train back afterwards.
Incidentally, temporal expressions display a fascinating metaphorical conceptual
organization. Temporal expressions in English are frequently expressed in spatial
terms, as is illustrated by the various uses of at,in,somewhere , and near in these
examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations
such as these, in which one domain is systematically expressed in terms of another,
are very common in languages of the world.

--- Page 450 ---
450 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
20.5 Representing Aspect
A related notion to time is aspect , which is what we call the way events can be aspect
categorized by their internal temporal structure or temporal contour. By this we
mean questions like whether events are ongoing or have ended, or whether they are
conceptualized as happening at a point in time or over some interval. Such notions
of temporal contour have been used to divide event expressions into classes since
Aristotle, although the set of four classes we’ll introduce here is due to Vendler
(1967) (you may also see the German term aktionsart used to refer to these classes). aktionsart
The most basic aspectual distinction is between events (which involve change) events
andstates (which do not involve change). Stative expressions represent the notion states
stative of an event participant being in a state , or having a particular property, at a given
point in time. Stative expressions capture aspects of the world at a single point in
time, and conceptualize the participant as unchanging and continuous. Consider the
following ATIS examples.
(20.23) I like express trains.
(20.24) I need the cheapest fare.
(20.25) I want to go ﬁrst class.
In examples like these, the event participant denoted by the subject can be seen as
experiencing something at a speciﬁc point in time, and don’t involve any kind of
internal change over time (the liking or needing is conceptualized as continuous and
unchanging).
Non-states (which we’ll refer to as events ) are divided into subclasses; we’ll
introduce three here. Activity expressions describe events undertaken by a partic- activity
ipant that occur over a span of time (rather than being conceptualized as a single
point in time like stative expressions), and have no particular end point. Of course
in practice all things end, but the meaning of the expression doesn’t represent this
fact. Consider the following examples:
(20.26) She drove a Mazda.
(20.27) I live in Brooklyn.
These examples both specify that the subject is engaged in, or has engaged in, the
activity speciﬁed by the verb for some period of time, but doesn’t specify when the
driving or living might have stopped.
Two more classes of expressions, achievement expressions and accomplish-
ment expressions, describe events that take place over time, but also conceptualize
the event as having a particular kind of endpoint or goal. The Greek word telos
means ‘end’ or ’goal’ and so the events described by these kinds of expressions are
often called telic events. telic
Accomplishment expressions describe events that have a natural end point andaccomplishment
expressions
result in a particular state. Consider the following examples:
(20.28) He booked me a reservation.
(20.29) The 7:00 train got me to New York City.
In these examples, an event is seen as occurring over some period of time that ends
when the intended state is accomplished (i.e., the state of me having a reservation,
or me being in New York City).
The ﬁnal aspectual class, achievement expressions , is only subtly different thanachievement
expressions
accomplishments. Consider the following:

--- Page 451 ---
20.6 • T EMPORALLY ANNOTATED DATASETS : TIMEBANK 451
(20.30) She found her gate.
(20.31) I reached New York.
Like accomplishment expressions, achievement expressions result in a state. But
unlike accomplishments, achievement events are ‘punctual’: they are thought of as
happening in an instant and the verb doesn’t conceptualize the process or activ-
ity leading up the state. Thus the events in these examples may in fact have been
preceded by extended searching ortraveling events, but the verb doesn’t conceptu-
alize these preceding processes, but rather conceptualizes the events corresponding
toﬁnding andreaching as points, not intervals.
In summary, a standard way of categorizing event expressions by their temporal
contours is via these four general classes:
Stative: I know my departure gate.
Activity: John is ﬂying.
Accomplishment: Sally booked her ﬂight.
Achievement: She found her gate.
Before moving on, note that event expressions can easily be shifted from one
class to another. Consider the following examples:
(20.32) I ﬂew.
(20.33) I ﬂew to New York.
The ﬁrst example is a simple activity; it has no natural end point. The second ex-
ample is clearly an accomplishment event since it has an end point, and results in a
particular state. Clearly, the classiﬁcation of an event is not solely governed by the
verb, but by the semantics of the entire expression in context.
20.6 Temporally Annotated Datasets: TimeBank
TheTimeBank corpus consists of American English text annotated with temporal TimeBank
information (Pustejovsky et al., 2003). The annotations use TimeML (Saur ´ı et al.,
2006), a markup language for time based on Allen’s interval algebra discussed above
(Allen, 1984). There are three types of TimeML objects: an E VENT represent events
and states, a T IME represents time expressions like dates, and a L INK represents
various relationships between events and times (event-event, event-time, and time-
time). The links include temporal links (TL INK) for the 13 Allen relations, aspec-
tual links (AL INK) for aspectual relationships between events and subevents, and
SLINKS which mark factuality.
Consider the following sample sentence and its corresponding markup shown in
Fig. 20.13, selected from one of the TimeBank documents.
(20.34) Delta Air Lines earnings soared 33% to a record in the ﬁscal ﬁrst quarter,
bucking the industry trend toward declining proﬁts.
This text has three events and two temporal expressions (including the creation
time of the article, which serves as the document time), and four temporal links that
capture the using the Allen relations:
• Soaring e1isincluded in the ﬁscal ﬁrst quarter t58
• Soaring e1isbefore 1989-10-26 t57
• Soaring e1issimultaneous with the bucking e3

--- Page 452 ---
452 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
<TIMEX3 tid="t57" type="DATE" value="1989-10-26" functionInDocument="CREATION_TIME">
10/26/89 </TIMEX3>
Delta Air Lines earnings <EVENT eid="e1" class="OCCURRENCE"> soared </EVENT> 33% to a
record in <TIMEX3 tid="t58" type="DATE" value="1989-Q1" anchorTimeID="t57"> the
fiscal first quarter </TIMEX3>, <EVENT eid="e3" class="OCCURRENCE">bucking</EVENT>
the industry trend toward <EVENT eid="e4" class="OCCURRENCE">declining</EVENT>
profits.
Figure 20.13 Example from the TimeBank corpus.
• Declining e4includes soaring e1
We can also visualize the links as a graph. The TimeBank snippet in Eq. 20.35
would be represented with a graph like Fig. 20.14.
(20.35) [DCT:11/02/891] 1: Paciﬁc First Financial Corp. said 2shareholders
approved 3itsacquisition 4by Royal Trustco Ltd. of Toronto for $27 a share,
or $212 million. The thrift holding company said 5itexpects 6toobtain 7
regulatory approval 8andcomplete 9thetransaction 10byyear-end 11.
1234567811910BEFOREBEFOREAFTERSIMULTANEOUSENDSCULMINATESBEFOREEVIDENTIALMODALFACTIVEMODALEVIDENTIALMODAL
Figure 20.14 A graph of the text in Eq. 20.35, adapted from (Ocal et al., 2022). TL INKS
are shown in blue, AL INKS in red, and SL INKS in green.
20.7 Automatic Temporal Analysis
Here we introduce the three common steps used in analyzing time in text:
1. Extracting temporal expressions
2.Normalizing these expressions, by converting them to a standard format.
3.Linking events to times and extracting time graphs and timelines
20.7.1 Extracting Temporal Expressions
Temporal expressions are phrases that refer to absolute points in time, relative times,
durations, and sets of these. Absolute temporal expressions are those that can be absolute
mapped directly to calendar dates, times of day, or both. Relative temporal expres- relative
sions map to particular times through some other reference point (as in a week from
last Tuesday ). Finally, durations denote spans of time at varying levels of granular- duration
ity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample
temporal expressions in each of these categories.
Temporal expressions are grammatical constructions that often have temporal
lexical triggers as their heads, making them easy to ﬁnd. Lexical triggers might lexical triggers

--- Page 453 ---
20.7 • A UTOMATIC TEMPORAL ANALYSIS 453
Absolute Relative Durations
April 24, 1916 yesterday four hours
The summer of ’77 next semester three weeks
10:15 AM two weeks from yesterday six days
The 3rd quarter of 2006 last quarter the last three quarters
Figure 20.15 Examples of absolute, relational and durational temporal expressions.
be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist
of their phrasal projections: noun phrases, adjective phrases, and adverbial phrases
(Figure 20.16).
Category Examples
Noun morning ,noon ,night ,winter ,dusk,dawn
Proper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet
Adjective recent, past, annual, former
Adverb hourly, daily, monthly, yearly
Figure 20.16 Examples of temporal lexical triggers.
The task is to detect temporal expressions in running text, like this examples,
shown with TIMEX3 tags (Pustejovsky et al. 2005, Ferro et al. 2005).
A fare increase initiated <TIMEX3>last week</TIMEX3>by UAL
Corp’s United Airlines was matched by competitors over <TIMEX3>the
weekend</TIMEX3>, marking the second successful fare increase in
<TIMEX3>two weeks</TIMEX3>.
Rule-based approaches use cascades of regular expressions to recognize larger
and larger chunks from previous stages, based on patterns containing parts of speech,
trigger words (e.g., February ) or classes (e.g., MONTH ) (Chang and Manning, 2012;
Str¨otgen and Gertz, 2013; Chambers, 2013). Here’s a rule from SUTime (Chang and
Manning, 2012) for detecting expressions like 3 years old :
/(\d+)[-\s]($TEUnits)(s)?([-\s]old)?/
Sequence-labeling approaches use the standard IOB scheme, marking words
that are either (I)nside, (O)utside or at the (B)eginning of a temporal expression:
A
Ofare
Oincrease
Oinitiated
Olast
Bweek
Iby
OUAL
OCorp’s...
O
A statistical sequence labeler is trained, using either embeddings or a ﬁne-tuned
encoder, or classic features extracted from the token and context including words,
lexical triggers, and POS.
Temporal expression recognizers are evaluated with the usual recall, precision,
andF-measures. A major difﬁculty for all of these very lexicalized approaches is
avoiding expressions that trigger false positives:
(20.36) 1984 tells the story of Winston Smith...
(20.37) ...U2’s classic Sunday Bloody Sunday
20.7.2 Temporal Normalization
Temporal normalization is the task of mapping a temporal expression to a pointtemporal
normalization
in time or to a duration. Points in time correspond to calendar dates, to times of
day, or both. Durations primarily consist of lengths of time. Normalized times

--- Page 454 ---
454 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
<TIMEX3 i d =” t1 ’ ’ t y p e =”DATE” v a l u e =” 2007 −07 −02 ” f u n c t i o n I n D o c u m e n t =”CREATION TIME” >
J u l y 2 , 2007 </TIMEX3 >A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d =” t 2 ” t y p e =”DATE”
v a l u e =” 2007 −W26” anchorTimeID=” t 1 ” >l a s t week </TIMEX3 >by U n i t e d A i r l i n e s was
matched by c o m p e t i t o r s o ve r <TIMEX3 i d =” t 3 ” t y p e =”DURATION” v a l u e =”P1WE”
anchorTimeID=” t 1 ” >t h e weekend </TIMEX3 >, marking t h e second s u c c e s s f u l f a r e
i n c r e a s e i n <TIMEX3 i d =” t 4 ” t y p e =”DURATION” v a l u e =”P2W” anchorTimeID=” t 1 ” >two
weeks </TIMEX3 >.
Figure 20.17 TimeML markup including normalized values for temporal expressions.
are represented via the ISO 8601 standard for encoding temporal values (ISO8601,
2004). Fig. 20.17 reproduces our earlier example with these value attributes.
The dateline, or document date, for this text was July 2, 2007 . The ISO repre-
sentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.
The encodings for the temporal expressions in our sample text all follow from this
date, and are shown here as values for the VALUE attribute.
The ﬁrst temporal expression in the text proper refers to a particular week of the
year. In the ISO standard, weeks are numbered from 01 to 53, with the ﬁrst week
of the year being the one that has the ﬁrst Thursday of the year. These weeks are
represented with the template YYYY-Wnn. The ISO week for our document date is
week 27; thus the value for last week is represented as “2007-W26”.
The next temporal expression is the weekend . ISO weeks begin on Monday;
thus, weekends occur at the end of a week and are fully contained within a single
week. Weekends are treated as durations, so the value of the VALUE attribute has
to be a length. Durations are represented according to the pattern P nx, where nis
an integer denoting the length and xrepresents the unit, as in P3Y for three years
or P2D for two days . In this example, one weekend is captured as P1WE. In this
case, there is also sufﬁcient information to anchor this particular weekend as part of
a particular week. Such information is encoded in the ANCHOR TIMEID attribute.
Finally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18
give some more examples, but there is a lot more to the various temporal annotation
standards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005)
for more details.
Unit Pattern Sample Value
Fully speciﬁed dates YYYY-MM-DD 1991-09-28
Weeks YYYY-Wnn 2007-W27
Weekends PnWE P1WE
24-hour clock times HH:MM:SS 11:13:45
Dates and times YYYY-MM-DDTHH:MM:SS 1991-09-28T11:00:00
Financial quarters Qn 1999-Q3
Figure 20.18 Sample ISO patterns for representing various times and durations.
Most current approaches to temporal normalization are rule-based (Chang and
Manning 2012, Str ¨otgen and Gertz 2013). Patterns that match temporal expressions
are associated with semantic analysis procedures. For example, the pattern above for
recognizing phrases like 3 years old can be associated with the predicate Duration
that takes two arguments, the length and the unit of time:
pattern: /(\d+)[-\s]($TEUnits)(s)?([-\s]old)?/
result: Duration($1, $2)
The task is difﬁcult because fully qualiﬁed temporal expressions are fairly rare
in real texts. Most temporal expressions in news articles are incomplete and are only
implicitly anchored, often with respect to the dateline of the article, which we refer

--- Page 455 ---
20.7 • A UTOMATIC TEMPORAL ANALYSIS 455
to as the document’s temporal anchor . The values of temporal expressions suchtemporal
anchor
astoday ,yesterday , ortomorrow can all be computed with respect to this temporal
anchor. The semantic procedure for today simply assigns the anchor, and the attach-
ments for tomorrow andyesterday add a day and subtract a day from the anchor,
respectively. Of course, given the cyclic nature of our representations for months,
weeks, days, and times of day, our temporal arithmetic procedures must use modulo
arithmetic appropriate to the time unit being used.
Unfortunately, even simple expressions such as the weekend orWednesday in-
troduce a fair amount of complexity. In our current example, the weekend clearly
refers to the weekend of the week that immediately precedes the document date. But
this won’t always be the case, as is illustrated in the following example.
(20.38) Random security checks that began yesterday at Sky Harbor will continue
at least through the weekend.
In this case, the expression the weekend refers to the weekend of the week that the
anchoring date is part of (i.e., the coming weekend). The information that signals
this meaning comes from the tense of continue , the verb governing the weekend .
Relative temporal expressions are handled with temporal arithmetic similar to
that used for today andyesterday . The document date indicates that our example
article is ISO week 27, so the expression last week normalizes to the current week
minus 1. To resolve ambiguous next andlastexpressions we consider the distance
from the anchoring date to the nearest unit. Next Friday can refer either to the
immediately next Friday or to the Friday following that, but the closer the document
date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such
ambiguities are handled by encoding language and domain-speciﬁc heuristics into
the temporal attachments.
20.7.3 Temporal Ordering of Events
The goal of temporal analysis, is to link times to events and then ﬁt all these events
into a complete timeline. This ambitious task is the subject of considerable current
research but solving it with a high level of accuracy is beyond the capabilities of
current systems. A somewhat simpler, but still useful, task is to impose a partial or-
dering on the events and temporal expressions mentioned in a text. Such an ordering
can provide many of the same beneﬁts as a true timeline. An example of such a par-
tial ordering is the determination that the fare increase by American Airlines came
after the fare increase by United in our sample text. Determining such an ordering
can be viewed as a binary relation detection and classiﬁcation task.
Even this partial ordering task assumes that in addition to the detecting and nor-
malizing time expressions steps described above, we have already detected all the
events in the text. Indeed, many temporal expressions are anchored to events men-
tioned in a text and not directly to other temporal expressions. Consider the follow-
ing example:
(20.39) One week after the storm, JetBlue issued its customer bill of rights.
To determine when JetBlue issued its customer bill of rights we need to determine
the time of the storm event, and then we need to modify that time by the temporal
expression one week after .
Thus once the events and times have been detected, our goal next is to assert links
between all the times and events: i.e. creating event-event, event-time, time-time,
DCT-event, and DCT-time TimeML TL INKS . This can be done by training time
relation classiﬁers to predict the correct T: INKbetween each pair of times/events,

--- Page 456 ---
456 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
supervised by the gold labels in the TimeBank corpus with features like words/em-
beddings, parse paths, tense and aspect The sieve-based architecture using precision-
ranked sets of classiﬁers, which we’ll introduce in Chapter 23, is also commonly
used.
Systems that perform all 4 tasks (time extraction creation and normalization,
event extraction, and time/event linking) include TARSQI (Verhagen et al., 2005)
CLEAR TK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza
and Tonelli, 2016).
20.8 Template Filling
Many texts contain reports of events, and possibly sequences of events, that often
correspond to fairly common, stereotypical situations in the world. These abstract
situations or stories, related to what have been called scripts (Schank and Abel- scripts
son, 1977), consist of prototypical sequences of sub-events, participants, and their
roles. The strong expectations provided by these scripts can facilitate the proper
classiﬁcation of entities, the assignment of entities into roles and relations, and most
critically, the drawing of inferences that ﬁll in things that have been left unsaid. In
their simplest form, such scripts can be represented as templates consisting of ﬁxed templates
sets of slots that take as values slot-ﬁllers belonging to particular classes. The task
oftemplate ﬁlling is to ﬁnd documents that invoke particular scripts and then ﬁll the template ﬁlling
slots in the associated templates with ﬁllers extracted from the text. These slot-ﬁllers
may consist of text segments extracted directly from the text, or they may consist of
concepts that have been inferred from text elements through some additional pro-
cessing.
A ﬁlled template from our original airline story might look like the following.
FARE-RAISE ATTEMPT :2
6664LEAD AIRLINE : U NITED AIRLINES
AMOUNT : $6
EFFECTIVE DATE: 2006-10-26
FOLLOWER : A MERICAN AIRLINES3
7775
This template has four slots ( LEAD AIRLINE ,AMOUNT ,EFFECTIVE DATE ,FOL-
LOWER ). The next section describes a standard sequence-labeling approach to ﬁlling
slots. Section 20.8.2 then describes an older system based on the use of cascades of
ﬁnite-state transducers and designed to address a more complex template-ﬁlling task
that current learning-based systems don’t yet address.
20.8.1 Machine Learning Approaches to Template Filling
In the standard paradigm for template ﬁlling, we are given training documents with
text spans annotated with predeﬁned templates and their slot ﬁllers. Our goal is to
create one template for each event in the input, ﬁlling in the slots with text spans.
The task is generally modeled by training two separate supervised systems. The
ﬁrst system decides whether the template is present in a particular sentence. This
task is called template recognition or sometimes, in a perhaps confusing bit oftemplate
recognition
terminology, event recognition . Template recognition can be treated as a text classi-
ﬁcation task, with features extracted from every sequence of words that was labeled
in training documents as ﬁlling any slot from the template being detected. The usual

--- Page 457 ---
20.8 • T EMPLATE FILLING 457
set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,
syntactic chunk tags, and named entity tags.
The second system has the job of role-ﬁller extraction . A separate classiﬁer isrole-ﬁller
extraction
trained to detect each role ( LEAD -AIRLINE ,AMOUNT , and so on). This can be a
binary classiﬁer that is run on every noun-phrase in the parsed input sentence, or a
sequence model run over sequences of words. Each role classiﬁer is trained on the
labeled data in the training set. Again, the usual set of features can be used, but now
trained only on an individual noun phrase or the ﬁllers of a single slot.
Multiple non-identical text segments might be labeled with the same slot la-
bel. For example in our sample text, the strings United orUnited Airlines might be
labeled as the L EAD AIRLINE . These are not incompatible choices and the corefer-
ence resolution techniques introduced in Chapter 23 can provide a path to a solution.
A variety of annotated collections have been used to evaluate this style of ap-
proach to template ﬁlling, including sets of job announcements, conference calls for
papers, restaurant guides, and biological texts. A key open question is extracting
templates in cases where there is no training data or even predeﬁned templates, by
inducing templates as sets of linked events (Chambers and Jurafsky, 2011).
20.8.2 Earlier Finite-State Template-Filling Systems
The templates above are relatively simple. But consider the task of producing a
template that contained all the information in a text like this one (Grishman and
Sundheim, 1995):
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan
with a local concern and a Japanese trading house to produce golf clubs to be
shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-
ized at 20 million new Taiwan dollars, will start production in January 1990
with production of 20,000 iron and “metal wood” clubs a month.
The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were
a series of U.S. government-organized information-extraction evaluations) was to
produce hierarchically linked templates describing joint ventures. Figure 20.19
shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how
the ﬁller of the ACTIVITY slot of the TIE-UPtemplate is itself a template with slots.
Tie-up-1 Activity-1 :
RELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.
ENTITIES Bridgestone Sports Co. P RODUCT iron and “metal wood” clubs
a local concern S TART DATE DURING: January 1990
a Japanese trading house
JOINT VENTURE Bridgestone Sports Taiwan Co.
ACTIVITY Activity-1
AMOUNT NT$20000000
Figure 20.19 The templates produced by FASTUS given the input text on page 457.
Early systems for dealing with these complex templates were based on cascades
of transducers based on handwritten rules, as sketched in Fig. 20.20.
The ﬁrst four stages use handwritten regular expression and grammar rules to
do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and
events with a recognizer based on ﬁnite-state transducers (FSTs), and inserts the rec-
ognized objects into the appropriate slots in templates. This FST recognizer is based

--- Page 458 ---
458 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
No. Step Description
1 Tokens Tokenize input stream of characters
2 Complex Words Multiword phrases, numbers, and proper names.
3 Basic phrases Segment sentences into noun and verb groups
4 Complex phrases Identify complex noun groups and verb groups
5 Semantic Patterns Identify entities and events, insert into templates.
6 Merging Merge references to the same entity or event
Figure 20.20 Levels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a
speciﬁc type of information which is then passed on to the next higher level.
on hand-built regular expressions like the following (NG indicates Noun-Group and
VG Verb-Group), which matches the ﬁrst sentence of the news story above.
NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)
VG(Produce) NG(Product)
The result of processing these two sentences is the ﬁve draft templates (Fig. 20.21)
that must then be merged into the single hierarchical structure shown in Fig. 20.19.
The merging algorithm, after performing coreference resolution, merges two activi-
ties that are likely to be describing the same events.
#Template/Slot Value
1RELATIONSHIP : TIE-UP
ENTITIES : Bridgestone Co., a local concern, a Japanese trading house
2ACTIVITY : PRODUCTION
PRODUCT : “golf clubs”
3RELATIONSHIP : TIE-UP
JOINT VENTURE : “Bridgestone Sports Taiwan Co.”
AMOUNT : NT$20000000
4ACTIVITY : PRODUCTION
COMPANY : “Bridgestone Sports Taiwan Co.”
START DATE: DURING : January 1990
5ACTIVITY : PRODUCTION
PRODUCT : “iron and “metal wood” clubs”
Figure 20.21 The ﬁve partial templates produced by stage 5 of FASTUS . These templates
are merged in stage 6 to produce the ﬁnal template shown in Fig. 20.19 on page 457.
20.9 Summary
This chapter has explored techniques for extracting limited forms of semantic con-
tent from texts.
•Relations among entities can be extracted by pattern-based approaches, su-
pervised learning methods when annotated training data is available, lightly
supervised bootstrapping methods when small numbers of seed tuples or
seed patterns are available, distant supervision when a database of relations
is available, and unsupervised orOpen IE methods.
• Reasoning about time can be facilitated by detection and normalization of
temporal expressions .

--- Page 459 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 459
•Events can be ordered in time using sequence models and classiﬁers trained
on temporally- and event-labeled data like the TimeBank corpus .
•Template-ﬁlling applications can recognize stereotypical situations in texts
and assign elements from the text to roles represented as ﬁxed sets of slots .
Bibliographical and Historical Notes
The earliest work on information extraction addressed the template-ﬁlling task in the
context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.
government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-
heim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert
et al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite inﬂuential and inspired
later systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the
MUC evaluation techniques.
Due to the difﬁculty of porting systems from one domain to another, attention
shifted to machine learning approaches. Early supervised learning approaches to
IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996)
focused on automating the knowledge acquisition process, mainly for ﬁnite-state
rule-based systems. Their success, and the earlier success of HMM-based speech
recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs
McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of fea-
tures (Zhou et al., 2005). Neural approaches followed from the pioneering results of
Collobert et al. (2011), who applied a CRF on top of a convolutional net.
Progress in this area continues to be stimulated by formal evaluations with shared
benchmark datasets, including the Automatic Content Extraction (ACE) evaluations
of 2000-2007 on named entity recognition, relation extraction, and temporal ex-
pressions1, the KBP (Knowledge Base Population ) evaluations (Ji et al. 2010, Sur- KBP
deanu 2013) of relation extraction tasks like slot ﬁlling (extracting attributes (‘slots’) slot ﬁlling
like age, birthplace, and spouse for a given entity) and a series of SemEval work-
shops (Hendrickx et al., 2009).
Semisupervised relation extraction was ﬁrst proposed by Hearst (1992b), and
extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-
BALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-
vision algorithm we describe was drawn from Mintz et al. (2009), who ﬁrst used
the term ‘distant supervision’ (which was suggested to them by Chris Manning)
but similar ideas had occurred in earlier systems like Craven and Kumlien (1999)
and Morgan et al. (2004) under the name weakly labeled data , as well as in Snow
et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and
Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include
KNOW ITALLEtzioni et al. (2005), TextRunner (Banko et al., 2007), and R EVERB
(Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines
the advantages of distant supervision and Open IE.
1www.nist.gov/speech/tests/ace/

--- Page 460 ---
460 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
Exercises
20.1 Acronym expansion, the process of associating a phrase with an acronym, can
be accomplished by a simple form of relational analysis. Develop a system
based on the relation analysis approaches described in this chapter to populate
a database of acronym expansions. If you focus on English Three Letter
Acronyms (TLAs) you can evaluate your system’s performance by comparing
it to Wikipedia’s TLA page.
20.2 Acquire the CMU seminar corpus and develop a template-ﬁlling system by
using any of the techniques mentioned in Section 20.8. Analyze how well
your system performs as compared with state-of-the-art results on this corpus.
20.3 A useful functionality in newer email and calendar applications is the ability
to associate temporal expressions connected with events in email (doctor’s
appointments, meeting planning, party invitations, etc.) with speciﬁc calendar
entries. Collect a corpus of email containing temporal expressions related to
event planning. How do these expressions compare to the kinds of expressions
commonly found in news text that we’ve been discussing in this chapter?
20.4 For the following sentences, give FOL translations that capture the temporal
relationships between the events.
1. When Mary’s ﬂight departed, I ate lunch.
2. When Mary’s ﬂight departed, I had eaten lunch.

--- Page 461 ---
CHAPTER
21Semantic Role Labeling
“Who, What, Where, When, With what, Why, How”
The seven circumstances, associated with Hermagoras and Aristotle (Sloan, 2010)
Sometime between the 7th and 4th centuries BCE, the Indian grammarian P ¯an.ini1
wrote a famous treatise on Sanskrit grammar, the As .t.¯adhy ¯ay¯ı (‘8 books’), a treatise
that has been called “one of the greatest monuments of hu-
man intelligence” (Bloomﬁeld, 1933, 11). The work de-
scribes the linguistics of the Sanskrit language in the form
of 3959 sutras, each very efﬁciently (since it had to be
memorized!) expressing part of a formal rule system that
brilliantly preﬁgured modern mechanisms of formal lan-
guage theory (Penn and Kiparsky, 2012). One set of rules
describes the k¯arakas , semantic relationships between a
verb and noun arguments, roles like agent ,instrument , or
destination . P¯an.ini’s work was the earliest we know of
that modeled the linguistic realization of events and their
participants. This task of understanding how participants relate to events—being
able to answer the question “Who did what to whom” (and perhaps also “when and
where”)—is a central question of natural language processing.
Let’s move forward 2.5 millennia to the present and consider the very mundane
goal of understanding text about a purchase of stock by XYZ Corporation. This
purchasing event and its participants can be described by a wide variety of surface
forms. The event can be described by a verb ( sold, bought ) or a noun ( purchase ),
and XYZ Corp can be the syntactic subject (of bought ), the indirect object (of sold),
or in a genitive or noun compound relation (with the noun purchase ) despite having
notionally the same role in all of them:
• XYZ corporation bought the stock.
• They sold the stock to XYZ corporation.
• The stock was bought by XYZ corporation.
• The purchase of the stock by XYZ corporation...
• The stock purchase by XYZ corporation...
In this chapter we introduce a level of representation that captures the common-
ality between these sentences: there was a purchase event, the participants were
XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic
representations , semantic roles , express the role that arguments of a predicate take
in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce
semantic role labeling , the task of assigning roles to spans in sentences, and selec-
tional restrictions , the preferences that predicates express about their arguments,
such as the fact that the theme of eatis generally something edible.
1Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based
on the Sanskrit grammar of Panini. Image from the Wellcome Collection.

--- Page 462 ---
462 CHAPTER 21 • S EMANTIC ROLE LABELING
21.1 Semantic Roles
Consider the meanings of the arguments Sasha ,Pat,the window , and the door in
these two sentences.
(21.1) Sasha broke the window.
(21.2) Pat opened the door.
The subjects Sasha andPat, what we might call the breaker of the window-
breaking event and the opener of the door-opening event, have something in com-
mon. They are both volitional actors, often animate, and they have direct causal
responsibility for their events.
Thematic roles are a way to capture this semantic commonality between break- thematic roles
ersandopeners . We say that the subjects of both these verbs are agents . Thus, agents
AGENT is the thematic role that represents an abstract idea such as volitional causa-
tion. Similarly, the direct objects of both these verbs, the BrokenThing andOpenedThing ,
are both prototypically inanimate objects that are affected in some way by the action.
The semantic role for these participants is theme . theme
Thematic Role Deﬁnition
AGENT The volitional causer of an event
EXPERIENCER The experiencer of an event
FORCE The non-volitional causer of the event
THEME The participant most directly affected by an event
RESULT The end product of an event
CONTENT The proposition or content of a propositional event
INSTRUMENT An instrument used in an event
BENEFICIARY The beneﬁciary of an event
SOURCE The origin of the object of a transfer event
GOAL The destination of an object of a transfer event
Figure 21.1 Some commonly used thematic roles with their deﬁnitions.
Although thematic roles are one of the oldest linguistic models, as we saw above,
their modern formulation is due to Fillmore (1968) and Gruber (1965). Although
there is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-
matic roles that have been used in various computational papers, together with rough
deﬁnitions and examples. Most thematic role sets have about a dozen roles, but we’ll
see sets with smaller numbers of roles with even more abstract meanings, and sets
with very large numbers of roles that are speciﬁc to situations. We’ll use the general
term semantic roles for all sets of roles, whether small or large. semantic roles
21.2 Diathesis Alternations
The main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple inferences that aren’t possible
from the pure surface string of words, or even from the parse tree. To extend the
earlier examples, if a document says that Company A acquired Company B , we’d
like to know that this answers the query Was Company B acquired? despite the fact
that the two sentences have very different surface syntax. Similarly, this shallow
semantics might act as a useful intermediate language in machine translation.

--- Page 463 ---
21.2 • D IATHESIS ALTERNATIONS 463
Thematic Role Example
AGENT The waiter spilled the soup.
EXPERIENCER John has a headache.
FORCE The wind blows debris from the mall into our yards.
THEME Only after Benjamin Franklin broke the ice ...
RESULT The city built a regulation-size baseball diamond ...
CONTENT Mona asked “You met Mary Ann at a supermarket?”
INSTRUMENT He poached catﬁsh, stunning them with a shocking device ...
BENEFICIARY Whenever Ann Callahan makes hotel reservations for her boss ...
SOURCE I ﬂew in from Boston .
GOAL I drove to Portland .
Figure 21.2 Some prototypical examples of various thematic roles.
Semantic roles thus help generalize over different surface realizations of pred-
icate arguments. For example, while the AGENT is often realized as the subject of
the sentence, in other cases the THEME can be the subject. Consider these possible
realizations of the thematic arguments of the verb break :
(21.3) John
AGENTbroke the window.
THEME
(21.4) John
AGENTbroke the window
THEMEwith a rock.
INSTRUMENT
(21.5) The rock
INSTRUMENTbroke the window.
THEME
(21.6) The window
THEMEbroke.
(21.7) The window
THEMEwas broken by John.
AGENT
These examples suggest that break has (at least) the possible arguments AGENT ,
THEME , and INSTRUMENT . The set of thematic role arguments taken by a verb is
often called the thematic grid ,q-grid, or case frame . We can see that there are thematic grid
case frame (among others) the following possibilities for the realization of these arguments of
break :
AGENT /Subject, THEME /Object
AGENT /Subject, THEME /Object, INSTRUMENT /PPwith
INSTRUMENT /Subject, THEME /Object
THEME /Subject
It turns out that many verbs allow their thematic roles to be realized in various
syntactic positions. For example, verbs like give can realize the THEME and GOAL
arguments in two different ways:
(21.8) a. Doris
AGENTgave the book
THEMEto Cary.
GOAL
b.Doris
AGENTgave Cary
GOALthe book.
THEME
These multiple argument structure realizations (the fact that break can take AGENT ,
INSTRUMENT , or THEME as subject, and give can realize its THEME and GOAL in
either order) are called verb alternations ordiathesis alternations . The alternationverb
alternation
we showed above for give, the dative alternation , seems to occur with particular se-dative
alternation
mantic classes of verbs, including “verbs of future having” ( advance ,allocate ,offer,

--- Page 464 ---
464 CHAPTER 21 • S EMANTIC ROLE LABELING
owe), “send verbs” ( forward ,hand ,mail), “verbs of throwing” ( kick,pass,throw ),
and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which
they belong (47 high-level classes, divided into 193 more speciﬁc classes) and the
various alternations in which they participate. These lists of verb classes have been
incorporated into the online resource VerbNet (Kipper et al., 2000), which links each
verb to both WordNet and FrameNet entries.
21.3 Semantic Roles: Problems with Thematic Roles
Representing meaning at the thematic role level seems like it should be useful in
dealing with complications like diathesis alternations. Yet it has proved quite difﬁ-
cult to come up with a standard set of roles, and equally difﬁcult to produce a formal
deﬁnition of roles like AGENT ,THEME , or INSTRUMENT .
For example, researchers attempting to deﬁne role sets often ﬁnd they need to
fragment a role like AGENT orTHEME into many speciﬁc roles. Levin and Rappa-
port Hovav (2005) summarize a number of such cases, such as the fact there seem
to be at least two kinds of INSTRUMENTS ,intermediary instruments that can appear
as subjects and enabling instruments that cannot:
(21.9) a. Shelly cut the banana with a knife.
b. The knife cut the banana.
(21.10) a. Shelly ate the sliced banana with a fork.
b. *The fork ate the sliced banana.
In addition to the fragmentation problem, there are cases in which we’d like to
reason about and generalize across semantic roles, but the ﬁnite discrete lists of roles
don’t let us do this.
Finally, it has proved difﬁcult to formally deﬁne the thematic roles. Consider the
AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any
individual noun phrase might not exhibit all of these properties.
These problems have led to alternative semantic role models that use either semantic role
many fewer or many more roles.
The ﬁrst of these options is to deﬁne generalized semantic roles that abstract
over the speciﬁc thematic roles. For example, PROTO -AGENT and PROTO -PATIENT proto-agent
proto-patient are generalized roles that express roughly agent-like and roughly patient-like mean-
ings. These roles are deﬁned, not by necessary and sufﬁcient conditions, but rather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. Thus, the more an argument displays agent-like properties (being voli-
tionally involved in the event, causing an event or a change of state in another par-
ticipant, being sentient or intentionally involved, moving) the greater the likelihood
that the argument can be labeled a PROTO -AGENT . The more patient-like the proper-
ties (undergoing change of state, causally affected by another participant, stationary
relative to other participants, etc.), the greater the likelihood that the argument can
be labeled a PROTO -PATIENT .
The second direction is instead to deﬁne semantic roles that are speciﬁc to a
particular verb or a particular group of semantically related verbs or nouns.
In the next two sections we describe two commonly used lexical resources that
make use of these alternative versions of semantic roles. PropBank uses both proto-
roles and verb-speciﬁc semantic roles. FrameNet uses semantic roles that are spe-
ciﬁc to a general semantic idea called a frame .

--- Page 465 ---
21.4 • T HEPROPOSITION BANK 465
21.4 The Proposition Bank
The Proposition Bank , generally referred to as PropBank , is a resource of sen- PropBank
tences annotated with semantic roles. The English PropBank labels all the sentences
in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese
TreeBank. Because of the difﬁculty of deﬁning a universal set of thematic roles,
the semantic roles in PropBank are deﬁned with respect to an individual verb sense.
Each sense of each verb thus has a speciﬁc set of roles, which are given only numbers
rather than names: Arg0 ,Arg1 ,Arg2 , and so on. In general, Arg0 represents the
PROTO -AGENT , and Arg1 , the PROTO -PATIENT . The semantics of the other roles
are less consistent, often being deﬁned speciﬁcally for each verb. Nonetheless there
are some generalization; the Arg2 is often the benefactive, instrument, attribute, or
end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4
the end point.
Here are some slightly simpliﬁed PropBank entries for one sense each of the
verbs agree andfall. Such PropBank entries are called frame ﬁles ; note that the
deﬁnitions in the frame ﬁle for each role (“Other entity agreeing”, “Extent, amount
fallen”) are informal glosses intended to be read by humans, rather than being formal
deﬁnitions.
(21.11) agree.01
Arg0: Agreer
Arg1: Proposition
Arg2: Other entity agreeing
Ex1: [ Arg0 The group] agreed [Arg1 it wouldn’t make an offer].
Ex2: [ ArgM-TMP Usually] [ Arg0 John] agrees [Arg2 with Mary]
[Arg1 on everything].
(21.12) fall.01
Arg1: Logical subject, patient, thing falling
Arg2: Extent, amount fallen
Arg3: start point
Arg4: end point, end state of arg1
Ex1: [ Arg1 Sales] fell[Arg4 to $25 million] [ Arg3 from $27 million].
Ex2: [ Arg1 The average junk bond] fell[Arg2 by 4.2%].
Note that there is no Arg0 role for fall, because the normal subject of fallis a
PROTO -PATIENT .
The PropBank semantic roles can be useful in recovering shallow semantic in-
formation about verbal arguments. Consider the verb increase :
(21.13) increase.01 “go up incrementally”
Arg0: causer of increase
Arg1: thing increasing
Arg2: amount increased by, EXT, or MNR
Arg3: start point
Arg4: end point
A PropBank semantic role labeling would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case Big
Fruit Co. is the AGENT andthe price of bananas is the THEME , despite the differing
surface forms.

--- Page 466 ---
466 CHAPTER 21 • S EMANTIC ROLE LABELING
(21.14) [ Arg0 Big Fruit Co. ] increased [ Arg1 the price of bananas].
(21.15) [ Arg1 The price of bananas] was increased again [ Arg0 by Big Fruit Co. ]
(21.16) [ Arg1 The price of bananas] increased [ Arg2 5%].
PropBank also has a number of non-numbered arguments called ArgMs , (ArgM-
TMP, ArgM-LOC, etc.) which represent modiﬁcation or adjunct meanings. These
are relatively stable across predicates, so aren’t listed with each frame ﬁle. Data
labeled with these modiﬁers can be helpful in training systems to detect temporal,
location, or directional modiﬁcation across predicates. Some of the ArgM’s include:
TMP when? yesterday evening, now
LOC where? at the museum, in San Francisco
DIR where to/from? down, to Bangkok
MNR how? clearly, with much enthusiasm
PRP/CAU why? because ... , in response to the ruling
REC themselves, each other
ADV miscellaneous
PRD secondary predication ...ate the meat raw
While PropBank focuses on verbs, a related project, NomBank (Meyers et al., NomBank
2004) adds annotations to noun predicates. For example the noun agreement in
Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as
the Arg2. This allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.
21.5 FrameNet
While making inferences about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
inferences in many more situations, across different verbs, and also between verbs
and nouns. For example, we’d like to extract the similarity among these three sen-
tences:
(21.17) [ Arg1 The price of bananas] increased [ Arg2 5%].
(21.18) [ Arg1 The price of bananas] rose [ Arg2 5%].
(21.19) There has been a [ Arg2 5%] rise [ Arg1 in the price of bananas].
Note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. We’d like a system to recognize that the
price of bananas is what went up, and that 5%is the amount it went up, no matter
whether the 5%appears as the object of the verb increased or as a nominal modiﬁer
of the noun rise.
The FrameNet project is another semantic-role-labeling project that attempts FrameNet
to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,
Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank
project are speciﬁc to an individual verb, roles in the FrameNet project are speciﬁc
to aframe .
What is a frame? Consider the following set of words:
reservation, ﬂight, travel, buy, price, cost, fare, rates, meal, plane
There are many individual lexical relations of hyponymy, synonymy, and so on
between many of the words in this list. The resulting set of relations does not,

--- Page 467 ---
21.5 • F RAME NET 467
however, add up to a complete account of how these words are related. They are
clearly all deﬁned with respect to a coherent chunk of common-sense background
information concerning air travel.
We call the holistic background knowledge that unites these words a frame (Fill- frame
more, 1985). The idea that groups of words are deﬁned with respect to some back-
ground information is widespread in artiﬁcial intelligence and cognitive science,
where besides frame we see related works like a model (Johnson-Laird, 1983), or model
even script (Schank and Abelson, 1977). script
A frame in FrameNet is a background knowledge structure that deﬁnes a set of
frame-speciﬁc semantic roles, called frame elements , and includes a set of predi- frame elements
cates that use these roles. Each word evokes a frame and proﬁles some aspect of the
frame and its elements. The FrameNet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. For example, the change position onascale frame is deﬁned as
follows:
This frame consists of words that indicate the change of an Item’s posi-
tion on a scale (the Attribute) from a starting point (Initial value) to an
end point (Final value).
Some of the semantic roles (frame elements) in the frame are deﬁned as in
Fig. 21.3. Note that these are separated into core roles , which are frame speciﬁc, and core roles
non-core roles , which are more like the Arg-M arguments in PropBank, expressing non-core roles
more general properties of time, location, and so on.
Core Roles
ATTRIBUTE The A TTRIBUTE is a scalar property that the I TEM possesses.
DIFFERENCE The distance by which an I TEM changes its position on the scale.
FINAL STATE A description that presents the I TEM’s state after the change in the A TTRIBUTE ’s
value as an independent predication.
FINAL VALUE The position on the scale where the I TEM ends up.
INITIAL STATE A description that presents the I TEM’s state before the change in the A T-
TRIBUTE ’s value as an independent predication.
INITIAL VALUE The initial position on the scale from which the I TEM moves away.
ITEM The entity that has a position on the scale.
VALUE RANGE A portion of the scale, typically identiﬁed by its end points, along which the
values of the A TTRIBUTE ﬂuctuate.
Some Non-Core Roles
DURATION The length of time over which the change takes place.
SPEED The rate of change of the V ALUE .
GROUP The G ROUP in which an I TEM changes the value of an
ATTRIBUTE in a speciﬁed way.
Figure 21.3 The frame elements in the change position onascale frame from the FrameNet Labelers
Guide (Ruppenhofer et al., 2016).
Here are some example sentences:
(21.20) [ ITEM Oil]rose [ATTRIBUTE in price] [ DIFFERENCE by 2%].
(21.21) [ ITEM It] has increased [FINAL STATE to having them 1 day a month].
(21.22) [ ITEM Microsoft shares] fell[FINAL VALUE to 7 5/8].
(21.23) [ ITEM Colon cancer incidence] fell[DIFFERENCE by 50%] [ GROUP among
men].

--- Page 468 ---
468 CHAPTER 21 • S EMANTIC ROLE LABELING
(21.24) a steady increase [INITIAL VALUE from 9.5] [ FINAL VALUE to 14.3] [ ITEM
in dividends]
(21.25) a [ DIFFERENCE 5%] [ ITEM dividend] increase ...
Note from these example sentences that the frame includes target words like rise,
fall, and increase . In fact, the complete frame consists of the following words:
VERBS: dwindle move soar escalation shift
advance edge mushroom swell explosion tumble
climb explode plummet swing fall
decline fall reach triple ﬂuctuation ADVERBS:
decrease ﬂuctuate rise tumble gain increasingly
diminish gain rocket growth
dip grow shift NOUNS: hike
double increase skyrocket decline increase
drop jump slide decrease rise
FrameNet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be represented by inheri-
tance as well). Thus, there is a Cause change ofposition onascale frame that is
linked to the Change ofposition onascale frame by the cause relation, but that
adds an A GENT role and is used for causative examples such as the following:
(21.26) [ AGENT They] raised [ITEM the price of their soda] [ DIFFERENCE by 2%].
Together, these two frames would allow an understanding system to extract the
common event semantics of all the verbal and nominal causative and non-causative
usages.
FrameNets have also been developed for many other languages including Span-
ish, German, Japanese, Portuguese, Italian, and Chinese.
21.6 Semantic Role Labeling
Semantic role labeling (sometimes shortened as SRL ) is the task of automaticallysemantic role
labeling
ﬁnding the semantic roles of each argument of each predicate in a sentence. Cur-
rent approaches to semantic role labeling are based on supervised machine learning,
often using the FrameNet and PropBank resources to specify what counts as a pred-
icate, deﬁne the set of roles used in the task, and provide training and test sets.
Recall that the difference between these two models of semantic roles is that
FrameNet (21.27) employs many frame-speciﬁc frame elements as roles, while Prop-
Bank (21.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speciﬁc labels, along with the more general ARGM labels. Some
examples:
(21.27)[You] can’t [blame] [the program] [for being unable to identify it]
COGNIZER TARGET EVALUEE REASON
(21.28)[The San Francisco Examiner] issued [a special edition] [yesterday]
ARG0 TARGET ARG 1 ARGM -TMP
21.6.1 A Feature-based Algorithm for Semantic Role Labeling
A simpliﬁed feature-based semantic role labeling algorithm is sketched in Fig. 21.4.
Feature-based algorithms—from the very earliest systems like (Simmons, 1973)—
begin by parsing, using broad-coverage parsers to assign a parse to the input string.

--- Page 469 ---
21.6 • S EMANTIC ROLE LABELING 469
Figure 21.5 shows a parse of (21.28) above. The parse is then traversed to ﬁnd all
words that are predicates.
For each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classiﬁcation to decide the semantic role (if any) it plays
for this predicate. Given a labeled training set such as PropBank or FrameNet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. A 1-of-N classiﬁer is then trained to predict a semantic role for
each constituent given these features, where N is the number of potential semantic
roles plus an extra NONE role for non-role constituents. Any standard classiﬁcation
algorithms can be used. Finally, for each test sentence to be labeled, the classiﬁer is
run on each relevant constituent.
function SEMANTIC ROLELABEL (words )returns labeled tree
parse PARSE (words )
for each predicate inparse do
for each node inparse do
featurevector EXTRACT FEATURES (node ,predicate ,parse )
CLASSIFY NODE(node ,featurevector ,parse )
Figure 21.4 A generic semantic-role-labeling algorithm. C LASSIFY NODE is a 1-of- Nclas-
siﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labeled data
such as FrameNet or PropBank.
S
NP-SBJ =A R G 0 VP
DT NNP NNP NNP
The San Francisco Examiner
VBD = TARGET NP=A R G 1 PP-TMP =A R G M - T M P
issued DT JJ NN IN NP
as p e c i a l e d i t i o n a r o u n d N N N P - T M P
noon yesterday
Figure 21.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line
shows the path feature NP"S#VP#VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.
Instead of training a single-stage classiﬁer as in Fig. 21.5, the node-level classi-
ﬁcation task can be broken down into multiple steps:
1.Pruning: Since only a small number of the constituents in a sentence are
arguments of any given predicate, many systems use simple heuristics to prune
unlikely constituents.
2.Identiﬁcation: a binary classiﬁcation of each node as an argument to be la-
beled or a NONE .
3.Classiﬁcation: a 1-of- Nclassiﬁcation of all the constituents that were labeled
as arguments by the previous stage

--- Page 470 ---
470 CHAPTER 21 • S EMANTIC ROLE LABELING
The separation of identiﬁcation and classiﬁcation may lead to better use of fea-
tures (different features may be useful for the two tasks) or to computational efﬁ-
ciency.
Global Optimization
The classiﬁcation algorithm of Fig. 21.5 classiﬁes each argument separately (‘lo-
cally’), making the simplifying assumption that each argument of a predicate can be
labeled independently. This assumption is false; there are interactions between argu-
ments that require a more ‘global’ assignment of labels to constituents. For example,
constituents in FrameNet and PropBank are required to be non-overlapping. More
signiﬁcantly, the semantic roles of constituents are not independent. For example
PropBank does not allow multiple identical arguments; two constituents of the same
verb cannot both be labeled ARG0 .
Role labeling systems thus often add a fourth step to deal with global consistency
across the labels in a sentence. For example, the local classiﬁers can return a list of
possible labels associated with probabilities for each constituent, and a second-pass
Viterbi decoding or re-ranking approach can be used to choose the best consensus
label. Integer linear programming (ILP) is another common way to choose a solution
that conforms best to multiple constraints.
Features for Semantic Role Labeling
Most systems use some generalization of the core set of features introduced by
Gildea and Jurafsky (2000). Common basic features templates (demonstrated on
theNP-SBJ constituent The San Francisco Examiner in Fig. 21.5) include:
• The governing predicate , in this case the verb issued . The predicate is a cru-
cial feature since labels are deﬁned only with respect to a particular predicate.
• The phrase type of the constituent, in this case, NP(orNP-SBJ ). Some se-
mantic roles tend to appear as NPs, others as SorPP, and so on.
• The headword of the constituent, Examiner . The headword of a constituent
can be computed with standard head rules, such as those given in Appendix D
in Fig. 18.17. Certain headwords (e.g., pronouns) place strong constraints on
the possible semantic roles they are likely to ﬁll.
• The headword part of speech of the constituent, NNP .
• The path in the parse tree from the constituent to the predicate. This path is
marked by the dotted line in Fig. 21.5. Following Gildea and Jurafsky (2000),
we can use a simple linear representation of the path, NP "S#VP#VBD."and
#represent upward and downward movement in the tree, respectively. The
path is very useful as a compact representation of many kinds of grammatical
function relationships between the constituent and the predicate.
• The voice of the clause in which the constituent appears, in this case, active
(as contrasted with passive ). Passive sentences tend to have strongly different
linkings of semantic roles to surface form than do active ones.
• The binary linear position of the constituent with respect to the predicate,
either before orafter .
• The subcategorization of the predicate, the set of expected arguments that
appear in the verb phrase. We can extract this information by using the phrase-
structure rule that expands the immediate parent of the predicate; VP !VBD
NP PP for the predicate in Fig. 21.5.
• The named entity type of the constituent.

--- Page 471 ---
21.6 • S EMANTIC ROLE LABELING 471
• The ﬁrst words and the last word of the constituent.
The following feature vector thus represents the ﬁrst NP in our example (recall
that most observations will have the value NONE rather than, for example, ARG0,
since most constituents in the parse tree will not bear a semantic role):
ARG0: [issued, NP, Examiner, NNP, NP "S#VP#VBD, active, before, VP !NP PP,
ORG, The, Examiner]
Other features are often used in addition, such as sets of n-grams inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).
It’s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.
21.6.2 A Neural Algorithm for Semantic Role Labeling
A simple neural approach to SRL is to treat it as a sequence labeling task like named-
entity recognition, using the BIO approach. Let’s assume that we are given the
predicate and the task is just detecting and labeling spans. Recall that with BIO
tagging, we have a begin and end tag for each possible role ( B-ARG0,I-ARG0;B-
ARG1,I-ARG1, and so on), plus an outside tag O.
ENCODER[CLS]thecatslovehats[SEP]love[SEP]FFNB-ARG0I-ARG0B-PREDconcatenate with predicateB-ARG1FFNSoftmaxFFNFFNFFN
Figure 21.6 A simple neural approach to semantic role labeling. The input sentence is
followed by [SEP] and an extra input for the predicate, in this case love. The encoder outputs
are concatenated to an indicator variable which is 1 for the predicate and 0 for all other words
After He et al. (2017) and Shi and Lin (2019).
As with all the taggers, the goal is to compute the highest probability tag se-
quence ˆ y, given the input sequence of words w:
ˆy=argmax
y2TP(yjw)
Fig. 21.6 shows a sketch of a standard algorithm from He et al. (2017). Here each
input word is mapped to pretrained embeddings, and then each token is concatenated
with the predicate embedding and then passed through a feedforward network with
a softmax which outputs a distribution over each SRL label. For decoding, a CRF
layer can be used instead of the MLP layer on top of the biLSTM output to do global
inference, but in practice this doesn’t seem to provide much beneﬁt.

--- Page 472 ---
472 CHAPTER 21 • S EMANTIC ROLE LABELING
21.6.3 Evaluation of Semantic Role Labeling
The standard evaluation for semantic role labeling is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and F-measure. Identiﬁcation and classiﬁcation can
also be evaluated separately. Two common datasets used for evaluation are CoNLL-
2005 (Carreras and M `arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).
21.7 Selectional Restrictions
We turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. A selectional restriction is a semantic type con-selectional
restriction
straint that a verb imposes on the kind of concepts that are allowed to ﬁll its argument
roles. Consider the two meanings associated with the following example:
(21.29) I want to eat someplace nearby.
There are two possible parses and semantic interpretations for this sentence. In
the sensible interpretation, eatis intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. In the nonsensical speaker-as-
Godzilla interpretation, eatis transitive and the phrase someplace nearby is the direct
object and the THEME of the eating, like the NP Malaysian food in the following
sentences:
(21.30) I want to eat Malaysian food.
How do we know that someplace nearby isn’t the direct object in this sentence?
One useful cue is the semantic fact that the THEME of E ATING events tends to be
something that is edible . This restriction placed by the verb eaton the ﬁller of its
THEME argument is a selectional restriction.
Selectional restrictions are associated with senses, not entire lexemes. We can
see this in the following examples of the lexeme serve :
(21.31) The restaurant serves green-lipped mussels.
(21.32) Which airlines serve Denver?
Example (21.31) illustrates the offering-food sense of serve , which ordinarily re-
stricts its THEME to be some kind of food Example (21.32) illustrates the provides a
commercial service to sense of serve , which constrains its THEME to be some type
of appropriate location.
Selectional restrictions vary widely in their speciﬁcity. The verb imagine , for
example, imposes strict requirements on its AGENT role (restricting it to humans
and other animate entities) but places very few semantic requirements on its THEME
role. A verb like diagonalize , on the other hand, places a very speciﬁc constraint
on the ﬁller of its THEME role: it has to be a matrix, while the arguments of the
adjective odorless are restricted to concepts that could possess an odor:
(21.33) In rehearsal, I often ask the musicians to imagine a tennis game.
(21.34) Radon is an odorless gas that can’t be detected by human senses.
(21.35) To diagonalize a matrix is to ﬁnd its eigenvalues.
These examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
This distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.

--- Page 473 ---
21.7 • S ELECTIONAL RESTRICTIONS 473
21.7.1 Representing Selectional Restrictions
One way to capture the semantics of selectional restrictions is to use and extend the
event representation of Appendix F. Recall that the neo-Davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. Ignoring the issue of
thel-structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eatmight look like the following:
9e;x;y Eating (e)^Agent (e;x)^T heme (e;y)
With this representation, all we know about y, the ﬁller of the THEME role, is that
it is associated with an Eating event through the Theme relation. To stipulate the
selectional restriction that ymust be something edible, we simply add a new term to
that effect:
9e;x;y Eating (e)^Agent (e;x)^T heme (e;y)^EdibleT hing (y)
When a phrase like ate a hamburger is encountered, a semantic analyzer can form
the following kind of representation:
9e;x;y Eating (e)^Eater (e;x)^T heme (e;y)^EdibleT hing (y)^Hamburger (y)
This representation is perfectly reasonable since the membership of yin the category
Hamburger is consistent with its membership in the category EdibleThing , assuming
a reasonable set of facts in the knowledge base. Correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as Takeoff would be inconsistent with membership in the
category EdibleThing .
While this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. First, using FOL to perform the simple
task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms
can do the job with far less computational cost. The second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. Unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.
A more practical approach is to state selectional restrictions in terms of WordNet
synsets rather than as logical concepts. Each predicate simply speciﬁes a WordNet
synset as the selectional restriction on each of its arguments. A meaning representa-
tion is well-formed if the role ﬁller word is a hyponym (subordinate) of this synset.
For our ate a hamburger example, for instance, we could set the selectional
restriction on the THEME role of the verb eatto the synset ffood, nutrient g, glossed
asany substance that can be metabolized by an animal to give energy and build
tissue . Luckily, the chain of hypernyms for hamburger shown in Fig. 21.7 reveals
that hamburgers are indeed food. Again, the ﬁller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
We can apply this approach to the THEME roles of the verbs imagine ,lift, and di-
agonalize , discussed earlier. Let us restrict imagine ’sTHEME to the synsetfentityg,
lift’sTHEME tofphysical entityg, and diagonalize tofmatrixg. This arrangement
correctly permits imagine a hamburger andlift a hamburger , while also correctly
ruling out diagonalize a hamburger .

--- Page 474 ---
474 CHAPTER 21 • S EMANTIC ROLE LABELING
Sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich
=> snack food
=> dish
=> nutriment, nourishment, nutrition...
=> food, nutrient
=> substance
=> matter
=> physical entity
=> entity
Figure 21.7 Evidence from WordNet that hamburgers are edible.
21.7.2 Selectional Preferences
In the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (Katz and Fodor 1963,
Hirst 1987). For example, the verb eatmight require that its THEME argument be
[+FOOD] . Early word sense disambiguation systems used this idea to rule out senses
that violated the selectional restrictions of their governing predicates.
Very quickly, however, it became clear that these selectional restrictions were
better represented as preferences rather than strict constraints (Wilks 1975b, Wilks
1975a). For example, selectional restriction violations (like inedible arguments of
eat) often occur in well-formed sentences, for example because they are negated
(21.36), or because selectional restrictions are overstated (21.37):
(21.36) But it fell apart in 1931, perhaps because people realized you can’t eat
gold for lunch if you’re hungry.
(21.37) In his two championship trials, Mr. Kulkarni ateglass on an empty
stomach, accompanied only by water and tea.
Modern systems for selectional preferences therefore specify the relation be-
tween a predicate and its possible arguments with soft constraints of some kind.
Selectional Association
One of the most inﬂuential has been the selectional association model of Resnik
(1993). Resnik deﬁnes the idea of selectional preference strength as the generalselectional
preference
strengthamount of information that a predicate tells us about the semantic class of its argu-
ments. For example, the verb eattells us a lot about the semantic class of its direct
objects, since they tend to be edible. The verb be, by contrast, tells us less about
its direct objects. The selectional preference strength can be deﬁned by the differ-
ence in information between two distributions: the distribution of expected semantic
classes P(c)(how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb P(cjv)(how likely is
it that the direct object of the speciﬁc verb vwill fall into semantic class c). The
greater the difference between these distributions, the more information the verb
is giving us about possible objects. The difference between these two distributions
can be quantiﬁed by relative entropy , or the Kullback-Leibler divergence (Kullback relative entropy
and Leibler, 1951). The Kullback-Leibler or KL divergence D(PjjQ)expresses the KL divergence

--- Page 475 ---
21.7 • S ELECTIONAL RESTRICTIONS 475
difference between two probability distributions PandQ
D(PjjQ) =X
xP(x)logP(x)
Q(x)(21.38)
The selectional preference SR(v)uses the KL divergence to express how much in-
formation, in bits, the verb vexpresses about the possible semantic class of its argu-
ment.
SR(v) = D(P(cjv)jjP(c))
=X
cP(cjv)logP(cjv)
P(c)(21.39)
Resnik then deﬁnes the selectional association of a particular class and verb as theselectional
association
relative contribution of that class to the general selectional preference of the verb:
AR(v;c) =1
SR(v)P(cjv)logP(cjv)
P(c)(21.40)
The selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
Resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the WordNet concepts containing the
word. The following table from Resnik (1996) shows some sample high and low
selectional associations for verbs and some WordNet semantic classes of their direct
objects.
Direct Object Direct Object
Verb Semantic Class Assoc Semantic Class Assoc
read WRITING 6.80 ACTIVITY -.20
write WRITING 7.26 COMMERCE 0
see ENTITY 5.79 METHOD -0.01
Selectional Preference via Conditional Probability
An alternative to using selectional association between a verb and the WordNet class
of its arguments is to use the conditional probability of an argument word given a
predicate verb, directly modeling the strength of association of one verb (predicate)
with one noun (argument).
The conditional probability model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. The conditional probability of an
argument noun given a verb for a particular relation P(njv;r)can then be used as a
selectional preference metric for that pair of words (Brockmann and Lapata 2003,
Keller and Lapata 2003):
P(njv;r) =(
C(n;v;r)
C(v;r)ifC(n;v;r)>0
0 otherwise
The inverse probability P(vjn;r)was found to have better performance in some cases
(Brockmann and Lapata, 2003):
P(vjn;r) =(
C(n;v;r)
C(n;r)ifC(n;v;r)>0
0 otherwise

--- Page 476 ---
476 CHAPTER 21 • S EMANTIC ROLE LABELING
An even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument log count (v;n;r)instead of conditional probability;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (Brockmann and Lapata, 2003).
Evaluating Selectional Preferences
One way to evaluate models of selectional preferences is to use pseudowords (Gale pseudowords
et al. 1992b, Sch ¨utze 1992a). A pseudoword is an artiﬁcial word created by concate-
nating a test word in some context (say banana ) with a confounder word (say door )
to create banana-door ). The task of the system is to identify which of the two words
is the original word. To evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. For each verb token (say drive ) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor , the noun with the
frequency closest to the original (say house ), to make car/house ). We then use the
selectional preference model to choose which of carandhouse are more preferred
objects of drive , and compute how often the model chooses the correct original ob-
ject (e.g., car) (Chambers and Jurafsky, 2010).
Another evaluation metric is to get human preferences for a test set of verb-
argument pairs, and have them rate their degree of plausibility. This is usually done
by using magnitude estimation, a technique from psychophysics, in which subjects
rate the plausibility of an argument proportional to a modulus item. A selectional
preference model can then be evaluated by its correlation with the human prefer-
ences (Keller and Lapata, 2003).
21.8 Primitive Decomposition of Predicates
One way of thinking about the semantic roles we have discussed through the chapter
is that they help us deﬁne the roles that arguments play in a decompositional way,
based on ﬁnite lists of thematic roles (agent, patient, instrument, proto-agent, proto-
patient, etc.). This idea of decomposing meaning into sets of primitive semantic
elements or features, called primitive decomposition orcomponential analysis ,componential
analysis
has been taken even further, and focused particularly on predicates.
Consider these examples of the verb kill:
(21.41) Jim killed his philodendron.
(21.42) Jim did something to cause his philodendron to become not alive.
There is a truth-conditional (‘propositional semantics’) perspective from which these
two sentences have the same meaning. Assuming this equivalence, we could repre-
sent the meaning of killas:
(21.43) KILL (x,y),CAUSE (x,BECOME (NOT(ALIVE (y))))
thus using semantic primitives like do,cause ,become not , and alive .
Indeed, one such set of potential semantic primitives has been used to account
for some of the verbal alternations discussed in Section 21.2 (Lakoff 1965, Dowty
1979). Consider the following examples.
(21.44) John opened the door. )CAUSE (John, BECOME (OPEN (door)))
(21.45) The door opened. )BECOME (OPEN (door))

--- Page 477 ---
21.9 • S UMMARY 477
(21.46) The door is open. )OPEN (door)
The decompositional approach asserts that a single state-like predicate associ-
ated with open underlies all of these examples. The differences among the meanings
of these examples arises from the combination of this single predicate with the prim-
itives CAUSE and BECOME .
While this approach to primitive decomposition can explain the similarity be-
tween states and actions or causative and non-causative predicates, it still relies on
having a large number of predicates like open . More radical approaches choose to
break down these predicates as well. One such approach to verbal predicate decom-
position that played a role in early natural language systems is conceptual depen-
dency (CD), a set of ten primitive predicates, shown in Fig. 21.8.conceptual
dependency
Primitive Deﬁnition
ATRANS The abstract transfer of possession or control from one entity to
another
PTRANS The physical transfer of an object from one location to another
MTRANS The transfer of mental concepts between entities or within an
entity
MBUILD The creation of new information within an entity
PROPEL The application of physical force to move an object
MOVE The integral movement of a body part by an animal
INGEST The taking in of a substance by an animal
EXPEL The expulsion of something from an animal
SPEAK The action of producing a sound
ATTEND The action of focusing a sense organ
Figure 21.8 A set of conceptual dependency primitives.
Below is an example sentence along with its CDrepresentation. The verb brought
is translated into the two primitives ATRANS and PTRANS to indicate that the waiter
both physically conveyed the check to Mary and passed control of it to her. Note
that CDalso associates a ﬁxed set of thematic roles with each primitive to represent
the various participants in the action.
(21.47) The waiter brought Mary the check.
9x;y Atrans (x)^Actor (x;Waiter )^Ob ject (x;Check )^To(x;Mary )
^Ptrans (y)^Actor (y;Waiter )^Ob ject (y;Check )^To(y;Mary )
21.9 Summary
•Semantic roles are abstract models of the role an argument plays in the event
described by the predicate.
•Thematic roles are a model of semantic roles based on a single ﬁnite list of
roles. Other semantic role models include per-verb semantic role lists and
proto-agent /proto-patient , both of which are implemented in PropBank ,
and per-frame role lists, implemented in FrameNet .

--- Page 478 ---
478 CHAPTER 21 • S EMANTIC ROLE LABELING
•Semantic role labeling is the task of assigning semantic role labels to the
constituents of a sentence. The task is generally treated as a supervised ma-
chine learning task, with models trained on PropBank or FrameNet. Algo-
rithms generally start by parsing a sentence and then automatically tag each
parse tree node with a semantic role. Neural models map straight from words
end-to-end.
• Semantic selectional restrictions allow words (particularly predicates) to post
constraints on the semantic properties of their argument words. Selectional
preference models (like selectional association or simple conditional proba-
bility) allow a weight or probability to be assigned to the association between
a predicate and an argument word or class.
Bibliographical and Historical Notes
Although the idea of semantic roles dates back to P ¯an.ini, they were re-introduced
into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fill-
more had become interested in argument structure by studying Lucien Tesni `ere’s
groundbreaking ´El´ements de Syntaxe Structurale (Tesni `ere, 1959) in which the term
‘dependency’ was introduced and the foundations were laid for dependency gram-
mar. Following Tesni `ere’s terminology, Fillmore ﬁrst referred to argument roles as
actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003))
and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument,
etc.), that could be taken on by the arguments of predicates. Verbs would be listed in
the lexicon with their case frame , the list of obligatory (or optional) case arguments.
The idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-speciﬁed representations of meaning was quickly adopted in natural language
processing, and systems for extracting case frames were created for machine transla-
tion (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language pro-
cessing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). General-
purpose semantic role labelers were developed. The earliest ones (Simmons, 1973)
ﬁrst parsed a sentence by means of an ATN (Augmented Transition Network) parser.
Each verb then had a set of rules specifying how the parse should be mapped to se-
mantic roles. These rules mainly made reference to grammatical functions (subject,
object, complement of speciﬁc prepositions) but also checked constituent internal
features such as the animacy of head nouns. Later systems assigned roles from pre-
built parse trees, again by using dictionaries with verb-speciﬁc case frames (Levin
1977, Marcus 1980).
By 1977 case representation was widely used and taught in AI and NLP courses,
and was described as a standard of natural language processing in the ﬁrst edition of
Winston’s 1977 textbook Artiﬁcial Intelligence .
In the 1980s Fillmore proposed his model of frame semantics , later describing
the intuition as follows:
“The idea behind frame semantics is that speakers are aware of possi-
bly quite complex situation types, packages of connected expectations,
that go by various names—frames, schemas, scenarios, scripts, cultural
narratives, memes—and the words in our language are understood with
such frames as their presupposed background.” (Fillmore, 2012, p. 712)

--- Page 479 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 479
The word frame seemed to be in the air for a suite of related notions proposed at
about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as
well as related notions with other names like scripts (Schank and Abelson, 1975)
andschemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).
Fillmore was also inﬂuenced by the semantic ﬁeld theorists and by a visit to the Yale
AI lab where he took notice of the lists of slots and ﬁllers used by early information
extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s
Fillmore drew on these insights to begin the FrameNet corpus annotation project.
At the same time, Beth Levin drew on her early case frame dictionaries (Levin,
1977) to develop her book which summarized sets of verb classes deﬁned by shared
argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper
et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus
created by Martha Palmer and colleagues (Palmer et al., 2005).
The combination of rich linguistic annotation and corpus-based approach in-
stantiated in FrameNet and PropBank led to a revival of automatic approaches to
semantic role labeling, ﬁrst on FrameNet (Gildea and Jurafsky, 2000) and then on
PropBank data (Gildea and Palmer, 2002, inter alia). The problem ﬁrst addressed in
the 1970s by handwritten rules was thus now generally recast as one of supervised
machine learning enabled by large and consistent databases. Many popular features
used for role labeling are deﬁned in Gildea and Jurafsky (2002), Surdeanu et al.
(2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao
et al. (2009). The use of dependency rather than constituency parses was introduced
in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer
et al. (2010) and M `arquez et al. (2008).
The use of neural approaches to semantic role labeling was pioneered by Col-
lobert et al. (2011), who applied a CRF on top of a convolutional net. Early work
like Foland, Jr. and Martin (2015) focused on using dependency features. Later work
eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of
a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to
augment the biLSTM architecture with highway networks and also replace the CRF
with A* decoding that make it possible to apply a wide variety of global constraints
in SRL decoding.
Most semantic role labeling schemes only work within a single sentence, fo-
cusing on the object of the verbal (or nominal, in the case of NomBank) predicate.
However, in many cases, a verbal or nominal predicate may have an implicit argu-
ment : one that appears only in a contextual sentence, or perhaps not at all and mustimplicit
argument
be inferred. In the two sentences This house has a new owner. The sale was ﬁnalized
10 days ago. thesale in the second sentence has no A RG1, but a reasonable reader
would infer that the Arg1 should be the house mentioned in the prior sentence. Find-
ing these arguments, implicit argument detection (sometimes shortened as iSRL ) iSRL
was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do
et al. (2017) for more recent neural models.
To avoid the need for huge labeled training sets, unsupervised approaches for
semantic role labeling attempt to induce the set of semantic roles by clustering over
arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier
and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev
(2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khod-
dam (2014).
Recent innovations in frame labeling include connotation frames , which mark
richer information about the argument of predicates. Connotation frames mark the

--- Page 480 ---
480 CHAPTER 21 • S EMANTIC ROLE LABELING
sentiment of the writer or reader toward the arguments (for example using the verb
survive inhe survived a bombing expresses the writer’s sympathy toward the subject
heand negative sentiment toward the bombing. See Chapter 22 for more details.
Selectional preference has been widely studied beyond the selectional associa-
tion models of Resnik (1993) and Resnik (1996). Methods have included clustering
(Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic mod-
els (S ´eaghdha 2010, Ritter et al. 2010b), and constraints can be expressed at the level
of words or classes (Agirre and Martinez, 2001). Selectional preferences have also
been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al.
2013, Do et al. 2017).
Exercises

--- Page 481 ---
CHAPTER
22Lexicons for Sentiment, Affect,
and Connotation
Some day we’ll be able to measure the power of words
Maya Angelou
In this chapter we turn to tools for interpreting affective meaning, extending our affective
study of sentiment analysis in Chapter 4. We use the word ‘affective’, following the
tradition in affective computing (Picard, 1995) to mean emotion, sentiment, per-
sonality, mood, and attitudes. Affective meaning is closely related to subjectivity , subjectivity
the study of a speaker or writer’s evaluations, opinions, emotions, and speculations
(Wiebe et al., 1999).
How should affective meaning be deﬁned? One inﬂuential typology of affec-
tive states comes from Scherer (2000), who deﬁnes each class of affective states by
factors like its cognitive realization and time course (Fig. 22.1).
Emotion: Relatively brief episode of response to the evaluation of an external
or internal event as being of major signiﬁcance.
(angry, sad, joyful, fearful, ashamed, proud, elated, desperate )
Mood: Diffuse affect state, most pronounced as change in subjective feeling, of
low intensity but relatively long duration, often without apparent cause.
(cheerful, gloomy, irritable, listless, depressed, buoyant )
Interpersonal stance: Affective stance taken toward another person in a spe-
ciﬁc interaction, coloring the interpersonal exchange in that situation.
(distant, cold, warm, supportive, contemptuous, friendly )
Attitude: Relatively enduring, affectively colored beliefs, preferences, and pre-
dispositions towards objects or persons.
(liking, loving, hating, valuing, desiring )
Personality traits: Emotionally laden, stable personality dispositions and be-
havior tendencies, typical for a person.
(nervous, anxious, reckless, morose, hostile, jealous )
Figure 22.1 The Scherer typology of affective states (Scherer, 2000).
We can design extractors for each of these kinds of affective states. Chapter 4
already introduced sentiment analysis , the task of extracting the positive or negative
orientation that a writer expresses in a text. This corresponds in Scherer’s typology
to the extraction of attitudes : ﬁguring out what people like or dislike, from affect-
rich texts like consumer reviews of books or movies, newspaper editorials, or public
sentiment in blogs or tweets.
Detecting emotion andmoods is useful for detecting whether a student is con-
fused, engaged, or certain when interacting with a tutorial system, whether a caller
to a help line is frustrated, whether someone’s blog posts or tweets indicated depres-
sion. Detecting emotions like fear in novels, for example, could help us trace what
groups or situations are feared and how that changes over time.

--- Page 482 ---
482 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
Detecting different interpersonal stances can be useful when extracting infor-
mation from human-human conversations. The goal here is to detect stances like
friendliness or awkwardness in interviews or friendly conversations, for example for
summarizing meetings or ﬁnding parts of a conversation where people are especially
excited or engaged, conversational hot spots that can help in meeting summariza-
tion. Detecting the personality of a user—such as whether the user is an extrovert
or the extent to which they are open to experience — can help improve conversa-
tional agents, which seem to work better if they match users’ personality expecta-
tions (Mairesse and Walker, 2008). And affect is important for generation as well
as recognition; synthesizing affect is important for conversational agents in various
domains, including literacy tutors such as children’s storybooks, or computer games.
In Chapter 4 we introduced the use of naive Bayes classiﬁcation to classify a
document’s sentiment. Various classiﬁers have been successfully applied to many of
these tasks, using all the words in the training set as input to a classiﬁer which then
determines the affect status of the text.
In this chapter we focus on an alternative model, in which instead of using every
word as a feature, we focus only on certain words, ones that carry particularly strong
cues to affect or sentiment. We call these lists of words affective lexicons orsenti-
ment lexicons . These lexicons presuppose a fact about semantics: that words have
affective meanings orconnotations . The word connotation has different meanings connotations
in different ﬁelds, but here we use it to mean the aspects of a word’s meaning that
are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. In
addition to their ability to help determine the affective status of a text, connotation
lexicons can be useful features for other kinds of affective tasks, and for computa-
tional social science analysis.
In the next sections we introduce basic theories of emotion, show how sentiment
lexicons are a special case of emotion lexicons, and mention some useful lexicons.
We then survey three ways for building lexicons: human labeling, semi-supervised,
and supervised. Finally, we talk about how to detect affect toward a particular entity,
and introduce connotation frames.
22.1 Deﬁning Emotion
One of the most important affective classes is emotion , which Scherer (2000) deﬁnes emotion
as a “relatively brief episode of response to the evaluation of an external or internal
event as being of major signiﬁcance”.
Detecting emotion has the potential to improve a number of language processing
tasks. Emotion recognition could help dialogue systems like tutoring systems detect
that a student was unhappy, bored, hesitant, conﬁdent, and so on. Automatically
detecting emotions in reviews or customer responses (anger, dissatisfaction, trust)
could help businesses recognize speciﬁc problem areas or ones that are going well.
Emotion can play a role in medical NLP tasks like helping diagnose depression or
suicidal intent. Detecting emotions expressed toward characters in novels might
play a role in understanding how different social groups were viewed by society at
different times.
Computational models of emotion in NLP have mainly been based on two fami-
lies of theories of emotion (out of the many studied in the ﬁeld of affective science).
In one of these families, emotions are viewed as ﬁxed atomic units, limited in num-
ber, and from which others are generated, often called basic emotions (Tomkins basic emotions

--- Page 483 ---
22.1 • D EFINING EMOTION 483
1962, Plutchik 1962), a model dating back to Darwin. Perhaps the most well-known
of this family of theories are the 6 emotions proposed by Ekman (e.g., Ekman 1999)
to be universally present in all cultures: surprise, happiness, anger, fear, disgust,
sadness . Another atomic theory is the Plutchik (1980) wheel of emotion, consisting
of 8 basic emotions in four opposing pairs: joy–sadness ,anger–fear ,trust–disgust ,
andanticipation–surprise , together with the emotions derived from them, shown in
Fig. 22.2.
Figure 22.2 Plutchik wheel of emotion.
The second class of emotion theories widely used in NLP views emotion as a
space in 2 or 3 dimensions (Russell, 1980). Most models include the two dimensions
valence andarousal , and many add a third, dominance . These can be deﬁned as:
valence: the pleasantness of the stimulus
arousal: the level of alertness, activeness, or energy provoked by the stimulus
dominance: the degree of control or dominance exerted by the stimulus or the
emotion
Sentiment can be viewed as a special case of this second view of emotions as points
in space. In particular, the valence dimension, measuring how pleasant or unpleasant
a word is, is often used directly as a measure of sentiment.
In these lexicon-based models of affect, the affective meaning of a word is gen-
erally ﬁxed, irrespective of the linguistic context in which a word is used, or the
dialect or culture of the speaker. By contrast, other models in affective science repre-
sent emotions as much richer processes involving cognition (Barrett et al., 2007). In
appraisal theory , for example, emotions are complex processes, in which a person
considers how an event is congruent with their goals, taking into account variables
like the agency, certainty, urgency, novelty and control associated with the event
(Moors et al., 2013). Computational models in NLP taking into account these richer
theories of emotion will likely play an important role in future work.

--- Page 484 ---
484 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
22.2 Available Sentiment and Affect Lexicons
A wide variety of affect lexicons have been created and released. The most basic
lexicons label words along one dimension of semantic variability, generally called
“sentiment” or “valence”.
In the simplest lexicons this dimension is represented in a binary fashion, with
a wordlist for positive words and a wordlist for negative words. The oldest is the
General Inquirer (Stone et al., 1966), which drew on content analysis and on earlyGeneral
Inquirer
work in the cognitive psychology of word meaning (Osgood et al., 1957). The Gen-
eral Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative
words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon
(Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior
lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe,
2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for
reliability (strongly subjective or weakly subjective). The polarity lexicon of Hu
and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product
reviews, labeled using a bootstrapping method from WordNet.
Positive admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-
tastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,
rejoice, relief, respect, satisfactorily, sensational, super, terriﬁc, thank, vivid, wise, won-
derful, zest
Negative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,
defective, disappointment, embarrass, fake, fear, ﬁlthy, fool, guilt, hate, idiot, inﬂict, lazy,
miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,
vile, wicked
Figure 22.3 Some words with consistent sentiment across the General Inquirer (Stone et al., 1966), the
MPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of Hu and Liu (2004b).
Slightly more general than these sentiment lexicons are lexicons that assign each
word a value on all three affective dimensions. The NRC Valence, Arousal, and
Dominance (V AD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dom-
inance scores to 20,000 words. Some examples are shown in Fig. 22.4.
Valence Arousal Dominance
vacation .840 enraged .962 powerful .991
delightful .918 party .840 authority .935
whistle .653 organized .337 saxophone .482
consolation .408 effortless .120 discouraged .0090
torture .115 napping .046 weak .045
Figure 22.4 Values of sample words on the emotional dimensions of Mohammad (2018a).
The NRC Word-Emotion Association Lexicon, also called EmoLex (Moham- EmoLex
mad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions deﬁned above.
The lexicon includes around 14,000 words including words from prior lexicons as
well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for
some sample words:

--- Page 485 ---
22.3 • C REATING AFFECT LEXICONS BY HUMAN LABELING 485
Word
anger
anticipation
disgust
fear
joy
sadness
surprise
trust
positive
negative
reward 0100101110
worry 0101010001
tenderness 0000100010
sweetheart 0100110110
suddenly 0000001000
thirst 0100011000
garbage 0010000001
For a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon
(Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy,
and sadness; Fig. 22.5 shows examples.
Anger Fear Joy Sadness
outraged 0.964 horror 0.923 superb 0.864 sad 0.844
violence 0.742 anguish 0.703 cheered 0.773 guilt 0.750
coup 0.578 pestilence 0.625 rainbow 0.531 unkind 0.547
oust 0.484 stressed 0.531 gesture 0.387 difﬁculties 0.421
suspicious 0.484 failing 0.531 warms 0.391 beggar 0.422
nurture 0.059 conﬁdent 0.094 hardship .031 sing 0.017
Figure 22.5 Sample emotional intensities for words for anger, fear, joy, and sadness from
Mohammad (2018b).
LIWC ,Linguistic Inquiry and Word Count , is a widely used set of 73 lex- LIWC
icons containing over 2300 words (Pennebaker et al., 2007), designed to capture
aspects of lexical meaning relevant for social psychological tasks. In addition to
sentiment-related lexicons like ones for negative emotion ( bad, weird, hate, prob-
lem, tough ) and positive emotion ( love, nice, sweet ), LIWC includes lexicons for
categories like anger, sadness, cognitive mechanisms, perception, tentative, and in-
hibition, shown in Fig. 22.6.
There are various other hand-built affective lexicons. The General Inquirer in-
cludes additional lexicons for dimensions like strong vs. weak, active vs. passive,
overstated vs. understated, as well as lexicons for categories like pleasure, pain,
virtue, vice, motivation, and cognitive orientation.
Another useful feature for various tasks is the distinction between concrete concrete
words like banana orbathrobe andabstract words like belief andalthough . The abstract
lexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5
of the concreteness of 40,000 words, thus assigning banana ,bathrobe , and bagel 5,
belief 1.19, although 1.07, and in between words like brisk a 2.5.
22.3 Creating Affect Lexicons by Human Labeling
The earliest method used to build affect lexicons, and still in common use, is to have
humans label each word. This is now most commonly done via crowdsourcing : crowdsourcing
breaking the task into small pieces and distributing them to a large number of anno-

--- Page 486 ---
486 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
Positive Negative
Emotion Emotion Insight Inhibition Family Negate
appreciat* anger* aware* avoid* brother* aren’t
comfort* bore* believe careful* cousin* cannot
great cry decid* hesitat* daughter* didn’t
happy despair* feel limit* family neither
interest fail* ﬁgur* oppos* father* never
joy* fear know prevent* grandf* no
perfect* griev* knew reluctan* grandm* nobod*
please* hate* means safe* husband none
safe* panic* notice* stop mom nor
terriﬁc suffers recogni* stubborn* mother nothing
value terrify sense wait niece* nowhere
wow* violent* think wary wife without
Figure 22.6 Samples from 5 of the 73 lexical categories in LIWC (Pennebaker et al., 2007).
The * means the previous letters are a word preﬁx and all words with that preﬁx are included
in the category.
tators. Let’s take a look at some of the methodological choices for two crowdsourced
emotion lexicons.
The NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013), labeled
emotions in two steps. To ensure that the annotators were judging the correct sense
of the word, they ﬁrst answered a multiple-choice synonym question that primed
the correct sense of the word (without requiring the annotator to read a potentially
confusing sense deﬁnition). These were created automatically using the headwords
associated with the thesaurus category of the sense in question in the Macquarie
dictionary and the headwords of 3 random distractor categories. An example:
Which word is closest in meaning (most related) to startle?
•automobile
•shake
•honesty
•entertain
For each word (e.g. startle ), the annotator was then asked to rate how associated
that word is with each of the 8 emotions ( joy,fear,anger , etc.). The associations
were rated on a scale of not,weakly ,moderately , and strongly associated. Outlier
ratings were removed, and then each term was assigned the class chosen by the ma-
jority of the annotators, with ties broken by choosing the stronger intensity, and then
the 4 levels were mapped into a binary label for each word (no and weak mapped to
0, moderate and strong mapped to 1).
The NRC V AD Lexicon (Mohammad, 2018a) was built by selecting words and
emoticons from prior lexicons and annotating them with crowd-sourcing using best-
worst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In best-best-worst
scaling
worst scaling, annotators are given N items (usually 4) and are asked which item is
thebest (highest) and which is the worst (lowest) in terms of some property. The
set of words used to describe the ends of the scales are taken from prior literature.
For valence, for example, the raters were asked:
Q1. Which of the four words below is associated with the MOST happi-
ness / pleasure / positiveness / satisfaction / contentedness / hopefulness
OR LEAST unhappiness / annoyance / negativeness / dissatisfaction /

--- Page 487 ---
22.4 • S EMI-SUPERVISED INDUCTION OF AFFECT LEXICONS 487
melancholy / despair? (Four words listed as options.)
Q2. Which of the four words below is associated with the LEAST hap-
piness / pleasure / positiveness / satisfaction / contentedness / hopeful-
ness OR MOST unhappiness / annoyance / negativeness / dissatisfaction
/ melancholy / despair? (Four words listed as options.)
The score for each word in the lexicon is the proportion of times the item was chosen
as the best (highest V/A/D) minus the proportion of times the item was chosen as the
worst (lowest V/A/D). The agreement between annotations are evaluated by split-
half reliability : split the corpus in half and compute the correlations between thesplit-half
reliability
annotations in the two halves.
22.4 Semi-supervised Induction of Affect Lexicons
Another common way to learn sentiment lexicons is to start from a set of seed words
that deﬁne two poles of a semantic axis (words like good orbad), and then ﬁnd ways
to label each word wby its similarity to the two seed sets. Here we summarize two
families of seed-based semi-supervised lexicon induction algorithms, axis-based and
graph-based.
22.4.1 Semantic Axis Methods
One of the most well-known lexicon induction methods, the Turney and Littman
(2003) algorithm, is given seed words like good orbad, and then for each word wto
be labeled, measures both how similar it is to good and how different it is from bad.
Here we describe a slight extension of the algorithm due to An et al. (2018), which
is based on computing a semantic axis .
In the ﬁrst step, we choose seed words by hand. There are two methods for
dealing with the fact that the affect of a word is different in different contexts: (1)
start with a single large seed lexicon and rely on the induction algorithm to ﬁne-tune
it to the domain, or (2) choose different seed words for different genres. Hellrich
et al. (2019) suggests that for modeling affect across different historical time periods,
starting with a large modern affect dictionary is better than small seedsets tuned to be
stable across time. As an example of the second approach, Hamilton et al. (2016a)
deﬁne one set of seed words for general sentiment analysis, a different set for Twitter,
and yet another set for sentiment in ﬁnancial text:
Domain Positive seeds Negative seeds
General good, lovely, excellent, fortunate, pleas-
ant, delightful, perfect, loved, love,
happybad, horrible, poor, unfortunate, un-
pleasant, disgusting, evil, hated, hate,
unhappy
Twitter love, loved, loves, awesome, nice,
amazing, best, fantastic, correct, happyhate, hated, hates, terrible, nasty, awful,
worst, horrible, wrong, sad
Finance successful, excellent, proﬁt, beneﬁcial,
improving, improved, success, gains,
positivenegligent, loss, volatile, wrong, losses,
damages, bad, litigation, failure, down,
negative
In the second step, we compute embeddings for each of the pole words. These
embeddings can be off-the-shelf word2vec embeddings, or can be computed directly

--- Page 488 ---
488 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
on a speciﬁc corpus (for example using a ﬁnancial corpus if a ﬁnance lexicon is the
goal), or we can ﬁne-tune off-the-shelf embeddings to a corpus. Fine-tuning is espe-
cially important if we have a very speciﬁc genre of text but don’t have enough data
to train good embeddings. In ﬁne-tuning, we begin with off-the-shelf embeddings
like word2vec, and continue training them on the small target corpus.
Once we have embeddings for each pole word, we create an embedding that
represents each pole by taking the centroid of the embeddings of each of the seed
words; recall that the centroid is the multidimensional version of the mean. Given
a set of embeddings for the positive seed words S+=fE(w+
1);E(w+
2);:::;E(w+
n)g,
and embeddings for the negative seed words S =fE(w 
1);E(w 
2);:::;E(w 
m)g, the
pole centroids are:
V+=1
nnX
1E(w+
i)
V =1
mmX
1E(w 
i) (22.1)
The semantic axis deﬁned by the poles is computed just by subtracting the two vec-
tors:
Vaxis=V+ V (22.2)
Vaxis, the semantic axis, is a vector in the direction of positive sentiment. Finally,
we compute (via cosine similarity) the angle between the vector in the direction of
positive sentiment and the direction of w’s embedding. A higher cosine means that
wis more aligned with S+than S .
score(w) = cos 
E(w);Vaxis
=E(w)Vaxis
kE(w)kkVaxisk(22.3)
If a dictionary of words with sentiment scores is sufﬁcient, we’re done! Or if we
need to group words into a positive and a negative lexicon, we can use a threshold
or other method to give us discrete lexicons.
22.4.2 Label Propagation
An alternative family of methods deﬁnes lexicons by propagating sentiment labels
on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown
(1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of
Hamilton et al. (2016a), which has four steps:
1.Deﬁne a graph : Given word embeddings, build a weighted lexical graph by
connecting each word with its knearest neighbors (according to cosine simi-
larity). The weights of the edge between words wiandwjare set as:
Ei;j=arccos
 wi>wj
kwikkwjk
: (22.4)
2.Deﬁne a seed set: Choose positive and negative seed words.
3.Propagate polarities from the seed set: Now we perform a random walk on
this graph, starting at the seed set. In a random walk, we start at a node and

--- Page 489 ---
22.4 • S EMI-SUPERVISED INDUCTION OF AFFECT LEXICONS 489
then choose a node to move to with probability proportional to the edge prob-
ability. A word’s polarity score for a seed set is proportional to the probability
of a random walk from the seed set landing on that word (Fig. 22.7).
4.Create word scores : We walk from both positive and negative seed sets,
resulting in positive (rawscore+(wi)) and negative (rawscore (wi)) raw label
scores. We then combine these values into a positive-polarity score as:
score+(wi) =rawscore+(wi)
rawscore+(wi)+rawscore (wi)(22.5)
It’s often helpful to standardize the scores to have zero mean and unit variance
within a corpus.
5.Assign conﬁdence to each score: Because sentiment scores are inﬂuenced by
the seed set, we’d like to know how much the score of a word would change if
a different seed set is used. We can use bootstrap sampling to get conﬁdence
regions, by computing the propagation Btimes over random subsets of the
positive and negative seed sets (for example using B=50 and choosing 7 of
the 10 seed words each time). The standard deviation of the bootstrap sampled
polarity scores gives a conﬁdence measure.
idolize
love
adore
appreciate
like
ﬁnd
dislike
see
notice
disapprove
abhor
hate
loathe
despise
uncover
idolize
love
adore
appreciate
like
ﬁnd
dislike
see
notice
disapprove
abhor
hate
loathe
despise
uncover
(a) (b)
Figure 22.7 Intuition of the S ENTPROP algorithm. (a) Run random walks from the seed words. (b) Assign
polarity scores (shown here as colors green or red) based on the frequency of random walk visits.
22.4.3 Other Methods
The core of semisupervised algorithms is the metric for measuring similarity with
the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) ap-
proaches above used embedding cosine as the distance metric: words were labeled
as positive basically if their embeddings had high cosines with positive seeds and
low cosines with negative seeds. Other methods have chosen other kinds of distance
metrics besides embedding cosine.
For example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic
cues; two adjectives are considered similar if they were frequently conjoined by and
and rarely conjoined by but. This is based on the intuition that adjectives conjoined
by the words andtend to have the same polarity; positive adjectives are generally
coordinated with positive, negative with negative:
fair and legitimate, corrupt and brutal
but less often positive adjectives coordinated with negative:
*fair and brutal, *corrupt and legitimate
By contrast, adjectives conjoined by butare likely to be of opposite polarity:

--- Page 490 ---
490 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
fair but brutal
Another cue to opposite polarity comes from morphological negation ( un-,im-,
-less). Adjectives with the same root but differing in a morphological negative ( ad-
equate/inadequate ,thoughtful/thoughtless ) tend to be of opposite polarity.
Yet another method for ﬁnding words that have a similar polarity to seed words is
to make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b).
A word’s synonyms presumably share its polarity while a word’s antonyms probably
have the opposite polarity. After a seed lexicon is built, each lexicon is updated as
follows, possibly iterated.
Lex+: Add synonyms of positive words ( well) and antonyms (like ﬁne) of negative
words
Lex : Add synonyms of negative words ( awful ) and antonyms (like evil) of positive
words
An extension of this algorithm assigns polarity to WordNet senses, called Senti-
WordNet (Baccianella et al., 2010). Fig. 22.8 shows some examples. SentiWordNet
Synset Pos Neg Obj
good#6 ‘agreeable or pleasing’ 1 0 0
respectable#2 honorable#4 good#4 estimable#2 ‘deserving of esteem’ 0.75 0 0.25
estimable#3 computable#1 ‘may be computed or estimated’ 0 0 1
sting#1 burn#4 bite#2 ‘cause a sharp or stinging pain’ 0 0.875 .125
acute#6 ‘of critical importance and consequence’ 0.625 0.125 .250
acute#4 ‘of an angle; less than 90 degrees’ 0 0 1
acute#1 ‘having or experiencing a rapid onset and short but severe course’ 0 0.5 0.5
Figure 22.8 Examples from SentiWordNet 3.0 (Baccianella et al., 2010). Note the differences between senses
of homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive
(acute#6 ), negative ( acute#1 ), or neutral ( acute #4 ).
In this algorithm, polarity is assigned to entire synsets rather than words. A
positive lexicon is built from all the synsets associated with 7 positive words, and a
negative lexicon from synsets associated with 7 negative words. A classiﬁer is then
trained from this data to take a WordNet gloss and decide if the sense being deﬁned
is positive, negative or neutral. A further step (involving a random-walk algorithm)
assigns a score to each WordNet synset for its degree of positivity, negativity, and
neutrality.
In summary, semisupervised algorithms use a human-deﬁned set of seed words
for the two poles of a dimension, and use similarity metrics like embedding cosine,
coordination, morphology, or thesaurus structure to score words by how similar they
are to the positive seeds and how dissimilar to the negative seeds.
22.5 Supervised Learning of Word Sentiment
Semi-supervised methods require only minimal human supervision (in the form of
seed sets). But sometimes a supervision signal exists in the world and can be made
use of. One such signal is the scores associated with online reviews .
The web contains an enormous number of online reviews for restaurants, movies,
books, or other products, each of which have the text of the review along with an

--- Page 491 ---
22.5 • S UPERVISED LEARNING OF WORD SENTIMENT 491
Movie review excerpts (IMDb)
10A great movie. This ﬁlm is just a wonderful experience. It’s surreal, zany, witty and slapstick
all at the same time. And terriﬁc performances too.
1This was probably the worst movie I have ever seen. The story went nowhere even though they
could have done some interesting stuff with it.
Restaurant review excerpts (Yelp)
5The service was impeccable. The food was cooked and seasoned perfectly... The watermelon
was perfectly square ... The grilled octopus was ... mouthwatering...
2...it took a while to get our waters, we got our entree before our starter, and we never received
silverware or napkins until we requested them...
Book review excerpts (GoodReads)
1I am going to try and stop being deceived by eye-catching titles. I so wanted to like this book
and was so disappointed by it.
5This book is hilarious. I would recommend it to anyone looking for a satirical read with a
romantic twist and a narrator that keeps butting in
Product review excerpts (Amazon)
5The lid on this blender though is probably what I like the best about it... enables you to pour
into something without even taking the lid off! ... the perfect pitcher! ... works fantastic.
1I hate this blender... It is nearly impossible to get frozen fruit and ice to turn into a smoothie...
You have to add a TON of liquid. I also wish it had a spout ...
Figure 22.9 Excerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except
IMDb, which is on a scale of 1 to 10 stars.
associated review score: a value that may range from 1 star to 5 stars, or scoring 1
to 10. Fig. 22.9 shows samples extracted from restaurant, book, and movie reviews.
We can use this review score as supervision: positive words are more likely to
appear in 5-star reviews; negative words in 1-star reviews. And instead of just a
binary polarity, this kind of supervision allows us to assign a word a more complex
representation of its polarity: its distribution over stars (or other scores).
Thus in a ten-star system we could represent the sentiment of each word as a
10-tuple, each number a score representing the word’s association with that polarity
level. This association can be a raw count, or a likelihood P(wjc), or some other
function of the count, for each class cfrom 1 to 10.
For example, we could compute the IMDb likelihood of a word like disap-
point(ed/ing) occurring in a 1 star review by dividing the number of times disap-
point(ed/ing) occurs in 1-star reviews in the IMDb dataset (8,557) by the total num-
ber of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of
P(disappointingj1)is .0003.
A slight modiﬁcation of this weighting, the normalized likelihood, can be used
as an illuminating visualization (Potts, 2011)1
P(wjc) =count (w;c)P
w2Ccount (w;c)
PottsScore (w) =P(wjc)P
cP(wjc)(22.6)
Dividing the IMDb estimate P(disappointingj1)of .0003 by the sum of the likeli-
hood P(wjc)over all categories gives a Potts score of 0.10. The word disappointing
thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The
1Each element of the Potts score of a word wand category ccan be shown to be a variant of the
pointwise mutual information pmi (w;c)without the log term; see Exercise 22.1.

--- Page 492 ---
492 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
Potts diagram (Potts, 2011) is a visualization of these word scores, representing the Potts diagram
prior sentiment of a word as a distribution over the rating categories.
Fig. 22.10 shows the Potts diagrams for 3 positive and 3 negative scalar adjec-
tives. Note that the curve for strongly positive scalars have the shape of the letter
J, while strongly negative scalars look like a reverse J. By contrast, weakly posi-
tive and negative scalars have a hump-shape, with the maximum either below the
mean (weakly negative words like disappointing ) or above the mean (weakly pos-
itive words like good ). These shapes offer an illuminating typology of affective
meaning.
OverviewDataMethodsCategorizationScale inductionLooking aheadExample: attenuators
IMDB – 53,775 tokens
Category-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.15Cat = 0.33 (p = 0.004)Cat^2 = -4.02 (p < 0.001)OpenTable – 3,890 tokens
Category-0.50-0.250.000.250.500.080.38Cat = 0.11 (p = 0.707)Cat^2 = -6.2 (p = 0.014)Goodreads – 3,424 tokens
Category-0.50-0.250.000.250.500.080.190.36Cat = -0.55 (p = 0.128)Cat^2 = -5.04 (p = 0.016)Amazon/Tripadvisor – 2,060 tokens
Category-0.50-0.250.000.250.500.120.28Cat = 0.42 (p = 0.207)Cat^2 = -2.74 (p = 0.05)somewhat/r
IMDB – 33,515 tokens
Category-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17Cat = -0.13 (p = 0.284)Cat^2 = -5.37 (p < 0.001)OpenTable – 2,829 tokens
Category-0.50-0.250.000.250.500.080.31Cat = 0.2 (p = 0.265)Cat^2 = -4.16 (p = 0.007)Goodreads – 1,806 tokens
Category-0.50-0.250.000.250.500.050.120.180.35Cat = -0.87 (p = 0.016)Cat^2 = -5.74 (p = 0.004)Amazon/Tripadvisor – 2,158 tokens
Category-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r
IMDB – 176,264 tokens
Category-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13Cat = -0.43 (p < 0.001)Cat^2 = -3.6 (p < 0.001)OpenTable – 8,982 tokens
Category-0.50-0.250.000.250.500.080.140.190.32Cat = -0.64 (p = 0.035)Cat^2 = -4.47 (p = 0.007)Goodreads – 11,895 tokens
Category-0.50-0.250.000.250.500.070.150.34Cat = -0.71 (p = 0.072)Cat^2 = -4.59 (p = 0.018)Amazon/Tripadvisor – 5,980 tokens
Category-0.50-0.250.000.250.500.150.28Cat = 0.26 (p = 0.496)Cat^2 = -2.23 (p = 0.131)pretty/r“Potts&diagrams”Potts,&Christopher .& 2011.&NSF&workshop&on&restructuring& adjectives.goodgreatexcellentdisappointingbadterribletotallyabsolutelyutterlysomewhatfairlyprettyPositive scalarsNegative scalarsEmphaticsAttenuators1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating
Figure 22.10 Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, show-
ing the J-shape and reverse J-shape for strongly positive and negative adjectives, and the
hump-shape for more weakly polarized adjectives.
Fig. 22.11 shows the Potts diagrams for emphasizing and attenuating adverbs.
Note that emphatics tend to have a J-shape (most likely to occur in the most posi-
tive reviews) or a U-shape (most likely to occur in the strongly positive and nega-
tive). Attenuators all have the hump-shape, emphasizing the middle of the scale and
downplaying both extremes. The diagrams can be used both as a typology of lexical
sentiment, and also play a role in modeling sentiment compositionality.
In addition to functions like posterior P(cjw), likelihood P(wjc), or normalized
likelihood (Eq. 22.6) many other functions of the count of a word occurring with a
sentiment label have been used. We’ll introduce some of these on page 496, includ-
ing ideas like normalizing the counts per writer in Eq. 22.14.
22.5.1 Log Odds Ratio Informative Dirichlet Prior
One thing we often want to do with word polarity is to distinguish between words
that are more likely to be used in one category of texts than in another. We may, for
example, want to know the words most associated with 1 star reviews versus those
associated with 5 star reviews. These differences may not be just related to senti-
ment. We might want to ﬁnd words used more often by Democratic than Republican
members of Congress, or words used more often in menus of expensive restaurants

--- Page 493 ---
22.5 • S UPERVISED LEARNING OF WORD SENTIMENT 493
OverviewDataMethodsCategorizationScale inductionLooking aheadExample: attenuators
IMDB – 53,775 tokens
Category-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.15Cat = 0.33 (p = 0.004)Cat^2 = -4.02 (p < 0.001)OpenTable – 3,890 tokens
Category-0.50-0.250.000.250.500.080.38Cat = 0.11 (p = 0.707)Cat^2 = -6.2 (p = 0.014)Goodreads – 3,424 tokens
Category-0.50-0.250.000.250.500.080.190.36Cat = -0.55 (p = 0.128)Cat^2 = -5.04 (p = 0.016)Amazon/Tripadvisor – 2,060 tokens
Category-0.50-0.250.000.250.500.120.28Cat = 0.42 (p = 0.207)Cat^2 = -2.74 (p = 0.05)somewhat/r
IMDB – 33,515 tokens
Category-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17Cat = -0.13 (p = 0.284)Cat^2 = -5.37 (p < 0.001)OpenTable – 2,829 tokens
Category-0.50-0.250.000.250.500.080.31Cat = 0.2 (p = 0.265)Cat^2 = -4.16 (p = 0.007)Goodreads – 1,806 tokens
Category-0.50-0.250.000.250.500.050.120.180.35Cat = -0.87 (p = 0.016)Cat^2 = -5.74 (p = 0.004)Amazon/Tripadvisor – 2,158 tokens
Category-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r
IMDB – 176,264 tokens
Category-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13Cat = -0.43 (p < 0.001)Cat^2 = -3.6 (p < 0.001)OpenTable – 8,982 tokens
Category-0.50-0.250.000.250.500.080.140.190.32Cat = -0.64 (p = 0.035)Cat^2 = -4.47 (p = 0.007)Goodreads – 11,895 tokens
Category-0.50-0.250.000.250.500.070.150.34Cat = -0.71 (p = 0.072)Cat^2 = -4.59 (p = 0.018)Amazon/Tripadvisor – 5,980 tokens
Category-0.50-0.250.000.250.500.150.28Cat = 0.26 (p = 0.496)Cat^2 = -2.23 (p = 0.131)pretty/r“Potts&diagrams”Potts,&Christopher .& 2011.&NSF&workshop&on&restructuring& adjectives.goodgreatexcellentdisappointingbadterribletotallyabsolutelyutterlysomewhatfairlyprettyPositive scalarsNegative scalarsEmphaticsAttenuators1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating
Figure 22.11 Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.
than cheap restaurants.
Given two classes of documents, to ﬁnd words more associated with one cate-
gory than another, we could measure the difference in frequencies (is a word wmore
frequent in class Aor class B?). Or instead of the difference in frequencies we could
compute the ratio of frequencies, or compute the log odds ratio (the log of the ratio
between the odds of the two words). We could then sort words by whichever associ-
ation measure we pick, ranging from words overrepresented in category Ato words
overrepresented in category B.
The problem with simple log-likelihood or log odds methods is that they overem-
phasize differences in very rare words, and often also in very frequent words. Very
rare words will seem to occur very differently in the two corpora since with tiny
counts there may be statistical ﬂuctations, or even zero occurrences in one corpus
compared to non-zero occurrences in the other. Very frequent words will also seem
different since all counts are large.
In this section we walk through the details of one solution to this problem: the
“log odds ratio informative Dirichlet prior” method of Monroe et al. (2008) that is a
particularly useful method for ﬁnding words that are statistically overrepresented in
one particular category of texts compared to another. It’s based on the idea of using
another large corpus to get a prior estimate of what we expect the frequency of each
word to be.
Let’s start with the goal: assume we want to know whether the word horrible
occurs more in corpus ior corpus j. We could compute the log likelihood ratio ,log likelihood
ratio
using fi(w)to mean the frequency of word win corpus i, and nito mean the total
number of words in corpus i:
llr(horrible ) = logPi(horrible )
Pj(horrible )
=logPi(horrible ) logPj(horrible )
=logfi(horrible )
ni logfj(horrible )
nj(22.7)
Instead, let’s compute the log odds ratio : does horrible have higher odds in ior in log odds ratio

--- Page 494 ---
494 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
j:
lor(horrible ) = logPi(horrible )
1 Pi(horrible )
 logPj(horrible )
1 Pj(horrible )
=log0
B@fi(horrible )
ni
1 fi(horrible )
ni1
CA log0
B@fj(horrible )
nj
1 fj(horrible )
nj1
CA
=logfi(horrible )
ni fi(horrible )
 logfj(horrible )
nj fj(horrible )
(22.8)
The Dirichlet intuition is to use a large background corpus to get a prior estimate of
what we expect the frequency of each word wto be. We’ll do this very simply by
adding the counts from that corpus to the numerator and denominator, so that we’re
essentially shrinking the counts toward that prior. It’s like asking how large are the
differences between iand jgiven what we would expect given their frequencies in
a well-estimated large background corpus.
The method estimates the difference between the frequency of word win two
corpora iandjvia the prior-modiﬁed log odds ratio for w,d(i j)
w, which is estimated
as:
d(i j)
w=logfi
w+aw
ni+a0 (fiw+aw)
 log 
fj
w+aw
nj+a0 (fj
w+aw)!
(22.9)
(where niis the size of corpus i,njis the size of corpus j,fi
wis the count of word
win corpus i,fj
wis the count of word win corpus j,a0is the scaled size of the
background corpus, and awis the scaled count of word win the background corpus.)
In addition, Monroe et al. (2008) make use of an estimate for the variance of the
log–odds–ratio:
s2
ˆd(i j)
w
1
fiw+aw+1
fj
w+aw(22.10)
The ﬁnal statistic for a word is then the z–score of its log–odds–ratio:
ˆd(i j)
wr
s2
ˆd(i j)
w(22.11)
The Monroe et al. (2008) method thus modiﬁes the commonly used log odds ratio
in two ways: it uses the z-scores of the log odds ratio, which controls for the amount
of variance in a word’s frequency, and it uses counts from a background corpus to
provide a prior count for words.
Fig. 22.12 shows the method applied to a dataset of restaurant reviews from
Yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews
(Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the
1-star reviews using negative sentiment words like worse, bad, awful and the 5-star
reviews using positive sentiment words like great, best, amazing . But there are other
illuminating differences. 1-star reviews use logical negation ( no, not ), while 5-star
reviews use emphatics and emphasize universality ( very, highly, every, always ). 1-
star reviews use ﬁrst person plurals ( we, us, our ) while 5 star reviews use the second
person. 1-star reviews talk about people ( manager, waiter, customer ) while 5-star
reviews talk about dessert and properties of expensive restaurants like courses and
atmosphere. See Jurafsky et al. (2014) for more details.

--- Page 495 ---
22.6 • U SING LEXICONS FOR SENTIMENT RECOGNITION 495
Class Words in 1-star reviews Class Words in 5-star reviews
Negative worst, rude, terrible, horrible, bad,
awful, disgusting, bland, tasteless,
gross, mediocre, overpriced, worse,
poorPositive great, best, love(d), delicious, amazing,
favorite, perfect, excellent, awesome,
friendly, fantastic, fresh, wonderful, in-
credible, sweet, yum(my)
Negation no, not Emphatics/
universalsvery, highly, perfectly, deﬁnitely, abso-
lutely, everything, every, always
1Pl pro we, us, our 2 pro you
3 pro she, he, her, him Articles a, the
Past verb was, were, asked, told, said, did,
charged, waited, left, tookAdvice try, recommend
Sequencers after, then Conjunct also, as, well, with, and
Nouns manager, waitress, waiter, customer,
customers, attitude, waste, poisoning,
money, bill, minutesNouns atmosphere, dessert, chocolate, wine,
course, menu
Irrealis
modalswould, should Auxiliaries is/’s, can, ’ve, are
Comp to, that Prep, other in, of, die, city, mouth
Figure 22.12 The top 50 words associated with one–star and ﬁve-star restaurant reviews in a Yelp dataset of
900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).
22.6 Using Lexicons for Sentiment Recognition
In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The
lexicons we have focused on throughout the chapter so far can be used in a number
of ways to improve sentiment detection.
In the simplest case, lexicons can be used when we don’t have sufﬁcient training
data to build a supervised sentiment analyzer; it can often be expensive to have a
human assign sentiment to each document to train the supervised classiﬁer.
In such situations, lexicons can be used in a rule-based algorithm for classiﬁca-
tion. The simplest version is just to use the ratio of positive to negative words: if a
document has more positive than negative words (using the lexicon to decide the po-
larity of each word in the document), it is classiﬁed as positive. Often a threshold l
is used, in which a document is classiﬁed as positive only if the ratio is greater than
l. If the sentiment lexicon includes positive and negative weights for each word,
q+
wandq 
w, these can be used as well. Here’s a simple such sentiment algorithm:
f+=X
ws.t.w2positivelexiconq+
wcount (w)
f =X
ws.t.w2negativelexiconq 
wcount (w)
sentiment =8
>>><
>>>:+iff+
f >l
 iff 
f+>l
0 otherwise.(22.12)
If supervised training data is available, these counts computed from sentiment lex-
icons, sometimes weighted or normalized in various ways, can also be used as fea-
tures in a classiﬁer along with other lexical or non-lexical features. We return to
such algorithms in Section 22.7.

--- Page 496 ---
496 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
22.7 Using Lexicons for Affect Recognition
Detection of emotion (and the other kinds of affective meaning described by Scherer
(2000)) can be done by generalizing the algorithms described above for detecting
sentiment.
The most common algorithms involve supervised classiﬁcation: a training set is
labeled for the affective meaning to be detected, and a classiﬁer is built using features
extracted from the training set. As with sentiment analysis, if the training set is large
enough, and the test set is sufﬁciently similar to the training set, simply using all
the words or all the bigrams as features in a powerful classiﬁer like SVM or logistic
regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose
performance is hard to beat. Thus we can treat affective meaning classiﬁcation of a
text sample as simple document classiﬁcation.
Some modiﬁcations are nonetheless often necessary for very large datasets. For
example, the Schwartz et al. (2013) study of personality, gender, and age using 700
million words of Facebook posts used only a subset of the n-grams of lengths 1-
3. Only words and phrases used by at least 1% of the subjects were included as
features, and 2-grams and 3-grams were only kept if they had sufﬁciently high PMI
(PMI greater than 2 length , where length is the number of words):
pmi(phrase ) =logp(phrase )Y
w2phrasep(w)(22.13)
Various weights can be used for the features, including the raw count in the training
set, or some normalized probability or log probability. Schwartz et al. (2013), for
example, turn feature counts into phrase likelihoods by normalizing them by each
subject’s total word use.
p(phrasejsubject ) =freq(phrase;subject )X
phrase02vocab (subject )freq(phrase0;subject )(22.14)
If the training data is sparser, or not as similar to the test set, any of the lexicons
we’ve discussed can play a helpful role, either alone or in combination with all the
words and n-grams.
Many possible values can be used for lexicon features. The simplest is just an
indicator function, in which the value of a feature fLtakes the value 1 if a particular
text has any word from the relevant lexicon L. Using the notation of Chapter 4, in
which a feature value is deﬁned for a particular output class cand document x.
fL(c;x) =1 if9w:w2L&w2x&class =c
0 otherwise
Alternatively the value of a feature fLfor a particular lexicon Lcan be the total
number of word tokens in the document that occur in L:
fL=X
w2Lcount (w)
For lexica in which each word is associated with a score or weight, the count can be
multiplied by a weight qL
w:
fL=X
w2LqL
wcount (w)

--- Page 497 ---
22.8 • L EXICON -BASED METHODS FOR ENTITY -CENTRIC AFFECT 497
Counts can alternatively be logged or normalized per writer as in Eq. 22.14.
However they are deﬁned, these lexicon features are then used in a supervised
classiﬁer to predict the desired affective category for the text or document. Once
a classiﬁer is trained, we can examine which lexicon features are associated with
which classes. For a classiﬁer like logistic regression the feature weight gives an
indication of how associated the feature is with the class.
22.8 Lexicon-based methods for Entity-Centric Affect
What if we want to get an affect score not for an entire document, but for a particular
entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines
affect lexicons with contextual embeddings to assign an affect score to an entity in
text. In the context of affect about people, they relabel the Valence/Arousal/Domi-
nance dimension as Sentiment/Agency/Power. The algorithm ﬁrst trains classiﬁers
to map embeddings to scores:
1. For each word win the training corpus:
(a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual
embedding efor each instance of the word. No additional ﬁne-tuning is
done.
(b) Average over the eembeddings of each instance of wto obtain a single
embedding vector for one training point w.
(c) Use the NRC V AD Lexicon to get S, A, and P scores for w.
2. Train (three) regression models on all words wto predict V , A, D scores from
a word’s average embedding.
Now given an entity mention min a text, we assign affect scores as follows:
1. Use the same pretrained LM to get contextual embeddings for min context.
2. Feed this embedding through the 3 regression models to get S, A, P scores for
the entity.
This results in a (S,A,P) tuple for a given entity mention; To get scores for the rep-
resentation of an entity in a complete document, we can run coreference resolution
and average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores
from their algorithm for characters from the movie The Dark Knight when run on
Wikipedia plot summary texts with gold coreference.
22.9 Connotation Frames
The lexicons we’ve described so far deﬁne a word as a point in affective space. A
connotation frame , by contrast, is a lexicon that incorporates a richer kind of gram-connotation
frame
matical structure, by combining affective lexicons with the frame semantic lexicons
of Chapter 21. The basic insight of connotation frame lexicons is that a predicate
like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016,
Rashkin et al. 2017).
Consider sentences like:
(22.15) Country A violated the sovereignty of Country B

--- Page 498 ---
498 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
Power ScoreweaklyRachelDentGordanBatmanJokerpowerfully
Sentiment ScorenegativeJokerDentGordanRachelBatmanpositive
Agency ScoredullDentGordanRachelBatmanJokerscaryFigure 1: Power, sentiment, and agency scores for char-
acters in The Dark Night as learned through the regres-
sion model with ELMo embeddings. Scores generally
align with character archetypes, i.e. the antagonist has
the lowest sentiment score.
ment have resulted in his effective removal from
the industry. While articles about the #MeToo
movement portray men like Weinstein as unpow-
erful, we can speculate that the corpora used to
train ELMo and BERT portray them as powerful.
Thus, in a corpus where traditional power roles
have been inverted, the embeddings extracted
from ELMo and BERT perform worse than ran-
dom, as they are biased towards the power struc-
tures in the data they are trained on. Further ev-
idence of this exists in the performance of the
BERT-masked embeddings - whereas these em-
beddings generally capture power poorly as com-
pared to the unmasked embeddings (Table 2),
they outperform the unmasked embeddings on this
task, and even outperform the frequency baseline
in one setting. Nevertheless, they do not outper-
form Field et al. (2019 ), likely because they do not
capture affect information as well as the unmasked
embeddings (Table 2).
4.3 Qualitative Document-level Analysis
Finally, we qualitatively analyze how well our
method captures affect dimensions by analyzing
single documents in detail. We conduct this anal-
ysis in a domain where we expect entities to fulﬁll
traditional power roles and where entity portray-
als are known. Following Bamman et al. (2013 ),
we analyze the Wikipedia plot summary of the
movie The Dark Knight ,7focusing on Batman
(protagonist),8the Joker (antagonist), Jim Gordan
(law enforcement ofﬁcer, ally to Batman), Har-
7http://bit.ly/2XmhRDR
8We consider Batman/Bruce Wayne to be the same entity.Power Scoreweakly Rachel Joker Dent Gordan Batmanpowerfully
Sentiment Scorenegative Joker Gordan Batman Dent Rachel positive
Agency Scoredull Rachel Dent GordanBatman Joker scaryFigure 2: Power, sentiment, and agency scores for char-acters inThe Dark Nightas learned through ASP withELMo embeddings. These scores reﬂect the same pat-terns as the regression model with greater separationbetween characters.vey Dent (ally to Batman who turns evil) andRachel Dawes (primary love interest). To facil-itate extracting example sentences, we score eachinstance of these entities in the narrative separatelyand average across instances to obtain an entityscore for the document.9To maximize our databy capturing every mention of an entity, we per-form co-reference resolution by hand. Addition-ally, based on our results from Table3as well asthe use of Wikipedia data in training the ELMomodel (Peters et al.,2018), we use ELMo embed-dings for our analysis.Figures1and2show results. For refer-ence, we show the entity scores as compared toone polar opposite pair identiﬁed by ASP. Boththe regression model and ASP show similar pat-terns. Batman has high power, while Rachel haslow power. Additionally, the Joker is associatedwith the most negative sentiment, but the high-est agency. Throughout the plot summary, themovie progresses by the Joker taking an aggres-sive action and the other characters responding.We can see this dynamic reﬂected in the Joker’sproﬁle score, as a high-powered, high-agency,low-sentiment character, who is the primary plot-driver. In general, ASP shows a greater separationbetween characters than the regression model. Wehypothesize that this occurs because ASP isolatesthe dimensions of interest, while the regression ap-proach captures other confounds, such as that hu-9When we used this averaging metric in other evaluations,we found no signiﬁcant change in results. Thus, in other sce-narios, we compute scores over averaged embeddings, ratherthan averaging scores separately computed for each embed-ding to reduce computationally complexity.
Figure 22.13 Power (dominance), sentiment (valence) and agency (arousal) for characters
in the movie The Dark Knight computed from embeddings trained on the NRC V AD Lexicon.
Note the protagonist (Batman) and the antagonist (the Joker) have high power and agency
scores but differ in sentiment, while the love interest Rachel has low power and agency but
high sentiment.
(22.16) the teenager ... survived the Boston Marathon bombing”
By using the verb violate in (22.15), the author is expressing their sympathies with
Country B, portraying Country B as a victim, and expressing antagonism toward
the agent Country A. By contrast, in using the verb survive , the author of (22.16) is
expressing that the bombing is a negative experience, and the subject of the sentence,
the teenager, is a sympathetic character. These aspects of connotation are inherent
in the meaning of the verbs violate andsurvive , as shown in Fig. 22.14.
Writer
Role1
Role2Role1 is asympathetic victimThere issome typeof hardship
Reader+_+__S(writer→role1)S(writer→role2)Connotation Frame for “Role1 survives Role2” S(role1→role2)
Writer
Role1
Role2Role1 is the antagonistRole2 is asympathetic victim
Reader+_+__S(writer→role1)S(writer→role2)Connotation Frame for “Role1 violates Role2” S(role1→role2)
(a) (b)
Figure 22.14 Connotation frames for survive andviolate . (a) For survive , the writer and reader have positive
sentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate , the
writer and reader have positive sentiment instead toward Role2, the direct object.
The connotation frame lexicons of Rashkin et al. (2016) and Rashkin et al.
(2017) also express other connotative aspects of the predicate toward each argu-
ment, including the effect (something bad happened to x) value : (x is valuable), and
mental state : (x is distressed by the event). Connotation frames can also mark the
power differential between the arguments (using the verb implore means that the
theme argument has greater power than the agent), and the agency of each argument
(waited is low agency). Fig. 22.15 shows a visualization from Sap et al. (2017).
Connotation frames can be built by hand (Sap et al., 2017), or they can be learned
by supervised learning (Rashkin et al., 2016), for example using hand-labeled train-

--- Page 499 ---
22.10 • S UMMARY 499
AGENTTHEMEpower(AG < TH)VERBimploreHe implored the tribunal to show mercy.The princess waited for her prince.AGENTTHEMEagency(AG) = -VERBwaitFigure 2: The formal notation of the connotation
frames of power and agency. The ﬁrst example
shows the relative power differential implied by
the verb “implored” , i.e., the agent (“he”) is in
a position of less power than the theme (“the tri-
bunal”). In contrast, “He demanded the tribunal
show mercy” implies that the agent has authority
over the theme. The second example shows the
low level of agency implied by the verb “waited” .
interactive demo website of our ﬁndings (see Fig-
ure5in the appendix for a screenshot).2Further-
more, as will be seen in Section 4.1, connotation
frames offer new insights that complement and de-
viate from the well-known Bechdel test ( Bechdel ,
1986 ). In particular, we ﬁnd that high-agency
women through the lens of connotation frames are
rare in modern ﬁlms. It is, in part, because some
movies (e.g., Snow White) accidentally pass the
Bechdel test and also because even movies with
strong female characters are not entirely free from
the deeply ingrained biases in social norms.
2 Connotation Frames of Power and
Agency
We create two new connotation relations, power
andagency (examples in Figure 3), as an expan-
sion of the existing connotation frame lexicons.3
Three AMT crowdworkers annotated the verbs
with placeholders to avoid gender bias in the con-
text (e.g., Xrescued Y; an example task is shown
in the appendix in Figure 7). We deﬁne the anno-
tated constructs as follows:
Power Differentials Many verbs imply the au-
thority levels of the agent and theme relative to
2http://homes .cs.washington .edu/ ˜msap/
movie-bias/ .
3The lexicons and a demo are available at http://
homes .cs.washington .edu/ ˜msap/movie-bias/ .power (AG<TH)power (AG>TH)
agency (AG)= agency (AG)=+Figure 3: Sample verbs in the connotation frames
with high annotator agreement. Size is indicative
of verb frequency in our corpus (bigger =more
frequent), color differences are only for legibility.
one another. For example, if the agent “dom-
inates” the theme (denoted as power (AG>TH)),
then the agent is implied to have a level of control
over the theme. Alternatively, if the agent “hon-
ors” the theme (denoted as power (AG<TH)), the
writer implies that the theme is more important or
authoritative. We used AMT crowdsourcing to la-
bel 1700 transitive verbs for power differentials.
With three annotators per verb, the inter-annotator
agreement is 0.34 (Krippendorff’s ↵).
Agency The agency attributed to the agent of the
verb denotes whether the action being described
implies that the agent is powerful, decisive, and
capable of pushing forward their own storyline.
For example, a person who is described as “ex-
periencing” things does not seem as active and de-
cisive as someone who is described as “determin-
ing” things. AMT workers labeled 2000 transi-
tive verbs for implying high/moderate/low agency
(inter-annotator agreement of 0.27). We denote
high agency as agency (AG)=+, and low agency
asagency (AG)= .
Pairwise agreements on a hard constraint are
56% and 51% for power and agency, respec-
tively. Despite this, agreements reach 96% and
94% when moderate labels are counted as agree-
ing with either high or low labels, showing that an-
notators rarely strongly disagree with one another.
Some contributing factors in the lower KA scores
include the subtlety of choosing between neutral
Figure 22.15 The connotation frames of Sap et al. (2017), showing that the verb implore
implies the agent has lower power than the theme (in contrast, say, with a verb like demanded ),
and showing the low level of agency of the subject of waited . Figure from Sap et al. (2017).
ing data to supervise classiﬁers for each of the individual relations, e.g., whether
S(writer!Role1) is + or -, and then improving accuracy via global constraints
across all relations.
22.10 Summary
• Many kinds of affective states can be distinguished, including emotions ,moods ,
attitudes (which include sentiment ),interpersonal stance , and personality .
•Emotion can be represented by ﬁxed atomic units often called basic emo-
tions , or as points in space deﬁned by dimensions like valence andarousal .
• Words have connotational aspects related to these affective states, and this
connotational aspect of word meaning can be represented in lexicons.
• Affective lexicons can be built by hand, using crowd sourcing to label the
affective content of each word.
• Lexicons can be built with semi-supervised , bootstrapping from seed words
using similarity metrics like embedding cosine.
• Lexicons can be learned in a fully supervised manner, when a convenient
training signal can be found in the world, such as ratings assigned by users on
a review site.
• Words can be assigned weights in a lexicon by using various functions of word
counts in training texts, and ratio metrics like log odds ratio informative
Dirichlet prior .
• Affect can be detected, just like sentiment, by using standard supervised text
classiﬁcation techniques, using all the words or bigrams in a text as features.
Additional features can be drawn from counts of words in lexicons.
• Lexicons can also be used to detect affect in a rule-based classiﬁer by picking
the simple majority sentiment based on counts of words in each lexicon.
•Connotation frames express richer relations of affective meaning that a pred-
icate encodes about its arguments.

--- Page 500 ---
500 CHAPTER 22 • L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION
Bibliographical and Historical Notes
The idea of formally representing the subjective meaning of words began with Os-
good et al. (1957), the same pioneering study that ﬁrst proposed the vector space
model of meaning described in Chapter 6. Osgood et al. (1957) had participants rate
words on various scales, and ran factor analysis on the ratings. The most signiﬁcant
factor they uncovered was the evaluative dimension, which distinguished between
pairs like good/bad ,valuable/worthless ,pleasant/unpleasant . This work inﬂuenced
the development of early dictionaries of sentiment and affective meaning in the ﬁeld
ofcontent analysis (Stone et al., 1966).
Wiebe (1994) began an inﬂuential line of work on detecting subjectivity in text, subjectivity
beginning with the task of identifying subjective sentences and the subjective char-
acters who are described in the text as holding private states, beliefs or attitudes.
Learned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and
McKeown (1997) were shown to be a useful feature in subjectivity detection (Hatzi-
vassiloglou and Wiebe 2000, Wiebe 2000).
The term sentiment seems to have been introduced in 2001 by Das and Chen
(2001), to describe the task of measuring market sentiment by looking at the words in
stock trading message boards. In the same paper Das and Chen (2001) also proposed
the use of a sentiment lexicon. The list of words in the lexicon was created by
hand, but each word was assigned weights according to how much it discriminated
a particular class (say buy versus sell) by maximizing across-class variation and
minimizing within-class variation. The term sentiment , and the use of lexicons,
caught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) ﬁrst showed
the power of using all the words without a sentiment lexicon; see also Wang and
Manning (2012).
Most of the semi-supervised methods we describe for extending sentiment dic-
tionaries drew on the early idea that synonyms and antonyms tend to co-occur in the
same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shep-
herd 1997). Other semi-supervised methods for learning cues to affective mean-
ing rely on information extraction techniques, like the AutoSlog pattern extractors
(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were ﬁrst sug-
gested by Hatzivassiloglou and McKeown (1997), and graph propagation became a
standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a,
Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by
ﬁltering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,
Fast et al. 2016).
Much recent work focuses on ways to learn embeddings that directly encode sen-
timent or other properties, such as the D ENSIFIER algorithm of Rothe et al. (2016)
that learns to transform the embedding space to focus on sentiment (or other) infor-
mation.
Exercises
22.1 Show that the relationship between a word wand a category cin the Potts
Score in Eq. 22.6 is a variant of the pointwise mutual information pmi (w;c)
without the log term.

--- Page 501 ---
CHAPTER
23Coreference Resolution and
Entity Linking
and even Stigand, the patriotic archbishop of Canterbury, found it advisable–”’
‘Found WHAT?’ said the Duck.
‘Found IT, ’ the Mouse replied rather crossly: ‘of course you know what “it”means. ’
‘I know what “it”means well enough, when I ﬁnd a thing, ’ said the Duck: ‘it’s gener-
ally a frog or a worm. The question is, what did the archbishop ﬁnd?’
Lewis Carroll, Alice in Wonderland
An important component of language processing is knowing who is being talked
about in a text. Consider the following passage:
(23.1) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old became the company’s president. It is widely
known that she came to Megabucks from rival Lotsabucks.
Each of the underlined phrases in this passage is used by the writer to refer to
a person named Victoria Chen. We call linguistic expressions like herorVictoria
Chen mentions orreferring expressions , and the discourse entity that is referred mention
to (Victoria Chen) the referent . (To distinguish between referring expressions and referent
their referents, we italicize the former.)1Two or more referring expressions that are
used to refer to the same discourse entity are said to corefer ; thus, Victoria Chen corefer
andshecorefer in (23.1).
Coreference is an important component of natural language processing. A dia-
logue system that has just told the user “There is a 2pm ﬂight on United and a 4pm
one on Cathay Paciﬁc” must know which ﬂight the user means by “I’ll take the sec-
ond one” . A question answering system that uses Wikipedia to answer a question
about Marie Curie must know who shewas in the sentence “She was born in War-
saw” . And a machine translation system translating from a language like Spanish, in
which pronouns can be dropped, must use coreference from the previous sentence to
decide whether the Spanish sentence ‘ “Me encanta el conocimiento”, dice. ’ should
be translated as ‘ “I love knowledge”, he says ’, or ‘ “I love knowledge”, she says ’.
Indeed, this example comes from an actual news article in El Pa ´ısabout a female
professor and was mistranslated as “he” in machine translation because of inaccurate
coreference resolution (Schiebinger, 2013).
Natural language processing systems (and humans) interpret linguistic expres-
sions with respect to a discourse model (Karttunen, 1969). A discourse modeldiscourse
model
(Fig. 23.1) is a mental model that the understander builds incrementally when in-
terpreting a text, containing representations of the entities referred to in the text,
as well as properties of the entities and relations among them. When a referent is
ﬁrst mentioned in a discourse, we say that a representation for it is evoked into the evoked
model. Upon subsequent mention, this representation is accessed from the model. accessed
1As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,
saying that sherefers to Victoria Chen. However, the reader should keep in mind that what we really
mean is that the speaker is performing the act of referring to Victoria Chen by uttering she.

--- Page 502 ---
502 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
VDiscourse Model“Victoria”“she”coreferrefer (evoke)refer (access)$LotsabucksMegabuckspay
Figure 23.1 How mentions evoke and access discourse entities in a discourse model.
Reference in a text to an entity that has been previously introduced into the
discourse is called anaphora , and the referring expression used is said to be an anaphora
anaphor , or anaphoric.2In passage (23.1), the pronouns sheandherand the deﬁ- anaphor
nite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior
mention (in this case Victoria Chen ) that is called the antecedent . Not every refer- antecedent
ring expression is an antecedent. An entity that has only a single mention in a text
(like Lotsabucks in (23.1)) is called a singleton . singleton
In this chapter we focus on the task of coreference resolution . Coreferencecoreference
resolution
resolution is the task of determining whether two mentions corefer , by which we
mean they refer to the same entity in the discourse model (the same discourse entity ).
The set of coreferring expressions is often called a coreference chain or acluster .coreference
chain
cluster For example, in processing (23.1), a coreference resolution algorithm would need
to ﬁnd at least four coreference chains, corresponding to the four entities in the
discourse model in Fig. 23.1.
1.fVictoria Chen ,her,the 38-year-old ,Sheg
2.fMegabucks Banking ,the company ,Megabucksg
3.fher payg
4.fLotsabucksg
Note that mentions can be nested; for example the mention heris syntactically
part of another mention, her pay , referring to a completely different discourse entity.
Coreference resolution thus comprises two tasks (although they are often per-
formed jointly): (1) identifying the mentions, and (2) clustering them into corefer-
ence chains/discourse entities.
We said that two mentions corefered if they are associated with the same dis-
course entity . But often we’d like to go further, deciding which real world entity is
associated with this discourse entity. For example, the mention Washington might
refer to the US state, or the capital city, or the person George Washington; the inter-
pretation of the sentence will of course be very different for each of these. The task
ofentity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping entity linking
a discourse entity to some real-world individual.3We usually operationalize entity
2We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather
than the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the
antecedent (under the narrower interpretation, repeated names are not anaphors).
3Computational linguistics/NLP thus differs in its use of the term reference from the ﬁeld of formal
semantics, which uses the words reference andcoreference to describe the relation between a mention
and a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention
refers to adiscourse entity (Webber, 1978) and the relation between a discourse entity and the real world
individual requires an additional step of linking .

--- Page 503 ---
503
linking or resolution by mapping to an ontology : a list of entities in the world, like
a gazeteer (Appendix F). Perhaps the most common ontology used for this task is
Wikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus
the entity linking task of wikiﬁcation (Mihalcea and Csomai, 2007) is the task of de-
ciding which Wikipedia page corresponding to an individual is being referred to by
a mention. But entity linking can be done with any ontology; for example if we have
an ontology of genes, we can link mentions of genes in text to the disambiguated
gene name in the ontology.
In the next sections we introduce the task of coreference resolution in more de-
tail, and survey a variety of architectures for resolution. We also introduce two
architectures for the task of entity linking.
Before turning to algorithms, however, we mention some important tasks we
will only touch on brieﬂy at the end of this chapter. First are the famous Winograd
Schema problems (so-called because they were ﬁrst pointed out by Terry Winograd
in his dissertation). These entity coreference resolution problems are designed to be
too difﬁcult to be solved by the resolution methods we describe in this chapter, and
the kind of real-world knowledge they require has made them a kind of challenge
task for natural language processing. For example, consider the task of determining
the correct antecedent of the pronoun they in the following example:
(23.2) The city council denied the demonstrators a permit because
a. they feared violence.
b. they advocated violence.
Determining the correct antecedent for the pronoun they requires understanding
that the second clause is intended as an explanation of the ﬁrst clause, and also
that city councils are perhaps more likely than demonstrators to fear violence and
that demonstrators might be more likely to advocate violence. Solving Winograd
Schema problems requires ﬁnding way to represent or discover the necessary real
world knowledge.
A problem we won’t discuss in this chapter is the related task of event corefer-
ence , deciding whether two event mentions (such as the buyand the acquisition inevent
coreference
these two sentences from the ECB+ corpus) refer to the same event:
(23.3) AMD agreed to [ buy] Markham, Ontario-based ATI for around $5.4 billion
in cash and stock, the companies announced Monday.
(23.4) The [ acquisition ] would turn AMD into one of the world’s largest providers
of graphics chips.
Event mentions are much harder to detect than entity mentions, since they can be ver-
bal as well as nominal. Once detected, the same mention-pair and mention-ranking
models used for entities are often applied to events.
An even more complex kind of coreference is discourse deixis (Webber, 1988), discourse deixis
in which an anaphor refers back to a discourse segment, which can be quite hard to
delimit or categorize, like the examples in (23.5) adapted from Webber (1991):
(23.5) According to Soleil, Beau just opened a restaurant
a. But thatturned out to be a lie.
b. But thatwas false.
c.That struck me as a funny way to describe the situation.
The referent of that is a speech act (see Chapter 15) in (23.5a), a proposition in
(23.5b), and a manner of description in (23.5c). We don’t give algorithms in this
chapter for these difﬁcult types of non-nominal antecedents , but see Kolhatkar
et al. (2018) for a survey.

--- Page 504 ---
504 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
23.1 Coreference Phenomena: Linguistic Background
We now offer some linguistic background on reference phenomena. We introduce
the four types of referring expressions (deﬁnite and indeﬁnite NPs, pronouns, and
names), describe how these are used to evoke and access entities in the discourse
model, and talk about linguistic features of the anaphor/antecedent relation (like
number/gender agreement, or properties of verb semantics).
23.1.1 Types of Referring Expressions
Indeﬁnite Noun Phrases: The most common form of indeﬁnite reference in En-
glish is marked with the determiner a(oran), but it can also be marked by a quan-
tiﬁer such as some or even the determiner this. Indeﬁnite reference generally intro-
duces into the discourse context entities that are new to the hearer.
(23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose .
b. He had gone round one day to bring her some walnuts .
c. I saw this beautiful cauliﬂower today.
Deﬁnite Noun Phrases: Deﬁnite reference, such as via NPs that use the English
article the, refers to an entity that is identiﬁable to the hearer. An entity can be
identiﬁable to the hearer because it has been mentioned previously in the text and
thus is already represented in the discourse model:
(23.7) It concerns a white stallion which I have sold to an ofﬁcer. But the pedigree
ofthe white stallion was not fully established.
Alternatively, an entity can be identiﬁable because it is contained in the hearer’s
set of beliefs about the world, or the uniqueness of the object is implied by the
description itself, in which case it evokes a representation of the referent into the
discourse model, as in (23.9):
(23.8) I read about it in the New York Times .
(23.9) Have you seen the car keys?
These last uses are quite common; more than half of deﬁnite NPs in newswire
texts are non-anaphoric, often because they are the ﬁrst time an entity is mentioned
(Poesio and Vieira 1998, Bean and Riloff 1999).
Pronouns: Another form of deﬁnite reference is pronominalization, used for enti-
ties that are extremely salient in the discourse, (as we discuss below):
(23.10) Emma smiled and chatted as cheerfully as shecould,
Pronouns can also participate in cataphora , in which they are mentioned before cataphora
their referents are, as in (23.11).
(23.11) Even before shesawit, Dorothy had been thinking about the Emerald City
every day.
Here, the pronouns sheanditboth occur before their referents are introduced.
Pronouns also appear in quantiﬁed contexts in which they are considered to be
bound , as in (23.12). bound
(23.12) Every dancer brought herleft arm forward.
Under the relevant reading, herdoes not refer to some woman in context, but instead
behaves like a variable bound to the quantiﬁed expression every dancer . We are not
concerned with the bound interpretation of pronouns in this chapter.

--- Page 505 ---
23.1 • C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 505
In some languages, pronouns can appear as clitics attached to a word, like lo
(‘it’) in this Spanish example from AnCora (Recasens and Mart ´ı, 2010):
(23.13) La intenci ´on es reconocer el gran prestigio que tiene la marat ´on y unir lo
con esta gran carrera.
‘The aim is to recognize the great prestige that the Marathon has and join jit
with this great race.”
Demonstrative Pronouns: Demonstrative pronouns thisandthat can appear ei-
ther alone or as determiners, for instance, this ingredient ,that spice :
(23.14) I just bought a copy of Thoreau’s Walden . I had bought one ﬁve years ago.
That one had been very tattered; this one was in much better condition.
Note that this NP is ambiguous; in colloquial spoken English, it can be indeﬁnite,
as in (23.6), or deﬁnite, as in (23.14).
Zero Anaphora: Instead of using a pronoun, in some languages (including Chi-
nese, Japanese, and Italian) it is possible to have an anaphor that has no lexical
realization at all, called a zero anaphor or zero pronoun, as in the following Italian zero anaphor
and Japanese examples from Poesio et al. (2016):
(23.15) EN [John] iwent to visit some friends. On the way [he] ibought some
wine.
IT [Giovanni] iand`o a far visita a degli amici. Per via ficompr `o del vino.
JA [John] i-wa yujin-o houmon-sita. Tochu-de fiwain-o ka-tta.
or this Chinese example:
(23.16) [ 我]前一会精神上太紧张。[0]现在比较平静了
[I] was too nervous a while ago. ... [0] am now calmer.
Zero anaphors complicate the task of mention detection in these languages.
Names: Names (such as of people, locations, or organizations) can be used to refer
to both new and old entities in the discourse:
(23.17) a. Miss Woodhouse certainly had not done him justice.
b.International Business Machines sought patent compensation
from Amazon; IBM had previously sued other companies.
23.1.2 Information Status
The way referring expressions are used to evoke new referents into the discourse
(introducing new information), or access old entities from the model (old informa-
tion), is called their information status orinformation structure . Entities can beinformation
status
discourse-new ordiscourse-old , and indeed it is common to distinguish at least discourse-new
discourse-old three kinds of entities informationally (Prince, 1981):
new NPs:
brand new NPs: these introduce entities that are discourse-new and hearer-
new like a fruit orsome walnuts .
unused NPs: these introduce entities that are discourse-new but hearer-old
(like Hong Kong ,Marie Curie , orthe New York Times .
old NPs: also called evoked NPs , these introduce entities that already in the dis-
course model, hence are both discourse-old and hearer-old, like itin “I went
to a new restaurant. It was... ”.

--- Page 506 ---
506 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
inferrables: these introduce entities that are neither hearer-old nor discourse-old,
but the hearer can infer their existence by reasoning based on other entities
that are in the discourse. Consider the following examples:
(23.18) I went to a superb restaurant yesterday. The chef had just opened it.
(23.19) Mix ﬂour, butter and water. Knead the dough until shiny.
Neither the chef northe dough were in the discourse model based on the ﬁrst
sentence of either example, but the reader can make a bridging inferencebridging
inference
that these entities should be added to the discourse model and associated with
the restaurant and the ingredients, based on world knowledge that restaurants
have chefs and dough is the result of mixing ﬂour and liquid (Haviland and
Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).
The form of an NP gives strong clues to its information status. We often talk
about an entity’s position on the given-new dimension, the extent to which the refer- given-new
ent is given (salient in the discourse, easier for the hearer to call to mind, predictable
by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,
Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) accessible
i.e., very salient in the hearer’s mind or easy to call to mind, can be referred to with
less linguistic material. For example pronouns are used only when the referent has
a high degree of activation or salience in the discourse model.4By contrast, less salience
salient entities, like a new referent being introduced to the discourse, will need to be
introduced with a longer and more explicit referring expression to help the hearer
recover the referent.
Thus when an entity is ﬁrst introduced into a discourse its mentions are likely
to have full names, titles or roles, or appositive or restrictive relative clauses, as in
the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks
Banking . As an entity is discussed over a discourse, it becomes more salient to the
hearer and its mentions on average typically becomes shorter and less informative,
for example with a shortened name (for example Ms. Chen ), a deﬁnite description
(the 38-year-old ), or a pronoun ( sheorher) (Hawkins 1978). However, this change
in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b,
Reichman 1985, Fox 1993).
23.1.3 Complications: Non-Referring Expressions
Many noun phrases or other nominals are not referring expressions, although they
may bear a confusing superﬁcial resemblance. For example in some of the earliest
computational work on reference resolution, Karttunen (1969) pointed out that the
NPa car in the following example does not create a discourse referent:
(23.20) Janet doesn’t have a car .
and cannot be referred back to by anaphoric itorthe car :
(23.21) * Itis a Toyota.
(23.22) * The car is red.
We summarize here four common types of structures that are not counted as men-
tions in coreference tasks and hence complicate the task of mention-detection:
4Pronouns also usually (but not always) refer to entities that were introduced no further than one or two
sentences back in the ongoing discourse, whereas deﬁnite noun phrases can often refer further back.

--- Page 507 ---
23.1 • C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 507
Appositives: An appositional structure is a noun phrase that appears next to a
head noun phrase, describing the head. In English they often appear in commas, like
“a unit of UAL” appearing in apposition to the NP United , orCFO of Megabucks
Banking in apposition to Victoria Chen .
(23.23) Victoria Chen, CFO of Megabucks Banking, saw ...
(23.24) United, a unit of UAL, matched the fares.
Appositional NPs are not referring expressions, instead functioning as a kind of
supplementary parenthetical description of the head NP. Nonetheless, sometimes it
is useful to link these phrases to an entity they describe, and so some datasets like
OntoNotes mark appositional relationships.
Predicative and Prenominal NPs: Predicative or attributive NPs describe prop-
erties of the head noun. In United is a unit of UAL , the NP a unit of UAL describes
a property of United, rather than referring to a distinct entity. Thus they are not
marked as mentions in coreference tasks; in our example the NPs $2.3 million and
the company’s president , are attributive, describing properties of her pay andthe
38-year-old ; Example (23.27) shows a Chinese example in which the predicate NP
(中国最大的城市; China’s biggest city ) is not a mention.
(23.25) her pay jumped to $2.3 million
(23.26) the 38-year-old became the company’s president
(23.27)上海是[中国最大的城市] [Shanghai is China’s biggest city ]
Expletives: Many uses of pronouns like itin English and corresponding pronouns
in other languages are not referential. Such expletive orpleonastic cases include expletive
it is raining , in idioms like hit it off , or in particular syntactic situations like clefts clefts
(23.28a) or extraposition (23.28b):
(23.28) a. Itwas Emma Goldman who founded Mother Earth
b.Itsurprised me that there was a herring hanging on her wall.
Generics: Another kind of expression that does not refer back to an entity explic-
itly evoked in the text is generic reference. Consider (23.29).
(23.29) I love mangos. They are very tasty.
Here, they refers, not to a particular mango or set of mangos, but instead to the class
of mangos in general. The pronoun youcan also be used generically:
(23.30) In July in San Francisco youhave to wear a jacket.
23.1.4 Linguistic Properties of the Coreference Relation
Now that we have seen the linguistic properties of individual referring expressions
we turn to properties of the antecedent/anaphor pair. Understanding these properties
is helpful both in designing novel features and performing error analyses.
Number Agreement: Referring expressions and their referents must generally
agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plu-
ral, and youis unspeciﬁed for number. So a plural antecedent like the chefs cannot
generally corefer with a singular anaphor like she. However, algorithms cannot
enforce number agreement too strictly. First, semantically plural entities can be re-
ferred to by either itorthey:
(23.31) IBM announced a new machine translation product yesterday. They have
been working on it for 20 years.

--- Page 508 ---
508 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
Second, singular they has become much more common, in which they is used to singular they
describe singular individuals, often useful because they is gender neutral. Although
recently increasing, singular they is quite old, part of English for many centuries.5
Person Agreement: English distinguishes between ﬁrst, second, and third person,
and a pronoun’s antecedent must agree with the pronoun in person. Thus a third
person pronoun ( he, she, they, him, her, them, his, her, their ) must have a third person
antecedent (one of the above or any other noun phrase). However, phenomena like
quotation can cause exceptions; in this example I,my, and sheare coreferent:
(23.32) “I voted for Nader because he was most aligned with my values,” she said.
Gender or Noun Class Agreement: In many languages, all nouns have grammat-
ical gender or noun class6and pronouns generally agree with the grammatical gender
of their antecedent. In English this occurs only with third-person singular pronouns,
which distinguish between male (he, him, his ),female (she, her ), and nonpersonal
(it) grammatical genders. Non-binary pronouns like zeorhirmay also occur in more
recent texts. Knowing which gender to associate with a name in text can be complex,
and may require world knowledge about the individual. Some examples:
(23.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)
(23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)
Binding Theory Constraints: The binding theory is a name for syntactic con-
straints on the relations between a mention and an antecedent in the same sentence
(Chomsky, 1981). Oversimplifying a bit, reﬂexive pronouns like himself andher- reﬂexive
selfcorefer with the subject of the most immediate clause that contains them (23.35),
whereas nonreﬂexives cannot corefer with this subject (23.36).
(23.35) Janet bought herself a bottle of ﬁsh sauce. [herself =Janet]
(23.36) Janet bought her a bottle of ﬁsh sauce. [her 6=Janet]
Recency: Entities introduced in recent utterances tend to be more salient than
those introduced from utterances further back. Thus, in (23.37), the pronoun itis
more likely to refer to Jim’s map than the doctor’s map.
(23.37) The doctor found an old map in the captain’s chest. Jim found an even
older map hidden on the shelf. It described an island.
Grammatical Role: Entities mentioned in subject position are more salient than
those in object position, which are in turn more salient than those mentioned in
oblique positions. Thus although the ﬁrst sentence in (23.38) and (23.39) expresses
roughly the same propositional content, the preferred referent for the pronoun he
varies with the subject—John in (23.38) and Bill in (23.39).
(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of
rum. [ he = Billy ]
(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of
rum. [ he = Jim ]
5Here’s a bound pronoun example from Shakespeare’s Comedy of Errors :There’s not a man I meet but
doth salute me As if I were their well-acquainted friend
6The word “gender” is generally only used for languages with 2 or 3 noun classes, like most Indo-
European languages; many languages, like the Bantu languages or Chinese, have a much larger number
of noun classes.

--- Page 509 ---
23.2 • C OREFERENCE TASKS AND DATASETS 509
Verb Semantics: Some verbs semantically emphasize one of their arguments, bi-
asing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).
(23.40) John telephoned Bill. He lost the laptop.
(23.41) John criticized Bill. He lost the laptop.
These examples differ only in the verb used in the ﬁrst sentence, yet “he” in (23.40)
is typically resolved to John, whereas “he” in (23.41) is resolved to Bill. This may
be partly due to the link between implicit causality and saliency: the implicit cause
of a “criticizing” event is its object, whereas the implicit cause of a “telephoning”
event is its subject. In such verbs, the entity which is the implicit cause may be more
salient.
Selectional Restrictions: Many other kinds of semantic knowledge can play a role
in referent preference. For example, the selectional restrictions that a verb places on
its arguments (Chapter 21) can help eliminate referents, as in (23.42).
(23.42) I ate the soup in my new bowl after cooking it for hours
There are two possible referents for it, the soup and the bowl. The verb eat, however,
requires that its direct object denote something edible, and this constraint can rule
outbowl as a possible referent.
23.2 Coreference Tasks and Datasets
We can formulate the task of coreference resolution as follows: Given a text T, ﬁnd
all entities and the coreference links between them. We evaluate our task by com-
paring the links our system creates with those in human-created gold coreference
annotations on T.
Let’s return to our coreference example, now using superscript numbers for each
coreference chain (cluster), and subscript letters for individual mentions in the clus-
ter:
(23.43) [Victoria Chen]1
a, CFO of [Megabucks Banking]2
a, saw [[her]1
bpay]3
ajump
to $2.3 million, as [the 38-year-old]1
calso became [[the company]2
b’s
president. It is widely known that [she]1
dcame to [Megabucks]2
cfrom rival
[Lotsabucks]4
a.
Assuming example (23.43) was the entirety of the article, the chains for her pay and
Lotsabucks are singleton mentions:
1.fVictoria Chen ,her,the 38-year-old ,Sheg
2.fMegabucks Banking ,the company ,Megabucksg
3.fher payg
4.fLotsabucksg
For most coreference evaluation campaigns, the input to the system is the raw
text of articles, and systems must detect mentions and then link them into clusters.
Solving this task requires dealing with pronominal anaphora (ﬁguring out that her
refers to Victoria Chen ), ﬁltering out non-referential pronouns like the pleonastic It
inIt has been ten years ), dealing with deﬁnite noun phrases to ﬁgure out that the
38-year-old is coreferent with Victoria Chen , and that the company is the same as
Megabucks . And we need to deal with names, to realize that Megabucks is the same
asMegabucks Banking .

--- Page 510 ---
510 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
Exactly what counts as a mention and what links are annotated differs from task
to task and dataset to dataset. For example some coreference datasets do not label
singletons, making the task much simpler. Resolvers can achieve much higher scores
on corpora without singletons, since singletons constitute the majority of mentions in
running text, and they are often hard to distinguish from non-referential NPs. Some
tasks use gold mention-detection (i.e. the system is given human-labeled mention
boundaries and the task is just to cluster these gold mentions), which eliminates the
need to detect and segment mentions from running text.
Coreference is usually evaluated by the CoNLL F1 score, which combines three
metrics: MUC, B3, and CEAF e; Section 23.8 gives the details.
Let’s mention a few characteristics of one popular coreference dataset, OntoNotes
(Pradhan et al. 2007c, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task
based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese
and English coreference datasets of roughly one million words each, consisting of
newswire, magazine articles, broadcast news, broadcast conversations, web data and
conversational speech data, as well as about 300,000 words of annotated Arabic
newswire. The most important distinguishing characteristic of OntoNotes is that
it does not label singletons, simplifying the coreference task, since singletons rep-
resent 60%-70% of all entities. In other ways, it is similar to other coreference
datasets. Referring expression NPs that are coreferent are marked as mentions, but
generics and pleonastic pronouns are not marked. Appositive clauses are not marked
as separate mentions, but they are included in the mention. Thus in the NP, “Richard
Godown, president of the Industrial Biotechnology Association” the mention is the
entire phrase. Prenominal modiﬁers are annotated as separate entities only if they
are proper nouns. Thus wheat is not an entity in wheat ﬁelds , but UNis an entity in
UN policy (but not adjectives like American inAmerican policy ).
A number of corpora mark richer discourse phenomena. The ISNotes corpus
annotates a portion of OntoNotes for information status, include bridging examples
(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains
coreference annotations for 210,532 tokens from 100 different literary novels, in-
cluding singletons and quantiﬁed and negated noun phrases. The AnCora-CO coref-
erence corpus (Recasens and Mart ´ı, 2010) contains 400,000 words each of Spanish
(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for
complex phenomena like discourse deixis in both languages. The ARRAU corpus
(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which
means singleton clusters are available. ARRAU includes diverse genres like dialog
(the TRAINS data) and ﬁction (the Pear Stories), and has labels for bridging refer-
ences, discourse deixis, generics, and ambiguous anaphoric relations.
23.3 Mention Detection
The ﬁrst stage of coreference is mention detection : ﬁnding the spans of text thatmention
detection
constitute each mention. Mention detection algorithms are usually very liberal in
proposing candidate mentions (i.e., emphasizing recall), and only ﬁltering later. For
example many systems run parsers and named entity taggers on the text and extract
every span that is either an NP, apossessive pronoun , or a named entity .
Doing so from our sample text repeated in (23.44):
(23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3

--- Page 511 ---
23.3 • M ENTION DETECTION 511
million, as the 38-year-old also became the company’s president. It is
widely known that she came to Megabucks from rival Lotsabucks.
might result in the following list of 13 potential mentions:
Victoria Chen $2.3 million she
CFO of Megabucks Banking the 38-year-old Megabucks
Megabucks Banking the company Lotsabucks
her the company’s president
her pay It
More recent mention detection systems are even more generous; the span-based
algorithm we will describe in Section 23.6 ﬁrst extracts literally all n-gram spans
of words up to N=10. Of course recall from Section 23.1.3 that many NPs—and
the overwhelming majority of random n-gram spans—are not referring expressions.
Therefore all such mention detection systems need to eventually ﬁlter out pleonas-
tic/expletive pronouns like Itabove, appositives like CFO of Megabucks Banking
Inc, or predicate nominals like the company’s president or$2.3 million .
Some of this ﬁltering can be done by rules. Early rule-based systems designed
regular expressions to deal with pleonastic it, like the following rules from Lappin
and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe ,know ,antic-
ipate ) to capture pleonastic itin “It is thought that ketchup...”, or modal adjectives
(e.g., necessary ,possible ,certain ,important ), for, e.g., “It is likely that I...”. Such
rules are sometimes used as part of modern systems:
It is Modaladjective that S
It is Modaladjective (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S
Mention-detection rules are sometimes designed speciﬁcally for particular eval-
uation campaigns. For OntoNotes, for example, mentions are not embedded within
larger mentions, and while numeric quantities are annotated, they are rarely coref-
erential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a
common ﬁrst pass rule-based mention detection algorithm (Lee et al., 2013) is:
1.Take all NPs, possessive pronouns, and named entities.
2.Remove numeric quantities (100 dollars, 8%), mentions embedded in
larger mentions, adjectival forms of nations, and stop words (like there ).
3.Remove pleonastic itbased on regular expression patterns.
Rule-based systems, however, are generally insufﬁcient to deal with mention-
detection, and so modern systems incorporate some sort of learned mention detec-
tion component, such as a referentiality classiﬁer, an anaphoricity classiﬁer —
detecting whether an NP is an anaphor—or a discourse-new classiﬁer— detecting
whether a mention is discourse-new and a potential antecedent for a future anaphor.
Ananaphoricity detector , for example, can draw its positive training examplesanaphoricity
detector
from any span that is labeled as an anaphoric referring expression in hand-labeled
datasets like OntoNotes, ARRAU , or AnCora. Any other NP or named entity can be
marked as a negative training example. Anaphoricity classiﬁers use features of the
candidate mention such as its head word, surrounding words, deﬁniteness, animacy,
length, position in the sentence/discourse, many of which were ﬁrst proposed in
early work by Ng and Cardie (2002a); see Section 23.5 for more on features.

--- Page 512 ---
512 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
Referentiality or anaphoricity detectors can be run as ﬁlters, in which only men-
tions that are classiﬁed as anaphoric or referential are passed on to the coreference
system. The end result of such a ﬁltering mention detection system on our example
above might be the following ﬁltered set of 9 potential mentions:
Victoria Chen her pay she
Megabucks Bank the 38-year-old Megabucks
her the company Lotsabucks
It turns out, however, that hard ﬁltering of mentions based on an anaphoricity
or referentiality classiﬁer leads to poor performance. If the anaphoricity classiﬁer
threshold is set too high, too many mentions are ﬁltered out and recall suffers. If the
classiﬁer threshold is set too low, too many pleonastic or non-referential mentions
are included and precision suffers.
The modern approach is instead to perform mention detection, anaphoricity, and
coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge
2007, Rahman and Ng 2009). For example mention detection in the Lee et al.
(2017b),2018 system is based on a single end-to-end neural network that computes
a score for each mention being referential, a score for two mentions being corefer-
ence, and combines them to make a decision, training all these scores with a single
end-to-end loss. We’ll describe this method in detail in Section 23.6.7
Despite these advances, correctly detecting referential mentions seems to still be
an unsolved problem, since systems incorrectly marking pleonastic pronouns like
itand other non-referential NPs as coreferent is a large source of errors of modern
coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube
2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).
Mention, referentiality, or anaphoricity detection is thus an important open area
of investigation. Other sources of knowledge may turn out to be helpful, especially
in combination with unsupervised and semisupervised algorithms, which also mit-
igate the expense of labeled datasets. In early work, for example Bean and Riloff
(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-
tracting and generalizing over the ﬁrst NPs in a text, which are guaranteed to be
non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in
the training data but never appear as gold mentions to help ﬁnd non-referential NPs.
Bergsma et al. (2008b) use web counts as a semisupervised way to augment standard
features for anaphoricity detection for English it, an important task because itis both
common and ambiguous; between a quarter and half itexamples are non-anaphoric.
Consider the following two examples:
(23.45) You can make [it] in advance. [anaphoric]
(23.46) You can make [it] in Hollywood. [non-anaphoric]
Theitinmake it is non-anaphoric, part of the idiom make it . Bergsma et al. (2008b)
turn the context around each example into patterns, like “make * in advance” from
(23.45), and “make * in Hollywood” from (23.46). They then use Google n-grams to
enumerate all the words that can replace itin the patterns. Non-anaphoric contexts
tend to only have itin the wildcard positions, while anaphoric contexts occur with
many other NPs (for example make them in advance is just as frequent in their data
7Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like
OntoNotes which don’t label singletons, an alternative to ﬁltering out non-referential mentions is to run
coreference resolution, and then simply delete any candidate mentions which were not corefered with
another mention. This likely doesn’t work as well as explicitly modeling referentiality, and cannot solve
the problem of detecting singletons, which is important for tasks like entity linking.

--- Page 513 ---
23.4 • A RCHITECTURES FOR COREFERENCE ALGORITHMS 513
asmake it in advance , but make them in Hollywood did not occur at all). These
n-gram contexts can be used as features in a supervised anaphoricity classiﬁer.
23.4 Architectures for Coreference Algorithms
Modern systems for coreference are based on supervised neural machine learning,
supervised from hand-labeled datasets like OntoNotes. In this section we overview
the various architecture of modern systems, using the categorization of Ng (2010),
which distinguishes algorithms based on whether they make each coreference deci-
sion in a way that is entity-based —representing each entity in the discourse model—
or only mention-based —considering each mention independently, and whether they
useranking models to directly compare potential antecedents. Afterwards, we go
into more detail on one state-of-the-art algorithm in Section 23.6.
23.4.1 The Mention-Pair Architecture
We begin with the mention-pair architecture, the simplest and most inﬂuential mention-pair
coreference architecture, which introduces many of the features of more complex
algorithms, even though other architectures perform better. The mention-pair ar- mention-pair
chitecture is based around a classiﬁer that— as its name suggests—is given a pair
of mentions, a candidate anaphor and a candidate antecedent, and makes a binary
classiﬁcation decision: coreferring or not.
Let’s consider the task of this classiﬁer for the pronoun shein our example, and
assume the slightly simpliﬁed set of potential antecedents in Fig. 23.2.
Victoria ChenMegabucks Bankingherher paythe 37-year-oldshep(coref|”Victoria Chen”,”she”)
p(coref|”Megabucks Banking”,”she”)
Figure 23.2 For each pair of a mention (like she), and a potential antecedent mention (like
Victoria Chen orher), the mention-pair classiﬁer assigns a probability of a coreference link.
For each prior mention ( Victoria Chen ,Megabucks Banking ,her, etc.), the binary
classiﬁer computes a probability: whether or not the mention is the antecedent of
she. We want this probability to be high for actual antecedents ( Victoria Chen ,her,
the 38-year-old ) and low for non-antecedents ( Megabucks Banking ,her pay ).
Early classiﬁers used hand-built features (Section 23.5); more recent classiﬁers
use neural representation learning (Section 23.6)
For training, we need a heuristic for selecting training samples; since most pairs
of mentions in a document are not coreferent, selecting every pair would lead to
a massive overabundance of negative samples. The most common heuristic, from
(Soon et al., 2001), is to choose the closest antecedent as a positive example, and all
pairs in between as the negative examples. More formally, for each anaphor mention
miwe create
• one positive instance ( mi;mj) where mjis the closest antecedent to mi, and

--- Page 514 ---
514 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
• a negative instance ( mi;mk) for each mkbetween mjandmi
Thus for the anaphor she, we would choose ( she,her) as the positive example
and no negative examples. Similarly, for the anaphor the company we would choose
(the company ,Megabucks ) as the positive example and ( the company ,she) (the com-
pany ,the 38-year-old ) (the company ,her pay ) and ( the company ,her) as negative
examples.
Once the classiﬁer is trained, it is applied to each test sentence in a clustering
step. For each mention iin a document, the classiﬁer considers each of the prior i 1
mentions. In closest-ﬁrst clustering (Soon et al., 2001), the classiﬁer is run right to
left (from mention i 1 down to mention 1) and the ﬁrst antecedent with probability
>:5 is linked to i. If no antecedent has probably >0:5, no antecedent is selected for
i. Inbest-ﬁrst clustering, the classiﬁer is run on all i 1 antecedents and the most
probable preceding mention is chosen as the antecedent for i. The transitive closure
of the pairwise relation is taken as the cluster.
While the mention-pair model has the advantage of simplicity, it has two main
problems. First, the classiﬁer doesn’t directly compare candidate antecedents to
each other, so it’s not trained to decide, between two likely antecedents, which one
is in fact better. Second, it ignores the discourse model, looking only at mentions,
not entities. Each classiﬁer decision is made completely locally to the pair, without
being able to take into account other mentions of the same entity. The next two
models each address one of these two ﬂaws.
23.4.2 The Mention-Rank Architecture
The mention ranking model directly compares candidate antecedents to each other,
choosing the highest-scoring antecedent for each anaphor.
In early formulations, for mention i, the classiﬁer decides which of the f1;:::;i 
1gprior mentions is the antecedent (Denis and Baldridge, 2008). But suppose iis
in fact not anaphoric, and none of the antecedents should be chosen? Such a model
would need to run a separate anaphoricity classiﬁer on i. Instead, it turns out to be
better to jointly learn anaphoricity detection and coreference together with a single
loss (Rahman and Ng, 2009).
So in modern mention-ranking systems, for the ith mention (anaphor), we have
an associated random variable yiranging over the values Y(i) =f1;:::;i 1;g. The
valueis a special dummy mention meaning that idoes not have an antecedent (i.e.,
is either discourse-new and starts a new coref chain, or is non-anaphoric).
Victoria ChenMegabucks Bankingherher paythe 37-year-oldshep(”Victoria Chen”|”she”)
p(ϵ|”she”)ϵOne or more of theseshould be highAll of theseshould be low}p(”her pay”|she”)p(”her”|she”)p(”the 37-year-old”|she”)
p(”Megabucks Banking”|she”)}
Figure 23.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a proba-
bility distribution over all previous mentions plus the special dummy mention .
At test time, for a given mention ithe model computes one softmax over all the
antecedents (plus ) giving a probability for each candidate antecedent (or none).

--- Page 515 ---
23.5 • C LASSIFIERS USING HAND -BUILT FEATURES 515
Fig. 23.3 shows an example of the computation for the single candidate anaphor
she.
Once the antecedent is classiﬁed for each anaphor, transitive closure can be run
over the pairwise decisions to get a complete clustering.
Training is trickier in the mention-ranking model than the mention-pair model,
because for each anaphor we don’t know which of all the possible gold antecedents
to use for training. Instead, the best antecedent for each mention is latent ; that
is, for each mention we have a whole cluster of legal gold antecedents to choose
from. Early work used heuristics to choose an antecedent, for example choosing the
closest antecedent as the gold antecedent and all non-antecedents in a window of
two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds
of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,
Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent
by summing over all of them, with a loss function that optimizes the likelihood of
all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the
details in Section 23.6.
Mention-ranking models can be implemented with hand-build features or with
neural representation learning (which might also incorporate some hand-built fea-
tures). we’ll explore both directions in Section 23.5 and Section 23.6.
23.4.3 Entity-based Models
Both the mention-pair and mention-ranking models make their decisions about men-
tions . By contrast, entity-based models link each mention not to a previous mention
but to a previous discourse entity (cluster of mentions).
A mention-ranking model can be turned into an entity-ranking model simply
by having the classiﬁer make its decisions over clusters of mentions rather than
individual mentions (Rahman and Ng, 2009).
For traditional feature-based models, this can be done by extracting features over
clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the
list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun, so that a cluster composed of fVictoria ,her,the
38-year-oldgwould have the shape P-Pr-D (Bj¨orkelund and Kuhn, 2014). An entity-
based model that includes a mention-pair classiﬁer can use as features aggregates of
mention-pair probabilities, for example computing the average probability of coref-
erence over all mention-pairs in the two clusters (Clark and Manning 2015).
Neural models can learn representations of clusters automatically, for example
by using an RNN over the sequence of cluster mentions to encode a state correspond-
ing to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-
resentations for pairs of clusters by pooling over learned representations of mention
pairs (Clark and Manning, 2016b).
However, although entity-based models are more expressive, the use of cluster-
level information in practice has not led to large gains in performance, so mention-
ranking models are still more commonly used.
23.5 Classiﬁers using hand-built features
Feature-based classiﬁers, use hand-designed features in logistic regression, SVM,
or random forest classiﬁers for coreference resolution. These classiﬁers don’t per-

--- Page 516 ---
516 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
form as well as neural ones. Nonetheless, they are still sometimes useful to build
lightweight systems when compute or data are sparse, and the features themselves
are useful for error analysis even in neural systems.
Given an anaphor mention and a potential antecedent mention, feature based
classiﬁers make use of three types of features: (i) features of the anaphor, (ii) features
of the candidate antecedent, and (iii) features of the relationship between the pair.
Entity-based models can make additional use of two additional classes: (iv) feature
of all mentions from the antecedent’s entity cluster, and (v) features of the relation
between the anaphor and the mentions in the antecedent entity cluster.
Features of the Anaphor or Antecedent Mention
First (last) word Victoria/she First or last word (or embedding) of antecedent/anaphor
Head word Victoria/she Head word (or head embedding) of antecedent/anaphor
Attributes Sg-F-A-3-PER/
Sg-F-A-3-PERThe number, gender, animacy, person, named entity type
attributes of (antecedent/anaphor)
Length 2/1 length in words of (antecedent/anaphor)
Mention type P/Pr Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an-
tecedent/anaphor
Features of the Antecedent Entity
Entity shape P-Pr-D The ‘shape’ or list of types of the mentions in the
antecedent entity (cluster), i.e., sequences of (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.
Entity attributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type
attributes of the antecedent entity
Ant. cluster size 3 Number of mentions in the antecedent cluster
Features of the Pair of Mentions
Sentence distance 1 The number of sentences between antecedent and anaphor
Mention distance 4 The number of mentions between antecedent and anaphor
i-within-i F Anaphor has i-within-i relation with antecedent
Cosine Cosine between antecedent and anaphor embeddings
Features of the Pair of Entities
Exact String Match F True if the strings of any two mentions from the antecedent
and anaphor clusters are identical.
Head Word Match F True if any mentions from antecedent cluster has same
headword as any mention in anaphor cluster
Word Inclusion F All words in anaphor cluster included in antecedent cluster
Figure 23.4 Feature-based coreference: sample feature values for anaphor “she” and potential antecedent
“Victoria Chen”.
Figure 23.4 shows a selection of commonly used features, and shows the value
that would be computed for the potential anaphor “she” and potential antecedent
“Victoria Chen” in our example sentence, repeated below:
(23.47) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old also became the company’s president. It is
widely known that shecame to Megabucks from rival Lotsabucks.
Features that prior work has found to be particularly useful are exact string
match, entity headword agreement, mention distance, as well as (for pronouns) exact
attribute match and i-within-i, and (for nominals and proper names) word inclusion
and cosine. For lexical features (like head words) it is common to only use words
that appear enough times ( >20 times).

--- Page 517 ---
23.6 • A NEURAL MENTION -RANKING ALGORITHM 517
It is crucial in feature-based systems to use conjunctions of features; one exper-
iment suggested that moving from individual features in a classiﬁer to conjunctions
of multiple features increased F1 by 4 points (Lee et al., 2017a). Speciﬁc conjunc-
tions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be
conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using
decision tree or random forest classiﬁers (Ng and Cardie 2002a, Lee et al. 2017a).
Features can also be used in neural models as well. Neural systems use contex-
tual word embeddings so don’t beneﬁt from shallow features like string match or or
mention types. However features like mention length, distance between mentions,
or genre can complement neural contextual embedding models.
23.6 A neural mention-ranking algorithm
In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)
(simpliﬁed and extended a bit, drawing on Joshi et al. (2019) and others). This is
amention-ranking algorithm that considers all possible spans of text in the docu-
ment, assigns a mention-score to each span, prunes the mentions based on this score,
then assigns coreference links to the remaining mentions.
More formally, given a document Dwith Twords, the model considers all of
theT(T+1)
2text spans in D(unigrams, bigrams, trigrams, 4-grams, etc; in practice
we only consider spans up a maximum length around 10). The task is to assign
to each span ian antecedent yi, a random variable ranging over the values Y(i) =
f1;:::;i 1;g; each previous span and a special dummy token . Choosing the
dummy token means that idoes not have an antecedent, either because iis discourse-
new and starts a new coreference chain, or because iis non-anaphoric.
For each pair of spans iand j, the system assigns a score s(i;j)for the coref-
erence link between span iand span j. The system then learns a distribution P(yi)
over the antecedents for span i:
P(yi) =exp(s(i;yi))P
y02Y(i)exp(s(i;y0))(23.48)
This score s(i;j)includes three factors that we’ll deﬁne below: m(i); whether span
iis a mention; m(j); whether span jis a mention; and c(i;j); whether jis the
antecedent of i:
s(i;j) =m(i)+m(j)+c(i;j) (23.49)
For the dummy antecedent , the score s(i;)is ﬁxed to 0. This way if any non-
dummy scores are positive, the model predicts the highest-scoring antecedent, but if
all the scores are negative it abstains.
23.6.1 Computing span representations
To compute the two functions m(i)andc(i;j)which score a span ior a pair of spans
(i;j), we’ll need a way to represent a span. The e2e-coref family of algorithms
represents each span by trying to capture 3 words/tokens: the ﬁrst word, the last
word, and the most important word. We ﬁrst run each paragraph or subdocument
through an encoder (like BERT) to generate embeddings hifor each token i. The
span iis then represented by a vector githat is a concatenation of the encoder output

--- Page 518 ---
518 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
embedding for the ﬁrst (start) token of the span, the encoder output for the last (end)
token of the span, and a third vector which is an attention-based representation:
gi= [hSTART (i);hEND (i);hATT(i)] (23.50)
The goal of the attention vector is to represent which word/token is the likely
syntactic head-word of the span; we saw in the prior section that head-words are
a useful feature; a matching head-word is a good indicator of coreference. The
attention representation is computed as usual; the system learns a weight vector wa,
and computes its dot product with the hidden state httransformed by a FFN:
at=waFFNa(ht) (23.51)
The attention score is normalized into a distribution via a softmax:
ai;t=exp(at)
PEND (i)
k=START (i)exp(ak)(23.52)
And then the attention distribution is used to create a vector hATT(i)which is an
attention-weighted sum of the embeddings etof each of the words in span i:
hATT(i)=END (i)X
t=START (i)ai;tet (23.53)
Fig. 23.5 shows the computation of the span representation and the mention
score.
Encodings (h)…Encoder GeneralElectricsaidthePostalServicecontactedthecompanySpan head (hATT) Span representation (g) Mention score (m)+++++General ElectricElectric said thethe Postal ServiceService contacted thethe company
Figure 23.5 Computation of the span representation g(and the mention score m) in a BERT version of the
e2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of
say 10; the ﬁgure shows a small subset of the bigram and trigram spans.
23.6.2 Computing the mention and antecedent scores mandc
Now that we know how to compute the vector gifor representing span i, we can
see the details of the two scoring functions m(i)andc(i;j). Both are computed by
feedforward networks:
m(i) = wmFFN m(gi) (23.54)
c(i;j) = wcFFN c([gi;gj;gigj;]) (23.55)
At inference time, this mention score mis used as a ﬁlter to keep only the best few
mentions.

--- Page 519 ---
23.6 • A NEURAL MENTION -RANKING ALGORITHM 519
We then compute the antecedent score for high-scoring mentions. The antecedent
score c(i;j)takes as input a representation of the spans iandj, but also the element-
wise similarity of the two spans to each other gigj(hereis element-wise mul-
tiplication). Fig. 23.6 shows the computation of the score sfor the three possible
antecedents of the company in the example sentence from Fig. 23.5.
Figure 23.6 The computation of the score sfor the three possible antecedents of the com-
pany in the example sentence from Fig. 23.5. Figure after Lee et al. (2017b).
Given the set of mentions, the joint distribution of antecedents for each docu-
ment is computed in a forward pass, and we can then do transitive closure on the
antecedents to create a ﬁnal clustering for the document.
Fig. 23.7 shows example predictions from the model, showing the attention
weights, which Lee et al. (2017b) ﬁnd correlate with traditional semantic heads.
Note that the model gets the second example wrong, presumably because attendants
andpilot likely have nearby word embeddings.
Figure 23.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per
example, showing one correct example and one mistake. Bold, parenthesized spans are men-
tions in the predicted cluster. The amount of red color on a word indicates the head-ﬁnding
attention weight ai;tin (23.52). Figure adapted from Lee et al. (2017b).
23.6.3 Learning
For training, we don’t have a single gold antecedent for each mention; instead the
coreference labeling only gives us each entire cluster of coreferent mentions; so a
mention only has a latent antecedent. We therefore use a loss function that maxi-
mizes the sum of the coreference probability of any of the legal antecedents. For a
given mention iwith possible antecedents Y(i), let GOLD (i) be the set of mentions
in the gold cluster containing i. Since the set of mentions occurring before iisY(i),
the set of mentions in that gold cluster that also occur before iisY(i)\GOLD (i). We

--- Page 520 ---
520 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
therefore want to maximize:
X
ˆy2Y(i)\GOLD (i)P(ˆy) (23.56)
If a mention iis not in a gold cluster GOLD (i) =.
To turn this probability into a loss function, we’ll use the cross-entropy loss
function we deﬁned in Eq. 5.23 in Chapter 5, by taking the  log of the probability.
If we then sum over all mentions, we get the ﬁnal loss function for training:
L=NX
i=2 logX
ˆy2Y(i)\GOLD (i)P(ˆy) (23.57)
23.7 Entity Linking
Entity linking is the task of associating a mention in text with the representation of entity linking
some real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It
is the natural follow-on to coreference resolution; coreference resolution is the task
of associating textual mentions that corefer to the same entity. Entity linking takes
the further step of identifying who that entity is. It is especially important for any
NLP task that links to a knowledge base.
While there are all sorts of potential knowledge-bases, we’ll focus in this section
on Wikipedia, since it’s widely used as an ontology for NLP tasks. In this usage,
each unique Wikipedia page acts as the unique id for a particular entity. This task of
deciding which Wikipedia page corresponding to an individual is being referred to
by a text mention has its own name: wikiﬁcation (Mihalcea and Csomai, 2007). wikiﬁcation
Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne
and Witten 2008), entity linking is done in (roughly) two stages: mention detec-
tion andmention disambiguation . We’ll give two algorithms, one simple classic
baseline that uses anchor dictionaries and information from the Wikipedia graph
structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,
2020). We’ll focus here mainly on the application of entity linking to questions,
since a lot of the literature has been in that context.
23.7.1 Linking based on Anchor Dictionaries and Web Graph
As a simple baseline we introduce the T AGME linker (Ferragina and Scaiella, 2011)
for Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007,
Cucerzan 2007, Milne and Witten 2008). Wikiﬁcation algorithms deﬁne the set of
entities as the set of Wikipedia pages, so we’ll refer to each Wikipedia page as a
unique entity e. T AGME ﬁrst creates a catalog of all entities (i.e. all Wikipedia
pages, removing some disambiguation and other meta-pages) and indexes them in a
standard IR engine like Lucene. For each page e, the algorithm computes an in-link
count in (e): the total number of in-links from other Wikipedia pages that point to e.
These counts can be derived from Wikipedia dumps.
Finally, the algorithm requires an anchor dictionary . An anchor dictionary
lists for each Wikipedia page, its anchor texts : the hyperlinked spans of text on anchor texts
other pages that point to it. For example, the web page for Stanford University,
http://www.stanford.edu , might be pointed to from another page using anchor
texts like Stanford orStanford University :

--- Page 521 ---
23.7 • E NTITY LINKING 521
<a href="http://www.stanford.edu">Stanford University</a>
We compute a Wikipedia anchor dictionary by including, for each Wikipedia
page e,e’s title as well as all the anchor texts from all Wikipedia pages that point to e.
For each anchor string awe’ll also compute its total frequency freq (a)in Wikipedia
(including non-anchor uses), the number of times aoccurs as a link (which we’ll call
link(a)), and its link probability linkprob (a) =link(a)=freq(a). Some cleanup of the
ﬁnal anchor dictionary is required, for example removing anchor strings composed
only of numbers or single characters, that are very rare, or that are very unlikely to
be useful entities because they have a very low linkprob.
Mention Detection Given a question (or other text we are trying to link), T AGME
detects mentions by querying the anchor dictionary for each token sequence up to
6 words. This large set of sequences is pruned with some simple heuristics (for
example pruning substrings if they have small linkprobs). The question:
When was Ada Lovelace born?
might give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans
likeLovelace might be pruned as having too low a linkprob, and but spans like born
have such a low linkprob that they would not be in the anchor dictionary at all.
Mention Disambiguation If a mention span is unambiguous (points to only one
entity/Wikipedia page), we are done with entity linking! However, many spans are
ambiguous, matching anchors for multiple Wikipedia entities/pages. The T AGME
algorithm uses two factors for disambiguating ambiguous spans, which have been
referred to as prior probability andrelatedness/coherence . The ﬁrst factor is p(eja),
the probability with which the span refers to a particular entity. For each page e2
E(a), the probability p(eja)that anchor apoints to e, is the ratio of the number of
links into ewith anchor text ato the total number of occurrences of aas an anchor:
prior(a!e) = p(eja) =count (a!e)
link(a)(23.58)
Let’s see how that factor works in linking entities in the following question:
What Chinese Dynasty came before the Yuan?
The most common association for the span Yuan in the anchor dictionary is the name
of the Chinese currency, i.e., the probability p(Yuancurrencyjyuan)is very high.
Rarer Wikipedia associations for Yuan include the common Chinese last name, a
language spoken in Thailand, and the correct entity in this case, the name of the
Chinese dynasty. So if we chose based only on p(eja), we would make the wrong
disambiguation and miss the correct link, Yuandynasty .
To help in just this sort of case, T AGME uses a second factor, the relatedness of
this entity to other entities in the input question. In our example, the fact that the
question also contains the span Chinese Dynasty , which has a high probability link to
the pageDynasties inChinese history , ought to help match Yuandynasty .
Let’s see how this works. Given a question q, for each candidate anchors span
adetected in q, we assign a relatedness score to each possible entity e2E(a)ofa.
The relatedness score of the link a!eis the weighted average relatedness between
eand all other entities in q. Two entities are considered related to the extent their
Wikipedia pages share many in-links. More formally, the relatedness between two
entities AandBis computed as
rel(A;B) =log(max(jin(A)j;jin(B)j)) log(jin(A)\in(B)j)
log(jWj) log(min(jin(A)j;jin(B)j))(23.59)

--- Page 522 ---
522 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
where in (x)is the set of Wikipedia pages pointing to xandWis the set of all Wiki-
pedia pages in the collection.
The vote given by anchor bto the candidate annotation a!Xis the average,
over all the possible entities of b, of their relatedness to X, weighted by their prior
probability:
vote(b;X) =1
jE(b)jX
Y2E(b)rel(X;Y)p(Yjb) (23.60)
The total relatedness score for a!Xis the sum of the votes of all the other anchors
detected in q:
relatedness (a!X) =X
b2Xqnavote(b;X) (23.61)
To score a!X, we combine relatedness and prior by choosing the entity X
that has the highest relatedness (a!X), ﬁnding other entities within a small of
this value, and from this set, choosing the entity with the highest prior P(Xja). The
result of this step is a single entity assigned to each span in q.
The T AGME algorithm has one further step of pruning spurious anchor/entity
pairs, assigning a score averaging link probability with the coherence.
coherence (a!X) =1
jSj 1X
B2SnXrel(B;X)
score(a!X) =coherence (a!X)+linkprob (a)
2(23.62)
Finally, pairs are pruned if score (a!X)<l, where the threshold lis set on a
held-out set.
23.7.2 Neural Graph-based linking
More recent entity linking models are based on bi-encoders , encoding a candidate
mention span, encoding an entity, and computing the dot product between the en-
codings. This allows embeddings for all the entities in the knowledge base to be
precomputed and cached (Wu et al., 2020). Let’s sketch the ELQ linking algorithm
of Li et al. (2020), which is given a question qand a set of candidate entities from
Wikipedia with associated Wikipedia text, and outputs tuples (e;ms;me)of entity id,
mention start, and mention end. As Fig. 23.8 shows, it does this by encoding each
Wikipedia entity using text from Wikipedia, encoding each mention span using text
from the question, and computing their similarity, as we describe below.
Entity Mention Detection To get an h-dimensional embedding for each question
token, the algorithm runs the question through BERT in the normal way:
[q1qn] =BERT ([CLS]q1qn[SEP]) (23.63)
It then computes the likelihood of each span [i;j]inqbeing an entity mention, in
a way similar to the span-based algorithm we saw for the reader above. First we
compute the score for i/jbeing the start/end of a mention:
sstart(i) =wstartqi;send(j) =wendqj; (23.64)

--- Page 523 ---
23.7 • E NTITY LINKING 523
Figure 23.8 A sketch of the inference process in the ELQ algorithm for entity linking in
questions (Li et al., 2020). Each candidate question mention span and candidate entity are
separately encoded, and then scored by the entity/span dot product.
where wstartandwendare vectors learned during training. Next, another trainable
embedding, wmention is used to compute a score for each token being part of a men-
tion:
smention (t) =wmentionqt (23.65)
Mention probabilities are then computed by combining these three scores:
p([i;j]) =s 
sstart(i)+send(j)+jX
t=ismention (t)!
(23.66)
Entity Linking To link mentions to entities, we next compute embeddings for
each entity in the set E=e1;;ei;;ewof all Wikipedia entities. For each en-
tityeiwe’ll get text from the entity’s Wikipedia page, the title t(ei)and the ﬁrst
128 tokens of the Wikipedia page which we’ll call the description d(ei). This is
again run through BERT, taking the output of the CLStoken BERT [CLS] as the entity
representation:
xei=BERT [CLS]([CLS]t(ei)[ENT]d(ei)[SEP]) (23.67)
Mention spans can be linked to entities by computing, for each entity eand span
[i;j], the dot product similarity between the span encoding (the average of the token
embeddings) and the entity encoding.
yi;j=1
(j i+1)jX
t=iqt
s(e;[i;j]) =x
eyi;j (23.68)
Finally, we take a softmax to get a distribution over entities for each span:
p(ej[i;j]) =exp(s(e;[i;j]))P
e02Eexp(s(e0;[i;j]))(23.69)
Training The ELQ mention detection and entity linking algorithm is fully super-
vised. This means, unlike the anchor dictionary algorithms from Section 23.7.1,

--- Page 524 ---
524 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
it requires datasets with entity boundaries marked and linked. Two such labeled
datasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions
(Berant et al., 2013) dataset derived from Google search questions, and GraphQues-
tions (Su et al., 2016). Both have had entity spans in the questions marked and
linked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled ver-
sions WebQSP ELand GraphQ EL(Li et al., 2020).
Given a training set, the ELQ mention detection and entity linking phases are
trained jointly, optimizing the sum of their losses. The mention detection loss is
a binary cross-entropy loss, with Lthe length of the passage and Nthe number of
candidates:
LMD= 1
NX
1ijmin(i+L 1;n) 
y[i;j]logp([i;j])+( 1 y[i;j])log(1 p([i;j]))
(23.70)
with y[i;j]=1 if[i;j]is a gold mention span, else 0. The entity linking loss is:
LED= logp(egj[i;j]) (23.71)
where egis the gold entity for mention [i;j].
23.8 Evaluation of Coreference Resolution
We evaluate coreference algorithms model-theoretically, comparing a set of hypoth-
esischains or clusters Hproduced by the system against a set of gold or reference
chains or clusters Rfrom a human labeling, and reporting precision and recall.
However, there are a wide variety of methods for doing this comparison. In fact,
there are 5 common metrics used to evaluate coreference algorithms: the link based
MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)
metrics, the mention based B3metric (Bagga and Baldwin, 1998), the entity based
CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and
Strube, 2016).
Let’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)MUC
F-measure
is based on the number of coreference links (pairs of mentions) common to Hand
R. Precision is the number of common links divided by the number of links in H.
Recall is the number of common links divided by the number of links in R; This
makes MUC biased toward systems that produce large chains (and fewer entities),
and it ignores singletons, since they don’t involve links.
B3is mention-based rather than link-based. For each mention in the reference B3
chain, we compute a precision and recall, and then we take a weighted sum over all
Nmentions in the document to compute a precision and recall for the entire task. For
a given mention i, letRbe the reference chain that includes i, and Hthe hypothesis
chain that has i. The set of correct mentions in HisH\R. Precision for mention i
is thusjH\Rj
jHj, and recall for mention ithusjH\Rj
jRj. The total precision is the weighted
sum of the precision for mention i, weighted by a weight wi. The total recall is the
weighted sum of the recall for mention i, weighted by a weight wi. Equivalently:
Precision =NX
i=1wi# of correct mentions in hypothesis chain containing entity i
# of mentions in hypothesis chain containing entity i
Recall =NX
i=1wi# of correct mentions in hypothesis chain containing entity i
# of mentions in reference chain containing entity i

--- Page 525 ---
23.9 • W INOGRAD SCHEMA PROBLEMS 525
The weight wifor each entity can be set to different values to produce different
versions of the algorithm.
Following a proposal from Denis and Baldridge (2009), the CoNLL coreference
competitions were scored based on the average of MUC, CEAF-e, and B3(Pradhan
et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns
to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed
description of the entire set of metrics; reference implementations of these should
be used rather than attempting to reimplement from scratch (Pradhan et al., 2014).
Alternative metrics have been proposed that deal with particular coreference do-
mains or tasks. For example, consider the task of resolving mentions to named
entities (persons, organizations, geopolitical entities), which might be useful for in-
formation extraction or knowledge base completion. A hypothesis chain that cor-
rectly contains all the pronouns referring to an entity, but has no version of the name
itself, or is linked with a wrong name, is not useful for this task. We might instead
want a metric that weights each mention by how informative it is (with names being
most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to
match a gold chain only if it contains at least one variant of a name (the NEC F1
metric of Agarwal et al. (2019)).
23.9 Winograd Schema problems
From early on in the ﬁeld, researchers have noted that some cases of coreference
are quite difﬁcult, seeming to require world knowledge or sophisticated reasoning
to solve. The problem was most famously pointed out by Winograd (1972) with the
following example:
(23.72) The city council denied the demonstrators a permit because
a. they feared violence.
b. they advocated violence.
Winograd noticed that the antecedent that most readers preferred for the pro-
noun they in continuation (a) was the city council , but in (b) was the demonstrators .
He suggested that this requires understanding that the second clause is intended
as an explanation of the ﬁrst clause, and also that our cultural frames suggest that
city councils are perhaps more likely than demonstrators to fear violence and that
demonstrators might be more likely to advocate violence.
In an attempt to get the ﬁeld of NLP to focus more on methods involving world
knowledge and common-sense reasoning, Levesque (2011) proposed a challenge
task called the Winograd Schema Challenge .8The problems in the challenge taskWinograd
schema
are coreference problems designed to be easily disambiguated by the human reader,
but hopefully not solvable by simple techniques such as selectional restrictions, or
other basic word association methods.
The problems are framed as a pair of statements that differ in a single word or
phrase, and a coreference question:
(23.73) The trophy didn’t ﬁt into the suitcase because it was too large .
Question: What was too large ? Answer: The trophy
8Levesque’s call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a
competition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the
problem called WNLI (Wang et al., 2018a).

--- Page 526 ---
526 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
(23.74) The trophy didn’t ﬁt into the suitcase because it was too small .
Question: What was too small ? Answer: The suitcase
The problems have the following characteristics:
1. The problems each have two parties
2. A pronoun preferentially refers to one of the parties, but could grammatically
also refer to the other
3. A question asks which party the pronoun refers to
4. If one word in the question is changed, the human-preferred answer changes
to the other party
The kind of world knowledge that might be needed to solve the problems can
vary. In the trophy/suitcase example, it is knowledge about the physical world; that
a bigger object cannot ﬁt into a smaller object. In the original Winograd sentence,
it is stereotypes about social actors like politicians and protesters. In examples like
the following, it is knowledge about human actions like turn-taking or thanking.
(23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose
turn was [over/next]? Answers: Bill/John
(23.76) Joan made sure to thank Susan for all the help she had [given/received].
Who had [given/received] help? Answers: Susan/Joan.
Although the Winograd Schema was designed to require common-sense rea-
soning, a large percentage of the original set of problems can be solved by pre-
trained language models, ﬁne-tuned on Winograd Schema sentences (Kocijan et al.,
2019). Large pretrained language models encode an enormous amount of world or
common-sense knowledge! The current trend is therefore to propose new datasets
with increasingly difﬁcult Winograd-like coreference resolution problems like K NOW REF
(Emami et al., 2019), with examples like:
(23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the
gap wasn’t all that big.
In the end, it seems likely that some combination of language modeling and knowl-
edge will prove fruitful; indeed, it seems that knowledge-based models overﬁt less
to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),
23.10 Gender Bias in Coreference
As with other aspects of language processing, coreference models exhibit gender and
other biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018). For exam-
ple the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Winograd Schema
paradigm to test the extent to which coreference algorithms are biased toward link-
ing gendered pronouns with antecedents consistent with cultural stereotypes. As we
summarized in Chapter 6, embeddings replicate societal biases in their training test,
such as associating men with historically sterotypical male occupations like doctors,
and women with stereotypical female occupations like secretaries (Caliskan et al.
2017, Garg et al. 2018).
A WinoBias sentence contain two mentions corresponding to stereotypically-
male and stereotypically-female occupations and a gendered pronoun that must be
linked to one of them. The sentence cannot be disambiguated by the gender of the
pronoun, but a biased model might be distracted by this cue. Here is an example
sentence:

--- Page 527 ---
23.11 • S UMMARY 527
(23.78) The secretary called the physician iand told him iabout a new patient
[pro-stereotypical]
(23.79) The secretary called the physician iand told her iabout a new patient
[anti-stereotypical]
Zhao et al. (2018a) consider a coreference system to be biased if it is more accu-
rate at linking pronouns consistent with gender stereotypical occupations (e.g., him
with physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical
occupations (e.g., herwith physician in (23.79)). They show that coreference sys-
tems of all architectures (rule-based, feature-based machine learned, and end-to-
end-neural) all show signiﬁcant bias, performing on average 21 F 1points worse in
the anti-stereotypical cases.
One possible source of this bias is that female entities are signiﬁcantly un-
derrepresented in the OntoNotes dataset, used to train most coreference systems.
Zhao et al. (2018a) propose a way to overcome this bias: they generate a second
gender-swapped dataset in which all male entities in OntoNotes are replaced with
female ones and vice versa, and retrain coreference systems on the combined orig-
inal and swapped OntoNotes data, also using debiased GloVE embeddings (Boluk-
basi et al., 2016). The resulting coreference systems no longer exhibit bias on the
WinoBias dataset, without signiﬁcantly impacting OntoNotes coreference accuracy.
In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo
contextualized word vector representations and coref systems that use them. They
showed that retraining ELMo with data augmentation again reduces or removes bias
in coreference systems on WinoBias.
Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered
Pronoun Resolution as a tool for developing improved coreference algorithms for
gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences
with gendered ambiguous pronouns (by contrast, only 20% of the gendered pro-
nouns in the English OntoNotes training data are feminine). The examples were
created by drawing on naturally occurring sentences from Wikipedia pages to create
hard to resolve cases with two named entities of the same gender and an ambiguous
pronoun that may refer to either person (or neither), like the following:
(23.80) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving
back from Karuizawa to Kitami where shehad spent her junior days.
Webster et al. (2018) show that modern coreference algorithms perform signif-
icantly worse on resolving feminine pronouns than masculine pronouns in GAP.
Kurita et al. (2019) shows that a system based on BERT contextualized word repre-
sentations shows similar bias.
23.11 Summary
This chapter introduced the task of coreference resolution .
• This is the task of linking together mentions in text which corefer , i.e. refer
to the same discourse entity in the discourse model , resulting in a set of
coreference chains (also called clusters orentities ).
• Mentions can be deﬁnite NPs orindeﬁnite NPs ,pronouns (including zero
pronouns ) ornames .

--- Page 528 ---
528 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
• The surface form of an entity mention is linked to its information status
(new,old, orinferrable ), and how accessible orsalient the entity is.
• Some NPs are not referring expressions, such as pleonastic itinIt is raining .
• Many corpora have human-labeled coreference annotations that can be used
for supervised learning, including OntoNotes for English, Chinese, and Ara-
bic, ARRAU for English, and AnCora for Spanish and Catalan.
• Mention detection can start with all nouns and named entities and then use
anaphoricity classiﬁers orreferentiality classiﬁers to ﬁlter out non-mentions.
• Three common architectures for coreference are mention-pair ,mention-rank ,
andentity-based , each of which can make use of feature-based or neural clas-
siﬁers.
• Modern coreference systems tend to be end-to-end, performing mention de-
tection and coreference in a single end-to-end architecture.
• Algorithms learn representations for text spans and heads, and learn to com-
pare anaphor spans with candidate antecedent spans.
• Entity linking is the task of associating a mention in text with the representa-
tion of some real-world entity in an ontology .
• Coreference systems are evaluated by comparing with gold entity labels using
precision/recall metrics like MUC ,B3,CEAF ,BLANC , orLEA .
• The Winograd Schema Challenge problems are difﬁcult coreference prob-
lems that seem to require world knowledge or sophisticated reasoning to solve.
• Coreference systems exhibit gender bias which can be evaluated using datasets
like Winobias and GAP.
Bibliographical and Historical Notes
Coreference has been part of natural language processing since the 1970s (Woods
et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation
of coreference was formulated by Karttunen (1969) (at the 3rd COLING confer-
ence), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But
it was Bonnie Webber’s 1978 dissertation and following work (Webber 1983) that
explored the model’s computational aspects, providing fundamental insights into
how entities are represented in the discourse model and the ways in which they can
license subsequent reference. Many of the examples she provided continue to chal-
lenge theories of reference to this day.
The Hobbs algorithm9is a tree-search algorithm that was the ﬁrst in a longHobbs
algorithm
series of syntax-based methods for identifying reference robustly in naturally occur-
ring text. The input to the Hobbs algorithm is a pronoun to be resolved, together
with a syntactic (constituency) parse of the sentences up to and including the cur-
rent sentence. The details of the algorithm depend on the grammar used, but can be
understood from a simpliﬁed version due to Kehler et al. (2004) that just searches
through the list of NPs in the current and prior sentences. This simpliﬁed Hobbs
algorithm searches NPs in the following order: “(i) in the current sentence from
right-to-left, starting with the ﬁrst NP to the left of the pronoun, (ii) in the previous
sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in
9The simpler of two algorithms presented originally in Hobbs (1978).

--- Page 529 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 529
the current sentence from left-to-right, starting with the ﬁrst noun group to the right
of the pronoun (for cataphora). The ﬁrst noun group that agrees with the pronoun
with respect to number, gender, and person is chosen as the antecedent” (Kehler
et al., 2004).
Lappin and Leass (1994) was an inﬂuential entity-based system that used weights
to combine syntactic and other features, extended soon after by Kennedy and Bogu-
raev (1996) whose system avoids the need for full syntactic parses.
Approximately contemporaneously centering (Grosz et al., 1995) was applied
to pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of
work followed focused on centering’s use in coreference (Kameyama 1986, Di Eu-
genio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler
1997a, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how center-
ing can be integrated with coherence-driven theories of pronoun interpretation. See
Chapter 24 for the use of centering in measuring discourse coherence.
Coreference competitions as part of the US DARPA-sponsored MUC confer-
ences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-
7 corpora), and set the tone for much later work, choosing to focus exclusively
on the simplest cases of identity coreference (ignoring difﬁcult cases like bridging,
metonymy, and part-whole) and drawing the community toward supervised machine
learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE eval-
uations produced labeled coreference corpora in English, Chinese, and Arabic that
were widely used for model training and evaluation.
This DARPA work inﬂuenced the community toward supervised learning begin-
ning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and
Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and
Cardie (2002b), and a series of machine learning models followed over the next 15
years. These often focused separately on pronominal anaphora resolution (Kehler
et al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,
Ng and Cardie 2002b, Ng 2005a) and deﬁnite NP reference (Poesio and Vieira 1998,
Vieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff
1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection
(de Marneffe et al., 2015).
The move from mention-pair to mention-ranking approaches was pioneered by
Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,
then extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-
max over all prior mentions. The idea of doing mention detection, anaphoricity, and
coreference jointly in a single end-to-end model grew out of the early proposal of Ng
(2005b) to use a dummy antecedent for mention-ranking, allowing ‘non-referential’
to be a choice for coreference classiﬁers, Denis and Baldridge’s 2007 joint system
combining anaphoricity classiﬁer probabilities with coreference probabilities, the
Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) pro-
posal to train the two models jointly with a single objective.
Simple rule-based systems for coreference returned to prominence in the 2010s,
partly because of their ability to encode entity-based features in a high-precision way
(Zhou et al. 2004b, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.
2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an
inability to deal with the semantics necessary to correctly handle cases of common
noun coreference.
A return to supervised learning led to a number of advances in mention-ranking
models which were also extended into neural architectures, for example using re-

--- Page 530 ---
530 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
inforcement learning to directly optimize coreference evaluation models Clark and
Manning (2016a), doing end-to-end coreference all the way from span extraction
(Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take
advantage of global entity-level information (Clark and Manning 2016b, Wiseman
et al. 2016, Lee et al. 2018).
Coreference is also related to the task of entity linking discussed in Chapter 14.
Coreference can help entity linking by giving more possible surface forms to help
link to the right Wikipedia page, and conversely entity linking can help improve
coreference resolution. Consider this example from Hajishirzi et al. (2013):
(23.81) [Michael Eisner] 1and [Donald Tsang] 2announced the grand opening of
[[Hong Kong] 3Disneyland] 4yesterday. [Eisner] 1thanked [the President] 2
and welcomed [fans] 5to [the park] 4.
Integrating entity linking into coreference can help draw encyclopedic knowl-
edge (like the fact that Donald Tsang is a president) to help disambiguate the men-
tionthe President . Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012)
showed that such attributes extracted from Wikipedia pages could be used to build
richer models of entity mentions in coreference. More recent research shows how to
do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even
jointly with named entity tagging as well (Durrett and Klein 2014).
The coreference task as we introduced it involves a simplifying assumption that
the relationship between an anaphor and its antecedent is one of identity : the two
coreferring mentions refer to the identical discourse referent. In real texts, the rela-
tionship can be more complex, where different aspects of a discourse referent can
be neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an
example of metonymy , in which the capital city Washington is used metonymically metonymy
to refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):
(23.82) a strict interpretation of a policy requires The U.S. to notify foreign
dictators of certain coup plots ... Washington rejected the bid ...
(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New
Year. In the South, everyone was celebrating New Year ; to the North, it
was a regular day.
(23.84) In France, the president is elected for a term of seven years, while in the
United States heis elected for a term of four years.
For further linguistic discussions of these complications of coreference see Puste-
jovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and
Turner (2008), Versley (2008), and Barker (2010).
Ng (2017) offers a useful compact history of machine learning models in coref-
erence resolution. There are three excellent book-length surveys of anaphora/coref-
erence resolution, covering different time periods: Hirst (1981) (early work until
about 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).
Andy Kehler wrote the Discourse chapter for the 2000 ﬁrst edition of this text-
book, which we used as the starting point for the second-edition chapter, and there
are some remnants of Andy’s lovely prose still in this third-edition coreference chap-
ter.
Exercises

--- Page 531 ---
CHAPTER
24Discourse Coherence
And even in our wildest and most wandering reveries, nay in our very dreams,
we shall ﬁnd, if we reﬂect, that the imagination ran not altogether at adven-
tures, but that there was still a connection upheld among the different ideas,
which succeeded each other. Were the loosest and freest conversation to be
transcribed, there would immediately be transcribed, there would immediately
be observed something which connected it in all its transitions.
David Hume, An enquiry concerning human understanding , 1748
Orson Welles’ movie Citizen Kane was groundbreaking in many ways, perhaps most
notably in its structure. The story of the life of ﬁctional media magnate Charles
Foster Kane, the movie does not proceed in chronological order through Kane’s
life. Instead, the ﬁlm begins with Kane’s death (famously murmuring “Rosebud” )
and is structured around ﬂashbacks to his life inserted among scenes of a reporter
investigating his death. The novel idea that the structure of a movie does not have
to linearly follow the structure of the real timeline made apparent for 20th century
cinematography the inﬁnite possibilities and impact of different kinds of coherent
narrative structures.
But coherent structure is not just a fact about movies or works of art. Like
movies, language does not normally consist of isolated, unrelated sentences, but
instead of collocated, structured, coherent groups of sentences. We refer to such
a coherent structured group of sentences as a discourse , and we use the word co- discourse
herence to refer to the relationship between sentences that makes real discourses coherence
different than just random assemblages of sentences. The chapter you are now read-
ing is an example of a discourse, as is a news article, a conversation, a thread on
social media, a Wikipedia page, and your favorite novel.
What makes a discourse coherent? If you created a text by taking random sen-
tences each from many different sources and pasted them together, would that be a
coherent discourse? Almost certainly not. Real discourses exhibit both local coher- local
ence andglobal coherence . Let’s consider three ways in which real discourses are global
locally coherent;
First, sentences or clauses in real discourses are related to nearby sentences in
systematic ways. Consider this example from Hobbs (1979):
(24.1) John took a train from Paris to Istanbul. He likes spinach.
This sequence is incoherent because it is unclear to a reader why the second
sentence follows the ﬁrst; what does liking spinach have to do with train trips? In
fact, a reader might go to some effort to try to ﬁgure out how the discourse could be
coherent; perhaps there is a French spinach shortage? The very fact that hearers try
to identify such connections suggests that human discourse comprehension involves
the need to establish this kind of coherence.
By contrast, in the following coherent example:
(24.2) Jane took a train from Paris to Istanbul. She had to attend a conference.

--- Page 532 ---
532 CHAPTER 24 • D ISCOURSE COHERENCE
the second sentence gives a REASON for Jane’s action in the ﬁrst sentence. Struc-
tured relationships like REASON that hold between text units are called coherence
relations , and coherent discourses are structured by many such coherence relations.coherence
relations
Coherence relations are introduced in Section 24.1.
A second way a discourse can be locally coherent is by virtue of being “about”
someone or something. In a coherent discourse some entities are salient , and the
discourse focuses on them and doesn’t go back and forth between multiple entities.
This is called entity-based coherence . Consider the following incoherent passage,
in which the salient entity seems to wildly swing from John to Jenny to the piano
store to the living room, back to Jenny, then the piano again:
(24.3) John wanted to buy a piano for his living room.
Jenny also wanted to buy a piano.
He went to the piano store.
It was nearby.
The living room was on the second ﬂoor.
She didn’t ﬁnd anything she liked.
The piano he bought was hard to get up to that ﬂoor.
Entity-based coherence models measure this kind of coherence by tracking salient
entities across a discourse. For example Centering Theory (Grosz et al., 1995), theCentering
Theory
most inﬂuential theory of entity-based coherence, keeps track of which entities in
the discourse model are salient at any point (salient entities are more likely to be
pronominalized or to appear in prominent syntactic positions like subject or object).
In Centering Theory, transitions between sentences that maintain the same salient
entity are considered more coherent than ones that repeatedly shift between entities.
The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly entity grid
used model that realizes some of the intuitions of the Centering Theory framework.
Entity-based coherence is introduced in Section 24.3.
Finally, discourses can be locally coherent by being topically coherent : nearbytopically
coherent
sentences are generally about the same topic and use the same or similar vocab-
ulary to discuss these topics. Because topically coherent discourses draw from a
single semantic ﬁeld or topic, they tend to exhibit the surface property known as
lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semanti- lexical cohesion
cally related words in nearby sentences. For example, the fact that the words house ,
chimney ,garret ,closet , and window — all of which belong to the same semantic
ﬁeld— appear in the two sentences in (24.4), or that they share the identical word
shingled , is a cue that the two are tied together as a discourse:
(24.4) Before winter I built a chimney , and shingled the sides of my house ...
I have thus a tight shingled and plastered house ... with a garret and a
closet , a large window on each side....
In addition to the local coherence between adjacent or nearby sentences, dis-
courses also exhibit global coherence . Many genres of text are associated with
particular conventional discourse structures. Academic articles might have sections
describing the Methodology or Results. Stories might follow conventional plotlines
or motifs. Persuasive essays have a particular claim they are trying to argue for,
and an essay might express this claim together with a structured set of premises that
support the argument and demolish potential counterarguments. We’ll introduce
versions of each of these kinds of global coherence.
Why do we care about the local or global coherence of a discourse? Since co-
herence is a property of a well-written text, coherence detection plays a part in any

--- Page 533 ---
24.1 • C OHERENCE RELATIONS 533
task that requires measuring the quality of a text. For example coherence can help
in pedagogical tasks like essay grading or essay quality measurement that are trying
to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al.
2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing
the coherence relationship between sentences can help know how to select informa-
tion from them. Finally, detecting incoherent text may even play a role in mental
health tasks like measuring symptoms of schizophrenia or other kinds of disordered
language (Ditman and Kuperberg 2010, Elvev ˚ag et al. 2007, Bedi et al. 2015, Iter
et al. 2018).
24.1 Coherence Relations
Recall from the introduction the difference between passages (24.5) and (24.6).
(24.5) Jane took a train from Paris to Istanbul. She likes spinach.
(24.6) Jane took a train from Paris to Istanbul. She had to attend a conference.
The reason (24.6) is more coherent is that the reader can form a connection be-
tween the two sentences, in which the second sentence provides a potential REASON
for the ﬁrst sentences. This link is harder to form for (24.5). These connections
between text spans in a discourse can be speciﬁed as a set of coherence relations .coherence
relation
The next two sections describe two commonly used models of coherence relations
and associated corpora: Rhetorical Structure Theory (RST), and the Penn Discourse
TreeBank (PDTB).
24.1.1 Rhetorical Structure Theory
The most commonly used model of discourse organization is Rhetorical Structure
Theory (RST ) (Mann and Thompson, 1987). In RST relations are deﬁned between RST
two spans of text, generally a nucleus and a satellite . The nucleus is the unit that nucleus
satellite is more central to the writer’s purpose and that is interpretable independently; the
satellite is less central and generally is only interpretable with respect to the nucleus.
Some symmetric relations, however, hold between two nuclei.
Below are a few examples of RST coherence relations, with deﬁnitions adapted
from the RST Treebank Manual (Carlson and Marcu, 2001).
Reason: The nucleus is an action carried out by an animate agent and the satellite
is the reason for the nucleus.
(24.7) [ NUC Jane took a train from Paris to Istanbul.] [ SATShe had to attend a
conference.]
Elaboration: The satellite gives additional information or detail about the situation
presented in the nucleus.
(24.8) [ NUC Dorothy was from Kansas.] [ SATShe lived in the midst of the great
Kansas prairies.]
Evidence: The satellite gives additional information or detail about the situation
presented in the nucleus. The information is presented with the goal of convince the
reader to accept the information presented in the nucleus.
(24.9) [ NUC Kevin must be here.] [ SATHis car is parked outside.]

--- Page 534 ---
534 CHAPTER 24 • D ISCOURSE COHERENCE
Attribution: The satellite gives the source of attribution for an instance of reported
speech in the nucleus.
(24.10) [ SATAnalysts estimated] [ NUC that sales at U.S. stores declined in the
quarter, too]
List: In this multinuclear relation, a series of nuclei is given, without contrast or
explicit comparison:
(24.11) [ NUC Billy Bones was the mate; ] [ NUC Long John, he was quartermaster]
RST relations are traditionally represented graphically; the asymmetric Nucleus-
Satellite relation is represented with an arrow from the satellite to the nucleus:
Kevin must be here.His car is parked outsideevidence
We can also talk about the coherence of a larger text by considering the hierar-
chical structure between coherence relations. Figure 24.1 shows the rhetorical struc-
ture of a paragraph from Marcu (2000a) for the text in (24.12) from the Scientiﬁc
American magazine.
(24.12) With its distant orbit–50 percent farther from the sun than Earth–and slim
atmospheric blanket, Mars experiences frigid weather conditions. Surface
temperatures typically average about -60 degrees Celsius (-76 degrees
Fahrenheit) at the equator and can dip to -123 degrees C near the poles. Only
the midday sun at tropical latitudes is warm enough to thaw ice on occasion,
but any liquid water formed in this way would evaporate almost instantly
because of the low atmospheric pressure.
Title
(1)
Mars2-9
evidence
2-3
background
  (2)
WIth its 
distant orbit  
<p> -- 50
 percent 
farther from 
the sun than 
Earth -- </p> 
and slim 
atmospheric 
blanket,(3)
Mars
experiences
frigid weather
conditions.4-9
elaboration-additional
(4)
Surface 
temperatures
 typically average 
about -60 
degrees Celsius
 <p> (-76 degrees
Fahrenheit)</p>
 at the equator4-5
List
(5)
and can dip
to -123
degrees C
near the
poles.6-9
Contrast
6-7
(6)
Only the
midday sun at
tropical latitudes
is warm enough(7)
to thaw ice
on occasion,purpose8-9
explanation-argumentative
(8)
but any liquid water
formed in this way 
would evaporate 
almost instantly(9)
because of
the low
atmospheric
pressure.
Figure 24.1 A discourse tree for the Scientiﬁc American text in (24.12), from Marcu (2000a). Note that
asymmetric relations are represented with a curved arrow from the satellite to the nucleus.
The leaves in the Fig. 24.1 tree correspond to text spans of a sentence, clause or
phrase that are called elementary discourse units orEDU s in RST; these units can EDU
also be referred to as discourse segments . Because these units may correspond to
arbitrary spans of text, determining the boundaries of an EDU is an important task
for extracting coherence relations. Roughly speaking, one can think of discourse

--- Page 535 ---
24.1 • C OHERENCE RELATIONS 535
segments as being analogous to constituents in sentence syntax, and indeed as we’ll
see in Section 24.2 we generally draw on parsing algorithms to infer discourse struc-
ture.
There are corpora for many discourse coherence models; the RST Discourse
TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It con-
sists of 385 English language documents selected from the Penn Treebank, with full
RST parses for each one, using a large set of 78 distinct relations, grouped into 16
classes. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian
Portuguese (Braud et al., 2017).
Now that we’ve seen examples of coherence, we can see more clearly how a
coherence relation can play a role in summarization or information extraction. For
example, the nuclei of a text presumably express more important information than
the satellites, which might be dropped in a summary.
24.1.2 Penn Discourse TreeBank (PDTB)
The Penn Discourse TreeBank (PDTB ) is a second commonly used dataset that PDTB
embodies another model of coherence relations (Miltsakaki et al. 2004, Prasad et al.
2008, Prasad et al. 2014). PDTB labeling is lexically grounded . Instead of asking
annotators to directly tag the coherence relation between text spans, they were given
a list of discourse connectives , words that signal discourse relations, like because ,discourse
connectives
although ,when ,since , oras a result . In a part of a text where these words marked a
coherence relation between two text spans, the connective and the spans were then
annotated, as in Fig. 24.13, where the phrase as a result signals a causal relationship
between what PDTB calls Arg1 (the ﬁrst two sentences, here in italics) and Arg2
(the third sentence, here in bold).
(24.13) Jewelry displays in department stores were often cluttered and uninspired.
And the merchandise was, well, fake. As a result , marketers of faux gems
steadily lost space in department stores to more fashionable
rivals—cosmetics makers.
(24.14) In July, the Environmental Protection Agency imposed a gradual ban on
virtually all uses of asbestos. (implicit=as a result )By 1997, almost all
remaining uses of cancer-causing asbestos will be outlawed.
Not all coherence relations are marked by an explicit discourse connective, and
so the PDTB also annotates pairs of neighboring sentences with no explicit signal,
like (24.14). The annotator ﬁrst chooses the word or phrase that could have been its
signal (in this case as a result ), and then labels its sense. For example for the am-
biguous discourse connective since annotators marked whether it is using a C AUSAL
or a T EMPORAL sense.
The ﬁnal dataset contains roughly 18,000 explicit relations and 16,000 implicit
relations. Fig. 24.2 shows examples from each of the 4 major semantic classes, while
Fig. 24.3 shows the full tagset.
Unlike the RST Discourse Treebank, which integrates these pairwise coherence
relations into a global tree structure spanning an entire discourse, the PDTB does not
annotate anything above the span-pair level, making no commitment with respect to
higher-level discourse structure.
There are also treebanks using similar methods for other languages; (24.15)
shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015).
Because Chinese has a smaller percentage of explicit discourse connectives than
English (only 22% of all discourse relations are marked with explicit connectives,

--- Page 536 ---
536 CHAPTER 24 • D ISCOURSE COHERENCE
Class Type Example
TEMPORAL SYNCHRONOUS The parishioners of St. Michael and All Angels stop to chat at
the church door, as members here always have. (Implicit while )
In the tower, ﬁve men and women pull rhythmically on ropes
attached to the same ﬁve bells that ﬁrst sounded here in 1614.
CONTINGENCY REASON Also unlike Mr. Ruder, Mr. Breeden appears to be in a position
to get somewhere with his agenda. (implicit=because )As a for-
mer White House aide who worked closely with Congress,
he is savvy in the ways of Washington.
COMPARISON CONTRAST The U.S. wants the removal of what it perceives as barriers to
investment; Japan denies there are real barriers.
EXPANSION CONJUNCTION Not only do the actors stand outside their characters and make
it clear they are at odds with them, but they often literally stand
on their heads.
Figure 24.2 The four high-level semantic distinctions in the PDTB sense hierarchy
Temporal Comparison
Asynchronous Contrast (Juxtaposition, Opposition)
Synchronous (Precedence, Succession) Pragmatic Contrast (Juxtaposition, Opposition)
Concession (Expectation, Contra-expectation)
Pragmatic Concession
Contingency Expansion
Cause (Reason, Result) Exception
Pragmatic Cause (Justiﬁcation) Instantiation
Condition (Hypothetical, General, Unreal
Present/Past, Factual Present/Past)Restatement (Speciﬁcation, Equivalence, Generalization)
Pragmatic Condition (Relevance, Implicit As-
sertion)Alternative (Conjunction, Disjunction, Chosen Alterna-
tive)
List
Figure 24.3 The PDTB sense hierarchy. There are four top-level c¯lasses, 16 types, and 23 subtypes (not all
types have subtypes). 11 of the 16 types are commonly used for implicit argument classiﬁcation; the 5 types in
italics are too rare in implicit labeling to be used.
compared to 47% in English), annotators labeled this corpus by directly mapping
pairs of sentences to 11 sense tags, without starting with a lexical discourse connec-
tor.
(24.15) [ Conn为] [Arg2推动图们江地区开发]，[Arg1韩国捐款一百万美元
设立了图们江发展基金]
“[In order to] [ Arg2 promote the development of the Tumen River region],
[Arg1 South Korea donated one million dollars to establish the Tumen
River Development Fund].”
These discourse treebanks have been used for shared tasks on multilingual dis-
course parsing (Xue et al., 2016).
24.2 Discourse Structure Parsing
Given a sequence of sentences, how can we automatically determine the coherence
relations between them? This task is often called discourse parsing (even thoughdiscourse
parsing
for PDTB we are only assigning labels to leaf spans and not building a full parse

--- Page 537 ---
24.2 • D ISCOURSE STRUCTURE PARSING 537
tree as we do for RST).
24.2.1 EDU segmentation for RST parsing
RST parsing is generally done in two stages. The ﬁrst stage, EDU segmentation ,
extracts the start and end of each EDU. The output of this stage would be a labeling
like the following:
(24.16) [Mr. Rambo says] e1[that a 3.2-acre property] e2[overlooking the San
Fernando Valley] e3[is priced at $4 million] e4[because the late actor Erroll
Flynn once lived there.] e5
Since EDUs roughly correspond to clauses, early models of EDU segmentation
ﬁrst ran a syntactic parser, and then post-processed the output. Modern systems
generally use neural sequence models supervised by the gold EDU segmentation in
datasets like the RST Discourse Treebank. Fig. 24.4 shows an example architecture
simpliﬁed from the algorithm of Lukasik et al. (2020) that predicts for each token
whether or not it is a break. Here the input sentence is passed through an encoder
and then passed through a linear layer and a softmax to produce a sequence of 0s
and 1, where 1 indicates the start of an EDU.
Mr.RambosaysthatENCODER…0001linear layersoftmaxEDU break
Figure 24.4 Predicting EDU segment beginnings from encoded text.
24.2.2 RST parsing
Tools for building RST coherence structure for a discourse have long been based on
syntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern
RST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we
saw in Chapter 20, using representation learning to build representations for each
span, and training a parser to choose the correct shift and reduce actions based on
the gold parses in the training set.
We’ll describe the shift-reduce parser of Yu et al. (2018). The parser state con-
sists of a stack and a queue, and produces this structure by taking a series of actions
on the states. Actions include:
•shift : pushes the ﬁrst EDU in the queue onto the stack creating a single-node
subtree.
•reduce (l,d): merges the top two subtrees on the stack, where lis the coherence
relation label, and dis the nuclearity direction, d2fNN;NS;SNg.
As well as the pop root operation, to remove the ﬁnal tree from the stack.
Fig. 24.6 shows the actions the parser takes to build the structure in Fig. 24.5.

--- Page 538 ---
538 CHAPTER 24 • D ISCOURSE COHERENCE
560e1 e2 e3 e4attr elabelab e1: American Telephone & Telegraph Co. said it
e2: will lay off 75 to 85 technicians here , effective Nov. 1.
e3: The workers install , maintain and repair its private branch exchanges,
e4: which are large intracompany telephone networks.
Figure 1: An example of RST discourse tree, where {e1,e2,e3,e4}are EDUs, attr andelab are
discourse relation labels, and arrows indicate the nuclearities of discourse relations.
RST discourse parsing. Other studies still adopt discrete syntax features proposed by statistical models,
feeding them into neural network models (Braud et al., 2016; Braud et al., 2017).
The above approaches model syntax trees in an explicit way, requiring discrete syntax parsing outputs
as inputs for RST parsing. These approaches may suffer from the error propagation problem. Syntax trees
produced by a supervised syntax parsing model could have errors, which may propagate into discourse
parsing models. The problem could be extremely serious when inputs of discourse parsing have different
distributions with the training data of the supervised syntax parser. Recently, Zhang et al. (2017) suggest
an alternative method, which extracts syntax features from a Bi-Afﬁne dependency parser (Dozat and
Manning, 2016), and the method gives competitive performances on relation extraction. It actually
represents syntax trees implicitly, thus it can reduce the error propagation problem.
In this work, we investigate the implicit syntax feature extraction approach for RST parsing. In ad-
dition, we propose a transition-based neural model for this task, which is able to incorporate various
features ﬂexibly. We exploit hierarchical bi-directional LSTMs (Bi-LSTMs) to encode texts, and further
enhance the transition-based model with dynamic oracle. Based on the proposed model, we study the
effectiveness of our proposed implicit syntax features. We conduct experiments on a standard RST dis-
course TreeBank (Carlson et al., 2003). First, we evaluate the performance of our proposed transition-
based baseline, ﬁnding that the model is able to achieve strong performances after applying dynamic
oracle. Then we evaluate the effectiveness of implicit syntax features extracted from a Bi-Afﬁne depen-
dency parser. Results show that the implicit syntax features are effective, giving better performances than
explicit Tree-LSTM (Li et al., 2015b). Our codes will be released for public under the Apache License
2.0 at https://github.com/yunan4nlp/NNDisParser .
In summary, we mainly make the following two contributions in this work: (1) we propose a transition-
based neural RST discourse parsing model with dynamic oracle, (2) we compare three different syntactic
integration approaches proposed by us. The rest of the paper is organized as follows. Section 2 describes
our proposed models including the transition-based neural model, the dynamic oracle strategy and the
implicit syntax feature extraction approach. Section 3 presents the experiments to evaluate our models.
Section 4 shows the related work. Finally, section 5 draws conclusions.
2 Transition-based Discourse Parsing
We follow Ji and Eisenstein (2014), exploiting a transition-based framework for RST discourse parsing.
The framework is conceptually simple and ﬂexible to support arbitrary features, which has been widely
used in a number of NLP tasks (Zhu et al., 2013; Dyer et al., 2015; Zhang et al., 2016). In addition, a
transition-based model formalizes a certain task into predicting a sequence of actions, which is essential
similar to sequence-to-sequence models proposed recently (Bahdanau et al., 2014). In the following,
we ﬁrst describe the transition system for RST discourse parsing, and then introduce our neural network
model by its encoder and decoder parts, respectively. Thirdly, we present our proposed dynamic oracle
strategy aiming to enhance the transition-based model. Then we introduce the integration method of
implicit syntax features. Finally we describe the training method of our neural network models.
2.1 The Transition-based System
The transition-based framework converts a structural learning problem into a sequence of action predic-
tions, whose key point is a transition system. A transition system consists of two parts: states and actions.
The states are used to store partially-parsed results and the actions are used to control state transitions.
Figure 24.5 Example RST discourse tree, showing four EDUs. Figure from Yu et al. (2018).
561Step Stack Queue Action Relation
1 ?e1,e2,e3,e4 SH ?
2 e1 e2,e3,e4 SH ?
3 e1,e2 e3,e4 RD(attr,SN) ?
4 e1:2 e3,e4 SH de1e2
5 e1:2,e3 e4 SH de1e2
6 e1:2,e3,e4 ?RD(elab,NS) de1e2
7 e1:2,e3:4 ?RD(elab,SN) de1e2,de3e4
8 e1:4 ? PR de1e2,de3e4,\e1:2e3:4
Table 1: An example of the transition-based system for RST discourse parsing.
The initial state is an empty state, and the ﬁnal state represents a full result. There are three kinds of
actions in our transition system:
•Shift (SH), which removes the ﬁrst EDU in the queue onto the stack, forming a single-node subtree.
•Reduce (RD) ( l,d), which merges the top two subtrees on the stack, where lis a discourse relation
label, and d2{NN,NS,SN}indicates the relation nuclearity (nuclear (N) or satellite (S)).
•Pop Root (PR), which pops out the top tree on the stack, marking the decoding being completed,
when the stack holds only one subtree and the queue is empty.
Given the RST tree as shown in Figure 1, it can be generated by the following action sequence: {SH,
SH, RD (attr,SN) , SH, SH, RD (elab,NS) , RD (elab,SN) , PR}. Table 1 shows the decoding
process in detail. By this way, we naturally convert RST discourse parsing into predicting a sequence of
transition actions, where each line includes a state and next step action referring to the tree.
2.2 Encoder-Decoder
Previous transition-based RST discourse parsing studies exploit statistical models, using manually-
designed discrete features (Sagae, 2009; Heilman and Sagae, 2015; Wang et al., 2017). In this work, we
propose a transition-based neural model for RST discourse parsing, which follows an encoder-decoder
framework. Given an input sequence of EDUs {e1,e2,. . . ,e n}, the encoder computes the input represen-
tations {he
1,he
2,. . . ,he
n}, and the decoder predicts next step actions conditioned on the encoder outputs.
2.2.1 Encoder
We follow Li et al. (2016), using hierarchical Bi-LSTMs to encode the source EDU inputs, where the
ﬁrst-layer is used to represent sequencial words inside of EDUs, and the second layer is used to represent
sequencial EDUs. Given an input sentence {w1,w2,. . . ,w m}, ﬁrst we represent each word by its form
(e.g., wi) and POS tag (e.g. ti), concatenating their neural embeddings. By this way, the input vectors
of the ﬁrst-layer Bi-LSTM are {xw
1,xw
2,. . . ,xw
m}, where xw
i=emb (wi) emb (ti), and then we apply
Bi-LSTM directly, obtaining:
{hw
1,hw
2,. . . ,hw
m}=Bi-LSTM ({xw
1,xw
2,. . . ,xw
m}) (1)
The second-layer Bi-LSTM is built over sequential EDUs. We should ﬁrst obtain a suitable representa-
tion for each EDU, which is composed by a span of words inside a certain sentence. Assuming an EDU
with its words by {ws,ws+1,. . . ,w t}, after applying the ﬁrst-layer Bi-LSTM, we obtain their representa-
tions by {hw
s,hw
s+1...,hw
t}, then we calculate the EDU representation by average pooling:
xe=1
t s+1tX
shw
k (2)
When the EDU representations are ready, we apply the second-layer Bi-LSTM directly, resulting:
{he
1,he
2,. . . ,he
n}=Bi-LSTM ({xe
1,xe
2,. . . ,xe
n}) (3)
Figure 24.6 Parsing the example of Fig. 24.5 using a shift-reduce parser. Figure from Yu
et al. (2018).
The Yu et al. (2018) uses an encoder-decoder architecture, where the encoder
represents the input span of words and EDUs using a hierarchical biLSTM. The
ﬁrst biLSTM layer represents the words inside an EDU, and the second represents
the EDU sequence. Given an input sentence w1;w2;:::;wm, the words can be repre-
sented as usual (by static embeddings, combinations with character embeddings or
tags, or contextual embeddings) resulting in an input word representation sequence
xw
1;xw
2;:::;xw
m. The result of the word-level biLSTM is then a sequence of hwvalues:
hw
1;hw
2;:::;hw
m=biLSTM (xw
1;xw
2;:::;xw
m) (24.17)
An EDU of span ws;ws+1;:::;wtthen has biLSTM output representation hw
s;hw
s+1;:::;hw
t,
and is represented by average pooling:
xe=1
t s+1tX
k=shw
k (24.18)
The second layer uses this input to compute a ﬁnal representation of the sequence of
EDU representations he:
he
1;he
2;:::;he
n=biLSTM (xe
1;xe
2;:::;xe
n) (24.19)
The decoder is then a feedforward network Wthat outputs an action obased on a
concatenation of the top three subtrees on the stack ( so;s1;s2) plus the ﬁrst EDU in
the queue (q0):
o=W(ht
s0;ht
s1;ht
s2;he
q0) (24.20)
where the representation of the EDU on the queue he
q0comes directly from the
encoder, and the three hidden vectors representing partial trees are computed by
average pooling over the encoder output for the EDUs in those trees:
hts=1
j i+1jX
k=ihe
k (24.21)

--- Page 539 ---
24.2 • D ISCOURSE STRUCTURE PARSING 539
Training ﬁrst maps each RST gold parse tree into a sequence of oracle actions, and
then uses the standard cross-entropy loss (with l2regularization) to train the system
to take such actions. Give a state Sand oracle action a, we ﬁrst compute the decoder
output using Eq. 24.20, apply a softmax to get probabilities:
pa=exp(oa)P
a02Aexp(oa0)(24.22)
and then computing the cross-entropy loss:
LCE() = log(pa)+l
2jjQjj2(24.23)
RST discourse parsers are evaluated on the test section of the RST Discourse Tree-
bank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu,
2000b). It is standard to ﬁrst transform the gold RST trees into right-branching bi-
nary trees, and to report four metrics: trees with no labels (S for Span), labeled
with nuclei (N), with relations (R), or both (F for Full), for each metric computing
micro-averaged F 1over all spans from all documents (Marcu 2000b, Morey et al.
2017).
24.2.3 PDTB discourse parsing
PDTB discourse parsing, the task of detecting PDTB coherence relations between
spans, is sometimes called shallow discourse parsing because the task just involvesshallow
discourse
parsingﬂat relationships between text spans, rather than the full trees of RST parsing.
The set of four subtasks for PDTB discourse parsing was laid out by Lin et al.
(2014) in the ﬁrst complete system, with separate tasks for explicit (tasks 1-3) and
implicit (task 4) connectives:
1. Find the discourse connectives (disambiguating them from non-discourse uses)
2. Find the two spans for each connective
3. Label the relationship between these spans
4. Assign a relation between every adjacent pair of sentences
Many systems have been proposed for Task 4: taking a pair of adjacent sentences
as input and assign a coherence relation sense label as output. The setup often fol-
lows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each
adjacent span one of the 11 second-level PDTB tags or none (removing the 5 very
rare tags of the 16 shown in italics in Fig. 24.3).
A simple but very strong algorithm for Task 4 is to represent each of the two
spans by BERT embeddings and take the last layer hidden state corresponding to
the position of the [CLS] token, pass this through a single layer tanh feedforward
network and then a softmax for sense classiﬁcation (Nie et al., 2019).
Each of the other tasks also have been addressed. Task 1 is to disambiguat-
ing discourse connectives from their non-discourse use. For example as Pitler and
Nenkova (2009) point out, the word andis a discourse connective linking the two
clauses by an elaboration/expansion relation in (24.24) while it’s a non-discourse
NP conjunction in (24.25):
(24.24) Selling picked up as previous buyers bailed out of their positions and
aggressive short sellers—anticipating further declines—moved in.
(24.25) My favorite colors are blue and green.

--- Page 540 ---
540 CHAPTER 24 • D ISCOURSE COHERENCE
Similarly, once is a discourse connective indicating a temporal relation in (24.26),
but simply a non-discourse adverb meaning ‘formerly’ and modifying used in (24.27):
(24.26) The asbestos ﬁber, crocidolite, is unusually resilient once it enters the
lungs, with even brief exposures to it causing symptoms that show up
decades later, researchers said.
(24.27) A form of asbestos once used to make Kent cigarette ﬁlters has caused a
high percentage of cancer deaths among a group of workers exposed to it
more than 30 years ago, researchers reported.
Determining whether a word is a discourse connective is thus a special case
of word sense disambiguation. Early work on disambiguation showed that the 4
PDTB high-level sense classes could be disambiguated with high (94%) accuracy
used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent
work performs the task end-to-end from word inputs using a biLSTM-CRF with
BIO outputs ( B-CONN ,I-CONN ,O) (Yu et al., 2019).
For task 2, PDTB spans can be identiﬁed with the same sequence models used to
ﬁnd RST EDUs: a biLSTM sequence model with pretrained contextual embedding
(BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a base-
line at ﬁnding spans, since 93% of relations are either completely within a single
sentence or span two adjacent sentences, with one argument in each sentence (Biran
and McKeown, 2015).
24.3 Centering and Entity-Based Coherence
A second way a discourse can be coherent is by virtue of being “about” some entity.
This idea that at each point in the discourse some entity is salient, and a discourse
is coherent by continuing to discuss the same entity, appears early in functional lin-
guistics and the psychology of discourse (Chafe 1976, Kintsch and Van Dijk 1978),
and soon made its way to computational models. In this section we introduce two
models of this kind of entity-based coherence :Centering Theory (Grosz et al., entity-based
1995), and the entity grid model of Barzilay and Lapata (2008).
24.3.1 Centering
Centering Theory (Grosz et al., 1995) is a theory of both discourse salience andCentering
Theory
discourse coherence. As a model of discourse salience, Centering proposes that at
any given point in the discourse one of the entities in the discourse model is salient:
it is being “centered” on. As a model of discourse coherence, Centering proposes
that discourses in which adjacent sentences CONTINUE to maintain the same salient
entity are more coherent than those which SHIFT back and forth between multiple
entities (we will see that CONTINUE and SHIFT are technical terms in the theory).
The following two texts from Grosz et al. (1995) which have exactly the same
propositional content but different saliences, can help in understanding the main
Centering intuition.
(24.28) a. John went to his favorite music store to buy a piano.
b. He had frequented the store for many years.
c. He was excited that he could ﬁnally buy a piano.
d. He arrived just as the store was closing for the day.

--- Page 541 ---
24.3 • C ENTERING AND ENTITY -BASED COHERENCE 541
(24.29) a. John went to his favorite music store to buy a piano.
b. It was a store John had frequented for many years.
c. He was excited that he could ﬁnally buy a piano.
d. It was closing just as John arrived.
While these two texts differ only in how the two entities (John and the store) are
realized in the sentences, the discourse in (24.28) is intuitively more coherent than
the one in (24.29). As Grosz et al. (1995) point out, this is because the discourse
in (24.28) is clearly about one individual, John, describing his actions and feelings.
The discourse in (24.29), by contrast, focuses ﬁrst on John, then the store, then back
to John, then to the store again. It lacks the “aboutness” of the ﬁrst discourse.
Centering Theory realizes this intuition by maintaining two representations for
each utterance Un. The backward-looking center ofUn, denoted as Cb(Un), rep-backward-
looking
centerresents the current salient entity, the one being focused on in the discourse after Un
is interpreted. The forward-looking centers ofUn, denoted as Cf(Un), are a setforward-looking
center
of potential future salient entities, the discourse entities evoked by Unany of which
could serve as Cb(the salient entity) of the following utterance, i.e. Cb(Un+1).
The set of forward-looking centers Cf(Un)are ranked according to factors like
discourse salience and grammatical role (for example subjects are higher ranked
than objects, which are higher ranked than all other grammatical roles). We call the
highest-ranked forward-looking center Cp(for “preferred center”). Cpis a kind of
prediction about what entity will be talked about next. Sometimes the next utterance
indeed talks about this entity, but sometimes another entity becomes salient instead.
We’ll use here the algorithm for centering presented in Brennan et al. (1987),
which deﬁnes four intersentential relationships between a pair of utterances Unand
Un+1that depend on the relationship between Cb(Un+1),Cb(Un), and Cp(Un+1);
these are shown in Fig. 24.7.
Cb(Un+1) =Cb(Un) Cb(Un+1)6=Cb(Un)
or undeﬁned Cb(Un)
Cb(Un+1) =Cp(Un+1) Continue Smooth-Shift
Cb(Un+1)6=Cp(Un+1) Retain Rough-Shift
Figure 24.7 Centering Transitions for Rule 2 from Brennan et al. (1987).
The following rules are used by the algorithm:
Rule 1: If any element of Cf(Un)is realized by a pronoun in utterance
Un+1, then Cb(Un+1)must be realized as a pronoun also.
Rule 2 :Transition states are ordered. Continue is preferred to Retain is
preferred to Smooth-Shift is preferred to Rough-Shift.
Rule 1 captures the intuition that pronominalization (including zero-anaphora)
is a common way to mark discourse salience. If there are multiple pronouns in an
utterance realizing entities from the previous utterance, one of these pronouns must
realize the backward center Cb; if there is only one pronoun, it must be Cb.
Rule 2 captures the intuition that discourses that continue to center the same en-
tity are more coherent than ones that repeatedly shift to other centers. The transition
table is based on two factors: whether the backward-looking center Cbis the same
from UntoUn+1and whether this discourse entity is the one that is preferred ( Cp)
in the new utterance Un+1. If both of these hold, a CONTINUE relation, the speaker
has been talking about the same entity and is going to continue talking about that

--- Page 542 ---
542 CHAPTER 24 • D ISCOURSE COHERENCE
entity. In a RETAIN relation, the speaker intends to SHIFT to a new entity in a future
utterance and meanwhile places the current entity in a lower rank Cf. In a SHIFT
relation, the speaker is shifting to a new salient entity.
Let’s walk though the start of (24.28) again, repeated as (24.30), showing the
representations after each utterance is processed.
(24.30) John went to his favorite music store to buy a piano. ( U1)
He was excited that he could ﬁnally buy a piano. ( U2)
He arrived just as the store was closing for the day. ( U3)
It was closing just as John arrived ( U4)
Using the grammatical role hierarchy to order the C f, for sentence U1we get:
Cf(U1):fJohn, music store, piano g
Cp(U1): John
Cb(U1): undeﬁned
and then for sentence U2:
Cf(U2):fJohn, pianog
Cp(U2): John
Cb(U2): John
Result: Continue ( Cp(U2)=Cb(U2);Cb(U1)undeﬁned)
The transition from U1toU2is thus a CONTINUE . Completing this example is left
as exercise (1) for the reader
24.3.2 Entity Grid model
Centering embodies a particular theory of how entity mentioning leads to coher-
ence: that salient entities appear in subject position or are pronominalized, and that
discourses are salient by means of continuing to mention the same entity in such
ways.
The entity grid model of Barzilay and Lapata (2008) is an alternative way to entity grid
capture entity-based coherence: instead of having a top-down theory, the entity-grid
model using machine learning to induce the patterns of entity mentioning that make
a discourse more coherent.
The model is based around an entity grid , a two-dimensional array that repre-
sents the distribution of entity mentions across sentences. The rows represent sen-
tences, and the columns represent discourse entities (most versions of the entity grid
model focus just on nominal mentions). Each cell represents the possible appearance
of an entity in a sentence, and the values represent whether the entity appears and its
grammatical role. Grammatical roles are subject ( S), object ( O), neither ( X), or ab-
sent (–); in the implementation of Barzilay and Lapata (2008), subjects of passives
are represented with O, leading to a representation with some of the characteristics
of thematic roles.
Fig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in
Fig. 24.9. There is one row for each of the six sentences. The second column, for
the entity ‘trial’, is O– – – X, showing that the trial appears in the ﬁrst sentence as
direct object, in the last sentence as an oblique, and does not appear in the middle
sentences. The third column, for the entity Microsoft, shows that it appears as sub-
ject in sentence 1 (it also appears as the object of the preposition against , but entities
that appear multiple times are recorded with their highest-ranked grammatical func-
tion). Computing the entity grids requires extracting entities and doing coreference

--- Page 543 ---
24.3 • C ENTERING AND ENTITY -BASED COHERENCE 543
Computational Linguistics Volume 34, Number 1
these patterns can be encoded as feature vectors appropriate for performing coherence-
related ranking and classiﬁcation tasks.
3.1 The Entity-Grid Discourse Representation
Each text is represented by an entity grid ,at w o - d i m e n s i o n a la r r a yt h a tc a p t u r e s
the distribution of discourse entities across text sentences. We follow Miltsakaki and
Kukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., a
main clause with accompanying subordinate and adjunct clauses). The rows of the
grid correspond to sentences, and the columns correspond to discourse entities. By
discourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3
how coreferent entities are identiﬁed). For each occurrence of a discourse entity in the
text, the corresponding grid cell contains information about its presence or absence
in a sequence of sentences. In addition, for entities present in a given sentence, grid
cells contain information about their syntactic role. Such information can be expressed
in many ways (e.g., using constituent labels or thematic role information). Because
grammatical relations ﬁgure prominently in entity-based theories of local coherence (see
Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to
a string from a set of categories reﬂecting whether the entity in question is a subject ( S),
object ( O), or neither ( X). Entities absent from a sentence are signaled by gaps ( –).
Grammatical role information can be extracted from the output of a broad-coverage
dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical
parser (Collins 1997; Charniak 2000). We discuss how this information was computed
for our experiments in Section 3.3.
Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2.
Because the text contains six sentences, the grid columns are of length six. Consider
for instance the grid column for the entity trial ,[O–––– X].I tr e c o r d st h a t trial is
present in sentences 1 and 6 (as Oand X,r e s p e c t i v e l y )b u ti sa b s e n tf r o mt h er e s to ft h e
sentences. Also note that the grid in Table 1 takes coreference resolution into account.
Even though the same entity appears in different linguistic forms, for example, Microsoft
Corp. ,Microsoft ,a n d the company , it is mapped to a single entry in the grid (see the
column introduced by Microsoft in Table 1).
Table 1
A fragment of the entity grid. Noun phrases are represented by their head nouns. Grid cells
correspond to grammatical roles: subjects ( S), objects ( O), or neither ( X).Department
Trial
Microsoft
Evidence
Competitors
Markets
Products
Brands
Case
Netscape
Software
Tactics
Government
Suit
Earnings
1SO SXO –––––––––– 1
2–– O–– XSO –––– ––– 2
3–– SO –––– SOO ––– – 3
4–– S–––––––– S––– 4
5–––––––––––– SO –5
6–XS ––––– ––– – – – O6
6
Figure 24.8 Part of the entity grid for the text in Fig. 24.9. Entities are listed by their head
noun; each cell represents whether an entity appears as subject ( S), object ( O), neither ( X), or
is absent (–). Figure from Barzilay and Lapata (2008).
Barzilay and Lapata Modeling Local Coherence
Table 2
Summary augmented with syntactic annotations for grid computation.
1 [The Justice Department]Sis conducting an [anti-trust trial]Oagainst [Microsoft Corp.]X
with [evidence]Xthat [the company]Sis increasingly attempting to crush [competitors]O.
2[ M i c r o s o f t ]Ois accused of trying to forcefully buy into [markets]Xwhere [its own
products]Sare not competitive enough to unseat [established brands]O.
3[ T h e c a s e ]Srevolves around [evidence]Oof [Microsoft]Saggressively pressuring
[Netscape]Ointo merging [browser software]O.
4[ M i c r o s o f t ]Sclaims [its tactics]Sare commonplace and good economically.
5 [The government]Smay ﬁle [a civil suit]Oruling that [conspiracy]Sto curb [competition]O
through [collusion]Xis [a violation of the Sherman Act]O.
6[ M i c r o s o f t ]Scontinues to show [increased earnings]Odespite [the trial]X.
When a noun is attested more than once with a different grammatical role in the
same sentence, we default to the role with the highest grammatical ranking: subjects are
ranked higher than objects, which in turn are ranked higher than the rest. For example,
the entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x(for
Microsoft Corp. )a n d s(forthe company ), but is represented only by sin the grid (see
Tables 1 and 2).
3.2 Entity Grids as Feature Vectors
Af u n d a m e n t a la s s u m p t i o nu n d e r l y i n go u ra p p r o a c hi st h a tt h ed i s t r i b u t i o no fe n t i t i e s
in coherent texts exhibits certain regularities reﬂected in grid topology. Some of these
regularities are formalized in Centering Theory as constraints on transitions of the
local focus in adjacent sentences. Grids of coherent texts are likely to have some dense
columns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and many
sparse columns which will consist mostly of gaps (see markets andearnings in Table 1).
One would further expect that entities corresponding to dense columns are more often
subjects or objects. These characteristics will be less pronounced in low-coherence texts.
Inspired by Centering Theory, our analysis revolves around patterns of local entity
transitions. A local entity transition is a sequence {S,O,X,–}nthat represents entity
occurrences and their syntactic roles in nadjacent sentences. Local transitions can be
easily obtained from a grid as continuous subsequences of each column. Each transition
will have a certain probability in a given grid. For instance, the probability of the
transition [S–]in the grid from Table 1 is 0 .08 (computed as a ratio of its frequency
[i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text
can thus be viewed as a distribution deﬁned over transition types.
We can now go one step further and represent each text by a ﬁxed set of transition
sequences using a standard feature vector notation. Each grid rendering jof a document
dicorresponds to a feature vector Φ(xij)=(p1(xij),p2(xij),... ,pm(xij)), where mis the
number of all predeﬁned entity transitions, and pt(xij)t h ep r o b a b i l i t yo ft r a n s i t i o n t
in grid xij.T h i sf e a t u r ev e c t o rr e p r e s e n t a t i o ni su s e f u l l ya m e n a b l et om a c h i n el e a r n i n g
algorithms (see our experiments in Sections 4–6). Furthermore, it allows the consid-
eration of large numbers of transitions which could potentially uncover novel entity
distribution patterns relevant for coherence assessment or other coherence-related tasks.
Note that considerable latitude is available when specifying the transition types to
be included in a feature vector. These can be all transitions of a given length (e.g., two
or three) or the most frequent transitions within a document collection. An example of
7
Figure 24.9 A discourse with the entities marked and annotated with grammatical func-
tions. Figure from Barzilay and Lapata (2008).
resolution to cluster them into discourse entities (Chapter 23) as well as parsing the
sentences to get grammatical roles.
In the resulting grid, columns that are dense (like the column for Microsoft) in-
dicate entities that are mentioned often in the texts; sparse columns (like the column
for earnings) indicate entities that are mentioned rarely.
In the entity grid model, coherence is measured by patterns of local entity tran-
sition . For example, Department is a subject in sentence 1, and then not men-
tioned in sentence 2; this is the transition [ S–]. The transitions are thus sequences
fS,O X, –gnwhich can be extracted as continuous cells from each column. Each
transition has a probability; the probability of [ S–] in the grid from Fig. 24.8 is 0.08
(it occurs 6 times out of the 75 total transitions of length two). Fig. 24.10 shows the
distribution over transitions of length 2 for the text of Fig. 24.9 (shown as the ﬁrst
rowd1), and 2 other documents.
Computational Linguistics Volume 34, Number 1
af e a t u r es p a c ew i t ht r a n s i t i o n so fl e n g t ht w oi si l l u s t r a t e di nT a b l e3 .T h es e c o n dr o w
(introduced by d1)i st h ef e a t u r ev e c t o rr e p r e s e n t a t i o no ft h eg r i di nT a b l e1 .
3.3 Grid Construction: Linguistic Dimensions
One of the central research issues in developing entity-based models of coherence is
determining what sources of linguistic knowledge are essential for accurate prediction,
and how to encode them succinctly in a discourse representation. Previous approaches
tend to agree on the features of entity distribution related to local coherence—the
disagreement lies in the way these features are modeled.
Our study of alternative encodings is not a mere duplication of previous ef-
forts (Poesio et al. 2004) that focus on linguistic aspects of parameterization. Because we
are interested in an automatically constructed model, we have to take into account com-
putational and learning issues when considering alternative representations. Therefore,
our exploration of the parameter space is guided by three considerations: the linguistic
importance of a parameter, the accuracy of its automatic computation, and the size of the
resulting feature space. From the linguistic side, we focus on properties of entity distri-
bution that are tightly linked to local coherence, and at the same time allow for multiple
interpretations during the encoding process. Computational considerations prevent us
from considering discourse representations that cannot be computed reliably by exist-
ing tools. For instance, we could not experiment with the granularity of an utterance—
sentence versus clause—because available clause separators introduce substantial noise
into a grid construction. Finally, we exclude representations that will explode the size of
the feature space, thereby increasing the amount of data required for training the model.
Entity Ex traction. The accurate computation of entity classes is key to computing mean-
ingful entity grids. In previous implementations of entity-based models, classes of coref-
erent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis
et al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious
solution for identifying entity classes is to employ an automatic coreference resolution
tool that determines which noun phrases refer to the same entity in a document.
Current approaches recast coreference resolution as a classiﬁcation task. A pair
of NPs is classiﬁed as coreferring or not based on constraints that are learned from
an annotated corpus. A separate clustering mechanism then coordinates the possibly
contradictory pairwise classiﬁcations and constructs a partition on the set of NPs. In
our experiments, we employ Ng and Cardie’s (2002) coreference resolution system.
The system decides whether two NPs are coreferent by exploiting a wealth of lexical,
grammatical, semantic, and positional features. It is trained on the MUC (6–7) data sets
and yields state-of-the-art performance (70.4 F-measure on MUC-6 and 63.4 on MUC-7).
Table 3
Example of a feature-vector document representation using all transitions of length two given
syntactic categories S,O,X,a n d –.
SS SO SX S –OS OO OX O –XS XO XX X –– S–O–X––
d1.01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59
d2.02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36
d3.02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39
8
Figure 24.10 A feature vector for representing documents using all transitions of length 2.
Document d1is the text in Fig. 24.9. Figure from Barzilay and Lapata (2008).
The transitions and their probabilities can then be used as features for a machine
learning model. This model can be a text classiﬁer trained to produce human-labeled
coherence scores (for example from humans labeling each text as coherent or inco-
herent). But such data is expensive to gather. Barzilay and Lapata (2005) introduced
a simplifying innovation: coherence models can be trained by self-supervision :
trained to distinguish the natural original order of sentences in a discourse from

--- Page 544 ---
544 CHAPTER 24 • D ISCOURSE COHERENCE
a modiﬁed order (such as a randomized order). We turn to these evaluations in the
next section.
24.3.3 Evaluating Neural and Entity-based coherence
Entity-based coherence models, as well as the neural models we introduce in the
next section, are generally evaluated in one of two ways.
First, we can have humans rate the coherence of a document and train a classiﬁer
to predict these human ratings, which can be categorial (high/low, or high/mid/low)
or continuous. This is the best evaluation to use if we have some end task in mind,
like essay grading, where human raters are the correct deﬁnition of the ﬁnal label.
Alternatively, since it’s very expensive to get human labels, and we might not
yet have an end-task in mind, we can use natural texts to do self-supervision. In
self-supervision we pair up a natural discourse with a pseudo-document created by
changing the ordering. Since naturally-ordered discourses are more coherent than
random permutation (Lin et al., 2011), a successful coherence algorithm should pre-
fer the original ordering.
Self-supervision has been implemented in 3 ways. In the sentence order dis-
crimination task (Barzilay and Lapata, 2005), we compare a document to a random
permutation of its sentences. A model is considered correct for an (original, per-
muted) test pair if it ranks the original document higher. Given kdocuments, we can
compute npermutations, resulting in knpairs each with one original document and
one permutation, to use in training and testing.
In the sentence insertion task (Chen et al., 2007) we take a document, remove
one of the nsentences s, and create n 1 copies of the document with sinserted into
each position. The task is to decide which of the ndocuments is the one with the
original ordering, distinguishing the original position for sfrom all other positions.
Insertion is harder than discrimination since we are comparing documents that differ
by only one sentence.
Finally, in the sentence order reconstruction task (Lapata, 2003), we take a
document, randomize the sentences, and train the model to put them back in the
correct order. Again given kdocuments, we can compute npermutations, resulting
inknpairs each with one original document and one permutation, to use in training
and testing. Reordering is of course a much harder task than simple classiﬁcation.
24.4 Representation learning models for local coherence
The third kind of local coherence is topical or semantic ﬁeld coherence. Discourses
cohere by talking about the same topics and subtopics, and drawing on the same
semantic ﬁelds in doing so.
The ﬁeld was pioneered by a series of unsupervised models in the 1990s of this
kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976): lexical cohesion
the sharing of identical or semantically related words in nearby sentences. Morris
and Hirst (1991) computed lexical chains of words (like pine,bush trees ,trunk ) that
occurred through a discourse and that were related in Roget’s Thesaurus (by being in
the same category, or linked categories). They showed that the number and density
of chain correlated with the topic structure. The TextTiling algorithm of Hearst TextTiling
(1997) computed the cosine between neighboring text spans (the normalized dot
product of vectors of raw word counts), again showing that sentences or paragraph in

--- Page 545 ---
24.4 • R EPRESENTATION LEARNING MODELS FOR LOCAL COHERENCE 545
a subtopic have high cosine with each other, but not with sentences in a neighboring
subtopic.
A third early model, the LSA Coherence method of Foltz et al. (1998) was the
ﬁrst to use embeddings, modeling the coherence between two sentences as the co-
sine between their LSA sentence embedding vectors1, computing embeddings for a
sentence sby summing the embeddings of its words w:
sim(s;t) = cos(s;t)
=cos(X
w2sw;X
w2tw) (24.31)
and deﬁning the overall coherence of a text as the average similarity over all pairs of
adjacent sentences siandsi+1:
coherence (T) =1
n 1n 1X
i=1cos(si;si+1) (24.32)
Modern neural representation-learning coherence models, beginning with Li et al.
(2014), draw on the intuitions of these early unsupervised models for learning sen-
tence representations and measuring how they change between neighboring sen-
tences. But the new models also draw on the idea pioneered by Barzilay and Lapata
(2005) of self-supervision. That is, unlike say coherence relation models, which
train on hand-labeled representations for RST or PDTB, these models are trained to
distinguish natural discourses from unnatural discourses formed by scrambling the
order of sentences, thus using representation learning to discover the features that
matter for at least the ordering aspect of coherence.
Here we present one such model, the local coherence discriminator (LCD) (Xu
et al., 2019). Like early models, LCD computes the coherence of a text as the av-
erage of coherence scores between consecutive pairs of sentences. But unlike the
early unsupervised models, LCD is a self-supervised model trained to discriminate
consecutive sentence pairs (si;si+1)in the training documents (assumed to be coher-
ent) from (constructed) incoherent pairs (si;s0). All consecutive pairs are positive
examples, and the negative (incoherent) partner for a sentence siis another sentence
uniformly sampled from the same document as si.
Fig. 24.11 describes the architecture of the model fq, which takes a sentence
pair and returns a score, higher scores for more coherent pairs. Given an input
sentence pair sandt, the model computes sentence embeddings sandt(using any
sentence embeddings algorithm), and then concatenates four features of the pair: (1)
the concatenation of the two vectors (2) their difference s t; (3) the absolute value
of their difference js tj; (4) their element-wise product st. These are passed
through a one-layer feedforward network to output the coherence score.
The model is trained to make this coherence score higher for real pairs than for
negative pairs. More formally, the training objective for a corpus Cof documents d,
each of which consists of a list of sentences si, is:
Lq=X
d2CX
si2dE
p(s0jsi)[L(fq(si;si+1);fq(si;s0))] (24.33)
Ep(s0jsi)is the expectation with respect to the negative sampling distribution con-
ditioned on si: given a sentence sithe algorithms samples a negative sentence s0
1See Chapter 6 for more on LSA embeddings; they are computed by applying SVD to the term-
document matrix (each cell weighted by log frequency and normalized by entropy), and then the ﬁrst
300 dimensions are used as the embedding.

--- Page 546 ---
546 CHAPTER 24 • D ISCOURSE COHERENCE
681Loss function: The role of the loss function is
to encourage f+=f✓(si,si+1)to be high while
f =f✓(si,s0)to be low. Common losses such as
margin or log loss can all be used. Through exper-
imental validation, we found that margin loss to
be superior for this problem. Speciﬁcally, Ltakes
on the form: L(f+,f ) = max(0 ,⌘ f++f )
where ⌘is the margin hyperparameter.
Negative samples: Technically, we are free to
choose any sentence s0to form a negative pair
with si. However, because of potential differ-
ences in genre, topic and writing style, such neg-
atives might cause the discriminative model to
learn cues unrelated to coherence. Therefore, we
only select sentences from the same document to
construct negative pairs. Speciﬁcally, suppose si
comes from document dkwith length nk, then
p(s0|si)is a uniform distribution over the nk 1
sentences {sj}j6=ifrom dk. For a document with
nsentences, there are n 1positive pairs, and
(n 1)⇤(n 2)/2negative pairs. It turns out that
the quadratic number of negatives provides a rich
enough learning signal, while at the same time, is
not too prohibitively large to be effectively cov-
ered by a sampling procedure. In practice, we
sample a new set of negatives each time we see
a document, hence after many epochs, we can ef-
fectively cover the space for even very long doc-
uments. Section 5.7discusses further details on
sampling.
4.1 Model Architecture
The speciﬁc neural architecture that we use for f✓
is illustrated in Figure 1. We assume the use of
some pre-trained sentence encoder, which is dis-
cussed in the next section.
Given an input sentence pair, the sentence en-
coder maps the sentences to real-valued vectors S
andT. We then compute the concatenation of the
following features: (1)concatenation of the two
vectors (S, T);(2)element-wise difference S T;
(3)element-wise product S⇤T;(4)absolute value
of element-wise difference |S T|. The concate-
nated feature representation is then fed to a one-
layer MLP to output the coherence score.
In practice, we make our overall coherence
model bidirectional, by training a forward model
with input (S, T)and a backward model with in-
put(T,S)with the same architecture but separate
parameters. The coherence score is then the aver-
age from the two models.
Figure 1: Generic architecture for our proposed model.
4.2 Pre-trained Generative Model as the
Sentence Encoder
Our model can work with any pre-trained sen-
tence encoder, ranging from the most simplistic
average GloVe ( Pennington et al. ,2014 ) embed-
dings to more sophisticated supervised or unsu-
pervised pre-trained sentence encoders ( Conneau
et al. ,2017 ). As mentioned in the introduction,
since generative models can often be turned into
sentence encoder, generative coherence model can
be leveraged by our model to beneﬁt from the
advantages of both generative and discriminative
training, similar to ( Kiros et al. ,2015 ;Peters et al. ,
2018 ). After initialization, we freeze the genera-
tive model parameters to avoid overﬁtting.
In Section 5, we will experimentally show that
while we do beneﬁt from strong pre-trained en-
coders, the fact that our local discriminative model
improves over previous methods is independent of
the choice of sentence encoder.
5 Experiments
5.1 Evaluation Tasks
Following Nguyen and Joty (2017 ) and other pre-
vious work, we evaluate our models on the dis-
crimination and insertion tasks. Additionally, we
evaluate on the paragraph reconstruction task in
open-domain settings, in a similar manner to Li
and Jurafsky (2017 ).
In the discrimination task, a document is com-
pared to a random permutation of its sentences,
and the model is considered correct if it scores the
original document higher than the permuted one.
Twenty permutations are used in the test set in ac-
cordance with previous work.
Figure 24.11 The architecture of the LCD model of document coherence, showing the
computation of the score for a pair of sentences sandt. Figure from Xu et al. (2019).
uniformly over the other sentences in the same document. Lis a loss function that
takes two scores, one for a positive pair and one for a negative pair, with the goal of
encouraging f+=fq(si;si+1)to be high and f =fq(si;s0))to be low. Fig. 24.11
use the margin loss l(f+;f ) =max(0;h f++f )where his the margin hyper-
parameter.
Xu et al. (2019) also give a useful baseline algorithm that itself has quite high
performance in measuring perplexity: train an RNN language model on the data,
and compute the log likelihood of sentence siin two ways, once given the preceding
context (conditional log likelihood) and once with no context (marginal log likeli-
hood). The difference between these values tells us how much the preceding context
improved the predictability of si, a predictability measure of coherence.
Training models to predict longer contexts than just consecutive pairs of sen-
tences can result in even stronger discourse representations. For example a Trans-
former language model trained with a contrastive sentence objective to predict text
up to a distance of 2 sentences improves performance on various discourse coher-
ence tasks (Iter et al., 2020).
Language-model style models are generally evaluated by the methods of Sec-
tion 24.3.3, although they can also be evaluated on the RST and PDTB coherence
relation tasks.
24.5 Global Coherence
A discourse must also cohere globally rather than just at the level of pairs of sen-
tences. Consider stories, for example. The narrative structure of stories is one of
the oldest kinds of global coherence to be studied. In his inﬂuential Morphology of
the Folktale , Propp (1968) models the discourse structure of Russian folktales via
a kind of plot grammar. His model includes a set of character categories he called
dramatis personae , like Hero, Villain, Donor, or Helper, and a set of events he
called functions (like “Villain commits kidnapping”, “Donor tests Hero”, or “Hero
is pursued”) that have to occur in particular order, along with other components.
Propp shows that the plots of each of the fairy tales he studies can be represented as

--- Page 547 ---
24.5 • G LOBAL COHERENCE 547
a sequence of these functions, different tales choosing different subsets of functions,
but always in the same order. Indeed Lakoff (1972) showed that Propp’s model
amounted to a discourse grammar of stories, and in recent computational work Fin-
layson (2016) demonstrates that some of these Proppian functions could be induced
from corpora of folktale texts by detecting events that have similar actions across
stories. Bamman et al. (2013) showed that generalizations over dramatis personae
could be induced from movie plot summaries on Wikipedia. Their model induced
latent personae from features like the actions the character takes (e.g., Villains stran-
gle), the actions done to them (e.g., Villains are foiled and arrested) or the descriptive
words used of them (Villains are evil).
In this section we introduce two kinds of such global discourse structure that
have been widely studied computationally. The ﬁrst is the structure of arguments:
the way people attempt to convince each other in persuasive essays by offering
claims and supporting premises. The second is somewhat related: the structure of
scientiﬁc papers, and the way authors present their goals, results, and relationship to
prior work in their papers.
24.5.1 Argumentation Structure
The ﬁrst type of global discourse structure is the structure of arguments . Analyzing
people’s argumentation computationally is often called argumentation mining .argumentation
mining
The study of arguments dates back to Aristotle, who in his Rhetorics described
three components of a good argument: pathos (appealing to the emotions of the pathos
listener), ethos (appealing to the speaker’s personal character), and logos (the logical ethos
logos structure of the argument).
Most of the discourse structure studies of argumentation have focused on logos ,
particularly via building and training on annotated datasets of persuasive essays or
other arguments (Reed et al. 2008, Stab and Gurevych 2014a, Peldszus and Stede
2016, Habernal and Gurevych 2017, Musi et al. 2018). Such corpora, for exam-
ple, often include annotations of argumentative components like claims (the central claims
component of the argument that is controversial and needs support) and premises premises
(the reasons given by the author to persuade the reader by supporting or attacking
the claim or other premises), as well as the argumentative relations between themargumentative
relations
like SUPPORT and ATTACK .
Consider the following example of a persuasive essay from Stab and Gurevych
(2014b). The ﬁrst sentence (1) presents a claim (in bold). (2) and (3) present two
premises supporting the claim. (4) gives a premise supporting premise (3).
“(1) Museums and art galleries provide a better understanding
about arts than Internet. (2) In most museums and art galleries, de-
tailed descriptions in terms of the background, history and author are
provided. (3) Seeing an artwork online is not the same as watching it
with our own eyes, as (4) the picture online does not show the texture
or three-dimensional structure of the art, which is important to study.”
Thus this example has three argumentative relations: SUPPORT (2,1), SUPPORT (3,1)
and SUPPORT (4,3). Fig. 24.12 shows the structure of a much more complex argu-
ment.
While argumentation mining is clearly related to rhetorical structure and other
kinds of coherence relations, arguments tend to be much less local; often a persua-
sive essay will have only a single main claim, with premises spread throughout the
text, without the local coherence we see in coherence relations.

--- Page 548 ---
548 CHAPTER 24 • D ISCOURSE COHERENCE
Stab and Gurevych Parsing Argumentation Structures
cloning. This example illustrates that knowing argumentative relations is important for
separating several arguments in a paragraph. The example also shows that argument
components frequently exhibit preceding text units that are not relevant to the argument
but helpful for recognizing the argument component type. For example, preceding dis-
course connectors like “therefore”, “consequently”, or “thus” can signal a subsequent
claim. Discourse markers like “because”, “since”, or “furthermore” could indicate a
premise. Formally, these preceding tokens of an argument component starting at token
tiare deﬁned as the tokens ti m,...,ti 1that are not covered by another argument
component in the sentence s=t1,t2,...,tnwhere 1 inand i m 1. The third body
paragraph illustrates a contra argument and argumentative attack relations:
Admittedly, [ cloning could bemisused formilitary purposes] Claim 5. For example,
[:it:::::could:::be:::::used::to::::::::::manipulate:::::::human::::::genes::in::::::order::to::::::create::::::::obedient:::::::soldiers
::::with::::::::::::extraordinary:::::::abilities] Premise 9. However, because [::::moral::::and:::::::ethical::::::values:::are
::::::::::::internationally::::::shared] Premise 10,[:it:::is::::very::::::::unlikely::::that:::::::cloning::::will::be::::::::misused:::for
::::::militant:::::::::objectives] Premise 11.
The paragraph begins with Claim 5, which attacks the stance of the author. It is supported
byPremise 9in the second sentence. The third sentence includes two premises, both of
which defend the stance of the author. Premise 11is an attack of Claim 5, and Premise 10
supports Premise 11. The last paragraph (conclusion) restates the major claim and sum-
marizes the main aspects of the essay:
To sum up, although [ permitting cloning might bear some risks like misuse for
military purposes] Claim 6, I strongly believe that [ this technology is beneﬁcial to
humanity ]MajorClaim 2. It is likely that [ thistechnologybears some important cures which
will significantly improve lifeconditions] Claim 7.
The conclusion of the essay starts with an attacking claim followed by the restatement of
the major claim. The last sentence includes another claim that summarizes the most im-
portant points of the author’s argumentation. Figure 2 shows the entire argumentation
structure of the example essay.
Figure 2Argumentation structure of the example essay. Arrows indicate argumentative relations.Arrowheads denote argumentative support relations and circleheads attack relations. Dashedlines indicate relations that are encoded in the stance attributes of claims. “P” denotes premises.629
Figure 24.12 Argumentation structure of a persuasive essay. Arrows indicate argumentation relations, ei-
ther of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises. Figure from Stab and
Gurevych (2017).
Algorithms for detecting argumentation structure often include classiﬁers for
distinguishing claims, premises, or non-argumentation, together with relation clas-
siﬁers for deciding if two spans have the SUPPORT ,ATTACK , or neither relation
(Peldszus and Stede, 2013). While these are the main focus of much computational
work, there is also preliminary efforts on annotating and detecting richer semantic
relationships (Park and Cardie 2014, Hidey et al. 2017) such as detecting argumen-
tation schemes , larger-scale structures for argument like argument from example ,argumentation
schemes
orargument from cause to effect , orargument from consequences (Feng and
Hirst, 2011).
Another important line of research is studying how these argument structure (or
other features) are associated with the success or persuasiveness of an argument
(Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it
is Aristotle’s logos that is most related to discourse structure, Aristotle’s ethos and
pathos techniques are particularly relevant in the detection of mechanisms of this
sort of persuasion . For example scholars have investigated the linguistic realization persuasion
of features studied by social scientists like reciprocity (people return favors), social
proof (people follow others’ choices), authority (people are inﬂuenced by those
with power), and scarcity (people value things that are scarce), all of which can
be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown
(2017) showed that these features could be combined with argumentation structure
to predict who inﬂuences whom on social media, Althoff et al. (2014) found that
linguistic models of reciprocity and authority predicted success in online requests,
while the semisupervised model of Yang et al. (2019) detected mentions of scarcity,
commitment, and social identity to predict the success of peer-to-peer lending plat-
forms.
See Stede and Schneider (2018) for a comprehensive survey of argument mining.
24.5.2 The structure of scientiﬁc discourse
Scientiﬁc papers have a very speciﬁc global structure: somewhere in the course of
the paper the authors must indicate a scientiﬁc goal, develop a method for a solu-
tion, provide evidence for the solution, and compare to prior work. One popular

--- Page 549 ---
24.6 • S UMMARY 549
annotation scheme for modeling these rhetorical goals is the argumentative zon-
ingmodel of Teufel et al. (1999) and Teufel et al. (2009), which is informed by theargumentative
zoning
idea that each scientiﬁc paper tries to make a knowledge claim about a new piece
of knowledge being added to the repository of the ﬁeld (Myers, 1992). Sentences
in a scientiﬁc paper can be assigned one of 15 tags; Fig. 24.13 shows 7 (shortened)
examples of labeled sentences.
Category Description Example
AIM Statement of speciﬁc research goal, or
hypothesis of current paper“The aim of this process is to examine the role that
training plays in the tagging process”
OWNMETHOD New Knowledge claim, own work:
methods“In order for it to be useful for our purposes, the
following extensions must be made:”
OWNRESULTS Measurable/objective outcome of own
work“All the curves have a generally upward trend but
always lie far below backoff (51% error rate)”
USE Other work is used in own work “We use the framework for the allocation and
transfer of control of Whittaker....”
GAPWEAK Lack of solution in ﬁeld, problem with
other solutions“Here, we will produce experimental evidence
suggesting that this simple model leads to serious
overestimates”
SUPPORT Other work supports current work or is
supported by current work“Work similar to that described here has been car-
ried out by Merialdo (1994), with broadly similar
conclusions.”
ANTISUPPORT Clash with other’s results or theory; su-
periority of own work“This result challenges the claims of...”
Figure 24.13 Examples for 7 of the 15 labels from the Argumentative Zoning labelset (Teufel et al., 2009).
Teufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientiﬁc
articles from computational linguistics and chemistry, which can be used as supervi-
sion for training standard sentence-classiﬁcation architecture to assign the 15 labels.
24.6 Summary
In this chapter we introduced local and global models for discourse coherence .
• Discourses are not arbitrary collections of sentences; they must be coherent .
Among the factors that make a discourse coherent are coherence relations
between the sentences, entity-based coherence, and topical coherence.
• Various sets of coherence relations andrhetorical relations have been pro-
posed. The relations in Rhetorical Structure Theory ( RST ) hold between
spans of text and are structured into a tree. Because of this, shift-reduce
and other parsing algorithms are generally used to assign these structures.
The Penn Discourse Treebank ( PDTB ) labels only relations between pairs of
spans, and the labels are generally assigned by sequence models.
•Entity-based coherence captures the intuition that discourses are about an
entity, and continue mentioning the entity from sentence to sentence. Cen-
tering Theory is a family of models describing how salience is modeled for
discourse entities, and hence how coherence is achieved by virtue of keeping
the same discourse entities salient over the discourse. The entity grid model
gives a more bottom-up way to compute which entity realization transitions
lead to coherence.

--- Page 550 ---
550 CHAPTER 24 • D ISCOURSE COHERENCE
• Many different genres have different types of global coherence . Persuasive
essays have claims and premises that are extracted in the ﬁeld of argument
mining , scientiﬁc articles have structure related to aims, methods, results, and
comparisons.
Bibliographical and Historical Notes
Coherence relations arose from the independent development of a number of schol-
ars, including Hobbs (1979) idea that coherence relations play an inferential role for
the hearer, and the investigations by Mann and Thompson (1987) of the discourse
structure of large texts. Other approaches to coherence relations and their extrac-
tion include Segmented Discourse Representation Theory ( SDRT ) (Asher and Las- SDRT
carides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi
1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue
that coherence structure includes crossed bracketings, which make it impossible to
represent as a tree, and propose a graph representation instead. A compendium of
over 350 relations that have been proposed in the literature can be found in Hovy
(1990).
RST parsing was ﬁrst proposed by Marcu (1997), and early work was rule-based,
focused on discourse markers (Marcu, 2000a). The creation of the RST Discourse
TreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety
of machine learning algorithms, beginning with the shift-reduce parser of Marcu
(1999) that used decision trees to choose actions, and continuing with a wide variety
of machine learned parsing methods (Soricut and Marcu 2003, Sagae 2009, Hernault
et al. 2010, Feng and Hirst 2014, Surdeanu et al. 2015, Joty et al. 2015) and chunkers
(Sporleder and Lapata, 2005). Subba and Di Eugenio (2009) integrated sophisticated
semantic information into RST parsing. Ji and Eisenstein (2014) ﬁrst applied neural
models to RST parsing neural models, leading to the modern set of neural RST
models (Li et al. 2014, Li et al. 2016b, Braud et al. 2017, Yu et al. 2018, inter alia)
as well as neural segmenters (Wang et al. 2018b). and neural PDTB parsing models
(Ji and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017).
Barzilay and Lapata (2005) pioneered the idea of self-supervision for coher-
ence: training a coherence model to distinguish true orderings of sentences from
random permutations. Li et al. (2014) ﬁrst applied this paradigm to neural sentence-
representation, and many neural self-supervised models followed (Li and Jurafsky
2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al.
2020)
Another aspect of global coherence is the global topic structure of a text, the way
the topics shift over the course of the document. Barzilay and Lee (2004) introduced
an HMM model for capturing topics for coherence, and later work expanded this
intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li
and Jurafsky 2017).
The relationship between explicit and implicit discourse connectives has been
a fruitful one for research. Marcu and Echihabi (2002) ﬁrst proposed to use sen-
tences with explicit relations to help provide training data for implicit relations, by
removing the explicit relations and trying to re-predict them as a way of improv-
ing performance on implicit connectives; this idea was reﬁned by Sporleder and
Lascarides (2005), (Pitler et al., 2009), and Rutherford and Xue (2015). This rela-

--- Page 551 ---
BIBLIOGRAPHICAL AND HISTORICAL NOTES 551
tionship can also be used as a way to create discourse-aware representations. The
DisSent algorithm (Nie et al., 2019) creates the task of predicting explicit discourse
markers between two sentences. They show that representations learned to be good
at this task also function as powerful sentence representations for other discourse
tasks.
The idea of entity-based coherence seems to have arisen in multiple ﬁelds in the
mid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse
processing (Kintsch and Van Dijk, 1978), and in the roughly contemporaneous work
of Grosz, Sidner, Joshi, and their colleagues. Grosz (1977a) addressed the focus
of attention that conversational participants maintain as the discourse unfolds. She
deﬁned two levels of focus; entities relevant to the entire discourse were said to
be in global focus, whereas entities that are locally in focus (i.e., most central to
a particular utterance) were said to be in immediate focus. Sidner 1979; 1983 de-
scribed a method for tracking (immediate) discourse foci and their use in resolving
pronouns and demonstrative noun phrases. She made a distinction between the cur-
rent discourse focus and potential foci, which are the predecessors to the backward-
and forward-looking centers of Centering theory, respectively. The name and further
roots of the centering approach lie in papers by Joshi and Kuhn (1979) and Joshi and
Weinstein (1981), who addressed the relationship between immediate focus and the
inferences required to integrate the current utterance into the discourse model. Grosz
et al. (1983) integrated this work with the prior work of Sidner and Grosz. This led
to a manuscript on centering which, while widely circulated since 1986, remained
unpublished until Grosz et al. (1995). A collection of centering papers appears in
Walker et al. (1998b). See Karamanis et al. (2004) and Poesio et al. (2004) for a
deeper exploration of centering and its parameterizations, and the History section of
Chapter 23 for more on the use of centering on coreference.
The grid model of entity-based coherence was ﬁrst proposed by Barzilay and
Lapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then
extended by them Barzilay and Lapata (2008) and others with additional features
(Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that
projects entities into a global graph for the discourse (Guinaudeau and Strube 2013,
Mesgar and Strube 2016), and a convolutional model to capture longer-range entity
dependencies (Nguyen and Joty, 2017).
Theories of discourse coherence have also been used in algorithms for interpret-
ing discourse-level linguistic phenomena, including verb phrase ellipsis and gap-
ping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher
1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship
between coherence relations and discourse connectives can be found in Knott and
Dale (1994).
Useful surveys of discourse processing and structure include Stede (2011) and
Webber et al. (2012).
Andy Kehler wrote the Discourse chapter for the 2000 ﬁrst edition of this text-
book, which we used as the starting point for the second-edition chapter, and there
are some remnants of Andy’s lovely prose still in this third-edition coherence chap-
ter.

--- Page 552 ---
552 CHAPTER 24 • D ISCOURSE COHERENCE
Exercises
24.1 Finish the Centering Theory processing of the last two utterances of (24.30),
and show how (24.29) would be processed. Does the algorithm indeed mark
(24.29) as less coherent?
24.2 Select an editorial column from your favorite newspaper, and determine the
discourse structure for a 10–20 sentence portion. What problems did you
encounter? Were you helped by superﬁcial cues the speaker included (e.g.,
discourse connectives) in any places?

--- Page 553 ---
Bibliography
Abadi, M., A. Agarwal, P. Barham,
E. Brevdo, Z. Chen, C. Citro,
G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Good-
fellow, A. Harp, G. Irving, M. Is-
ard, Y . Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Man ´e,
R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar,
P. Tucker, V . Vanhoucke, V . Vasude-
van, F. Vi ´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke,
Y . Yu, and X. Zheng. 2015. Tensor-
Flow: Large-scale machine learning
on heterogeneous systems. Software
available from tensorﬂow.org.
Abney, S. P., R. E. Schapire, and
Y . Singer. 1999. Boosting ap-
plied to tagging and PP attachment.
EMNLP/VLC .
Agarwal, O., S. Subramanian,
A. Nenkova, and D. Roth. 2019.
Evaluation of named entity corefer-
ence. Workshop on Computational
Models of Reference, Anaphora and
Coreference .
Aggarwal, C. C. and C. Zhai. 2012.
A survey of text classiﬁcation al-
gorithms. In C. C. Aggarwal and
C. Zhai, eds, Mining text data , 163–
222. Springer.
Agichtein, E. and L. Gravano. 2000.
Snowball: Extracting relations from
large plain-text collections. Pro-
ceedings of the 5th ACM Interna-
tional Conference on Digital Li-
braries .
Agirre, E., C. Banea, C. Cardie, D. Cer,
M. Diab, A. Gonzalez-Agirre,
W. Guo, I. Lopez-Gazpio, M. Mar-
itxalar, R. Mihalcea, G. Rigau,
L. Uria, and J. Wiebe. 2015.
SemEval-2015 task 2: Semantic
textual similarity, English, Span-
ish and pilot on interpretability.
SemEval-15 .
Agirre, E., M. Diab, D. Cer,
and A. Gonzalez-Agirre. 2012.
SemEval-2012 task 6: A pilot on se-
mantic textual similarity. SemEval-
12.
Agirre, E. and D. Martinez. 2001.
Learning class-to-class selectional
preferences. CoNLL .
Aho, A. V . and J. D. Ullman. 1972. The
Theory of Parsing, Translation, and
Compiling , volume 1. Prentice Hall.
Algoet, P. H. and T. M. Cover. 1988.
A sandwich proof of the Shannon-
McMillan-Breiman theorem. The
Annals of Probability , 16(2):899–
909.Allen, J. 1984. Towards a general the-
ory of action and time. Artiﬁcial In-
telligence , 23(2):123–154.
Allen, J. and C. R. Perrault. 1980. An-
alyzing intention in utterances. Arti-
ﬁcial Intelligence , 15:143–178.
Allen, J., M. S. Hunnicut, and D. H.
Klatt. 1987. From Text to Speech:
The MITalk system . Cambridge Uni-
versity Press.
Althoff, T., C. Danescu-Niculescu-
Mizil, and D. Jurafsky. 2014. How
to ask for a favor: A case study
on the success of altruistic requests.
ICWSM 2014 .
An, J., H. Kwak, and Y .-Y . Ahn.
2018. SemAxis: A lightweight
framework to characterize domain-
speciﬁc word semantics beyond sen-
timent. ACL.
Anastasopoulos, A. and G. Neubig.
2020. Should all cross-lingual em-
beddings speak English? ACL.
Antoniak, M. and D. Mimno.
2018. Evaluating the stability of
embedding-based word similarities.
TACL , 6:107–119.
Aone, C. and S. W. Bennett. 1995. Eval-
uating automated and manual acqui-
sition of anaphora resolution strate-
gies. ACL.
Ariel, M. 2001. Accessibility the-
ory: An overview. In T. Sanders,
J. Schilperoord, and W. Spooren,
eds, Text Representation: Linguistic
and Psycholinguistic Aspects , 29–
87. Benjamins.
Arora, S., P. Lewis, A. Fan, J. Kahn, and
C. R ´e. 2023. Reasoning over pub-
lic and private data in retrieval-based
systems. TACL , 11:902–921.
Artetxe, M. and H. Schwenk. 2019.
Massively multilingual sentence em-
beddings for zero-shot cross-lingual
transfer and beyond. TACL , 7:597–
610.
Artstein, R., S. Gandhe, J. Gerten,
A. Leuski, and D. Traum. 2009.
Semi-formal evaluation of conver-
sational characters. In Languages:
From Formal to Natural , 22–35.
Springer.
Asher, N. 1993. Reference to Abstract
Objects in Discourse . Studies in Lin-
guistics and Philosophy (SLAP) 50,
Kluwer.
Asher, N. and A. Lascarides. 2003. Log-
ics of Conversation . Cambridge Uni-
versity Press.
Atal, B. S. and S. Hanauer. 1971.
Speech analysis and synthesis by
prediction of the speech wave. JASA ,
50:637–655.Austin, J. L. 1962. How to Do Things
with Words . Harvard University
Press.
Awadallah, A. H., R. G. Kulkarni,
U. Ozertem, and R. Jones. 2015.
Charaterizing and predicting voice
query reformulation. CIKM-15 .
Ba, J. L., J. R. Kiros, and G. E. Hinton.
2016. Layer normalization. NeurIPS
workshop .
Baayen, R. H. 2001. Word frequency
distributions . Springer.
Baccianella, S., A. Esuli, and F. Sebas-
tiani. 2010. Sentiwordnet 3.0: An
enhanced lexical resource for senti-
ment analysis and opinion mining.
LREC .
Bach, K. and R. Harnish. 1979. Linguis-
tic communication and speech acts .
MIT Press.
Backus, J. W. 1959. The syntax
and semantics of the proposed in-
ternational algebraic language of the
Zurich ACM-GAMM Conference.
Information Processing: Proceed-
ings of the International Conference
on Information Processing, Paris .
UNESCO.
Backus, J. W. 1996. Transcript of ques-
tion and answer session. In R. L.
Wexelblat, ed., History of Program-
ming Languages , page 162. Aca-
demic Press.
Bada, M., M. Eckert, D. Evans, K. Gar-
cia, K. Shipley, D. Sitnikov, W. A.
Baumgartner, K. B. Cohen, K. Ver-
spoor, J. A. Blake, and L. E. Hunter.
2012. Concept annotation in the
craft corpus. BMC bioinformatics ,
13(1):161.
Bagga, A. and B. Baldwin. 1998.
Algorithms for scoring coreference
chains. LREC Workshop on Linguis-
tic Coreference .
Bahdanau, D., K. H. Cho, and Y . Ben-
gio. 2015. Neural machine transla-
tion by jointly learning to align and
translate. ICLR 2015 .
Bahdanau, D., J. Chorowski,
D. Serdyuk, P. Brakel, and Y . Ben-
gio. 2016. End-to-end attention-
based large vocabulary speech
recognition. ICASSP .
Bahl, L. R. and R. L. Mercer. 1976.
Part of speech assignment by a sta-
tistical decision algorithm. Proceed-
ings IEEE International Symposium
on Information Theory .
Bahl, L. R., F. Jelinek, and R. L.
Mercer. 1983. A maximum likeli-
hood approach to continuous speech
recognition. IEEE Transactions on
Pattern Analysis and Machine Intel-
ligence , 5(2):179–190.
553

--- Page 554 ---
554 Bibliography
Bajaj, P., D. Campos, N. Craswell,
L. Deng, J. G. ando Xiaodong Liu,
R. Majumder, A. McNamara, B. Mi-
tra, T. Nguye, M. Rosenberg,
X. Song, A. Stoica, S. Tiwary, and
T. Wang. 2016. MS MARCO: A
human generated MAchine Reading
COmprehension dataset. NeurIPS .
Baker, C. F., C. J. Fillmore, and
J. B. Lowe. 1998. The Berkeley
FrameNet project. COLING/ACL .
Baker, J. K. 1975a. The DRAGON sys-
tem – An overview. IEEE Transac-
tions on ASSP , ASSP-23(1):24–29.
Baker, J. K. 1975b. Stochastic
modeling for automatic speech un-
derstanding. In D. R. Reddy,
ed.,Speech Recognition . Academic
Press.
Baldridge, J., N. Asher, and J. Hunter.
2007. Annotation for and robust
parsing of discourse structure on
unrestricted texts. Zeitschrift f ¨ur
Sprachwissenschaft , 26:213–239.
Bamman, D., O. Lewke, and A. Man-
soor. 2020. An annotated dataset
of coreference in English literature.
LREC .
Bamman, D., B. O’Connor, and N. A.
Smith. 2013. Learning latent per-
sonas of ﬁlm characters. ACL.
Bamman, D., S. Popat, and S. Shen.
2019. An annotated dataset of liter-
ary entities. NAACL HLT .
Banerjee, S. and A. Lavie. 2005. ME-
TEOR: An automatic metric for MT
evaluation with improved correla-
tion with human judgments. Pro-
ceedings of ACL Workshop on In-
trinsic and Extrinsic Evaluation
Measures for MT and/or Summa-
rization .
Banko, M., M. Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007.
Open information extraction for the
web. IJCAI .
Ba˜n´on, M., P. Chen, B. Haddow,
K. Heaﬁeld, H. Hoang, M. Espl `a-
Gomis, M. L. Forcada, A. Kamran,
F. Kirefu, P. Koehn, S. Ortiz Ro-
jas, L. Pla Sempere, G. Ram ´ırez-
S´anchez, E. Sarr ´ıas, M. Strelec,
B. Thompson, W. Waites, D. Wig-
gins, and J. Zaragoza. 2020.
ParaCrawl: Web-scale acquisition
of parallel corpora. ACL.
Bar-Hillel, Y . 1960. The present sta-
tus of automatic translation of lan-
guages. In F. Alt, ed., Advances
in Computers 1 , 91–163. Academic
Press.
Barker, C. 2010. Nominals don’t
provide criteria of identity. In
M. Rathert and A. Alexiadou, eds,
The Semantics of Nominalizations
across Languages and Frameworks ,
9–24. Mouton.Barrett, L. F., B. Mesquita, K. N.
Ochsner, and J. J. Gross. 2007. The
experience of emotion. Annual Re-
view of Psychology , 58:373–403.
Barzilay, R. and M. Lapata. 2005. Mod-
eling local coherence: An entity-
based approach. ACL.
Barzilay, R. and M. Lapata. 2008. Mod-
eling local coherence: An entity-
based approach. Computational Lin-
guistics , 34(1):1–34.
Barzilay, R. and L. Lee. 2004. Catching
the drift: Probabilistic content mod-
els, with applications to generation
and summarization. HLT-NAACL .
Baum, L. E. and J. A. Eagon. 1967. An
inequality with applications to sta-
tistical estimation for probabilistic
functions of Markov processes and
to a model for ecology. Bulletin of
the American Mathematical Society ,
73(3):360–363.
Baum, L. E. and T. Petrie. 1966. Statis-
tical inference for probabilistic func-
tions of ﬁnite-state Markov chains.
Annals of Mathematical Statistics ,
37(6):1554–1563.
Baum, L. F. 1900. The Wizard of Oz .
Available at Project Gutenberg.
Bayes, T. 1763. An Essay Toward Solv-
ing a Problem in the Doctrine of
Chances , volume 53. Reprinted in
Facsimiles of Two Papers by Bayes ,
Hafner Publishing, 1963.
Bazell, C. E. 1952/1966. The corre-
spondence fallacy in structural lin-
guistics. In E. P. Hamp, F. W.
Householder, and R. Austerlitz, eds,
Studies by Members of the English
Department, Istanbul University (3),
reprinted in Readings in Linguistics
II (1966) , 271–298. University of
Chicago Press.
Bean, D. and E. Riloff. 1999.
Corpus-based identiﬁcation of non-
anaphoric noun phrases. ACL.
Bean, D. and E. Riloff. 2004. Unsu-
pervised learning of contextual role
knowledge for coreference resolu-
tion. HLT-NAACL .
Bedi, G., F. Carrillo, G. A. Cecchi, D. F.
Slezak, M. Sigman, N. B. Mota,
S. Ribeiro, D. C. Javitt, M. Copelli,
and C. M. Corcoran. 2015. Auto-
mated analysis of free speech pre-
dicts psychosis onset in high-risk
youths. npj Schizophrenia , 1.
Bejˇcek, E., E. Haji ˇcov´a, J. Haji ˇc,
P. J ´ınov´a, V . Kettnerov ´a,
V . Kol ´aˇrov´a, M. Mikulov ´a,
J. M ´ırovsk ´y, A. Nedoluzhko,
J. Panevov ´a, L. Pol ´akov ´a,
M. ˇSevˇc´ıkov´a, J. ˇStˇep´anek, and
ˇS. Zik ´anov ´a. 2013. Prague de-
pendency treebank 3.0. Technical
report, Institute of Formal and Ap-
plied Linguistics, Charles Universityin Prague. LINDAT/CLARIN dig-
ital library at Institute of Formal
and Applied Linguistics, Charles
University in Prague.
Bellegarda, J. R. 1997. A latent se-
mantic analysis framework for large-
span language modeling. EU-
ROSPEECH .
Bellegarda, J. R. 2000. Exploiting la-
tent semantic information in statisti-
cal language modeling. Proceedings
of the IEEE , 89(8):1279–1296.
Bellegarda, J. R. 2013. Natural lan-
guage technology in mobile devices:
Two grounding frameworks. In
Mobile Speech and Advanced Nat-
ural Language Solutions , 185–196.
Springer.
Bellman, R. 1957. Dynamic Program-
ming . Princeton University Press.
Bellman, R. 1984. Eye of the Hurri-
cane: an autobiography . World Sci-
entiﬁc Singapore.
Bender, E. M. 2019. The #BenderRule:
On naming the languages we study
and why it matters. Blog post.
Bender, E. M., B. Friedman, and
A. McMillan-Major. 2021. A
guide for writing data statements
for natural language processing.
http://techpolicylab.uw.
edu/data-statements/ .
Bender, E. M. and A. Koller. 2020.
Climbing towards NLU: On mean-
ing, form, and understanding in the
age of data. ACL.
Bengio, Y ., A. Courville, and P. Vin-
cent. 2013. Representation learn-
ing: A review and new perspec-
tives. IEEE Transactions on Pattern
Analysis and Machine Intelligence ,
35(8):1798–1828.
Bengio, Y ., R. Ducharme, and P. Vin-
cent. 2000. A neural probabilistic
language model. NeurIPS .
Bengio, Y ., R. Ducharme, P. Vincent,
and C. Jauvin. 2003. A neural prob-
abilistic language model. JMLR ,
3:1137–1155.
Bengio, Y ., P. Lamblin, D. Popovici,
and H. Larochelle. 2007. Greedy
layer-wise training of deep net-
works. NeurIPS .
Bengio, Y ., H. Schwenk, J.-S. Sen ´ecal,
F. Morin, and J.-L. Gauvain. 2006.
Neural probabilistic language mod-
els. In Innovations in Machine
Learning , 137–186. Springer.
Bengtson, E. and D. Roth. 2008. Un-
derstanding the value of features for
coreference resolution. EMNLP .
Bentivogli, L., M. Cettolo, M. Federico,
and C. Federmann. 2018. Machine
translation human evaluation: an in-
vestigation of evaluation based on
post-editing and its relation with di-
rect assessment. ICSLT .

--- Page 555 ---
Bibliography 555
Berant, J., A. Chou, R. Frostig, and
P. Liang. 2013. Semantic parsing
on freebase from question-answer
pairs. EMNLP .
Berg-Kirkpatrick, T., D. Burkett, and
D. Klein. 2012. An empirical inves-
tigation of statistical signiﬁcance in
NLP. EMNLP .
Berger, A., S. A. Della Pietra, and V . J.
Della Pietra. 1996. A maximum en-
tropy approach to natural language
processing. Computational Linguis-
tics, 22(1):39–71.
Bergsma, S. and D. Lin. 2006. Boot-
strapping path-based pronoun reso-
lution. COLING/ACL .
Bergsma, S., D. Lin, and R. Goebel.
2008a. Discriminative learning of
selectional preference from unla-
beled text. EMNLP .
Bergsma, S., D. Lin, and R. Goebel.
2008b. Distributional identiﬁcation
of non-referential pronouns. ACL.
Bethard, S. 2013. ClearTK-TimeML:
A minimalist approach to TempEval
2013. SemEval-13 .
Bhat, I., R. A. Bhat, M. Shrivastava,
and D. Sharma. 2017. Joining
hands: Exploiting monolingual tree-
banks for parsing of code-mixing
data. EACL .
Bianchi, F., M. Suzgun, G. At-
tanasio, P. Rottger, D. Jurafsky,
T. Hashimoto, and J. Zou. 2024.
Safety-tuned LLaMAs: Lessons
from improving the safety of large
language models that follow instruc-
tions. ICLR .
Bickel, B. 2003. Referential density
in discourse and syntactic typology.
Language , 79(2):708–736.
Bickmore, T. W., H. Trinh, S. Olafsson,
T. K. O’Leary, R. Asadi, N. M. Rick-
les, and R. Cruz. 2018. Patient and
consumer safety risks when using
conversational assistants for medical
information: An observational study
of Siri, Alexa, and Google Assis-
tant. Journal of Medical Internet Re-
search , 20(9):e11510.
Bikel, D. M., S. Miller, R. Schwartz,
and R. Weischedel. 1997. Nymble:
A high-performance learning name-
ﬁnder. ANLP .
Biran, O. and K. McKeown. 2015.
PDTB discourse parsing as a tagging
task: The two taggers approach.
SIGDIAL .
Bird, S., E. Klein, and E. Loper. 2009.
Natural Language Processing with
Python . O’Reilly.
Bisani, M. and H. Ney. 2004. Boot-
strap estimates for conﬁdence inter-
vals in ASR performance evaluation.
ICASSP .
Bishop, C. M. 2006. Pattern recogni-
tion and machine learning . Springer.Bisk, Y ., A. Holtzman, J. Thomason,
J. Andreas, Y . Bengio, J. Chai,
M. Lapata, A. Lazaridou, J. May,
A. Nisnevich, N. Pinto, and
J. Turian. 2020. Experience grounds
language. EMNLP .
Bizer, C., J. Lehmann, G. Kobilarov,
S. Auer, C. Becker, R. Cyganiak,
and S. Hellmann. 2009. DBpedia—
A crystallization point for the Web
of Data. Web Semantics: science,
services and agents on the world
wide web , 7(3):154–165.
Bj¨orkelund, A. and J. Kuhn. 2014.
Learning structured perceptrons for
coreference resolution with latent
antecedents and non-local features.
ACL.
Black, A. W. and P. Taylor. 1994.
CHATR: A generic speech synthesis
system. COLING .
Black, E., S. P. Abney, D. Flickinger,
C. Gdaniec, R. Grishman, P. Har-
rison, D. Hindle, R. Ingria, F. Je-
linek, J. L. Klavans, M. Y . Liberman,
M. P. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. A
procedure for quantitatively compar-
ing the syntactic coverage of English
grammars. Speech and Natural Lan-
guage Workshop .
Blei, D. M., A. Y . Ng, and M. I. Jor-
dan. 2003. Latent Dirichlet alloca-
tion. JMLR , 3(5):993–1022.
Blodgett, S. L., S. Barocas,
H. Daum ´e III, and H. Wallach. 2020.
Language (technology) is power: A
critical survey of “bias” in NLP.
ACL.
Blodgett, S. L., L. Green, and
B. O’Connor. 2016. Demographic
dialectal variation in social media:
A case study of African-American
English. EMNLP .
Blodgett, S. L. and B. O’Connor. 2017.
Racial disparity in natural language
processing: A case study of so-
cial media African-American En-
glish. FAT/ML Workshop, KDD .
Bloomﬁeld, L. 1914. An Introduction to
the Study of Language . Henry Holt
and Company.
Bloomﬁeld, L. 1933. Language . Uni-
versity of Chicago Press.
Bobrow, D. G., R. M. Kaplan, M. Kay,
D. A. Norman, H. Thompson, and
T. Winograd. 1977. GUS, A frame
driven dialog system. Artiﬁcial In-
telligence , 8:155–173.
Bobrow, D. G. and D. A. Norman.
1975. Some principles of memory
schemata. In D. G. Bobrow and
A. Collins, eds, Representation and
Understanding . Academic Press.
Bojanowski, P., E. Grave, A. Joulin, and
T. Mikolov. 2017. Enriching word
vectors with subword information.
TACL , 5:135–146.Bollacker, K., C. Evans, P. Paritosh,
T. Sturge, and J. Taylor. 2008.
Freebase: a collaboratively created
graph database for structuring hu-
man knowledge. SIGMOD 2008 .
Bolukbasi, T., K.-W. Chang, J. Zou,
V . Saligrama, and A. T. Kalai. 2016.
Man is to computer programmer as
woman is to homemaker? Debiasing
word embeddings. NeurIPS .
Bommasani, R., D. A. Hudson,
E. Adeli, R. Altman, S. Arora,
S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brun-
skill, E. Brynjolfsson, S. Buch,
D. Card, R. Castellon, N. S. Chat-
terji, A. S. Chen, K. A. Creel,
J. Davis, D. Demszky, C. Don-
ahue, M. Doumbouya, E. Durmus,
S. Ermon, J. Etchemendy, K. Etha-
yarajh, L. Fei-Fei, C. Finn, T. Gale,
L. E. Gillespie, K. Goel, N. D.
Goodman, S. Grossman, N. Guha,
T. Hashimoto, P. Henderson, J. He-
witt, D. E. Ho, J. Hong, K. Hsu,
J. Huang, T. F. Icard, S. Jain, D. Ju-
rafsky, P. Kalluri, S. Karamcheti,
G. Keeling, F. Khani, O. Khat-
tab, P. W. Koh, M. S. Krass,
R. Krishna, R. Kuditipudi, A. Ku-
mar, F. Ladhak, M. Lee, T. Lee,
J. Leskovec, I. Levent, X. L. Li,
X. Li, T. Ma, A. Malik, C. D. Man-
ning, S. P. Mirchandani, E. Mitchell,
Z. Munyikwa, S. Nair, A. Narayan,
D. Narayanan, B. Newman, A. Nie,
J. C. Niebles, H. Nilforoshan, J. F.
Nyarko, G. Ogut, L. Orr, I. Papadim-
itriou, J. S. Park, C. Piech, E. Porte-
lance, C. Potts, A. Raghunathan,
R. Reich, H. Ren, F. Rong, Y . H.
Roohani, C. Ruiz, J. Ryan, C. R’e,
D. Sadigh, S. Sagawa, K. San-
thanam, A. Shih, K. P. Srinivasan,
A. Tamkin, R. Taori, A. W. Thomas,
F. Tram `er, R. E. Wang, W. Wang,
B. Wu, J. Wu, Y . Wu, S. M. Xie,
M. Yasunaga, J. You, M. A. Zaharia,
M. Zhang, T. Zhang, X. Zhang,
Y . Zhang, L. Zheng, K. Zhou, and
P. Liang. 2021. On the opportuni-
ties and risks of foundation models.
ArXiv .
Booth, T. L. 1969. Probabilistic
representation of formal languages.
IEEE Conference Record of the 1969
Tenth Annual Symposium on Switch-
ing and Automata Theory .
Borges, J. L. 1964. The analytical lan-
guage of john wilkins. In Other
inquisitions 1937–1952 . University
of Texas Press. Trans. Ruth L. C.
Simms.
Bostrom, K. and G. Durrett. 2020. Byte
pair encoding is suboptimal for lan-
guage model pretraining. EMNLP .
Bourlard, H. and N. Morgan. 1994.
Connectionist Speech Recognition:
A Hybrid Approach . Kluwer.

--- Page 556 ---
556 Bibliography
Brants, T. 2000. TnT: A statistical part-
of-speech tagger. ANLP .
Brants, T., A. C. Popat, P. Xu, F. J.
Och, and J. Dean. 2007. Large lan-
guage models in machine transla-
tion. EMNLP/CoNLL .
Braud, C., M. Coavoux, and
A. Søgaard. 2017. Cross-lingual
RST discourse parsing. EACL .
Br´eal, M. 1897. Essai de S ´emantique:
Science des signiﬁcations . Hachette.
Brennan, S. E., M. W. Friedman, and
C. Pollard. 1987. A centering ap-
proach to pronouns. ACL.
Brin, S. 1998. Extracting patterns and
relations from the World Wide Web.
Proceedings World Wide Web and
Databases International Workshop,
Number 1590 in LNCS . Springer.
Brockmann, C. and M. Lapata. 2003.
Evaluating and combining ap-
proaches to selectional preference
acquisition. EACL .
Broschart, J. 1997. Why Tongan does
it differently. Linguistic Typology ,
1:123–165.
Brown, P. F., J. Cocke, S. A.
Della Pietra, V . J. Della Pietra, F. Je-
linek, J. D. Lafferty, R. L. Mercer,
and P. S. Roossin. 1990. A statis-
tical approach to machine transla-
tion. Computational Linguistics ,
16(2):79–85.
Brown, P. F., S. A. Della Pietra, V . J.
Della Pietra, and R. L. Mercer. 1993.
The mathematics of statistical ma-
chine translation: Parameter esti-
mation. Computational Linguistics ,
19(2):263–311.
Brown, T., B. Mann, N. Ryder,
M. Subbiah, J. Kaplan, P. Dhari-
wal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agar-
wal, A. Herbert-V oss, G. Krueger,
T. Henighan, R. Child, A. Ramesh,
D. M. Ziegler, J. Wu, C. Win-
ter, C. Hesse, M. Chen, E. Sigler,
M. Litwin, S. Gray, B. Chess,
J. Clark, C. Berner, S. McCan-
dlish, A. Radford, I. Sutskever, and
D. Amodei. 2020. Language mod-
els are few-shot learners. NeurIPS ,
volume 33.
Bruce, B. C. 1975. Generation as a so-
cial action. Proceedings of TINLAP-
1 (Theoretical Issues in Natural
Language Processing) .
Brysbaert, M., A. B. Warriner, and
V . Kuperman. 2014. Concrete-
ness ratings for 40 thousand gen-
erally known English word lem-
mas. Behavior Research Methods ,
46(3):904–911.
Bu, H., J. Du, X. Na, B. Wu, and
H. Zheng. 2017. AISHELL-1: An
open-source Mandarin speech cor-
pus and a speech recognition base-
line. O-COCOSDA Proceedings .Buchholz, S. and E. Marsi. 2006. Conll-
x shared task on multilingual depen-
dency parsing. CoNLL .
Budanitsky, A. and G. Hirst. 2006.
Evaluating WordNet-based mea-
sures of lexical semantic related-
ness. Computational Linguistics ,
32(1):13–47.
Budzianowski, P., T.-H. Wen, B.-
H. Tseng, I. Casanueva, S. Ultes,
O. Ramadan, and M. Ga ˇsi´c. 2018.
MultiWOZ - a large-scale multi-
domain wizard-of-Oz dataset for
task-oriented dialogue modelling.
EMNLP .
Bullinaria, J. A. and J. P. Levy. 2007.
Extracting semantic representations
from word co-occurrence statistics:
A computational study. Behavior re-
search methods , 39(3):510–526.
Bullinaria, J. A. and J. P. Levy.
2012. Extracting semantic repre-
sentations from word co-occurrence
statistics: stop-lists, stemming, and
SVD. Behavior research methods ,
44(3):890–907.
Bulyko, I., K. Kirchhoff, M. Osten-
dorf, and J. Goldberg. 2005. Error-
sensitive response generation in a
spoken language dialogue system.
Speech Communication , 45(3):271–
288.
Caliskan, A., J. J. Bryson, and
A. Narayanan. 2017. Semantics de-
rived automatically from language
corpora contain human-like biases.
Science , 356(6334):183–186.
Callison-Burch, C., M. Osborne, and
P. Koehn. 2006. Re-evaluating the
role of BLEU in machine translation
research. EACL .
Canavan, A., D. Graff, and G. Zip-
perlen. 1997. CALLHOME Ameri-
can English speech LDC97S42. Lin-
guistic Data Consortium.
Carbonell, J. R. 1970. AI in
CAI: An artiﬁcial-intelligence ap-
proach to computer-assisted instruc-
tion. IEEE transactions on man-
machine systems , 11(4):190–202.
Cardie, C. 1993. A case-based approach
to knowledge acquisition for domain
speciﬁc sentence analysis. AAAI .
Cardie, C. 1994. Domain-Speciﬁc
Knowledge Acquisition for Concep-
tual Sentence Analysis . Ph.D. the-
sis, University of Massachusetts,
Amherst, MA. Available as CMP-
SCI Technical Report 94-74.
Cardie, C. and K. Wagstaff. 1999.
Noun phrase coreference as cluster-
ing. EMNLP/VLC .
Carlini, N., F. Tramer, E. Wal-
lace, M. Jagielski, A. Herbert-V oss,
K. Lee, A. Roberts, T. Brown,
D. Song, U. Erlingsson, et al. 2021.
Extracting training data from largelanguage models. 30th USENIX Se-
curity Symposium (USENIX Security
21).
Carlson, G. N. 1977. Reference to kinds
in English . Ph.D. thesis, Univer-
sity of Massachusetts, Amherst. For-
ward.
Carlson, L. and D. Marcu. 2001. Dis-
course tagging manual. Technical
Report ISI-TR-545, ISI.
Carlson, L., D. Marcu, and M. E.
Okurowski. 2001. Building a
discourse-tagged corpus in the
framework of rhetorical structure
theory. SIGDIAL .
Carreras, X. and L. M `arquez. 2005.
Introduction to the CoNLL-2005
shared task: Semantic role labeling.
CoNLL .
Chafe, W. L. 1976. Givenness, con-
trastiveness, deﬁniteness, subjects,
topics, and point of view. In C. N. Li,
ed.,Subject and Topic , 25–55. Aca-
demic Press.
Chambers, N. 2013. NavyTime: Event
and time ordering from raw text.
SemEval-13 .
Chambers, N., T. Cassidy, B. McDow-
ell, and S. Bethard. 2014. Dense
event ordering with a multi-pass ar-
chitecture. TACL , 2:273–284.
Chambers, N. and D. Jurafsky. 2010.
Improving the use of pseudo-words
for evaluating selectional prefer-
ences. ACL.
Chambers, N. and D. Jurafsky. 2011.
Template-based information extrac-
tion without the templates. ACL.
Chan, W., N. Jaitly, Q. Le, and
O. Vinyals. 2016. Listen, at-
tend and spell: A neural network
for large vocabulary conversational
speech recognition. ICASSP .
Chandioux, J. 1976. M ´ET´EO: un
syst`eme op ´erationnel pour la tra-
duction automatique des bulletins
m´et´eorologiques destin ´es au grand
public. Meta , 21:127–133.
Chang, A. X. and C. D. Manning. 2012.
SUTime: A library for recognizing
and normalizing time expressions.
LREC .
Chang, K.-W., R. Samdani, and
D. Roth. 2013. A constrained la-
tent variable model for coreference
resolution. EMNLP .
Chang, K.-W., R. Samdani, A. Ro-
zovskaya, M. Sammons, and
D. Roth. 2012. Illinois-Coref:
The UI system in the CoNLL-2012
shared task. CoNLL .
Chaplot, D. S. and R. Salakhutdinov.
2018. Knowledge-based word sense
disambiguation using topic models.
AAAI .

--- Page 557 ---
Bibliography 557
Charniak, E. 1997. Statistical pars-
ing with a context-free grammar and
word statistics. AAAI .
Charniak, E., C. Hendrickson, N. Ja-
cobson, and M. Perkowitz. 1993.
Equations for part-of-speech tag-
ging. AAAI .
Che, W., Z. Li, Y . Li, Y . Guo, B. Qin,
and T. Liu. 2009. Multilingual
dependency-based syntactic and se-
mantic parsing. CoNLL .
Chen, C. and V . Ng. 2013. Linguis-
tically aware coreference evaluation
metrics. IJCNLP .
Chen, D., A. Fisch, J. Weston, and
A. Bordes. 2017a. Reading Wiki-
pedia to answer open-domain ques-
tions. ACL.
Chen, D. and C. Manning. 2014. A fast
and accurate dependency parser us-
ing neural networks. EMNLP .
Chen, E., B. Snyder, and R. Barzi-
lay. 2007. Incremental text structur-
ing with online hierarchical ranking.
EMNLP/CoNLL .
Chen, S. F. and J. Goodman. 1999.
An empirical study of smoothing
techniques for language modeling.
Computer Speech and Language ,
13:359–394.
Chen, X., Z. Shi, X. Qiu, and X. Huang.
2017b. Adversarial multi-criteria
learning for Chinese word segmen-
tation. ACL.
Cheng, J., L. Dong, and M. La-
pata. 2016. Long short-term
memory-networks for machine read-
ing. EMNLP .
Cheng, M., E. Durmus, and D. Juraf-
sky. 2023. Marked personas: Using
natural language prompts to mea-
sure stereotypes in language models.
ACL.
Chiang, D. 2005. A hierarchical phrase-
based model for statistical machine
translation. ACL.
Chinchor, N., L. Hirschman, and D. L.
Lewis. 1993. Evaluating Message
Understanding systems: An analy-
sis of the third Message Understand-
ing Conference. Computational Lin-
guistics , 19(3):409–449.
Chiticariu, L., M. Danilevsky, Y . Li,
F. Reiss, and H. Zhu. 2018. Sys-
temT: Declarative text understand-
ing for enterprise. NAACL HLT , vol-
ume 3.
Chiticariu, L., Y . Li, and F. R. Reiss.
2013. Rule-Based Information Ex-
traction is Dead! Long Live Rule-
Based Information Extraction Sys-
tems! EMNLP .
Chiu, J. P. C. and E. Nichols. 2016.
Named entity recognition with bidi-
rectional LSTM-CNNs. TACL ,
4:357–370.Cho, K., B. van Merri ¨enboer, C. Gul-
cehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y . Bengio. 2014.
Learning phrase representations us-
ing RNN encoder–decoder for statis-
tical machine translation. EMNLP .
Choe, D. K. and E. Charniak. 2016.
Parsing as language modeling.
EMNLP .
Choi, J. D. and M. Palmer. 2011a. Get-
ting the most out of transition-based
dependency parsing. ACL.
Choi, J. D. and M. Palmer. 2011b.
Transition-based semantic role la-
beling using predicate argument
clustering. Proceedings of the ACL
2011 Workshop on Relational Mod-
els of Semantics .
Choi, J. D., J. Tetreault, and A. Stent.
2015. It depends: Dependency
parser comparison using a web-
based evaluation tool. ACL.
Chomsky, N. 1956. Three models for
the description of language. IRE
Transactions on Information The-
ory, 2(3):113–124.
Chomsky, N. 1956/1975. The Logi-
cal Structure of Linguistic Theory .
Plenum.
Chomsky, N. 1957. Syntactic Struc-
tures . Mouton.
Chomsky, N. 1963. Formal proper-
ties of grammars. In R. D. Luce,
R. Bush, and E. Galanter, eds, Hand-
book of Mathematical Psychology ,
volume 2, 323–418. Wiley.
Chomsky, N. 1981. Lectures on Gov-
ernment and Binding . Foris.
Chorowski, J., D. Bahdanau, K. Cho,
and Y . Bengio. 2014. End-to-end
continuous speech recognition using
attention-based recurrent NN: First
results. NeurIPS Deep Learning and
Representation Learning Workshop .
Chou, W., C.-H. Lee, and B. H. Juang.
1993. Minimum error rate train-
ing based on n-best string models.
ICASSP .
Christiano, P. F., J. Leike, T. Brown,
M. Martic, S. Legg, and D. Amodei.
2017. Deep reinforcement learning
from human preferences. NeurIPS ,
volume 30.
Christodoulopoulos, C., S. Goldwa-
ter, and M. Steedman. 2010. Two
decades of unsupervised POS in-
duction: How far have we come?
EMNLP .
Chu, Y .-J. and T.-H. Liu. 1965. On the
shortest arborescence of a directed
graph. Science Sinica , 14:1396–
1400.
Chu-Carroll, J. 1998. A statistical
model for discourse act recognition
in dialogue interactions. ApplyingMachine Learning to Discourse Pro-
cessing. Papers from the 1998 AAAI
Spring Symposium. Tech. rep. SS-
98-01. AAAI Press.
Chu-Carroll, J. and S. Carberry. 1998.
Collaborative response generation in
planning dialogues. Computational
Linguistics , 24(3):355–400.
Church, K. W. 1988. A stochastic parts
program and noun phrase parser for
unrestricted text. ANLP .
Church, K. W. 1989. A stochastic parts
program and noun phrase parser for
unrestricted text. ICASSP .
Church, K. W. 1994. Unix for Poets.
Slides from 2nd ELSNET Summer
School and unpublished paper ms.
Church, K. W. and W. A. Gale. 1991. A
comparison of the enhanced Good-
Turing and deleted estimation meth-
ods for estimating probabilities of
English bigrams. Computer Speech
and Language , 5:19–54.
Church, K. W. and P. Hanks. 1989.
Word association norms, mutual in-
formation, and lexicography. ACL.
Church, K. W. and P. Hanks. 1990.
Word association norms, mutual in-
formation, and lexicography. Com-
putational Linguistics , 16(1):22–29.
Cialdini, R. B. 1984. Inﬂuence: The
psychology of persuasion . Morrow.
Cieri, C., D. Miller, and K. Walker.
2004. The Fisher corpus: A resource
for the next generations of speech-
to-text. LREC .
Clark, E. 1987. The principle of con-
trast: A constraint on language ac-
quisition. In B. MacWhinney, ed.,
Mechanisms of language acquisi-
tion, 1–33. LEA.
Clark, H. H. 1996. Using Language .
Cambridge University Press.
Clark, H. H. and J. E. Fox Tree. 2002.
Using uh and um in spontaneous
speaking. Cognition , 84:73–111.
Clark, H. H. and C. Marshall. 1981.
Deﬁnite reference and mutual
knowledge. In A. K. Joshi, B. L.
Webber, and I. A. Sag, eds, Ele-
ments of Discourse Understanding ,
10–63. Cambridge.
Clark, H. H. and D. Wilkes-Gibbs.
1986. Referring as a collaborative
process. Cognition , 22:1–39.
Clark, J. H., E. Choi, M. Collins,
D. Garrette, T. Kwiatkowski,
V . Nikolaev, and J. Palomaki.
2020a. TyDi QA: A benchmark
for information-seeking question
answering in typologically diverse
languages. TACL , 8:454–470.
Clark, K., M.-T. Luong, Q. V . Le, and
C. D. Manning. 2020b. Electra: Pre-
training text encoders as discrimina-
tors rather than generators. ICLR .

--- Page 558 ---
558 Bibliography
Clark, K. and C. D. Manning. 2015.
Entity-centric coreference resolution
with model stacking. ACL.
Clark, K. and C. D. Manning. 2016a.
Deep reinforcement learning for
mention-ranking coreference mod-
els.EMNLP .
Clark, K. and C. D. Manning. 2016b.
Improving coreference resolution by
learning entity-level distributed rep-
resentations. ACL.
Clark, S., J. R. Curran, and M. Osborne.
2003. Bootstrapping POS-taggers
using unlabelled data. CoNLL .
Cobbe, K., V . Kosaraju, M. Bavar-
ian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, and J. Schul-
man. 2021. Training veriﬁers to
solve math word problems. ArXiv
preprint.
Coccaro, N. and D. Jurafsky. 1998. To-
wards better integration of seman-
tic predictors in statistical language
modeling. ICSLP .
Coenen, A., E. Reif, A. Yuan, B. Kim,
A. Pearce, F. Vi ´egas, and M. Watten-
berg. 2019. Visualizing and measur-
ing the geometry of bert. NeurIPS .
Cohen, A. D., A. Roberts, A. Molina,
A. Butryna, A. Jin, A. Kulshreshtha,
B. Hutchinson, B. Zevenbergen,
B. H. Aguera-Arcas, C. ching
Chang, C. Cui, C. Du, D. D. F.
Adiwardana, D. Chen, D. D. Lep-
ikhin, E. H. Chi, E. Hoffman-John,
H.-T. Cheng, H. Lee, I. Krivokon,
J. Qin, J. Hall, J. Fenton, J. Soraker,
K. Meier-Hellstern, K. Olson, L. M.
Aroyo, M. P. Bosma, M. J. Pickett,
M. A. Menegali, M. Croak, M. D ´ıaz,
M. Lamm, M. Krikun, M. R. Mor-
ris, N. Shazeer, Q. V . Le, R. Bern-
stein, R. Rajakumar, R. Kurzweil,
R. Thoppilan, S. Zheng, T. Bos,
T. Duke, T. Doshi, V . Y . Zhao,
V . Prabhakaran, W. Rusch, Y . Li,
Y . Huang, Y . Zhou, Y . Xu, and
Z. Chen. 2022. Lamda: Lan-
guage models for dialog applica-
tions. ArXiv preprint.
Cohen, M. H., J. P. Giangola, and
J. Balogh. 2004. Voice User Inter-
face Design . Addison-Wesley.
Cohen, P. R. and C. R. Perrault. 1979.
Elements of a plan-based theory of
speech acts. Cognitive Science ,
3(3):177–212.
Colby, K. M., S. Weber, and F. D. Hilf.
1971. Artiﬁcial paranoia. Artiﬁcial
Intelligence , 2(1):1–25.
Cole, R. A., D. G. Novick, P. J. E. Ver-
meulen, S. Sutton, M. Fanty, L. F. A.
Wessels, J. H. de Villiers, J. Schalk-
wyk, B. Hansen, and D. Burnett.
1997. Experiments with a spo-
ken dialogue system for taking theUS census. Speech Communication ,
23:243–260.
Collins, M. 1999. Head-Driven Statis-
tical Models for Natural Language
Parsing . Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Collobert, R. and J. Weston. 2007. Fast
semantic extraction using a novel
neural network architecture. ACL.
Collobert, R. and J. Weston. 2008.
A uniﬁed architecture for natural
language processing: Deep neural
networks with multitask learning.
ICML .
Collobert, R., J. Weston, L. Bottou,
M. Karlen, K. Kavukcuoglu, and
P. Kuksa. 2011. Natural language
processing (almost) from scratch.
JMLR , 12:2493–2537.
Comrie, B. 1989. Language Universals
and Linguistic Typology , 2nd edi-
tion. Blackwell.
Conneau, A., K. Khandelwal,
N. Goyal, V . Chaudhary, G. Wen-
zek, F. Guzm ´an, E. Grave, M. Ott,
L. Zettlemoyer, and V . Stoyanov.
2020. Unsupervised cross-lingual
representation learning at scale.
ACL.
Connolly, D., J. D. Burger, and D. S.
Day. 1994. A machine learning ap-
proach to anaphoric reference. Pro-
ceedings of the International Con-
ference on New Methods in Lan-
guage Processing (NeMLaP) .
Cooley, J. W. and J. W. Tukey. 1965.
An algorithm for the machine cal-
culation of complex Fourier se-
ries. Mathematics of Computation ,
19(90):297–301.
Cooper, F. S., A. M. Liberman, and
J. M. Borst. 1951. The interconver-
sion of audible and visible patterns
as a basis for research in the per-
ception of speech. Proceedings of
the National Academy of Sciences ,
37(5):318–325.
Cordier, B. 1965. Factor-analysis of
correspondences. COLING 1965 .
Costa-juss `a, M. R., J. Cross, O. C ¸ elebi,
M. Elbayad, K. Heaﬁeld, K. Hef-
fernan, E. Kalbassi, J. Lam,
D. Licht, J. Maillard, A. Sun,
S. Wang, G. Wenzek, A. Young-
blood, B. Akula, L. Barrault,
G. M. Gonzalez, P. Hansanti,
J. Hoffman, S. Jarrett, K. R.
Sadagopan, D. Rowe, S. Spruit,
C. Tran, P. Andrews, N. F. Ayan,
S. Bhosale, S. Edunov, A. Fan,
C. Gao, V . Goswami, F. Guzm ´an,
P. Koehn, A. Mourachko, C. Ropers,
S. Saleem, H. Schwenk, J. Wang,
and NLLB Team. 2022. No lan-
guage left behind: Scaling human-
centered machine translation. ArXiv.Cover, T. M. and J. A. Thomas. 1991.
Elements of Information Theory .
Wiley.
Covington, M. 2001. A fundamen-
tal algorithm for dependency pars-
ing. Proceedings of the 39th Annual
ACM Southeast Conference .
Cox, D. 1969. Analysis of Binary Data .
Chapman and Hall, London.
Craven, M. and J. Kumlien. 1999.
Constructing biological knowledge
bases by extracting information
from text sources. ISMB-99 .
Crawford, K. 2017. The trouble with
bias. Keynote at NeurIPS.
Croft, W. 1990. Typology and Univer-
sals. Cambridge University Press.
Crosbie, J. and E. Shutova. 2022. In-
duction heads as an essential mech-
anism for pattern matching in in-
context learning. ArXiv preprint.
Cross, J. and L. Huang. 2016. Span-
based constituency parsing with a
structure-label system and provably
optimal dynamic oracles. EMNLP .
Cruse, D. A. 2004. Meaning in Lan-
guage: an Introduction to Semantics
and Pragmatics . Oxford University
Press. Second edition.
Cucerzan, S. 2007. Large-scale
named entity disambiguation based
on Wikipedia data. EMNLP/CoNLL .
Dagan, I., S. Marcus, and
S. Markovitch. 1993. Contextual
word similarity and estimation from
sparse data. ACL.
Dahl, G. E., T. N. Sainath, and G. E.
Hinton. 2013. Improving deep
neural networks for LVCSR using
rectiﬁed linear units and dropout.
ICASSP .
Dahl, G. E., D. Yu, L. Deng, and
A. Acero. 2012. Context-dependent
pre-trained deep neural networks
for large-vocabulary speech recog-
nition. IEEE Transactions on au-
dio, speech, and language process-
ing, 20(1):30–42.
Dahl, M., V . Magesh, M. Suzgun, and
D. E. Ho. 2024. Large legal ﬁc-
tions: Proﬁling legal hallucinations
in large language models. Journal of
Legal Analysis , 16:64–93.
Dai, A. M. and Q. V . Le. 2015.
Semi-supervised sequence learning.
NeurIPS .
Danieli, M. and E. Gerbino. 1995. Met-
rics for evaluating dialogue strate-
gies in a spoken language system.
AAAI Spring Symposium on Empir-
ical Methods in Discourse Interpre-
tation and Generation .
Das, S. R. and M. Y . Chen. 2001. Ya-
hoo! for Amazon: Sentiment pars-
ing from small talk on the web. EFA
2001 Barcelona Meetings. http://
ssrn.com/abstract=276189 .

--- Page 559 ---
Bibliography 559
David, Jr., E. E. and O. G. Selfridge.
1962. Eyes and ears for computers.
Proceedings of the IRE (Institute of
Radio Engineers) , 50:1093–1101.
Davidson, T., D. Bhattacharya, and
I. Weber. 2019. Racial bias in hate
speech and abusive language detec-
tion datasets. Third Workshop on
Abusive Language Online .
Davies, M. 2012. Expanding hori-
zons in historical linguistics with the
400-million word Corpus of Histor-
ical American English. Corpora ,
7(2):121–157.
Davies, M. 2015. The Wiki-
pedia Corpus: 4.6 million arti-
cles, 1.9 billion words. Adapted
from Wikipedia. https://www.
english-corpora.org/wiki/ .
Davies, M. 2020. The Corpus
of Contemporary American En-
glish (COCA): One billion words,
1990-2019. https://www.
english-corpora.org/coca/ .
Davis, E., L. Morgenstern, and C. L.
Ortiz. 2017. The ﬁrst Winograd
schema challenge at IJCAI-16. AI
Magazine , 38(3):97–98.
Davis, K. H., R. Biddulph, and S. Bal-
ashek. 1952. Automatic recognition
of spoken digits. JASA , 24(6):637–
642.
Davis, S. and P. Mermelstein. 1980.
Comparison of parametric repre-
sentations for monosyllabic word
recognition in continuously spoken
sentences. IEEE Transactions on
ASSP , 28(4):357–366.
Deerwester, S. C., S. T. Dumais, G. W.
Furnas, R. A. Harshman, T. K.
Landauer, K. E. Lochbaum, and
L. Streeter. 1988. Computer infor-
mation retrieval using latent seman-
tic structure: US Patent 4,839,853.
Deerwester, S. C., S. T. Dumais, T. K.
Landauer, G. W. Furnas, and R. A.
Harshman. 1990. Indexing by la-
tent semantics analysis. JASIS ,
41(6):391–407.
Deibel, D. and R. Evanhoe. 2021. Con-
versations with Things: UX Design
for Chat and Voice . Rosenfeld.
DeJong, G. F. 1982. An overview of the
FRUMP system. In W. G. Lehnert
and M. H. Ringle, eds, Strategies for
Natural Language Processing , 149–
176. LEA.
Demberg, V . 2006. Letter-to-phoneme
conversion for a German text-to-
speech system. Diplomarbeit Nr. 47,
Universit ¨at Stuttgart.
Denes, P. 1959. The design and oper-
ation of the mechanical speech rec-
ognizer at University College Lon-
don. Journal of the British Institu-
tion of Radio Engineers , 19(4):219–
234. Appears together with compan-
ion paper (Fry 1959).Deng, L., G. Hinton, and B. Kingsbury.
2013. New types of deep neural
network learning for speech recog-
nition and related applications: An
overview. ICASSP .
Deng, Y . and W. Byrne. 2005. HMM
word and phrase alignment for sta-
tistical machine translation. HLT-
EMNLP .
Denis, P. and J. Baldridge. 2007. Joint
determination of anaphoricity and
coreference resolution using integer
programming. NAACL-HLT .
Denis, P. and J. Baldridge. 2008. Spe-
cialized models and ranking for
coreference resolution. EMNLP .
Denis, P. and J. Baldridge. 2009. Global
joint models for coreference resolu-
tion and named entity classiﬁcation.
Procesamiento del Lenguaje Natu-
ral, 42.
DeRose, S. J. 1988. Grammatical cat-
egory disambiguation by statistical
optimization. Computational Lin-
guistics , 14:31–39.
Devlin, J., M.-W. Chang, K. Lee, and
K. Toutanova. 2019. BERT: Pre-
training of deep bidirectional trans-
formers for language understanding.
NAACL HLT .
Di Eugenio, B. 1990. Centering theory
and the Italian pronominal system.
COLING .
Di Eugenio, B. 1996. The discourse
functions of Italian subjects: A cen-
tering approach. COLING .
Dias Oliva, T., D. Antonialli, and
A. Gomes. 2021. Fighting hate
speech, silencing drag queens? arti-
ﬁcial intelligence in content modera-
tion and risks to lgbtq voices online.
Sexuality & Culture , 25:700–732.
Dinan, E., G. Abercrombie, A. S.
Bergman, S. Spruit, D. Hovy, Y .-L.
Boureau, and V . Rieser. 2021. Antic-
ipating safety issues in e2e conver-
sational ai: Framework and tooling.
ArXiv.
Dinan, E., A. Fan, A. Williams, J. Ur-
banek, D. Kiela, and J. Weston.
2020. Queens are powerful too: Mit-
igating gender bias in dialogue gen-
eration. EMNLP .
Ditman, T. and G. R. Kuperberg.
2010. Building coherence: A frame-
work for exploring the breakdown
of links across clause boundaries in
schizophrenia. Journal of neurolin-
guistics , 23(3):254–269.
Dixon, L., J. Li, J. Sorensen, N. Thain,
and L. Vasserman. 2018. Measuring
and mitigating unintended bias in
text classiﬁcation. 2018 AAAI/ACM
Conference on AI, Ethics, and Soci-
ety.Dixon, N. and H. Maxey. 1968. Termi-
nal analog synthesis of continuous
speech using the diphone method of
segment assembly. IEEE Transac-
tions on Audio and Electroacoustics ,
16(1):40–50.
Do, Q. N. T., S. Bethard, and M.-F.
Moens. 2017. Improving implicit
semantic role labeling by predicting
semantic frame arguments. IJCNLP .
Doddington, G. 2002. Automatic eval-
uation of machine translation quality
using n-gram co-occurrence statis-
tics. HLT.
Dodge, J., S. Gururangan, D. Card,
R. Schwartz, and N. A. Smith. 2019.
Show your work: Improved report-
ing of experimental results. EMNLP .
Dodge, J., M. Sap, A. Marasovi ´c,
W. Agnew, G. Ilharco, D. Groen-
eveld, M. Mitchell, and M. Gardner.
2021. Documenting large webtext
corpora: A case study on the colos-
sal clean crawled corpus. EMNLP .
Dong, L. and M. Lapata. 2016. Lan-
guage to logical form with neural at-
tention. ACL.
Dorr, B. 1994. Machine translation di-
vergences: A formal description and
proposed solution. Computational
Linguistics , 20(4):597–633.
Dostert, L. 1955. The Georgetown-
I.B.M. experiment. In Machine
Translation of Languages: Fourteen
Essays , 124–135. MIT Press.
Dowty, D. R. 1979. Word Meaning and
Montague Grammar . D. Reidel.
Dozat, T. and C. D. Manning. 2017.
Deep biafﬁne attention for neural de-
pendency parsing. ICLR .
Dozat, T. and C. D. Manning. 2018.
Simpler but more accurate semantic
dependency parsing. ACL.
Dozat, T., P. Qi, and C. D. Manning.
2017. Stanford’s graph-based neu-
ral dependency parser at the CoNLL
2017 shared task. Proceedings of the
CoNLL 2017 Shared Task: Multilin-
gual Parsing from Raw Text to Uni-
versal Dependencies .
Dror, R., G. Baumer, M. Bogomolov,
and R. Reichart. 2017. Replicabil-
ity analysis for natural language pro-
cessing: Testing signiﬁcance with
multiple datasets. TACL , 5:471–
–486.
Dror, R., L. Peled-Cohen, S. Shlomov,
and R. Reichart. 2020. Statisti-
cal Signiﬁcance Testing for Natural
Language Processing , volume 45 of
Synthesis Lectures on Human Lan-
guage Technologies . Morgan &
Claypool.
Dryer, M. S. and M. Haspelmath, eds.
2013. The World Atlas of Language

--- Page 560 ---
560 Bibliography
Structures Online . Max Planck In-
stitute for Evolutionary Anthropol-
ogy, Leipzig. Available online at
http://wals.info .
Du Bois, J. W., W. L. Chafe, C. Meyer,
S. A. Thompson, R. Englebretson,
and N. Martey. 2005. Santa Barbara
corpus of spoken American English,
Parts 1-4. Philadelphia: Linguistic
Data Consortium.
Durrett, G. and D. Klein. 2013. Easy
victories and uphill battles in coref-
erence resolution. EMNLP .
Durrett, G. and D. Klein. 2014. A joint
model for entity analysis: Corefer-
ence, typing, and linking. TACL ,
2:477–490.
Earley, J. 1968. An Efﬁcient Context-
Free Parsing Algorithm . Ph.D.
thesis, Carnegie Mellon University,
Pittsburgh, PA.
Earley, J. 1970. An efﬁcient context-
free parsing algorithm. CACM ,
6(8):451–455.
Ebden, P. and R. Sproat. 2015. The
Kestrel TTS text normalization sys-
tem. Natural Language Engineer-
ing, 21(3):333.
Edmonds, J. 1967. Optimum branch-
ings. Journal of Research of the
National Bureau of Standards B ,
71(4):233–240.
Edunov, S., M. Ott, M. Auli, and
D. Grangier. 2018. Understanding
back-translation at scale. EMNLP .
Efron, B. and R. J. Tibshirani. 1993. An
introduction to the bootstrap . CRC
press.
Egghe, L. 2007. Untangling Herdan’s
law and Heaps’ law: Mathematical
and informetric arguments. JASIST ,
58(5):702–709.
Eisner, J. 1996. Three new probabilistic
models for dependency parsing: An
exploration. COLING .
Ekman, P. 1999. Basic emotions. In
T. Dalgleish and M. J. Power, eds,
Handbook of Cognition and Emo-
tion, 45–60. Wiley.
Elhage, N., N. Nanda, C. Olsson,
T. Henighan, N. Joseph, B. Mann,
A. Askell, Y . Bai, A. Chen, T. Con-
erly, N. DasSarma, D. Drain,
D. Ganguli, Z. Hatﬁeld-Dodds,
D. Hernandez, A. Jones, J. Kernion,
L. Lovitt, K. Ndousse, D. Amodei,
T. Brown, J. Clark, J. Kaplan, S. Mc-
Candlish, and C. Olah. 2021. A
mathematical framework for trans-
former circuits. White paper.
Elman, J. L. 1990. Finding structure in
time. Cognitive science , 14(2):179–
211.
Elsner, M., J. Austerweil, and E. Char-
niak. 2007. A uniﬁed local and
global model for discourse coher-
ence. NAACL-HLT .Elsner, M. and E. Charniak. 2008.
Coreference-inspired coherence
modeling. ACL.
Elsner, M. and E. Charniak. 2011. Ex-
tending the entity grid with entity-
speciﬁc features. ACL.
Elvev ˚ag, B., P. W. Foltz, D. R.
Weinberger, and T. E. Goldberg.
2007. Quantifying incoherence in
speech: an automated methodology
and novel application to schizophre-
nia. Schizophrenia research , 93(1-
3):304–316.
Emami, A. and F. Jelinek. 2005. A neu-
ral syntactic language model. Ma-
chine learning , 60(1):195–227.
Emami, A., P. Trichelair, A. Trischler,
K. Suleman, H. Schulz, and J. C. K.
Cheung. 2019. The KNOWREF
coreference corpus: Removing gen-
der and number cues for difﬁ-
cult pronominal anaphora resolu-
tion. ACL.
Erk, K. 2007. A simple, similarity-
based model for selectional prefer-
ences. ACL.
van Esch, D. and R. Sproat. 2018.
An expanded taxonomy of semiotic
classes for text normalization. IN-
TERSPEECH .
Ethayarajh, K. 2019. How contextual
are contextualized word representa-
tions? Comparing the geometry of
BERT, ELMo, and GPT-2 embed-
dings. EMNLP .
Ethayarajh, K., D. Duvenaud, and
G. Hirst. 2019a. Towards un-
derstanding linear word analogies.
ACL.
Ethayarajh, K., D. Duvenaud, and
G. Hirst. 2019b. Understanding un-
desirable word embedding associa-
tions. ACL.
Ethayarajh, K. and D. Jurafsky. 2020.
Utility is in the eye of the user:
A critique of NLP leaderboards.
EMNLP .
Etzioni, O., M. Cafarella, D. Downey,
A.-M. Popescu, T. Shaked, S. Soder-
land, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity
extraction from the web: An experi-
mental study. Artiﬁcial Intelligence ,
165(1):91–134.
Evans, N. 2000. Word classes in the
world’s languages. In G. Booij,
C. Lehmann, and J. Mugdan, eds,
Morphology: A Handbook on Inﬂec-
tion and Word Formation , 708–732.
Mouton.
Fader, A., S. Soderland, and O. Etzioni.
2011. Identifying relations for open
information extraction. EMNLP .
Fan, A., S. Bhosale, H. Schwenk,
Z. Ma, A. El-Kishky, S. Goyal,
M. Baines, O. Celebi, G. Wenzek,V . Chaudhary, N. Goyal, T. Birch,
V . Liptchinsky, S. Edunov, M. Auli,
and A. Joulin. 2021. Beyond
english-centric multilingual ma-
chine translation. JMLR , 22(107):1–
48.
Fano, R. M. 1961. Transmission of In-
formation: A Statistical Theory of
Communications . MIT Press.
Fant, G. M. 1951. Speech communica-
tion research. Ing. Vetenskaps Akad.
Stockholm, Sweden , 24:331–337.
Fant, G. M. 1986. Glottal ﬂow: Models
and interaction. Journal of Phonet-
ics, 14:393–399.
Fast, E., B. Chen, and M. S. Bernstein.
2016. Empath: Understanding Topic
Signals in Large-Scale Text. CHI.
Fauconnier, G. and M. Turner. 2008.
The way we think: Conceptual
blending and the mind’s hidden
complexities . Basic Books.
Feldman, J. A. and D. H. Ballard.
1982. Connectionist models and
their properties. Cognitive Science ,
6:205–254.
Fellbaum, C., ed. 1998. WordNet: An
Electronic Lexical Database . MIT
Press.
Feng, V . W. and G. Hirst. 2011. Classi-
fying arguments by scheme. ACL.
Feng, V . W. and G. Hirst. 2014.
A linear-time bottom-up discourse
parser with constraints and post-
editing. ACL.
Feng, V . W., Z. Lin, and G. Hirst. 2014.
The impact of deep hierarchical dis-
course structures in the evaluation of
text coherence. COLING .
Fernandes, E. R., C. N. dos Santos, and
R. L. Milidi ´u. 2012. Latent struc-
ture perceptron with feature induc-
tion for unrestricted coreference res-
olution. CoNLL .
Ferragina, P. and U. Scaiella. 2011.
Fast and accurate annotation of short
texts with wikipedia pages. IEEE
Software , 29(1):70–75.
Ferro, L., L. Gerber, I. Mani, B. Sund-
heim, and G. Wilson. 2005. Tides
2005 standard for the annotation of
temporal expressions. Technical re-
port, MITRE.
Ferrucci, D. A. 2012. Introduction
to “This is Watson”. IBM Jour-
nal of Research and Development ,
56(3/4):1:1–1:15.
Fessler, L. 2017. We tested bots like Siri
and Alexa to see who would stand
up to sexual harassment. Quartz .
Feb 22, 2017. https://qz.com/
911681/ .
Field, A. and Y . Tsvetkov. 2019. Entity-
centric contextual affective analysis.
ACL.

--- Page 561 ---
Bibliography 561
Fikes, R. E. and N. J. Nilsson. 1971.
STRIPS: A new approach to the
application of theorem proving to
problem solving. Artiﬁcial Intelli-
gence , 2:189–208.
Fillmore, C. J. 1966. A proposal con-
cerning English prepositions. In F. P.
Dinneen, ed., 17th annual Round Ta-
ble, volume 17 of Monograph Series
on Language and Linguistics , 19–
34. Georgetown University Press.
Fillmore, C. J. 1968. The case for case.
In E. W. Bach and R. T. Harms, eds,
Universals in Linguistic Theory , 1–
88. Holt, Rinehart & Winston.
Fillmore, C. J. 1985. Frames and the se-
mantics of understanding. Quaderni
di Semantica , VI(2):222–254.
Fillmore, C. J. 2003. Valency and se-
mantic roles: the concept of deep
structure case. In V . Agel, L. M.
Eichinger, H. W. Eroms, P. Hell-
wig, H. J. Heringer, and H. Lobin,
eds, Dependenz und Valenz: Ein
internationales Handbuch der zeit-
gen¨ossischen Forschung , chapter 36,
457–475. Walter de Gruyter.
Fillmore, C. J. 2012. ACL life-
time achievement award: Encoun-
ters with language. Computational
Linguistics , 38(4):701–718.
Fillmore, C. J. and C. F. Baker. 2009. A
frames approach to semantic analy-
sis. In B. Heine and H. Narrog, eds,
The Oxford Handbook of Linguistic
Analysis , 313–340. Oxford Univer-
sity Press.
Fillmore, C. J., C. R. Johnson, and
M. R. L. Petruck. 2003. Background
to FrameNet. International journal
of lexicography , 16(3):235–250.
Finkelstein, L., E. Gabrilovich, Y . Ma-
tias, E. Rivlin, Z. Solan, G. Wolf-
man, and E. Ruppin. 2002. Placing
search in context: The concept revis-
ited. ACM Transactions on Informa-
tion Systems , 20(1):116—-131.
Finlayson, M. A. 2016. Inferring
Propp’s functions from semantically
annotated text. The Journal of Amer-
ican Folklore , 129(511):55–77.
Firth, J. R. 1935. The technique of se-
mantics. Transactions of the philo-
logical society , 34(1):36–73.
Firth, J. R. 1957. A synopsis of linguis-
tic theory 1930–1955. In Studies in
Linguistic Analysis . Philological So-
ciety. Reprinted in Palmer, F. (ed.)
1968. Selected Papers of J. R. Firth.
Longman, Harlow.
Flanagan, J. L. 1972. Speech Analysis,
Synthesis, and Perception . Springer.
Flanagan, J. L., K. Ishizaka, and K. L.
Shipley. 1975. Synthesis of speech
from a dynamic model of the vocal
cords and vocal tract. The Bell Sys-
tem Technical Journal , 54(3):485–
506.Foland, W. and J. H. Martin. 2016.
CU-NLP at SemEval-2016 task 8:
AMR parsing using LSTM-based re-
current neural networks. SemEval-
2016 .
Foland, Jr., W. R. and J. H. Martin.
2015. Dependency-based seman-
tic role labeling using convolutional
neural networks. *SEM 2015 .
Foltz, P. W., W. Kintsch, and T. K. Lan-
dauer. 1998. The measurement of
textual coherence with latent seman-
tic analysis. Discourse processes ,
25(2-3):285–307.
8, W. Nekoto, V . Marivate, T. Matsila,
T. Fasubaa, T. Kolawole, T. Fag-
bohungbe, S. O. Akinola, S. H.
Muhammad, S. Kabongo, S. Osei,
S. Freshia, R. A. Niyongabo,
R. M. P. Ogayo, O. Ahia, M. Mer-
essa, M. Adeyemi, M. Mokgesi-
Selinga, L. Okegbemi, L. J. Mar-
tinus, K. Tajudeen, K. Degila,
K. Ogueji, K. Siminyu, J. Kreutzer,
J. Webster, J. T. Ali, J. A. I.
Orife, I. Ezeani, I. A. Dangana,
H. Kamper, H. Elsahar, G. Duru,
G. Kioko, E. Murhabazi, E. van
Biljon, D. Whitenack, C. Onye-
fuluchi, C. Emezue, B. Dossou,
B. Sibanda, B. I. Bassey, A. Olabiyi,
A. Ramkilowan, A. ¨Oktem, A. Akin-
faderin, and A. Bashir. 2020. Partic-
ipatory research for low-resourced
machine translation: A case study
in African languages. Findings of
EMNLP .
Fox, B. A. 1993. Discourse Structure
and Anaphora: Written and Conver-
sational English . Cambridge.
Francis, W. N. and H. Ku ˇcera. 1982.
Frequency Analysis of English Us-
age. Houghton Mifﬂin, Boston.
Franz, A. and T. Brants. 2006. All our
n-gram are belong to you. https:
//research.google/blog/
all-our-n-gram-are-belong-to-you/ .
Fraser, N. M. and G. N. Gilbert. 1991.
Simulating speech systems. Com-
puter Speech and Language , 5:81–
99.
Friedman, B. and D. G. Hendry. 2019.
Value Sensitive Design: Shaping
Technology with Moral Imagination .
MIT Press.
Friedman, B., D. G. Hendry, and
A. Borning. 2017. A survey
of value sensitive design methods.
Foundations and Trends in Human-
Computer Interaction , 11(2):63–
125.
Fry, D. B. 1959. Theoretical as-
pects of mechanical speech recogni-
tion. Journal of the British Institu-
tion of Radio Engineers , 19(4):211–
218. Appears together with compan-
ion paper (Denes 1959).Furnas, G. W., T. K. Landauer, L. M.
Gomez, and S. T. Dumais. 1987.
The vocabulary problem in human-
system communication. Commu-
nications of the ACM , 30(11):964–
971.
Gabow, H. N., Z. Galil, T. Spencer, and
R. E. Tarjan. 1986. Efﬁcient algo-
rithms for ﬁnding minimum span-
ning trees in undirected and directed
graphs. Combinatorica , 6(2):109–
122.
Gaddy, D., M. Stern, and D. Klein.
2018. What’s going on in neural
constituency parsers? an analysis.
NAACL HLT .
Gale, W. A. and K. W. Church. 1994.
What is wrong with adding one? In
N. Oostdijk and P. de Haan, eds,
Corpus-Based Research into Lan-
guage , 189–198. Rodopi.
Gale, W. A. and K. W. Church. 1991.
A program for aligning sentences in
bilingual corpora. ACL.
Gale, W. A. and K. W. Church. 1993.
A program for aligning sentences in
bilingual corpora. Computational
Linguistics , 19:75–102.
Gale, W. A., K. W. Church, and
D. Yarowsky. 1992a. One sense per
discourse. HLT.
Gale, W. A., K. W. Church, and
D. Yarowsky. 1992b. Work on sta-
tistical methods for word sense dis-
ambiguation. AAAI Fall Symposium
on Probabilistic Approaches to Nat-
ural Language .
Gao, L., T. Hoppe, A. Thite, S. Bi-
derman, C. Foster, N. Nabeshima,
S. Black, J. Phang, S. Presser,
L. Golding, H. He, and C. Leahy.
2020. The Pile: An 800GB dataset
of diverse text for language model-
ing. ArXiv preprint.
Garg, N., L. Schiebinger, D. Jurafsky,
and J. Zou. 2018. Word embeddings
quantify 100 years of gender and
ethnic stereotypes. Proceedings of
the National Academy of Sciences ,
115(16):E3635–E3644.
Garside, R. 1987. The CLAWS word-
tagging system. In R. Garside,
G. Leech, and G. Sampson, eds, The
Computational Analysis of English ,
30–41. Longman.
Garside, R., G. Leech, and A. McEnery.
1997. Corpus Annotation . Long-
man.
Gebru, T., J. Morgenstern, B. Vec-
chione, J. W. Vaughan, H. Wal-
lach, H. Daum ´e III, and K. Craw-
ford. 2020. Datasheets for datasets.
ArXiv.
Gehman, S., S. Gururangan, M. Sap,
Y . Choi, and N. A. Smith. 2020. Re-
alToxicityPrompts: Evaluating neu-
ral toxic degeneration in language
models. Findings of EMNLP .

--- Page 562 ---
562 Bibliography
Gerber, M. and J. Y . Chai. 2010. Be-
yond nombank: A study of implicit
arguments for nominal predicates.
ACL.
Gers, F. A., J. Schmidhuber, and
F. Cummins. 2000. Learning to for-
get: Continual prediction with lstm.
Neural computation , 12(10):2451–
2471.
Gil, D. 2000. Syntactic categories,
cross-linguistic variation and univer-
sal grammar. In P. M. V ogel and
B. Comrie, eds, Approaches to the
Typology of Word Classes , 173–216.
Mouton.
Gildea, D. and D. Jurafsky. 2000. Au-
tomatic labeling of semantic roles.
ACL.
Gildea, D. and D. Jurafsky. 2002.
Automatic labeling of semantic
roles. Computational Linguistics ,
28(3):245–288.
Gildea, D. and M. Palmer. 2002.
The necessity of syntactic parsing
for predicate argument recognition.
ACL.
Giles, C. L., G. M. Kuhn, and R. J.
Williams. 1994. Dynamic recurrent
neural networks: Theory and appli-
cations. IEEE Trans. Neural Netw.
Learning Syst. , 5(2):153–156.
Gillick, L. and S. J. Cox. 1989. Some
statistical issues in the comparison
of speech recognition algorithms.
ICASSP .
Girard, G. 1718. La justesse de la
langue franc ¸oise: ou les diff ´erentes
signiﬁcations des mots qui passent
pour synonimes . Laurent d’Houry,
Paris.
Giuliano, V . E. 1965. The inter-
pretation of word associations.
Statistical Association Methods
For Mechanized Documentation.
Symposium Proceedings. Wash-
ington, D.C., USA, March 17,
1964 .https://nvlpubs.nist.
gov/nistpubs/Legacy/MP/
nbsmiscellaneouspub269.pdf .
Gladkova, A., A. Drozd, and S. Mat-
suoka. 2016. Analogy-based de-
tection of morphological and se-
mantic relations with word embed-
dings: what works and what doesn’t.
NAACL Student Research Workshop .
Glaese, A., N. McAleese, M. Trebacz,
J. Aslanides, V . Firoiu, T. Ewalds,
M. Rauh, L. Weidinger, M. Chad-
wick, P. Thacker, L. Campbell-
Gillingham, J. Uesato, P.-S. Huang,
R. Comanescu, F. Yang, A. See,
S. Dathathri, R. Greig, C. Chen,
D. Fritz, J. Sanchez Elias, R. Green,
S. Mokr ´a, N. Fernando, B. Wu,
R. Foley, S. Young, I. Gabriel,
W. Isaac, J. Mellor, D. Hassabis,
K. Kavukcuoglu, L. A. Hendricks,and G. Irving. 2022. Improving
alignment of dialogue agents via tar-
geted human judgements. ArXiv
preprint.
Glenberg, A. M. and D. A. Robert-
son. 2000. Symbol grounding and
meaning: A comparison of high-
dimensional and embodied theories
of meaning. Journal of memory and
language , 43(3):379–401.
Godfrey, J., E. Holliman, and J. Mc-
Daniel. 1992. SWITCHBOARD:
Telephone speech corpus for re-
search and development. ICASSP .
Goel, V . and W. Byrne. 2000. Minimum
bayes-risk automatic speech recog-
nition. Computer Speech & Lan-
guage , 14(2):115–135.
Goffman, E. 1974. Frame analysis: An
essay on the organization of experi-
ence. Harvard University Press.
Goldberg, J., M. Ostendorf, and
K. Kirchhoff. 2003. The impact of
response wording in error correction
subdialogs. ISCA Tutorial and Re-
search Workshop on Error Handling
in Spoken Dialogue Systems .
Goldberg, Y . 2017. Neural Network
Methods for Natural Language Pro-
cessing , volume 10 of Synthesis Lec-
tures on Human Language Tech-
nologies . Morgan & Claypool.
Gonen, H. and Y . Goldberg. 2019. Lip-
stick on a pig: Debiasing methods
cover up systematic gender biases in
word embeddings but do not remove
them. NAACL HLT .
Good, M. D., J. A. Whiteside, D. R.
Wixon, and S. J. Jones. 1984. Build-
ing a user-derived interface. CACM ,
27(10):1032–1043.
Goodfellow, I., Y . Bengio, and
A. Courville. 2016. Deep Learn-
ing. MIT Press.
Goodman, J. 2006. A bit of progress
in language modeling: Extended
version. Technical Report MSR-
TR-2001-72, Machine Learning and
Applied Statistics Group, Microsoft
Research, Redmond, WA.
Goodwin, C. 1996. Transparent vi-
sion. In E. Ochs, E. A. Schegloff,
and S. A. Thompson, eds, Interac-
tion and Grammar , 370–404. Cam-
bridge University Press.
Gopalakrishnan, K., B. Hedayatnia,
Q. Chen, A. Gottardi, S. Kwa-
tra, A. Venkatesh, R. Gabriel, and
D. Hakkani-T ¨ur. 2019. Topical-
chat: Towards knowledge-grounded
open-domain conversations. INTER-
SPEECH .
Gould, J. D., J. Conti, and T. Ho-
vanyecz. 1983. Composing let-
ters with a simulated listening type-
writer. CACM , 26(4):295–308.Gould, J. D. and C. Lewis. 1985. De-
signing for usability: Key principles
and what designers think. CACM ,
28(3):300–311.
Gould, S. J. 1980. The Panda’s Thumb .
Penguin Group.
Graff, D. 1997. The 1996 Broadcast
News speech and language-model
corpus. Proceedings DARPA Speech
Recognition Workshop .
Gravano, A., J. Hirschberg, and
ˇS. Be ˇnuˇs. 2012. Afﬁrmative cue
words in task-oriented dialogue.
Computational Linguistics , 38(1):1–
39.
Graves, A. 2012. Sequence transduc-
tion with recurrent neural networks.
ICASSP .
Graves, A. 2013. Generating se-
quences with recurrent neural net-
works. ArXiv.
Graves, A., S. Fern ´andez, F. Gomez,
and J. Schmidhuber. 2006. Con-
nectionist temporal classiﬁcation:
Labelling unsegmented sequence
data with recurrent neural networks.
ICML .
Graves, A., S. Fern ´andez, M. Li-
wicki, H. Bunke, and J. Schmidhu-
ber. 2007. Unconstrained on-line
handwriting recognition with recur-
rent neural networks. NeurIPS .
Graves, A. and N. Jaitly. 2014. Towards
end-to-end speech recognition with
recurrent neural networks. ICML .
Graves, A., A.-r. Mohamed, and
G. Hinton. 2013. Speech recognition
with deep recurrent neural networks.
ICASSP .
Graves, A. and J. Schmidhuber. 2005.
Framewise phoneme classiﬁcation
with bidirectional LSTM and other
neural network architectures. Neu-
ral Networks , 18(5-6):602–610.
Graves, A., G. Wayne, and I. Dani-
helka. 2014. Neural Turing ma-
chines. ArXiv.
Green, B. F., A. K. Wolf, C. Chom-
sky, and K. Laughery. 1961. Base-
ball: An automatic question an-
swerer. Proceedings of the Western
Joint Computer Conference 19 .
Greene, B. B. and G. M. Rubin. 1971.
Automatic grammatical tagging of
English. Department of Linguis-
tics, Brown University, Providence,
Rhode Island.
Greenwald, A. G., D. E. McGhee, and
J. L. K. Schwartz. 1998. Measur-
ing individual differences in implicit
cognition: the implicit association
test. Journal of personality and so-
cial psychology , 74(6):1464–1480.
Grenager, T. and C. D. Manning. 2006.
Unsupervised discovery of a statisti-
cal verb lexicon. EMNLP .

--- Page 563 ---
Bibliography 563
Grice, H. P. 1975. Logic and conversa-
tion. In P. Cole and J. L. Morgan,
eds, Speech Acts: Syntax and Se-
mantics Volume 3 , 41–58. Academic
Press.
Grice, H. P. 1978. Further notes on
logic and conversation. In P. Cole,
ed.,Pragmatics: Syntax and Seman-
tics Volume 9 , 113–127. Academic
Press.
Grishman, R. and B. Sundheim. 1995.
Design of the MUC-6 evaluation.
MUC-6 .
Grosz, B. J. 1977a. The representation
and use of focus in a system for un-
derstanding dialogs. IJCAI-77 . Mor-
gan Kaufmann.
Grosz, B. J. 1977b. The Representation
and Use of Focus in Dialogue Un-
derstanding . Ph.D. thesis, Univer-
sity of California, Berkeley.
Grosz, B. J., A. K. Joshi, and S. Wein-
stein. 1983. Providing a uniﬁed ac-
count of deﬁnite noun phrases in En-
glish. ACL.
Grosz, B. J., A. K. Joshi, and S. Wein-
stein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguis-
tics, 21(2):203–225.
Grosz, B. J. and C. L. Sidner. 1980.
Plans for discourse. In P. R. Cohen,
J. Morgan, and M. E. Pollack, eds,
Intentions in Communication , 417–
444. MIT Press.
Gruber, J. S. 1965. Studies in Lexical
Relations . Ph.D. thesis, MIT.
Gr¨unewald, S., A. Friedrich, and
J. Kuhn. 2021. Applying Occam’s
razor to transformer-based depen-
dency parsing: What works, what
doesn’t, and what is really neces-
sary. IWPT .
Guinaudeau, C. and M. Strube. 2013.
Graph-based local coherence model-
ing. ACL.
Guindon, R. 1988. A multidisciplinary
perspective on dialogue structure in
user-advisor dialogues. In R. Guin-
don, ed., Cognitive Science and Its
Applications for Human-Computer
Interaction , 163–200. Lawrence Erl-
baum.
Gundel, J. K., N. Hedberg, and
R. Zacharski. 1993. Cognitive status
and the form of referring expressions
in discourse. Language , 69(2):274–
307.
Gururangan, S., A. Marasovi ´c,
S. Swayamdipta, K. Lo, I. Belt-
agy, D. Downey, and N. A. Smith.
2020. Don’t stop pretraining: Adapt
language models to domains and
tasks. ACL.
Gusﬁeld, D. 1997. Algorithms on
Strings, Trees, and Sequences . Cam-
bridge University Press.Guyon, I. and A. Elisseeff. 2003. An
introduction to variable and feature
selection. JMLR , 3:1157–1182.
Haber, J. and M. Poesio. 2020. As-
sessing polyseme sense similarity
through co-predication acceptability
and contextualised embedding dis-
tance. *SEM .
Habernal, I. and I. Gurevych. 2016.
Which argument is more convinc-
ing? Analyzing and predicting con-
vincingness of Web arguments using
bidirectional LSTM. ACL.
Habernal, I. and I. Gurevych. 2017.
Argumentation mining in user-
generated web discourse. Computa-
tional Linguistics , 43(1):125–179.
Haghighi, A. and D. Klein. 2009.
Simple coreference resolution with
rich syntactic and semantic features.
EMNLP .
Hajishirzi, H., L. Zilles, D. S. Weld,
and L. Zettlemoyer. 2013. Joint
coreference resolution and named-
entity linking with multi-pass sieves.
EMNLP .
Hajiˇc, J. 1998. Building a Syn-
tactically Annotated Corpus: The
Prague Dependency Treebank , 106–
132. Karolinum.
Hajiˇc, J. 2000. Morphological tagging:
Data vs. dictionaries. NAACL .
Hajiˇc, J., M. Ciaramita, R. Johans-
son, D. Kawahara, M. A. Mart ´ı,
L. M `arquez, A. Meyers, J. Nivre,
S. Pad ´o, J. ˇStˇep´anek, P. Stran ˇa´k,
M. Surdeanu, N. Xue, and Y . Zhang.
2009. The conll-2009 shared task:
Syntactic and semantic dependen-
cies in multiple languages. CoNLL .
Hakkani-T ¨ur, D., K. Oﬂazer, and
G. T ¨ur. 2002. Statistical morpholog-
ical disambiguation for agglutinative
languages. Journal of Computers
and Humanities , 36(4):381–410.
Halliday, M. A. K. and R. Hasan. 1976.
Cohesion in English . Longman. En-
glish Language Series, Title No. 9.
Hamilton, W. L., K. Clark, J. Leskovec,
and D. Jurafsky. 2016a. Inducing
domain-speciﬁc sentiment lexicons
from unlabeled corpora. EMNLP .
Hamilton, W. L., J. Leskovec, and
D. Jurafsky. 2016b. Diachronic word
embeddings reveal statistical laws of
semantic change. ACL.
Hannun, A. 2017. Sequence modeling
with CTC. Distill , 2(11).
Hannun, A. Y ., A. L. Maas, D. Juraf-
sky, and A. Y . Ng. 2014. First-pass
large vocabulary continuous speech
recognition using bi-directional re-
current DNNs. ArXiv preprint
arXiv:1408.2873.
Harris, C. M. 1953. A study of the
building blocks in speech. JASA ,
25(5):962–969.Harris, R. A. 2005. Voice Interaction
Design: Crafting the New Conver-
sational Speech Systems . Morgan
Kaufmann.
Harris, Z. S. 1946. From morpheme
to utterance. Language , 22(3):161–
183.
Harris, Z. S. 1954. Distributional struc-
ture. Word , 10:146–162.
Harris, Z. S. 1962. String Analysis of
Sentence Structure . Mouton, The
Hague.
Hashimoto, T., M. Srivastava,
H. Namkoong, and P. Liang. 2018.
Fairness without demographics in
repeated loss minimization. ICML .
Hastie, T., R. J. Tibshirani, and J. H.
Friedman. 2001. The Elements of
Statistical Learning . Springer.
Hatzivassiloglou, V . and K. McKeown.
1997. Predicting the semantic orien-
tation of adjectives. ACL.
Hatzivassiloglou, V . and J. Wiebe.
2000. Effects of adjective orienta-
tion and gradability on sentence sub-
jectivity. COLING .
Haviland, S. E. and H. H. Clark. 1974.
What’s new? Acquiring new infor-
mation as a process in comprehen-
sion. Journal of Verbal Learning and
Verbal Behaviour , 13:512–521.
Hawkins, J. A. 1978. Deﬁniteness
and indeﬁniteness: a study in refer-
ence and grammaticality prediction .
Croom Helm Ltd.
Hayashi, T., R. Yamamoto, K. In-
oue, T. Yoshimura, S. Watanabe,
T. Toda, K. Takeda, Y . Zhang,
and X. Tan. 2020. ESPnet-TTS:
Uniﬁed, reproducible, and integrat-
able open source end-to-end text-to-
speech toolkit. ICASSP .
He, L., K. Lee, M. Lewis, and L. Zettle-
moyer. 2017. Deep semantic role la-
beling: What works and what’s next.
ACL.
He, W., K. Liu, J. Liu, Y . Lyu, S. Zhao,
X. Xiao, Y . Liu, Y . Wang, H. Wu,
Q. She, X. Liu, T. Wu, and H. Wang.
2018. DuReader: a Chinese machine
reading comprehension dataset from
real-world applications. Workshop
on Machine Reading for Question
Answering .
Heaﬁeld, K. 2011. KenLM: Faster
and smaller language model queries.
Workshop on Statistical Machine
Translation .
Heaﬁeld, K., I. Pouzyrevsky, J. H.
Clark, and P. Koehn. 2013. Scal-
able modiﬁed Kneser-Ney language
model estimation. ACL.
Heaps, H. S. 1978. Information re-
trieval. Computational and theoret-
ical aspects . Academic Press.

--- Page 564 ---
564 Bibliography
Hearst, M. A. 1992a. Automatic acqui-
sition of hyponyms from large text
corpora. COLING .
Hearst, M. A. 1992b. Automatic acqui-
sition of hyponyms from large text
corpora. COLING .
Hearst, M. A. 1997. Texttiling: Seg-
menting text into multi-paragraph
subtopic passages. Computational
Linguistics , 23:33–64.
Hearst, M. A. 1998. Automatic discov-
ery of WordNet relations. In C. Fell-
baum, ed., WordNet: An Electronic
Lexical Database . MIT Press.
Heckerman, D., E. Horvitz, M. Sahami,
and S. T. Dumais. 1998. A bayesian
approach to ﬁltering junk e-mail.
AAAI-98 Workshop on Learning for
Text Categorization .
Heim, I. 1982. The semantics of deﬁnite
and indeﬁnite noun phrases . Ph.D.
thesis, University of Massachusetts
at Amherst.
Hellrich, J., S. Buechel, and U. Hahn.
2019. Modeling word emotion in
historical language: Quantity beats
supposed stability in seed word se-
lection. 3rd Joint SIGHUM Work-
shop on Computational Linguistics
for Cultural Heritage, Social Sci-
ences, Humanities and Literature .
Hellrich, J. and U. Hahn. 2016. Bad
company—Neighborhoods in neural
embedding spaces considered harm-
ful.COLING .
Henderson, J. 1994. Description Based
Parsing in a Connectionist Network .
Ph.D. thesis, University of Pennsyl-
vania, Philadelphia, PA.
Henderson, J. 2003. Inducing history
representations for broad coverage
statistical parsing. HLT-NAACL-03 .
Henderson, J. 2004. Discriminative
training of a neural network statisti-
cal parser. ACL.
Henderson, P., J. Hu, J. Romoff,
E. Brunskill, D. Jurafsky, and
J. Pineau. 2020. Towards the sys-
tematic reporting of the energy and
carbon footprints of machine learn-
ing. Journal of Machine Learning
Research , 21(248):1–43.
Henderson, P., X. Li, D. Jurafsky,
T. Hashimoto, M. A. Lemley, and
P. Liang. 2023. Foundation models
and fair use. JMLR , 24(400):1–79.
Henderson, P., K. Sinha, N. Angelard-
Gontier, N. R. Ke, G. Fried,
R. Lowe, and J. Pineau. 2017. Eth-
ical challenges in data-driven dia-
logue systems. AAAI/ACM AI Ethics
and Society Conference .
Hendrickx, I., S. N. Kim, Z. Kozareva,
P. Nakov, D. ´O S´eaghdha, S. Pad ´o,
M. Pennacchiotti, L. Romano, and
S. Szpakowicz. 2009. Semeval-2010task 8: Multi-way classiﬁcation of
semantic relations between pairs of
nominals. 5th International Work-
shop on Semantic Evaluation .
Hendrix, G. G., C. W. Thompson, and
J. Slocum. 1973. Language process-
ing via canonical verbs and semantic
models. Proceedings of IJCAI-73 .
Herdan, G. 1960. Type-token mathe-
matics . Mouton.
Hermann, K. M., T. Kocisky, E. Grefen-
stette, L. Espeholt, W. Kay, M. Su-
leyman, and P. Blunsom. 2015a.
Teaching machines to read and com-
prehend. NeurIPS .
Hermann, K. M., T. Ko ˇcisk´y,
E. Grefenstette, L. Espeholt, W. Kay,
M. Suleyman, and P. Blunsom.
2015b. Teaching machines to read
and comprehend. NeurIPS .
Hernault, H., H. Prendinger, D. A. du-
Verle, and M. Ishizuka. 2010. Hilda:
A discourse parser using support
vector machine classiﬁcation. Dia-
logue & Discourse , 1(3).
Hidey, C., E. Musi, A. Hwang, S. Mure-
san, and K. McKeown. 2017. Ana-
lyzing the semantic types of claims
and premises in an online persuasive
forum. 4th Workshop on Argument
Mining .
Hill, F., R. Reichart, and A. Korhonen.
2015. Simlex-999: Evaluating se-
mantic models with (genuine) sim-
ilarity estimation. Computational
Linguistics , 41(4):665–695.
Hinkelman, E. A. and J. Allen. 1989.
Two constraints on speech act ambi-
guity. ACL.
Hinton, G. E. 1986. Learning dis-
tributed representations of concepts.
COGSCI .
Hinton, G. E., S. Osindero, and Y .-W.
Teh. 2006. A fast learning algorithm
for deep belief nets. Neural compu-
tation , 18(7):1527–1554.
Hinton, G. E., N. Srivastava,
A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. 2012. Improv-
ing neural networks by preventing
co-adaptation of feature detectors.
ArXiv preprint arXiv:1207.0580.
Hirschberg, J., D. J. Litman, and
M. Swerts. 2001. Identifying user
corrections automatically in spoken
dialogue systems. NAACL .
Hirschman, L., M. Light, E. Breck, and
J. D. Burger. 1999. Deep Read:
A reading comprehension system.
ACL.
Hirst, G. 1981. Anaphora in Natu-
ral Language Understanding: A sur-
vey. Number 119 in Lecture notes in
computer science. Springer-Verlag.
Hirst, G. 1987. Semantic Interpreta-
tion and the Resolution of Ambigu-
ity. Cambridge University Press.Hjelmslev, L. 1969. Prologomena to
a Theory of Language . University
of Wisconsin Press. Translated by
Francis J. Whitﬁeld; original Danish
edition 1943.
Hobbs, J. R. 1978. Resolving pronoun
references. Lingua , 44:311–338.
Hobbs, J. R. 1979. Coherence and
coreference. Cognitive Science ,
3:67–90.
Hobbs, J. R., D. E. Appelt, J. Bear,
D. Israel, M. Kameyama, M. E.
Stickel, and M. Tyson. 1997. FAS-
TUS: A cascaded ﬁnite-state trans-
ducer for extracting information
from natural-language text. In
E. Roche and Y . Schabes, eds,
Finite-State Language Processing ,
383–406. MIT Press.
Hochreiter, S. and J. Schmidhuber.
1997. Long short-term memory.
Neural Computation , 9(8):1735–
1780.
Hofmann, T. 1999. Probabilistic latent
semantic indexing. SIGIR-99 .
Holtzman, A., J. Buys, L. Du,
M. Forbes, and Y . Choi. 2020. The
curious case of neural text degener-
ation. ICLR .
Honovich, O., U. Shaham, S. R. Bow-
man, and O. Levy. 2023. Instruction
induction: From few examples to
natural language task descriptions.
ACL.
Hopcroft, J. E. and J. D. Ullman.
1979. Introduction to Automata The-
ory, Languages, and Computation .
Addison-Wesley.
Hou, Y ., K. Markert, and M. Strube.
2018. Unrestricted bridging reso-
lution. Computational Linguistics ,
44(2):237–284.
Householder, F. W. 1995. Dionysius
Thrax, the technai , and Sextus Em-
piricus. In E. F. K. Koerner and
R. E. Asher, eds, Concise History of
the Language Sciences , 99–103. El-
sevier Science.
Hovy, E. H. 1990. Parsimonious
and proﬂigate approaches to the
question of discourse structure rela-
tions. Proceedings of the 5th Inter-
national Workshop on Natural Lan-
guage Generation .
Hovy, E. H., M. P. Marcus, M. Palmer,
L. A. Ramshaw, and R. Weischedel.
2006. OntoNotes: The 90% solu-
tion. HLT-NAACL .
Hu, M. and B. Liu. 2004a. Mining
and summarizing customer reviews.
KDD .
Hu, M. and B. Liu. 2004b. Mining
and summarizing customer reviews.
SIGKDD-04 .
Huang, E. H., R. Socher, C. D. Man-
ning, and A. Y . Ng. 2012. Improving

--- Page 565 ---
Bibliography 565
word representations via global con-
text and multiple word prototypes.
ACL.
Huang, Z., W. Xu, and K. Yu. 2015.
Bidirectional LSTM-CRF models
for sequence tagging. arXiv preprint
arXiv:1508.01991 .
Huffman, S. 1996. Learning informa-
tion extraction patterns from exam-
ples. In S. Wertmer, E. Riloff, and
G. Scheller, eds, Connectionist, Sta-
tistical, and Symbolic Approaches
to Learning Natural Language Pro-
cessing , 246–260. Springer.
Hunt, A. J. and A. W. Black. 1996.
Unit selection in a concatenative
speech synthesis system using a
large speech database. ICASSP .
Hutchins, W. J. 1986. Machine Trans-
lation: Past, Present, Future . Ellis
Horwood, Chichester, England.
Hutchins, W. J. 1997. From ﬁrst con-
ception to ﬁrst demonstration: The
nascent years of machine transla-
tion, 1947–1954. A chronology. Ma-
chine Translation , 12:192–252.
Hutchins, W. J. and H. L. Somers. 1992.
An Introduction to Machine Transla-
tion. Academic Press.
Hutchinson, B., V . Prabhakaran,
E. Denton, K. Webster, Y . Zhong,
and S. Denuyl. 2020. Social bi-
ases in NLP models as barriers for
persons with disabilities. ACL.
Hymes, D. 1974. Ways of speaking.
In R. Bauman and J. Sherzer, eds,
Explorations in the ethnography of
speaking , 433–451. Cambridge Uni-
versity Press.
Iida, R., K. Inui, H. Takamura, and
Y . Matsumoto. 2003. Incorporating
contextual cues in trainable models
for coreference resolution. EACL
Workshop on The Computational
Treatment of Anaphora .
Irsoy, O. and C. Cardie. 2014. Opin-
ion mining with deep recurrent neu-
ral networks. EMNLP .
Ischen, C., T. Araujo, H. V oorveld,
G. van Noort, and E. Smit. 2019.
Privacy concerns in chatbot interac-
tions. International Workshop on
Chatbot Research and Design .
ISO8601. 2004. Data elements and
interchange formats—information
interchange—representation of
dates and times. Technical report,
International Organization for Stan-
dards (ISO).
Itakura, F. 1975. Minimum prediction
residual principle applied to speech
recognition. IEEE Transactions on
ASSP , ASSP-32:67–72.
Iter, D., K. Guu, L. Lansing, and
D. Jurafsky. 2020. Pretraining
with contrastive sentence objectivesimproves discourse performance of
language models. ACL.
Iter, D., J. Yoon, and D. Jurafsky. 2018.
Automatic detection of incoherent
speech for diagnosing schizophre-
nia. Fifth Workshop on Computa-
tional Linguistics and Clinical Psy-
chology .
Ito, K. and L. Johnson. 2017.
The LJ speech dataset.
https://keithito.com/
LJ-Speech-Dataset/ .
Iyer, S., I. Konstas, A. Cheung, J. Krish-
namurthy, and L. Zettlemoyer. 2017.
Learning a neural semantic parser
from user feedback. ACL.
Iyer, S., X. V . Lin, R. Pasunuru, T. Mi-
haylov, D. Simig, P. Yu, K. Shus-
ter, T. Wang, Q. Liu, P. S. Koura,
X. Li, B. O’Horo, G. Pereyra,
J. Wang, C. Dewan, A. Celikyil-
maz, L. Zettlemoyer, and V . Stoy-
anov. 2022. Opt-iml: Scaling lan-
guage model instruction meta learn-
ing through the lens of generaliza-
tion. ArXiv preprint.
Izacard, G., P. Lewis, M. Lomeli,
L. Hosseini, F. Petroni, T. Schick,
J. Dwivedi-Yu, A. Joulin, S. Riedel,
and E. Grave. 2022. Few-shot learn-
ing with retrieval augmented lan-
guage models. ArXiv preprint.
Jackendoff, R. 1983. Semantics and
Cognition . MIT Press.
Jacobs, P. S. and L. F. Rau. 1990.
SCISOR: A system for extract-
ing information from on-line news.
CACM , 33(11):88–97.
Jaech, A., G. Mulcaire, S. Hathi, M. Os-
tendorf, and N. A. Smith. 2016.
Hierarchical character-word models
for language identiﬁcation. ACL
Workshop on NLP for Social Media .
Jaitly, N., P. Nguyen, A. Senior, and
V . Vanhoucke. 2012. Application of
pretrained deep neural networks to
large vocabulary speech recognition.
INTERSPEECH .
Jauhiainen, T., M. Lui, M. Zampieri,
T. Baldwin, and K. Lind ´en. 2019.
Automatic language identiﬁcation in
texts: A survey. JAIR , 65(1):675–
682.
Jefferson, G. 1972. Side sequences. In
D. Sudnow, ed., Studies in social in-
teraction , 294–333. Free Press, New
York.
Jeffreys, H. 1948. Theory of Probabil-
ity, 2nd edition. Clarendon Press.
Section 3.23.
Jelinek, F. 1969. A fast sequential de-
coding algorithm using a stack. IBM
Journal of Research and Develop-
ment , 13:675–685.
Jelinek, F. 1990. Self-organized lan-
guage modeling for speech recogni-
tion. In A. Waibel and K.-F. Lee,eds, Readings in Speech Recogni-
tion, 450–506. Morgan Kaufmann.
Originally distributed as IBM tech-
nical report in 1985.
Jelinek, F. and R. L. Mercer. 1980.
Interpolated estimation of Markov
source parameters from sparse data.
In E. S. Gelsema and L. N. Kanal,
eds, Proceedings, Workshop on Pat-
tern Recognition in Practice , 381–
397. North Holland.
Jelinek, F., R. L. Mercer, and L. R.
Bahl. 1975. Design of a linguis-
tic statistical decoder for the recog-
nition of continuous speech. IEEE
Transactions on Information The-
ory, IT-21(3):250–256.
Ji, H. and R. Grishman. 2011. Knowl-
edge base population: Successful
approaches and challenges. ACL.
Ji, H., R. Grishman, and H. T. Dang.
2010. Overview of the tac 2011
knowledge base population track.
TAC-11 .
Ji, Y . and J. Eisenstein. 2014. Repre-
sentation learning for text-level dis-
course parsing. ACL.
Ji, Y . and J. Eisenstein. 2015. One vec-
tor is not enough: Entity-augmented
distributed semantics for discourse
relations. TACL , 3:329–344.
Jia, R. and P. Liang. 2016. Data recom-
bination for neural semantic parsing.
ACL.
Jia, S., T. Meng, J. Zhao, and K.-W.
Chang. 2020. Mitigating gender bias
ampliﬁcation in distribution by pos-
terior regularization. ACL.
Johnson, J., M. Douze, and H. J ´egou.
2017. Billion-scale similarity
search with GPUs. ArXiv preprint
arXiv:1702.08734.
Johnson, W. E. 1932. Probability: de-
ductive and inductive problems (ap-
pendix to). Mind , 41(164):421–423.
Johnson-Laird, P. N. 1983. Mental
Models . Harvard University Press,
Cambridge, MA.
Jones, M. P. and J. H. Martin. 1997.
Contextual spelling correction using
latent semantic analysis. ANLP .
Jones, R., A. McCallum, K. Nigam, and
E. Riloff. 1999. Bootstrapping for
text learning tasks. IJCAI-99 Work-
shop on Text Mining: Foundations,
Techniques and Applications .
Jones, T. 2015. Toward a descrip-
tion of African American Vernac-
ular English dialect regions using
“Black Twitter”. American Speech ,
90(4):403–440.
Joos, M. 1950. Description of language
design. JASA , 22:701–708.
Jordan, M. 1986. Serial order: A paral-
lel distributed processing approach.
Technical Report ICS Report 8604,
University of California, San Diego.

--- Page 566 ---
566 Bibliography
Joshi, A. K. and P. Hopely. 1999. A
parser from antiquity. In A. Kor-
nai, ed., Extended Finite State Mod-
els of Language , 6–15. Cambridge
University Press.
Joshi, A. K. and S. Kuhn. 1979. Cen-
tered logic: The role of entity cen-
tered sentence representation in nat-
ural language inferencing. IJCAI-79 .
Joshi, A. K. and S. Weinstein. 1981.
Control of inference: Role of some
aspects of discourse structure – cen-
tering. IJCAI-81 .
Joshi, M., D. Chen, Y . Liu, D. S.
Weld, L. Zettlemoyer, and O. Levy.
2020. SpanBERT: Improving pre-
training by representing and predict-
ing spans. TACL , 8:64–77.
Joshi, M., O. Levy, D. S. Weld, and
L. Zettlemoyer. 2019. BERT for
coreference resolution: Baselines
and analysis. EMNLP .
Joty, S., G. Carenini, and R. T. Ng.
2015. CODRA: A novel discrimi-
native framework for rhetorical anal-
ysis. Computational Linguistics ,
41(3):385–435.
Jurafsky, D. 2014. The Language of
Food . W. W. Norton, New York.
Jurafsky, D., V . Chahuneau, B. R. Rout-
ledge, and N. A. Smith. 2014. Narra-
tive framing of consumer sentiment
in online restaurant reviews. First
Monday , 19(4).
Jurafsky, D., C. Wooters, G. Tajchman,
J. Segal, A. Stolcke, E. Fosler, and
N. Morgan. 1994. The Berkeley
restaurant project. ICSLP .
Jurgens, D., S. M. Mohammad,
P. Turney, and K. Holyoak. 2012.
SemEval-2012 task 2: Measur-
ing degrees of relational similarity.
*SEM 2012 .
Jurgens, D., Y . Tsvetkov, and D. Juraf-
sky. 2017. Incorporating dialectal
variability for socially equitable lan-
guage identiﬁcation. ACL.
Justeson, J. S. and S. M. Katz. 1991.
Co-occurrences of antonymous ad-
jectives and their contexts. Compu-
tational linguistics , 17(1):1–19.
Kalchbrenner, N. and P. Blunsom.
2013. Recurrent continuous transla-
tion models. EMNLP .
Kameyama, M. 1986. A property-
sharing constraint in centering. ACL.
Kamp, H. 1981. A theory of truth and
semantic representation. In J. Groe-
nendijk, T. Janssen, and M. Stokhof,
eds, Formal Methods in the Study
of Language , 189–222. Mathemati-
cal Centre, Amsterdam.
Kamphuis, C., A. P. de Vries,
L. Boytsov, and J. Lin. 2020. Which
BM25 do you mean? a large-scale
reproducibility study of scoringvariants. European Conference on
Information Retrieval .
Kane, S. K., M. R. Morris, A. Paradiso,
and J. Campbell. 2017. “at times
avuncular and cantankerous, with
the reﬂexes of a mongoose”: Un-
derstanding self-expression through
augmentative and alternative com-
munication devices. CSCW .
Kaplan, J., S. McCandlish,
T. Henighan, T. B. Brown, B. Chess,
R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. 2020. Scaling laws
for neural language models. ArXiv
preprint.
Kaplan, R. M. 1973. A general syntac-
tic processor. In R. Rustin, ed., Natu-
ral Language Processing , 193–241.
Algorithmics Press.
Karamanis, N., M. Poesio, C. Mellish,
and J. Oberlander. 2004. Evaluat-
ing centering-based metrics of co-
herence for text structuring using a
reliably annotated corpus. ACL.
Karita, S., N. Chen, T. Hayashi,
T. Hori, H. Inaguma, Z. Jiang,
M. Someki, N. E. Y . Soplin, R. Ya-
mamoto, X. Wang, S. Watanabe,
T. Yoshimura, and W. Zhang. 2019.
A comparative study on transformer
vs RNN in speech applications.
IEEE ASRU-19 .
Karlsson, F., A. V outilainen,
J. Heikkil ¨a, and A. Anttila, eds.
1995. Constraint Grammar: A
Language-Independent System for
Parsing Unrestricted Text . Mouton
de Gruyter.
Karpukhin, V ., B. O ˘guz, S. Min,
P. Lewis, L. Wu, S. Edunov,
D. Chen, and W.-t. Yih. 2020. Dense
passage retrieval for open-domain
question answering. EMNLP .
Karttunen, L. 1969. Discourse refer-
ents. COLING . Preprint No. 70.
Karttunen, L. 1999. Comments on
Joshi. In A. Kornai, ed., Extended
Finite State Models of Language ,
16–18. Cambridge University Press.
Kasami, T. 1965. An efﬁcient recog-
nition and syntax analysis algorithm
for context-free languages. Tech-
nical Report AFCRL-65-758, Air
Force Cambridge Research Labora-
tory, Bedford, MA.
Katz, J. J. and J. A. Fodor. 1963. The
structure of a semantic theory. Lan-
guage , 39:170–210.
Kay, M. 1967. Experiments with a pow-
erful parser. COLING .
Kay, M. 1973. The MIND system.
In R. Rustin, ed., Natural Language
Processing , 155–188. Algorithmics
Press.
Kay, M. 1982. Algorithm schemata and
data structures in syntactic process-
ing. In S. All ´en, ed., Text Process-
ing: Text Analysis and Generation,Text Typology and Attribution , 327–
358. Almqvist and Wiksell, Stock-
holm.
Kay, M. and M. R ¨oscheisen. 1988.
Text-translation alignment. Techni-
cal Report P90-00143, Xerox Palo
Alto Research Center, Palo Alto,
CA.
Kay, M. and M. R ¨oscheisen. 1993.
Text-translation alignment. Compu-
tational Linguistics , 19:121–142.
Kehler, A. 1993. The effect of es-
tablishing coherence in ellipsis and
anaphora resolution. ACL.
Kehler, A. 1994. Temporal relations:
Reference or discourse coherence?
ACL.
Kehler, A. 1997a. Current theories of
centering for pronoun interpretation:
A critical evaluation. Computational
Linguistics , 23(3):467–475.
Kehler, A. 1997b. Probabilistic coref-
erence in information extraction.
EMNLP .
Kehler, A. 2000. Coherence, Reference,
and the Theory of Grammar . CSLI
Publications.
Kehler, A., D. E. Appelt, L. Taylor, and
A. Simma. 2004. The (non)utility
of predicate-argument frequencies
for pronoun interpretation. HLT-
NAACL .
Kehler, A. and H. Rohde. 2013. A prob-
abilistic reconciliation of coherence-
driven and centering-driven theories
of pronoun interpretation. Theoreti-
cal Linguistics , 39(1-2):1–37.
Keller, F. and M. Lapata. 2003. Using
the web to obtain frequencies for un-
seen bigrams. Computational Lin-
guistics , 29:459–484.
Kendall, T. and C. Farrington. 2020.
The Corpus of Regional African
American Language. Version
2020.05. Eugene, OR: The On-
line Resources for African Amer-
ican Language Project. http:
//oraal.uoregon.edu/coraal.
Kennedy, C. and B. K. Boguraev. 1996.
Anaphora for everyone: Pronomi-
nal anaphora resolution without a
parser. COLING .
Khandelwal, U., O. Levy, D. Juraf-
sky, L. Zettlemoyer, and M. Lewis.
2019. Generalization through mem-
orization: Nearest neighbor lan-
guage models. ICLR .
Khattab, O., C. Potts, and M. Zaharia.
2021. Relevance-guided supervision
for OpenQA with ColBERT. TACL ,
9:929–944.
Khattab, O., A. Singhvi, P. Mahesh-
wari, Z. Zhang, K. Santhanam,
S. Haq, A. Sharma, T. T. Joshi,
H. Moazam, H. Miller, M. Zaharia,
and C. Potts. 2024. DSPy: Compil-
ing declarative language model calls
into self-improving pipelines. ICLR .

--- Page 567 ---
Bibliography 567
Khattab, O. and M. Zaharia. 2020. Col-
BERT: Efﬁcient and effective pas-
sage search via contextualized late
interaction over BERT. SIGIR .
Kiela, D., M. Bartolo, Y . Nie,
D. Kaushik, A. Geiger, Z. Wu,
B. Vidgen, G. Prasad, A. Singh,
P. Ringshia, et al. 2021. Dynabench:
Rethinking benchmarking in nlp.
NAACL HLT .
Kiela, D. and S. Clark. 2014. A system-
atic study of semantic vector space
model parameters. EACL 2nd Work-
shop on Continuous Vector Space
Models and their Compositionality
(CVSC) .
Kim, E. 2019. Optimize com-
putational efﬁciency of skip-
gram with negative sampling.
https://aegis4048.github.
io/optimize_computational_
efficiency_of_skip-gram_
with_negative_sampling .
Kim, S. M. and E. H. Hovy. 2004. De-
termining the sentiment of opinions.
COLING .
King, S. 2020. From African Amer-
ican Vernacular English to African
American Language: Rethinking
the study of race and language in
African Americans’ speech. Annual
Review of Linguistics , 6:285–300.
Kingma, D. and J. Ba. 2015. Adam: A
method for stochastic optimization.
ICLR 2015 .
Kintsch, W. and T. A. Van Dijk. 1978.
Toward a model of text comprehen-
sion and production. Psychological
review , 85(5):363–394.
Kiperwasser, E. and Y . Goldberg. 2016.
Simple and accurate dependency
parsing using bidirectional LSTM
feature representations. TACL ,
4:313–327.
Kipper, K., H. T. Dang, and M. Palmer.
2000. Class-based construction of a
verb lexicon. AAAI .
Kiritchenko, S. and S. M. Mohammad.
2017. Best-worst scaling more re-
liable than rating scales: A case
study on sentiment intensity annota-
tion. ACL.
Kiritchenko, S. and S. M. Mohammad.
2018. Examining gender and race
bias in two hundred sentiment anal-
ysis systems. *SEM .
Kiss, T. and J. Strunk. 2006. Unsuper-
vised multilingual sentence bound-
ary detection. Computational Lin-
guistics , 32(4):485–525.
Kitaev, N., S. Cao, and D. Klein.
2019. Multilingual constituency
parsing with self-attention and pre-
training. ACL.
Kitaev, N. and D. Klein. 2018. Con-
stituency parsing with a self-
attentive encoder. ACL.Klatt, D. H. 1975. V oice onset time,
friction, and aspiration in word-
initial consonant clusters. Journal
of Speech and Hearing Research ,
18:686–706.
Klatt, D. H. 1977. Review of the ARPA
speech understanding project. JASA ,
62(6):1345–1366.
Klatt, D. H. 1982. The Klattalk text-to-
speech conversion system. ICASSP .
Kleene, S. C. 1951. Representation of
events in nerve nets and ﬁnite au-
tomata. Technical Report RM-704,
RAND Corporation. RAND Re-
search Memorandum.
Kleene, S. C. 1956. Representation of
events in nerve nets and ﬁnite au-
tomata. In C. Shannon and J. Mc-
Carthy, eds, Automata Studies , 3–41.
Princeton University Press.
Klein, S. and R. F. Simmons. 1963.
A computational approach to gram-
matical coding of English words.
Journal of the ACM , 10(3):334–347.
Knott, A. and R. Dale. 1994. Using
linguistic phenomena to motivate a
set of coherence relations. Discourse
Processes , 18(1):35–62.
Kocijan, V ., A.-M. Cretu, O.-M.
Camburu, Y . Yordanov, and
T. Lukasiewicz. 2019. A surpris-
ingly robust trick for the Winograd
Schema Challenge. ACL.
Kocmi, T., C. Federmann, R. Grund-
kiewicz, M. Junczys-Dowmunt,
H. Matsushita, and A. Menezes.
2021. To ship or not to ship: An
extensive evaluation of automatic
metrics for machine translation.
ArXiv.
Koehn, P. 2005. Europarl: A parallel
corpus for statistical machine trans-
lation. MT summit, vol. 5 .
Koehn, P., H. Hoang, A. Birch,
C. Callison-Burch, M. Federico,
N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bo-
jar, A. Constantin, and E. Herbst.
2006. Moses: Open source toolkit
for statistical machine translation.
ACL.
Koehn, P., F. J. Och, and D. Marcu.
2003. Statistical phrase-based trans-
lation. HLT-NAACL .
Kolhatkar, V ., A. Roussel, S. Dipper,
and H. Zinsmeister. 2018. Anaphora
with non-nominal antecedents in
computational linguistics: A sur-
vey. Computational Linguistics ,
44(3):547–612.
Kreutzer, J., I. Caswell, L. Wang,
A. Wahab, D. van Esch, N. Ulzii-
Orshikh, A. Tapo, N. Subra-
mani, A. Sokolov, C. Sikasote,
M. Setyawan, S. Sarin, S. Samb,
B. Sagot, C. Rivera, A. Rios, I. Pa-
padimitriou, S. Osei, P. O. Suarez,I. Orife, K. Ogueji, A. N. Rubungo,
T. Q. Nguyen, M. M ¨uller, A. M ¨uller,
S. H. Muhammad, N. Muham-
mad, A. Mnyakeni, J. Mirzakhalov,
T. Matangira, C. Leong, N. Lawson,
S. Kudugunta, Y . Jernite, M. Jenny,
O. Firat, B. F. P. Dossou, S. Dlamini,
N. de Silva, S. C ¸ abuk Ballı, S. Bi-
derman, A. Battisti, A. Baruwa,
A. Bapna, P. Baljekar, I. A. Az-
ime, A. Awokoya, D. Ataman,
O. Ahia, O. Ahia, S. Agrawal, and
M. Adeyemi. 2022. Quality at a
glance: An audit of web-crawled
multilingual datasets. TACL , 10:50–
72.
Krovetz, R. 1993. Viewing morphology
as an inference process. SIGIR-93 .
Kruskal, J. B. 1983. An overview of se-
quence comparison. In D. Sankoff
and J. B. Kruskal, eds, Time
Warps, String Edits, and Macro-
molecules: The Theory and Prac-
tice of Sequence Comparison , 1–44.
Addison-Wesley.
Kudo, T. 2018. Subword regularization:
Improving neural network transla-
tion models with multiple subword
candidates. ACL.
Kudo, T. and Y . Matsumoto. 2002.
Japanese dependency analysis using
cascaded chunking. CoNLL .
Kudo, T. and J. Richardson. 2018a.
SentencePiece: A simple and lan-
guage independent subword tok-
enizer and detokenizer for neural
text processing. EMNLP .
Kudo, T. and J. Richardson. 2018b.
SentencePiece: A simple and lan-
guage independent subword tok-
enizer and detokenizer for neural
text processing. EMNLP .
Kullback, S. and R. A. Leibler. 1951.
On information and sufﬁciency.
Annals of Mathematical Statistics ,
22:79–86.
Kulmizev, A., M. de Lhoneux,
J. Gontrum, E. Fano, and J. Nivre.
2019. Deep contextualized word
embeddings in transition-based and
graph-based dependency parsing
- a tale of two parsers revisited.
EMNLP .
Kumar, S. and W. Byrne. 2004. Min-
imum Bayes-risk decoding for sta-
tistical machine translation. HLT-
NAACL .
Kummerfeld, J. K. and D. Klein. 2013.
Error-driven analysis of challenges
in coreference resolution. EMNLP .
Kuno, S. 1965. The predictive ana-
lyzer and a path elimination tech-
nique. CACM , 8(7):453–462.
Kupiec, J. 1992. Robust part-of-speech
tagging using a hidden Markov
model. Computer Speech and Lan-
guage , 6:225–242.

--- Page 568 ---
568 Bibliography
Kurita, K., N. Vyas, A. Pareek, A. W.
Black, and Y . Tsvetkov. 2019. Quan-
tifying social biases in contextual
word representations. 1st ACL Work-
shop on Gender Bias for Natural
Language Processing .
Kuˇcera, H. and W. N. Francis. 1967.
Computational Analysis of Present-
Day American English . Brown Univ.
Press.
Kwiatkowski, T., J. Palomaki, O. Red-
ﬁeld, M. Collins, A. Parikh, C. Al-
berti, D. Epstein, I. Polosukhin,
J. Devlin, K. Lee, K. Toutanova,
L. Jones, M. Kelcey, M.-W. Chang,
A. M. Dai, J. Uszkoreit, Q. Le, and
S. Petrov. 2019. Natural questions:
A benchmark for question answer-
ing research. TACL , 7:452–466.
Lafferty, J. D., A. McCallum, and
F. C. N. Pereira. 2001. Conditional
random ﬁelds: Probabilistic mod-
els for segmenting and labeling se-
quence data. ICML .
Lai, A. and J. Tetreault. 2018. Dis-
course coherence in the wild: A
dataset, evaluation and methods.
SIGDIAL .
Lake, B. M. and G. L. Murphy. 2021.
Word meaning in minds and ma-
chines. Psychological Review . In
press.
Lakoff, G. 1965. On the Nature of Syn-
tactic Irregularity . Ph.D. thesis, In-
diana University. Published as Irreg-
ularity in Syntax . Holt, Rinehart, and
Winston, New York, 1970.
Lakoff, G. 1972. Structural complexity
in fairy tales. In The Study of Man ,
128–50. School of Social Sciences,
University of California, Irvine, CA.
Lakoff, G. and M. Johnson. 1980.
Metaphors We Live By . University
of Chicago Press, Chicago, IL.
Lample, G., M. Ballesteros, S. Subra-
manian, K. Kawakami, and C. Dyer.
2016. Neural architectures for
named entity recognition. NAACL
HLT.
Lample, G. and A. Conneau. 2019.
Cross-lingual language model pre-
training. NeurIPS , volume 32.
Lan, Z., M. Chen, S. Goodman,
K. Gimpel, P. Sharma, and R. Sori-
cut. 2020. ALBERT: A lite BERT
for self-supervised learning of lan-
guage representations. ICLR .
Landauer, T. K., ed. 1995. The Trou-
ble with Computers: Usefulness, Us-
ability, and Productivity . MIT Press.
Landauer, T. K. and S. T. Dumais. 1997.
A solution to Plato’s problem: The
Latent Semantic Analysis theory of
acquisition, induction, and represen-
tation of knowledge. Psychological
Review , 104:211–240.Landauer, T. K., D. Laham, B. Rehder,
and M. E. Schreiner. 1997. How
well can passage meaning be derived
without using word order? A com-
parison of Latent Semantic Analysis
and humans. COGSCI .
Lang, J. and M. Lapata. 2014.
Similarity-driven semantic role in-
duction via graph partitioning. Com-
putational Linguistics , 40(3):633–
669.
Lang, K. J., A. H. Waibel, and G. E.
Hinton. 1990. A time-delay neu-
ral network architecture for isolated
word recognition. Neural networks ,
3(1):23–43.
Lapata, M. 2003. Probabilistic text
structuring: Experiments with sen-
tence ordering. ACL.
Lapesa, G. and S. Evert. 2014. A large
scale evaluation of distributional se-
mantic models: Parameters, interac-
tions and model selection. TACL ,
2:531–545.
Lappin, S. and H. Leass. 1994. An algo-
rithm for pronominal anaphora res-
olution. Computational Linguistics ,
20(4):535–561.
Larsson, S. and D. R. Traum. 2000. In-
formation state and dialogue man-
agement in the trindi dialogue move
engine toolkit. Natural Language
Engineering , 6(323-340):97–114.
Lascarides, A. and N. Asher. 1993.
Temporal interpretation, discourse
relations, and common sense entail-
ment. Linguistics and Philosophy ,
16(5):437–493.
Lawrence, W. 1953. The synthesis of
speech from signals which have a
low information rate. In W. Jackson,
ed., Communication Theory , 460–
469. Butterworth.
LDC. 1998. LDC Catalog: Hub4
project . University of Penn-
sylvania. www.ldc.upenn.edu/
Catalog/LDC98S71.html .
LeCun, Y ., B. Boser, J. S. Denker,
D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel. 1989.
Backpropagation applied to hand-
written zip code recognition. Neural
computation , 1(4):541–551.
Lee, D. D. and H. S. Seung. 1999.
Learning the parts of objects by non-
negative matrix factorization. Na-
ture, 401(6755):788–791.
Lee, H., A. Chang, Y . Peirsman,
N. Chambers, M. Surdeanu, and
D. Jurafsky. 2013. Determin-
istic coreference resolution based
on entity-centric, precision-ranked
rules. Computational Linguistics ,
39(4):885–916.
Lee, H., Y . Peirsman, A. Chang,
N. Chambers, M. Surdeanu, andD. Jurafsky. 2011. Stanford’s multi-
pass sieve coreference resolution
system at the CoNLL-2011 shared
task. CoNLL .
Lee, H., M. Surdeanu, and D. Juraf-
sky. 2017a. A scaffolding approach
to coreference resolution integrat-
ing statistical and rule-based mod-
els.Natural Language Engineering ,
23(5):733–762.
Lee, K., M.-W. Chang, and
K. Toutanova. 2019. Latent re-
trieval for weakly supervised open
domain question answering. ACL.
Lee, K., L. He, M. Lewis, and L. Zettle-
moyer. 2017b. End-to-end neural
coreference resolution. EMNLP .
Lee, K., L. He, and L. Zettlemoyer.
2018. Higher-order coreference
resolution with coarse-to-ﬁne infer-
ence. NAACL HLT .
Lehnert, W. G., C. Cardie, D. Fisher,
E. Riloff, and R. Williams. 1991.
Description of the CIRCUS system
as used for MUC-3. MUC-3 .
Lemon, O., K. Georgila, J. Henderson,
and M. Stuttle. 2006. An ISU di-
alogue system exhibiting reinforce-
ment learning of dialogue policies:
Generic slot-ﬁlling in the TALK in-
car system. EACL .
Levenshtein, V . I. 1966. Binary codes
capable of correcting deletions, in-
sertions, and reversals. Cybernetics
and Control Theory , 10(8):707–710.
Original in Doklady Akademii Nauk
SSSR 163(4): 845–848 (1965).
Levesque, H. 2011. The Winograd
Schema Challenge. Logical Formal-
izations of Commonsense Reason-
ing — Papers from the AAAI 2011
Spring Symposium (SS-11-06) .
Levesque, H., E. Davis, and L. Morgen-
stern. 2012. The Winograd Schema
Challenge. KR-12 .
Levesque, H. J., P. R. Cohen, and
J. H. T. Nunes. 1990. On acting to-
gether. AAAI . Morgan Kaufmann.
Levin, B. 1977. Mapping sentences to
case frames. Technical Report 167,
MIT AI Laboratory. AI Working Pa-
per 143.
Levin, B. 1993. English Verb Classes
and Alternations: A Preliminary In-
vestigation . University of Chicago
Press.
Levin, B. and M. Rappaport Hovav.
2005. Argument Realization . Cam-
bridge University Press.
Levin, E., R. Pieraccini, and W. Eckert.
2000. A stochastic model of human-
machine interaction for learning dia-
log strategies. IEEE Transactions on
Speech and Audio Processing , 8:11–
23.
Levinson, S. C. 1983. Conversational
Analysis , chapter 6. Cambridge Uni-
versity Press.

--- Page 569 ---
Bibliography 569
Levow, G.-A. 1998. Characterizing
and recognizing spoken corrections
in human-computer dialogue. COL-
ING/ACL .
Levy, O. and Y . Goldberg. 2014a.
Dependency-based word embed-
dings. ACL.
Levy, O. and Y . Goldberg. 2014b. Lin-
guistic regularities in sparse and ex-
plicit word representations. CoNLL .
Levy, O. and Y . Goldberg. 2014c. Neu-
ral word embedding as implicit ma-
trix factorization. NeurIPS .
Levy, O., Y . Goldberg, and I. Da-
gan. 2015. Improving distributional
similarity with lessons learned from
word embeddings. TACL , 3:211–
225.
Li, B. Z., S. Min, S. Iyer, Y . Mehdad,
and W.-t. Yih. 2020. Efﬁcient one-
pass end-to-end entity linking for
questions. EMNLP .
Li, J., X. Chen, E. H. Hovy, and D. Ju-
rafsky. 2015. Visualizing and un-
derstanding neural models in NLP.
NAACL HLT .
Li, J. and D. Jurafsky. 2017. Neu-
ral net models of open-domain dis-
course coherence. EMNLP .
Li, J., R. Li, and E. H. Hovy. 2014.
Recursive deep models for discourse
parsing. EMNLP .
Li, J., W. Monroe, A. Ritter, D. Juraf-
sky, M. Galley, and J. Gao. 2016a.
Deep reinforcement learning for di-
alogue generation. EMNLP .
Li, M., J. Weston, and S. Roller. 2019a.
Acute-eval: Improved dialogue eval-
uation with optimized questions and
multi-turn comparisons. NeurIPS19
Workshop on Conversational AI .
Li, Q., T. Li, and B. Chang. 2016b.
Discourse parsing with attention-
based hierarchical neural networks.
EMNLP .
Li, X., Y . Meng, X. Sun, Q. Han,
A. Yuan, and J. Li. 2019b. Is
word segmentation necessary for
deep learning of Chinese representa-
tions? ACL.
Liang, P., R. Bommasani, T. Lee,
D. Tsipras, D. Soylu, M. Yasunaga,
Y . Zhang, D. Narayanan, Y . Wu,
A. Kumar, B. Newman, B. Yuan,
B. Yan, C. Zhang, C. Cosgrove,
C. D. Manning, C. R ´e, D. Acosta-
Navas, D. A. Hudson, E. Zelikman,
E. Durmus, F. Ladhak, F. Rong,
H. Ren, H. Yao, J. Wang, K. San-
thanam, L. Orr, L. Zheng, M. Yuk-
sekgonul, M. Suzgun, N. Kim,
N. Guha, N. Chatterji, O. Khattab,
P. Henderson, Q. Huang, R. Chi,
S. M. Xie, S. Santurkar, S. Ganguli,
T. Hashimoto, T. Icard, T. Zhang,
V . Chaudhary, W. Wang, X. Li,
Y . Mai, Y . Zhang, and Y . Koreeda.2023. Holistic evaluation of lan-
guage models. Transactions on Ma-
chine Learning Research .
Lin, C.-Y . 2004. ROUGE: A pack-
age for automatic evaluation of sum-
maries. ACL 2004 Workshop on Text
Summarization Branches Out .
Lin, D. 2003. Dependency-based eval-
uation of minipar. Workshop on the
Evaluation of Parsing Systems .
Lin, Y ., J.-B. Michel, E. Aiden Lieber-
man, J. Orwant, W. Brockman, and
S. Petrov. 2012a. Syntactic annota-
tions for the Google books NGram
corpus. ACL.
Lin, Y ., J.-B. Michel, E. Lieber-
man Aiden, J. Orwant, W. Brock-
man, and S. Petrov. 2012b. Syntac-
tic annotations for the Google Books
NGram corpus. ACL.
Lin, Z., A. Madotto, J. Shin, P. Xu, and
P. Fung. 2019. MoEL: Mixture of
empathetic listeners. EMNLP .
Lin, Z., M.-Y . Kan, and H. T. Ng. 2009.
Recognizing implicit discourse rela-
tions in the Penn Discourse Tree-
bank. EMNLP .
Lin, Z., H. T. Ng, and M.-Y . Kan. 2011.
Automatically evaluating text coher-
ence using discourse relations. ACL.
Lin, Z., H. T. Ng, and M.-Y . Kan. 2014.
A pdtb-styled end-to-end discourse
parser. Natural Language Engineer-
ing, 20(2):151–184.
Ling, W., C. Dyer, A. W. Black,
I. Trancoso, R. Fermandez, S. Amir,
L. Marujo, and T. Lu ´ıs. 2015. Find-
ing function in form: Compositional
character models for open vocabu-
lary word representation. EMNLP .
Linzen, T. 2016. Issues in evaluating se-
mantic spaces using word analogies.
1st Workshop on Evaluating Vector-
Space Representations for NLP .
Lison, P. and J. Tiedemann. 2016.
Opensubtitles2016: Extracting large
parallel corpora from movie and tv
subtitles. LREC .
Litman, D. J. 1985. Plan Recognition
and Discourse Analysis: An Inte-
grated Approach for Understanding
Dialogues . Ph.D. thesis, University
of Rochester, Rochester, NY .
Litman, D. J. and J. Allen. 1987. A plan
recognition model for subdialogues
in conversation. Cognitive Science ,
11:163–200.
Litman, D. J., M. A. Walker, and
M. Kearns. 1999. Automatic detec-
tion of poor speech recognition at
the dialogue level. ACL.
Liu, B. and L. Zhang. 2012. A sur-
vey of opinion mining and sentiment
analysis. In C. C. Aggarwal and
C. Zhai, eds, Mining text data , 415–
464. Springer.Liu, H., J. Dacon, W. Fan, H. Liu,
Z. Liu, and J. Tang. 2020. Does gen-
der matter? Towards fairness in dia-
logue systems. COLING .
Liu, J., S. Min, L. Zettlemoyer, Y . Choi,
and H. Hajishirzi. 2024. Inﬁni-gram:
Scaling unbounded n-gram language
models to a trillion tokens. ArXiv
preprint.
Liu, Y ., C. Sun, L. Lin, and X. Wang.
2016. Learning natural language
inference using bidirectional LSTM
model and inner-attention. ArXiv.
Liu, Y ., P. Fung, Y . Yang, C. Cieri,
S. Huang, and D. Graff. 2006.
HKUST/MTS: A very large scale
Mandarin telephone speech corpus.
International Conference on Chi-
nese Spoken Language Processing .
Liu, Y ., M. Ott, N. Goyal, J. Du,
M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and
V . Stoyanov. 2019. RoBERTa:
A robustly optimized BERT pre-
training approach. ArXiv preprint
arXiv:1907.11692.
Llama Team. 2024. The llama 3 herd of
models.
Lochbaum, K. E., B. J. Grosz, and
C. L. Sidner. 2000. Discourse struc-
ture and intention recognition. In
R. Dale, H. Moisl, and H. L. Somers,
eds,Handbook of Natural Language
Processing . Marcel Dekker.
Logeswaran, L., H. Lee, and D. Radev.
2018. Sentence ordering and coher-
ence modeling using recurrent neu-
ral networks. AAAI .
Longpre, S., L. Hou, T. Vu, A. Webson,
H. W. Chung, Y . Tay, D. Zhou, Q. V .
Le, B. Zoph, J. Wei, and A. Roberts.
2023. The Flan collection: Design-
ing data and methods for effective
instruction tuning. ICML .
Longpre, S., R. Mahari, A. Lee,
C. Lund, H. Oderinwale, W. Bran-
non, N. Saxena, N. Obeng-Marnu,
T. South, C. Hunter, et al. 2024a.
Consent in crisis: The rapid decline
of the ai data commons. ArXiv
preprint.
Longpre, S., G. Yauney, E. Reif, K. Lee,
A. Roberts, B. Zoph, D. Zhou,
J. Wei, K. Robinson, D. Mimno, and
D. Ippolito. 2024b. A pretrainer’s
guide to training data: Measuring
the effects of data age, domain cov-
erage, quality, & toxicity. NAACL
HLT.
Louis, A. and A. Nenkova. 2012. A
coherence model based on syntactic
patterns. EMNLP .
Loureiro, D. and A. Jorge. 2019.
Language modelling makes sense:
Propagating representations through
WordNet for full-coverage word
sense disambiguation. ACL.

--- Page 570 ---
570 Bibliography
Louviere, J. J., T. N. Flynn, and A. A. J.
Marley. 2015. Best-worst scaling:
Theory, methods and applications .
Cambridge University Press.
Lovins, J. B. 1968. Development of
a stemming algorithm. Mechanical
Translation and Computational Lin-
guistics , 11(1–2):9–13.
Lowerre, B. T. 1976. The Harpy Speech
Recognition System . Ph.D. thesis,
Carnegie Mellon University, Pitts-
burgh, PA.
Luhn, H. P. 1957. A statistical ap-
proach to the mechanized encoding
and searching of literary informa-
tion. IBM Journal of Research and
Development , 1(4):309–317.
Lui, M. and T. Baldwin. 2011. Cross-
domain feature selection for lan-
guage identiﬁcation. IJCNLP .
Lui, M. and T. Baldwin. 2012.
langid.py : An off-the-shelf lan-
guage identiﬁcation tool. ACL.
Lukasik, M., B. Dadachev, K. Papineni,
and G. Sim ˜oes. 2020. Text seg-
mentation by cross segment atten-
tion. EMNLP .
Luo, X. 2005. On coreference resolu-
tion performance metrics. EMNLP .
Luo, X. and S. Pradhan. 2016. Eval-
uation metrics. In M. Poesio,
R. Stuckardt, and Y . Versley, eds,
Anaphora resolution: Algorithms,
resources, and applications , 141–
163. Springer.
Luo, X., S. Pradhan, M. Recasens, and
E. H. Hovy. 2014. An extension of
BLANC to system mentions. ACL.
Ma, X. and E. H. Hovy. 2016. End-
to-end sequence labeling via bi-
directional LSTM-CNNs-CRF.
ACL.
Maas, A., Z. Xie, D. Jurafsky, and A. Y .
Ng. 2015. Lexicon-free conversa-
tional speech recognition with neu-
ral networks. NAACL HLT .
Maas, A. L., A. Y . Hannun, and A. Y .
Ng. 2013. Rectiﬁer nonlineari-
ties improve neural network acoustic
models. ICML .
Maas, A. L., P. Qi, Z. Xie, A. Y . Han-
nun, C. T. Lengerich, D. Jurafsky,
and A. Y . Ng. 2017. Building dnn
acoustic models for large vocabu-
lary speech recognition. Computer
Speech & Language , 41:195–213.
Magerman, D. M. 1995. Statisti-
cal decision-tree models for parsing.
ACL.
Mairesse, F. and M. A. Walker. 2008.
Trainable generation of big-ﬁve per-
sonality styles through data-driven
parameter estimation. ACL.
Mann, W. C. and S. A. Thompson.
1987. Rhetorical structure theory: Atheory of text organization. Techni-
cal Report RS-87-190, Information
Sciences Institute.
Manning, C. D. 2011. Part-of-speech
tagging from 97% to 100%: Is it
time for some linguistics? CICLing
2011 .
Manning, C. D., P. Raghavan, and
H. Sch ¨utze. 2008. Introduction to In-
formation Retrieval . Cambridge.
Manning, C. D., M. Surdeanu, J. Bauer,
J. Finkel, S. Bethard, and D. Mc-
Closky. 2014. The Stanford
CoreNLP natural language process-
ing toolkit. ACL.
Marcu, D. 1997. The rhetorical parsing
of natural language texts. ACL.
Marcu, D. 1999. A decision-based ap-
proach to rhetorical parsing. ACL.
Marcu, D. 2000a. The rhetorical pars-
ing of unrestricted texts: A surface-
based approach. Computational Lin-
guistics , 26(3):395–448.
Marcu, D., ed. 2000b. The Theory and
Practice of Discourse Parsing and
Summarization . MIT Press.
Marcu, D. and A. Echihabi. 2002. An
unsupervised approach to recogniz-
ing discourse relations. ACL.
Marcu, D. and W. Wong. 2002.
A phrase-based, joint probability
model for statistical machine trans-
lation. EMNLP .
Marcus, M. P. 1980. A Theory of Syn-
tactic Recognition for Natural Lan-
guage . MIT Press.
Marcus, M. P., B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a
large annotated corpus of English:
The Penn treebank. Computational
Linguistics , 19(2):313–330.
Marie, B., A. Fujita, and R. Rubino.
2021. Scientiﬁc credibility of ma-
chine translation research: A meta-
evaluation of 769 papers. ACL.
Markov, A. A. 1913. Essai d’une
recherche statistique sur le texte du
roman “Eugene Onegin” illustrant la
liaison des epreuve en chain (‘Ex-
ample of a statistical investigation
of the text of “Eugene Onegin” il-
lustrating the dependence between
samples in chain’). Izvistia Impera-
torskoi Akademii Nauk (Bulletin de
l’Acad ´emie Imp ´eriale des Sciences
de St.-P ´etersbourg) , 7:153–162.
de Marneffe, M.-C., T. Dozat, N. Sil-
veira, K. Haverinen, F. Ginter,
J. Nivre, and C. D. Manning. 2014.
Universal Stanford dependencies: A
cross-linguistic typology. LREC .
de Marneffe, M.-C., B. MacCartney,
and C. D. Manning. 2006. Gener-
ating typed dependency parses from
phrase structure parses. LREC .de Marneffe, M.-C. and C. D. Man-
ning. 2008. The Stanford typed de-
pendencies representation. COLING
Workshop on Cross-Framework and
Cross-Domain Parser Evaluation .
de Marneffe, M.-C., C. D. Manning,
J. Nivre, and D. Zeman. 2021. Uni-
versal Dependencies. Computa-
tional Linguistics , 47(2):255–308.
de Marneffe, M.-C., M. Recasens, and
C. Potts. 2015. Modeling the lifes-
pan of discourse entities with ap-
plication to coreference resolution.
JAIR , 52:445–475.
Maron, M. E. 1961. Automatic index-
ing: an experimental inquiry. Jour-
nal of the ACM , 8(3):404–417.
M`arquez, L., X. Carreras, K. C.
Litkowski, and S. Stevenson. 2008.
Semantic role labeling: An introduc-
tion to the special issue. Computa-
tional linguistics , 34(2):145–159.
Marshall, I. 1983. Choice of grammat-
ical word-class without global syn-
tactic analysis: Tagging words in the
LOB corpus. Computers and the Hu-
manities , 17:139–150.
Marshall, I. 1987. Tag selection using
probabilistic methods. In R. Garside,
G. Leech, and G. Sampson, eds, The
Computational Analysis of English ,
42–56. Longman.
Martschat, S. and M. Strube. 2014. Re-
call error analysis for coreference
resolution. EMNLP .
Martschat, S. and M. Strube. 2015. La-
tent structures for coreference reso-
lution. TACL , 3:405–418.
Mathis, D. A. and M. C. Mozer. 1995.
On the computational utility of con-
sciousness. NeurIPS . MIT Press.
McCallum, A., D. Freitag, and F. C. N.
Pereira. 2000. Maximum entropy
Markov models for information ex-
traction and segmentation. ICML .
McCallum, A. and W. Li. 2003. Early
results for named entity recogni-
tion with conditional random ﬁelds,
feature induction and web-enhanced
lexicons. CoNLL .
McCallum, A. and K. Nigam. 1998.
A comparison of event models
for naive bayes text classiﬁcation.
AAAI/ICML-98 Workshop on Learn-
ing for Text Categorization .
McCarthy, J. F. and W. G. Lehnert.
1995. Using decision trees for coref-
erence resolution. IJCAI-95 .
McClelland, J. L. and J. L. Elman.
1986. The TRACE model of speech
perception. Cognitive Psychology ,
18:1–86.
McClelland, J. L. and D. E. Rumel-
hart, eds. 1986. Parallel Dis-
tributed Processing: Explorations
in the Microstructure of Cognition ,
volume 2: Psychological and Bio-
logical Models . MIT Press.

--- Page 571 ---
Bibliography 571
McCulloch, W. S. and W. Pitts. 1943. A
logical calculus of ideas immanent
in nervous activity. Bulletin of Math-
ematical Biophysics , 5:115–133.
McDonald, R., K. Crammer, and
F. C. N. Pereira. 2005a. Online
large-margin training of dependency
parsers. ACL.
McDonald, R. and J. Nivre. 2011. An-
alyzing and integrating dependency
parsers. Computational Linguistics ,
37(1):197–230.
McDonald, R., F. C. N. Pereira, K. Rib-
arov, and J. Haji ˇc. 2005b. Non-
projective dependency parsing us-
ing spanning tree algorithms. HLT-
EMNLP .
McGufﬁe, K. and A. Newhouse.
2020. The radicalization risks of
GPT-3 and advanced neural lan-
guage models. ArXiv preprint
arXiv:2009.06807.
McLuhan, M. 1964. Understanding
Media: The Extensions of Man . New
American Library.
Melamud, O., J. Goldberger, and I. Da-
gan. 2016. context2vec: Learn-
ing generic context embedding with
bidirectional LSTM. CoNLL .
Merialdo, B. 1994. Tagging En-
glish text with a probabilistic
model. Computational Linguistics ,
20(2):155–172.
Mesgar, M. and M. Strube. 2016. Lexi-
cal coherence graph modeling using
word embeddings. ACL.
Metsis, V ., I. Androutsopoulos, and
G. Paliouras. 2006. Spam ﬁlter-
ing with naive bayes-which naive
bayes? CEAS .
Meyers, A., R. Reeves, C. Macleod,
R. Szekely, V . Zielinska, B. Young,
and R. Grishman. 2004. The nom-
bank project: An interim report.
NAACL/HLT Workshop: Frontiers in
Corpus Annotation .
Mihalcea, R. and A. Csomai. 2007.
Wikify!: Linking documents to en-
cyclopedic knowledge. CIKM 2007 .
Mikheev, A., M. Moens, and C. Grover.
1999. Named entity recognition
without gazetteers. EACL .
Mikolov, T. 2012. Statistical lan-
guage models based on neural net-
works . Ph.D. thesis, Brno University
of Technology.
Mikolov, T., K. Chen, G. S. Corrado,
and J. Dean. 2013a. Efﬁcient estima-
tion of word representations in vec-
tor space. ICLR 2013 .
Mikolov, T., M. Karaﬁ ´at, L. Bur-
get, J. ˇCernock `y, and S. Khudan-
pur. 2010. Recurrent neural net-
work based language model. IN-
TERSPEECH .Mikolov, T., S. Kombrink, L. Burget,
J. H. ˇCernock `y, and S. Khudanpur.
2011. Extensions of recurrent neural
network language model. ICASSP .
Mikolov, T., I. Sutskever, K. Chen,
G. S. Corrado, and J. Dean. 2013b.
Distributed representations of words
and phrases and their compositional-
ity.NeurIPS .
Mikolov, T., W.-t. Yih, and G. Zweig.
2013c. Linguistic regularities in
continuous space word representa-
tions. NAACL HLT .
Miller, G. A. and J. G. Beebe-Center.
1956. Some psychological methods
for evaluating the quality of trans-
lations. Mechanical Translation ,
3:73–80.
Miller, G. A. and W. G. Charles. 1991.
Contextual correlates of semantics
similarity. Language and Cognitive
Processes , 6(1):1–28.
Miller, G. A. and N. Chomsky. 1963.
Finitary models of language users.
In R. D. Luce, R. R. Bush, and
E. Galanter, eds, Handbook of Math-
ematical Psychology , volume II,
419–491. John Wiley.
Miller, G. A. and J. A. Selfridge.
1950. Verbal context and the recall
of meaningful material. American
Journal of Psychology , 63:176–185.
Miller, S., R. J. Bobrow, R. Ingria, and
R. Schwartz. 1994. Hidden under-
standing models of natural language.
ACL.
Milne, D. and I. H. Witten. 2008.
Learning to link with wikipedia.
CIKM 2008 .
Miltsakaki, E., R. Prasad, A. K. Joshi,
and B. L. Webber. 2004. The Penn
Discourse Treebank. LREC .
Min, S., X. Lyu, A. Holtzman,
M. Artetxe, M. Lewis, H. Hajishirzi,
and L. Zettlemoyer. 2022. Rethink-
ing the role of demonstrations: What
makes in-context learning work?
EMNLP .
Minsky, M. 1961. Steps toward artiﬁ-
cial intelligence. Proceedings of the
IRE, 49(1):8–30.
Minsky, M. 1974. A framework for rep-
resenting knowledge. Technical Re-
port 306, MIT AI Laboratory. Memo
306.
Minsky, M. and S. Papert. 1969. Per-
ceptrons . MIT Press.
Mintz, M., S. Bills, R. Snow, and D. Ju-
rafsky. 2009. Distant supervision for
relation extraction without labeled
data. ACL IJCNLP .
Mirza, P. and S. Tonelli. 2016.
CATENA: CAusal and TEmporal
relation extraction from NAtural
language texts. COLING .Mishra, S., D. Khashabi, C. Baral,
and H. Hajishirzi. 2022. Cross-task
generalization via natural language
crowdsourcing instructions. ACL.
Mitchell, M., S. Wu, A. Zal-
divar, P. Barnes, L. Vasserman,
B. Hutchinson, E. Spitzer, I. D. Raji,
and T. Gebru. 2019. Model cards for
model reporting. ACM FAccT .
Mitkov, R. 2002. Anaphora Resolution .
Longman.
Mohamed, A., G. E. Dahl, and G. E.
Hinton. 2009. Deep Belief Networks
for phone recognition. NIPS Work-
shop on Deep Learning for Speech
Recognition and Related Applica-
tions .
Mohammad, S. M. 2018a. Obtaining
reliable human ratings of valence,
arousal, and dominance for 20,000
English words. ACL.
Mohammad, S. M. 2018b. Word affect
intensities. LREC .
Mohammad, S. M. and P. D. Tur-
ney. 2013. Crowdsourcing a word-
emotion association lexicon. Com-
putational Intelligence , 29(3):436–
465.
Monroe, B. L., M. P. Colaresi, and
K. M. Quinn. 2008. Fightin’words:
Lexical feature selection and evalu-
ation for identifying the content of
political conﬂict. Political Analysis ,
16(4):372–403.
Moors, A., P. C. Ellsworth, K. R.
Scherer, and N. H. Frijda. 2013. Ap-
praisal theories of emotion: State
of the art and future development.
Emotion Review , 5(2):119–124.
Moosavi, N. S. and M. Strube. 2016.
Which coreference evaluation met-
ric do you trust? A proposal for a
link-based entity aware metric. ACL.
Morey, M., P. Muller, and N. Asher.
2017. How much progress have we
made on RST discourse parsing? a
replication study of recent results on
the rst-dt. EMNLP .
Morgan, A. A., L. Hirschman,
M. Colosimo, A. S. Yeh, and J. B.
Colombe. 2004. Gene name iden-
tiﬁcation and normalization using a
model organism database. Journal of
Biomedical Informatics , 37(6):396–
410.
Morgan, N. and H. Bourlard. 1990.
Continuous speech recognition us-
ing multilayer perceptrons with hid-
den markov models. ICASSP .
Morgan, N. and H. A. Bourlard.
1995. Neural networks for sta-
tistical recognition of continuous
speech. Proceedings of the IEEE ,
83(5):742–772.

--- Page 572 ---
572 Bibliography
Morris, J. and G. Hirst. 1991. Lexical
cohesion computed by thesaural re-
lations as an indicator of the struc-
ture of text. Computational Linguis-
tics, 17(1):21–48.
Mosteller, F. and D. L. Wallace. 1963.
Inference in an authorship problem:
A comparative study of discrimina-
tion methods applied to the author-
ship of the disputed federalist pa-
pers. Journal of the American Statis-
tical Association , 58(302):275–309.
Mosteller, F. and D. L. Wallace. 1964.
Inference and Disputed Authorship:
The Federalist . Springer-Verlag.
1984 2nd edition: Applied Bayesian
and Classical Inference .
Mrkˇsi´c, N., D. ´O S´eaghdha, T.-H. Wen,
B. Thomson, and S. Young. 2017.
Neural belief tracker: Data-driven
dialogue state tracking. ACL.
Muller, P., C. Braud, and M. Morey.
2019. ToNy: Contextual embed-
dings for accurate multilingual dis-
course segmentation of full docu-
ments. Workshop on Discourse Re-
lation Parsing and Treebanking .
Murphy, K. P. 2012. Machine learning:
A probabilistic perspective . MIT
Press.
Musi, E., M. Stede, L. Kriese, S. Mure-
san, and A. Rocci. 2018. A multi-
layer annotated corpus of argumen-
tative text: From argument schemes
to discourse relations. LREC .
Myers, G. 1992. “In this paper we
report...”: Speech acts and scien-
tiﬁc facts. Journal of Pragmatics ,
17(4):295–313.
N´adas, A. 1984. Estimation of prob-
abilities in the language model of
the IBM speech recognition sys-
tem. IEEE Transactions on ASSP ,
32(4):859–861.
Nadeem, M., A. Bethke, and S. Reddy.
2021. StereoSet: Measuring stereo-
typical bias in pretrained language
models. ACL.
Nagata, M. and T. Morimoto. 1994.
First steps toward statistical model-
ing of dialogue to predict the speech
act type of the next utterance. Speech
Communication , 15:193–203.
Nallapati, R., B. Zhou, C. dos San-
tos, C ¸ . Gulc ¸ehre, and B. Xiang.
2016. Abstractive text summa-
rization using sequence-to-sequence
RNNs and beyond. CoNLL .
Nash-Webber, B. L. 1975. The role of
semantics in automatic speech un-
derstanding. In D. G. Bobrow and
A. Collins, eds, Representation and
Understanding , 351–382. Academic
Press.
Naur, P., J. W. Backus, F. L. Bauer,
J. Green, C. Katz, J. McCarthy, A. J.Perlis, H. Rutishauser, K. Samelson,
B. Vauquois, J. H. Wegstein, A. van
Wijnagaarden, and M. Woodger.
1960. Report on the algorith-
mic language ALGOL 60. CACM ,
3(5):299–314. Revised in CACM
6:1, 1-17, 1963.
Nayak, N., D. Hakkani-T ¨ur, M. A.
Walker, and L. P. Heck. 2017. To
plan or not to plan? discourse
planning in slot-value informed se-
quence to sequence models for lan-
guage generation. INTERSPEECH .
Neff, G. and P. Nagy. 2016. Talking
to bots: Symbiotic agency and the
case of Tay. International Journal
of Communication , 10:4915–4931.
Ng, A. Y . and M. I. Jordan. 2002. On
discriminative vs. generative classi-
ﬁers: A comparison of logistic re-
gression and naive bayes. NeurIPS .
Ng, H. T., L. H. Teo, and J. L. P. Kwan.
2000. A machine learning approach
to answering questions for reading
comprehension tests. EMNLP .
Ng, V . 2004. Learning noun phrase
anaphoricity to improve coreference
resolution: Issues in representation
and optimization. ACL.
Ng, V . 2005a. Machine learning for
coreference resolution: From lo-
cal classiﬁcation to global ranking.
ACL.
Ng, V . 2005b. Supervised ranking
for pronoun resolution: Some recent
improvements. AAAI .
Ng, V . 2010. Supervised noun phrase
coreference research: The ﬁrst ﬁf-
teen years. ACL.
Ng, V . 2017. Machine learning for en-
tity coreference resolution: A retro-
spective look at two decades of re-
search. AAAI .
Ng, V . and C. Cardie. 2002a. Identi-
fying anaphoric and non-anaphoric
noun phrases to improve coreference
resolution. COLING .
Ng, V . and C. Cardie. 2002b. Improv-
ing machine learning approaches to
coreference resolution. ACL.
Nguyen, D. T. and S. Joty. 2017. A neu-
ral local coherence model. ACL.
Nickerson, R. S. 1976. On con-
versational interaction with comput-
ers. Proceedings of the ACM/SIG-
GRAPH workshop on User-oriented
design of interactive graphics sys-
tems.
Nie, A., E. Bennett, and N. Good-
man. 2019. DisSent: Learning sen-
tence representations from explicit
discourse relations. ACL.
Nielsen, J. 1992. The usability engi-
neering life cycle. IEEE Computer ,
25(3):12–22.Nielsen, M. A. 2015. Neural networks
and Deep learning . Determination
Press USA.
Nigam, K., J. D. Lafferty, and A. Mc-
Callum. 1999. Using maximum en-
tropy for text classiﬁcation. IJCAI-
99 workshop on machine learning
for information ﬁltering .
Nirenburg, S., H. L. Somers, and
Y . Wilks, eds. 2002. Readings in
Machine Translation . MIT Press.
Nissim, M., S. Dingare, J. Carletta, and
M. Steedman. 2004. An annotation
scheme for information status in di-
alogue. LREC .
NIST. 2005. Speech recognition
scoring toolkit (sctk) version 2.1.
http://www.nist.gov/speech/
tools/ .
NIST. 2007. Matched Pairs Sentence-
Segment Word Error (MAPSSWE)
Test.
Nivre, J. 2007. Incremental non-
projective dependency parsing.
NAACL-HLT .
Nivre, J. 2003. An efﬁcient algorithm
for projective dependency parsing.
Proceedings of the 8th International
Workshop on Parsing Technologies
(IWPT) .
Nivre, J. 2006. Inductive Dependency
Parsing . Springer.
Nivre, J. 2009. Non-projective de-
pendency parsing in expected linear
time. ACL IJCNLP .
Nivre, J., J. Hall, S. K ¨ubler, R. Mc-
Donald, J. Nilsson, S. Riedel, and
D. Yuret. 2007a. The conll 2007
shared task on dependency parsing.
EMNLP/CoNLL .
Nivre, J., J. Hall, J. Nilsson, A. Chanev,
G. Eryigit, S. K ¨ubler, S. Mari-
nov, and E. Marsi. 2007b. Malt-
parser: A language-independent
system for data-driven dependency
parsing. Natural Language Engi-
neering , 13(02):95–135.
Nivre, J. and J. Nilsson. 2005. Pseudo-
projective dependency parsing. ACL.
Nivre, J. and M. Scholz. 2004. Deter-
ministic dependency parsing of en-
glish text. COLING .
Niwa, Y . and Y . Nitta. 1994. Co-
occurrence vectors from corpora vs.
distance vectors from dictionaries.
COLING .
Noreen, E. W. 1989. Computer Inten-
sive Methods for Testing Hypothesis .
Wiley.
Norman, D. A. 1988. The Design of Ev-
eryday Things . Basic Books.
Norvig, P. 1991. Techniques for au-
tomatic memoization with applica-
tions to context-free parsing. Com-
putational Linguistics , 17(1):91–98.

--- Page 573 ---
Bibliography 573
Nosek, B. A., M. R. Banaji, and
A. G. Greenwald. 2002a. Harvest-
ing implicit group attitudes and be-
liefs from a demonstration web site.
Group Dynamics: Theory, Research,
and Practice , 6(1):101.
Nosek, B. A., M. R. Banaji, and A. G.
Greenwald. 2002b. Math=male,
me=female, therefore math 6=me.
Journal of personality and social
psychology , 83(1):44.
Nostalgebraist. 2020. Interpreting gpt:
the logit lens. White paper.
Ocal, M., A. Perez, A. Radas, and
M. Finlayson. 2022. Holistic eval-
uation of automatic TimeML anno-
tators. LREC .
Och, F. J. 1998. Ein beispiels-
basierter und statistischer Ansatz
zum maschinellen Lernen von
nat¨urlichsprachlicher ¨Ubersetzung .
Ph.D. thesis, Universit ¨at Erlangen-
N¨urnberg, Germany. Diplomarbeit
(diploma thesis).
Och, F. J. 2003. Minimum error rate
training in statistical machine trans-
lation. ACL.
Och, F. J. and H. Ney. 2002. Discrim-
inative training and maximum en-
tropy models for statistical machine
translation. ACL.
Och, F. J. and H. Ney. 2003. A system-
atic comparison of various statistical
alignment models. Computational
Linguistics , 29(1):19–51.
Och, F. J. and H. Ney. 2004. The align-
ment template approach to statistical
machine translation. Computational
Linguistics , 30(4):417–449.
O’Connor, B., M. Krieger, and D. Ahn.
2010. Tweetmotif: Exploratory
search and topic summarization for
twitter. ICWSM .
Olive, J. P. 1977. Rule synthe-
sis of speech from dyadic units.
ICASSP77 .
Olsson, C., N. Elhage, N. Nanda,
N. Joseph, N. DasSarma,
T. Henighan, B. Mann, A. Askell,
Y . Bai, A. Chen, et al. 2022. In-
context learning and induction
heads. ArXiv preprint.
Olteanu, A., F. Diaz, and G. Kazai.
2020. When are search completion
suggestions problematic? CSCW .
van den Oord, A., S. Dieleman, H. Zen,
K. Simonyan, O. Vinyals, A. Graves,
N. Kalchbrenner, A. Senior, and
K. Kavukcuoglu. 2016. WaveNet:
A Generative Model for Raw Audio.
ISCA Workshop on Speech Synthesis
Workshop .
Oppenheim, A. V ., R. W. Schafer, and
T. G. J. Stockham. 1968. Nonlinear
ﬁltering of multiplied and convolved
signals. Proceedings of the IEEE ,
56(8):1264–1291.Oravecz, C. and P. Dienes. 2002. Ef-
ﬁcient stochastic part-of-speech tag-
ging for Hungarian. LREC .
Osgood, C. E., G. J. Suci, and P. H. Tan-
nenbaum. 1957. The Measurement
of Meaning . University of Illinois
Press.
Ouyang, L., J. Wu, X. Jiang,
D. Almeida, C. Wainwright,
P. Mishkin, C. Zhang, S. Agar-
wal, K. Slama, A. Ray, J. Schul-
man, J. Hilton, F. Kelton, L. Miller,
M. Simens, A. Askell, P. Welinder,
P. Christiano, J. Leike, and R. Lowe.
2022. Training language models
to follow instructions with human
feedback. NeurIPS , volume 35.
Packard, D. W. 1973. Computer-
assisted morphological analysis of
ancient Greek. COLING .
Palmer, D. 2012. Text preprocessing.
In N. Indurkhya and F. J. Damerau,
eds,Handbook of Natural Language
Processing , 9–30. CRC Press.
Palmer, M., D. Gildea, and N. Xue.
2010. Semantic role labeling. Syn-
thesis Lectures on Human Language
Technologies , 3(1):1–103.
Palmer, M., P. Kingsbury, and
D. Gildea. 2005. The proposi-
tion bank: An annotated corpus
of semantic roles. Computational
Linguistics , 31(1):71–106.
Panayotov, V ., G. Chen, D. Povey, and
S. Khudanpur. 2015. Librispeech: an
ASR corpus based on public domain
audio books. ICASSP .
Pang, B. and L. Lee. 2008. Opin-
ion mining and sentiment analysis.
Foundations and trends in informa-
tion retrieval , 2(1-2):1–135.
Pang, B., L. Lee, and S. Vaithyanathan.
2002. Thumbs up? Sentiment
classiﬁcation using machine learn-
ing techniques. EMNLP .
Paolino, J. 2017. Google Home
vs Alexa: Two simple user
experience design gestures
that delighted a female user.
Medium . Jan 4, 2017. https:
//medium.com/startup-grind/
google-home-vs-alexa-56e26f69ac77 .
Papadimitriou, I., K. Lopez, and D. Ju-
rafsky. 2023. Multilingual BERT has
an accent: Evaluating English in-
ﬂuences on ﬂuency in multilingual
models. EACL Findings .
Papineni, K., S. Roukos, T. Ward, and
W.-J. Zhu. 2002. Bleu: A method
for automatic evaluation of machine
translation. ACL.
Park, J. H., J. Shin, and P. Fung. 2018.
Reducing gender bias in abusive lan-
guage detection. EMNLP .
Park, J. and C. Cardie. 2014. Identify-
ing appropriate support for proposi-
tions in online user comments. First
workshop on argumentation mining .Parrish, A., A. Chen, N. Nangia, V . Pad-
makumar, J. Phang, J. Thompson,
P. M. Htut, and S. Bowman. 2022.
BBQ: A hand-built bias benchmark
for question answering. Findings of
ACL 2022 .
Paszke, A., S. Gross, S. Chintala,
G. Chanan, E. Yang, Z. DeVito,
Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. 2017. Automatic dif-
ferentiation in pytorch. NIPS-W .
Pearl, C. 2017. Designing Voice User
Interfaces: Principles of Conversa-
tional Experiences . O’Reilly.
Peldszus, A. and M. Stede. 2013. From
argument diagrams to argumentation
mining in texts: A survey. In-
ternational Journal of Cognitive In-
formatics and Natural Intelligence
(IJCINI) , 7(1):1–31.
Peldszus, A. and M. Stede. 2016. An
annotated corpus of argumentative
microtexts. 1st European Confer-
ence on Argumentation .
Penn, G. and P. Kiparsky. 2012. On
P¯an.ini and the generative capacity of
contextualized replacement systems.
COLING .
Pennebaker, J. W., R. J. Booth, and
M. E. Francis. 2007. Linguistic In-
quiry and Word Count: LIWC 2007 .
Austin, TX.
Pennington, J., R. Socher, and C. D.
Manning. 2014. GloVe: Global
vectors for word representation.
EMNLP .
Percival, W. K. 1976. On the his-
torical source of immediate con-
stituent analysis. In J. D. McCawley,
ed., Syntax and Semantics Volume
7, Notes from the Linguistic Under-
ground , 229–242. Academic Press.
Perrault, C. R. and J. Allen. 1980.
A plan-based analysis of indirect
speech acts. American Journal
of Computational Linguistics , 6(3-
4):167–182.
Peters, M., M. Neumann, M. Iyyer,
M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer. 2018. Deep
contextualized word representations.
NAACL HLT .
Peterson, G. E., W. S.-Y . Wang, and
E. Sivertsen. 1958. Segmenta-
tion techniques in speech synthesis.
JASA , 30(8):739–742.
Peterson, J. C., D. Chen, and T. L. Grif-
ﬁths. 2020. Parallelograms revisited:
Exploring the limitations of vector
space models for simple analogies.
Cognition , 205.
Petroni, F., T. Rockt ¨aschel, S. Riedel,
P. Lewis, A. Bakhtin, Y . Wu, and
A. Miller. 2019. Language models
as knowledge bases? EMNLP .

--- Page 574 ---
574 Bibliography
Petrov, S., D. Das, and R. McDonald.
2012. A universal part-of-speech
tagset. LREC .
Petrov, S. and R. McDonald. 2012.
Overview of the 2012 shared task on
parsing the web. Notes of the First
Workshop on Syntactic Analysis of
Non-Canonical Language (SANCL) ,
volume 59.
Phillips, A. V . 1960. A question-
answering routine. Technical Re-
port 16, MIT AI Lab.
Picard, R. W. 1995. Affective comput-
ing. Technical Report 321, MIT Me-
dia Lab Perceputal Computing Tech-
nical Report. Revised November 26,
1995.
Pieraccini, R., E. Levin, and C.-H.
Lee. 1991. Stochastic representation
of conceptual structure in the ATIS
task. Speech and Natural Language
Workshop .
Pierce, J. R., J. B. Carroll, E. P.
Hamp, D. G. Hays, C. F. Hockett,
A. G. Oettinger, and A. J. Perlis.
1966. Language and Machines:
Computers in Translation and Lin-
guistics . ALPAC report. National
Academy of Sciences, National Re-
search Council, Washington, DC.
Pilehvar, M. T. and J. Camacho-
Collados. 2019. WiC: the word-
in-context dataset for evaluating
context-sensitive meaning represen-
tations. NAACL HLT .
Pitler, E., A. Louis, and A. Nenkova.
2009. Automatic sense prediction
for implicit discourse relations in
text. ACL IJCNLP .
Pitler, E. and A. Nenkova. 2009. Us-
ing syntax to disambiguate explicit
discourse connectives in text. ACL
IJCNLP .
Plutchik, R. 1962. The emotions: Facts,
theories, and a new model . Random
House.
Plutchik, R. 1980. A general psycho-
evolutionary theory of emotion. In
R. Plutchik and H. Kellerman, eds,
Emotion: Theory, Research, and Ex-
perience, Volume 1 , 3–33. Academic
Press.
Poesio, M., R. Stevenson, B. Di Euge-
nio, and J. Hitzeman. 2004. Center-
ing: A parametric theory and its in-
stantiations. Computational Linguis-
tics, 30(3):309–363.
Poesio, M., R. Stuckardt, and Y . Ver-
sley. 2016. Anaphora resolution:
Algorithms, resources, and applica-
tions . Springer.
Poesio, M., P. Sturt, R. Artstein, and
R. Filik. 2006. Underspeciﬁcation
and anaphora: Theoretical issues
and preliminary evidence. Discourse
processes , 42(2):157–175.Poesio, M. and R. Vieira. 1998. A
corpus-based investigation of deﬁ-
nite description use. Computational
Linguistics , 24(2):183–216.
Polanyi, L. 1988. A formal model of
the structure of discourse. Journal
of Pragmatics , 12.
Polanyi, L., C. Culy, M. van den Berg,
G. L. Thione, and D. Ahn. 2004.
A rule based approach to discourse
parsing. Proceedings of SIGDIAL .
Pollard, C. and I. A. Sag. 1994. Head-
Driven Phrase Structure Grammar .
University of Chicago Press.
Ponzetto, S. P. and M. Strube. 2006.
Exploiting semantic role labeling,
WordNet and Wikipedia for corefer-
ence resolution. HLT-NAACL .
Ponzetto, S. P. and M. Strube. 2007.
Knowledge derived from Wikipedia
for computing semantic relatedness.
JAIR , 30:181–212.
Popovi ´c, M. 2015. chrF: charac-
ter n-gram F-score for automatic
MT evaluation. Proceedings of the
Tenth Workshop on Statistical Ma-
chine Translation .
Popp, D., R. A. Donovan, M. Craw-
ford, K. L. Marsh, and M. Peele.
2003. Gender, race, and speech style
stereotypes. Sex Roles , 48(7-8):317–
325.
Porter, M. F. 1980. An algorithm
for sufﬁx stripping. Program ,
14(3):130–137.
Post, M. 2018. A call for clarity in re-
porting BLEU scores. WMT 2018 .
Potts, C. 2011. On the negativity of
negation. In N. Li and D. Lutz,
eds, Proceedings of Semantics and
Linguistic Theory 20 , 636–659. CLC
Publications, Ithaca, NY .
Povey, D., A. Ghoshal, G. Boulianne,
L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek,
Y . Qian, P. Schwarz, J. Silovsk ´y,
G. Stemmer, and K. Vesel ´y. 2011.
The Kaldi speech recognition
toolkit. ASRU .
Pradhan, S., E. H. Hovy, M. P. Mar-
cus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2007a. OntoNotes:
A uniﬁed relational semantic repre-
sentation. Proceedings of ICSC .
Pradhan, S., E. H. Hovy, M. P. Mar-
cus, M. Palmer, L. A. Ramshaw,
and R. M. Weischedel. 2007b.
Ontonotes: a uniﬁed relational se-
mantic representation. Int. J. Seman-
tic Computing , 1(4):405–419.
Pradhan, S., X. Luo, M. Recasens,
E. H. Hovy, V . Ng, and M. Strube.
2014. Scoring coreference partitions
of predicted mentions: A reference
implementation. ACL.Pradhan, S., A. Moschitti, N. Xue, H. T.
Ng, A. Bj ¨orkelund, O. Uryupina,
Y . Zhang, and Z. Zhong. 2013. To-
wards robust linguistic analysis us-
ing OntoNotes. CoNLL .
Pradhan, S., A. Moschitti, N. Xue,
O. Uryupina, and Y . Zhang. 2012a.
CoNLL-2012 shared task: Model-
ing multilingual unrestricted coref-
erence in OntoNotes. CoNLL .
Pradhan, S., A. Moschitti, N. Xue,
O. Uryupina, and Y . Zhang. 2012b.
Conll-2012 shared task: Model-
ing multilingual unrestricted coref-
erence in OntoNotes. CoNLL .
Pradhan, S., L. Ramshaw, M. P. Mar-
cus, M. Palmer, R. Weischedel, and
N. Xue. 2011. CoNLL-2011 shared
task: Modeling unrestricted corefer-
ence in OntoNotes. CoNLL .
Pradhan, S., L. Ramshaw, R. Wei-
schedel, J. MacBride, and L. Mic-
ciulla. 2007c. Unrestricted corefer-
ence: Identifying entities and events
in OntoNotes. Proceedings of
ICSC 2007 .
Pradhan, S., W. Ward, K. Hacioglu,
J. H. Martin, and D. Jurafsky. 2005.
Semantic role labeling using differ-
ent syntactic views. ACL.
Prasad, A., P. Hase, X. Zhou, and
M. Bansal. 2023. GrIPS: Gradient-
free, edit-based instruction search
for prompting large language mod-
els.EACL .
Prasad, R., N. Dinesh, A. Lee, E. Milt-
sakaki, L. Robaldo, A. K. Joshi, and
B. L. Webber. 2008. The Penn Dis-
course TreeBank 2.0. LREC .
Prasad, R., B. L. Webber, and A. Joshi.
2014. Reﬂections on the Penn Dis-
course Treebank, comparable cor-
pora, and complementary annota-
tion. Computational Linguistics ,
40(4):921–950.
Prates, M. O. R., P. H. Avelar, and L. C.
Lamb. 2019. Assessing gender bias
in machine translation: a case study
with Google Translate. Neural Com-
puting and Applications , 32:6363–
6381.
Price, P. J., W. Fisher, J. Bern-
stein, and D. Pallet. 1988. The
DARPA 1000-word resource man-
agement database for continuous
speech recognition. ICASSP .
Prince, E. 1981. Toward a taxonomy of
given-new information. In P. Cole,
ed., Radical Pragmatics , 223–255.
Academic Press.
Propp, V . 1968. Morphology of the
Folktale , 2nd edition. University of
Texas Press. Original Russian 1928.
Translated by Laurence Scott.
Pryzant, R., D. Iter, J. Li, Y . Lee,
C. Zhu, and M. Zeng. 2023. Au-
tomatic prompt optimization with

--- Page 575 ---
Bibliography 575
“gradient descent” and beam search.
EMNLP .
Pundak, G. and T. N. Sainath. 2016.
Lower frame rate neural network
acoustic models. INTERSPEECH .
Pustejovsky, J. 1991. The generative
lexicon. Computational Linguistics ,
17(4).
Pustejovsky, J., P. Hanks, R. Saur ´ı,
A. See, R. Gaizauskas, A. Setzer,
D. Radev, B. Sundheim, D. S. Day,
L. Ferro, and M. Lazo. 2003. The
TIMEBANK corpus. Proceedings
of Corpus Linguistics 2003 Confer-
ence. UCREL Technical Paper num-
ber 16.
Pustejovsky, J., R. Ingria,
R. Saur ´ı, J. Casta ˜no, J. Littman,
R. Gaizauskas, A. Setzer, G. Katz,
and I. Mani. 2005. The Speciﬁca-
tion Language TimeML , chapter 27.
Oxford.
Qin, L., Z. Zhang, and H. Zhao. 2016.
A stacking gated neural architecture
for implicit discourse relation classi-
ﬁcation. EMNLP .
Qin, L., Z. Zhang, H. Zhao, Z. Hu,
and E. Xing. 2017. Adversarial
connective-exploiting networks for
implicit discourse relation classiﬁca-
tion. ACL.
Radford, A., J. Wu, R. Child, D. Luan,
D. Amodei, and I. Sutskever. 2019.
Language models are unsupervised
multitask learners. OpenAI tech re-
port.
Rafailov, R., A. Sharma, E. Mitchell,
S. Ermon, C. D. Manning, and
C. Finn. 2023. Direct preference op-
timization: Your language model is
secretly a reward model. NeurIPS .
Raffel, C., N. Shazeer, A. Roberts,
K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu.
2020. Exploring the limits of trans-
fer learning with a uniﬁed text-to-
text transformer. JMLR , 21(140):1–
67.
Raghunathan, K., H. Lee, S. Rangara-
jan, N. Chambers, M. Surdeanu,
D. Jurafsky, and C. D. Manning.
2010. A multi-pass sieve for coref-
erence resolution. EMNLP .
Rahman, A. and V . Ng. 2009. Super-
vised models for coreference resolu-
tion. EMNLP .
Rahman, A. and V . Ng. 2012. Resolv-
ing complex cases of deﬁnite pro-
nouns: the Winograd Schema chal-
lenge. EMNLP .
Rajpurkar, P., R. Jia, and P. Liang.
2018. Know what you don’t
know: Unanswerable questions for
SQuAD. ACL.
Rajpurkar, P., J. Zhang, K. Lopyrev, and
P. Liang. 2016. SQuAD: 100,000+questions for machine comprehen-
sion of text. EMNLP .
Ram, O., Y . Levine, I. Dalmedigos,
D. Muhlgay, A. Shashua, K. Leyton-
Brown, and Y . Shoham. 2023.
In-context retrieval-augmented lan-
guage models. ArXiv preprint.
Ramshaw, L. A. and M. P. Mar-
cus. 1995. Text chunking using
transformation-based learning. Pro-
ceedings of the 3rd Annual Work-
shop on Very Large Corpora .
Rashkin, H., E. Bell, Y . Choi, and
S. V olkova. 2017. Multilingual con-
notation frames: A case study on
social media for targeted sentiment
analysis and forecast. ACL.
Rashkin, H., S. Singh, and Y . Choi.
2016. Connotation frames: A data-
driven investigation. ACL.
Rashkin, H., E. M. Smith, M. Li,
and Y .-L. Boureau. 2019. Towards
empathetic open-domain conversa-
tion models: A new benchmark and
dataset. ACL.
Ratinov, L. and D. Roth. 2012.
Learning-based multi-sieve co-
reference resolution with knowl-
edge. EMNLP .
Ratnaparkhi, A. 1996. A maxi-
mum entropy part-of-speech tagger.
EMNLP .
Ratnaparkhi, A. 1997. A linear ob-
served time statistical parser based
on maximum entropy models.
EMNLP .
Rawls, J. 2001. Justice as fairness:
A restatement . Harvard University
Press.
Recasens, M. and E. H. Hovy. 2011.
BLANC: Implementing the Rand
index for coreference evaluation.
Natural Language Engineering ,
17(4):485–510.
Recasens, M., E. H. Hovy, and M. A.
Mart ´ı. 2011. Identity, non-identity,
and near-identity: Addressing the
complexity of coreference. Lingua ,
121(6):1138–1152.
Recasens, M. and M. A. Mart ´ı. 2010.
AnCora-CO: Coreferentially anno-
tated corpora for Spanish and Cata-
lan. Language Resources and Eval-
uation , 44(4):315–345.
Reed, C., R. Mochales Palau, G. Rowe,
and M.-F. Moens. 2008. Lan-
guage resources for studying argu-
ment. LREC .
Reeves, B. and C. Nass. 1996. The
Media Equation: How People Treat
Computers, Television, and New Me-
dia Like Real People and Places .
Cambridge University Press.
Rehder, B., M. E. Schreiner, M. B. W.
Wolfe, D. Laham, T. K. Landauer,
and W. Kintsch. 1998. Using LatentSemantic Analysis to assess knowl-
edge: Some technical considera-
tions. Discourse Processes , 25(2-
3):337–354.
Rei, R., C. Stewart, A. C. Farinha, and
A. Lavie. 2020. COMET: A neu-
ral framework for MT evaluation.
EMNLP .
Reichenbach, H. 1947. Elements of
Symbolic Logic . Macmillan, New
York.
Reichman, R. 1985. Getting Computers
to Talk Like You and Me . MIT Press.
Resnik, P. 1993. Semantic classes and
syntactic ambiguity. HLT.
Resnik, P. 1996. Selectional con-
straints: An information-theoretic
model and its computational realiza-
tion. Cognition , 61:127–159.
Reynolds, L. and K. McDonell. 2021.
Prompt programming for large lan-
guage models: Beyond the few-shot
paradigm. CHI 2021 .
Riedel, S., L. Yao, and A. McCallum.
2010. Modeling relations and their
mentions without labeled text. In
Machine Learning and Knowledge
Discovery in Databases , 148–163.
Springer.
Riedel, S., L. Yao, A. McCallum, and
B. M. Marlin. 2013. Relation extrac-
tion with matrix factorization and
universal schemas. NAACL HLT .
Riloff, E. 1993. Automatically con-
structing a dictionary for informa-
tion extraction tasks. AAAI .
Riloff, E. 1996. Automatically gen-
erating extraction patterns from un-
tagged text. AAAI .
Riloff, E. and R. Jones. 1999. Learning
dictionaries for information extrac-
tion by multi-level bootstrapping.
AAAI .
Riloff, E. and M. Schmelzenbach. 1998.
An empirical approach to conceptual
case frame acquisition. Proceedings
of the Sixth Workshop on Very Large
Corpora .
Riloff, E. and J. Shepherd. 1997. A
corpus-based approach for building
semantic lexicons. EMNLP .
Riloff, E. and M. Thelen. 2000. A rule-
based question answering system
for reading comprehension tests.
ANLP/NAACL workshop on reading
comprehension tests .
Riloff, E. and J. Wiebe. 2003. Learn-
ing extraction patterns for subjective
expressions. EMNLP .
Ritter, A., C. Cherry, and B. Dolan.
2010a. Unsupervised modeling of
twitter conversations. NAACL HLT .
Ritter, A., O. Etzioni, and Mausam.
2010b. A latent dirichlet allocation
method for selectional preferences.
ACL.

--- Page 576 ---
576 Bibliography
Ritter, A., L. Zettlemoyer, Mausam, and
O. Etzioni. 2013. Modeling miss-
ing data in distant supervision for in-
formation extraction. TACL , 1:367–
378.
Roberts, A., C. Raffel, and N. Shazeer.
2020. How much knowledge can
you pack into the parameters of a
language model? EMNLP .
Robertson, S., S. Walker, S. Jones,
M. M. Hancock-Beaulieu, and
M. Gatford. 1995. Okapi at TREC-3.
Overview of the Third Text REtrieval
Conference (TREC-3) .
Robinson, T. and F. Fallside. 1991.
A recurrent error propagation net-
work speech recognition system.
Computer Speech & Language ,
5(3):259–274.
Robinson, T., M. Hochberg, and S. Re-
nals. 1996. The use of recurrent neu-
ral networks in continuous speech
recognition. In C.-H. Lee, F. K.
Soong, and K. K. Paliwal, eds, Au-
tomatic speech and speaker recogni-
tion, 233–258. Springer.
Rogers, A., M. Gardner, and I. Au-
genstein. 2023. QA dataset explo-
sion: A taxonomy of NLP resources
for question answering and reading
comprehension. ACM Computing
Surveys , 55(10):1–45.
Rohde, D. L. T., L. M. Gonnerman, and
D. C. Plaut. 2006. An improved
model of semantic similarity based
on lexical co-occurrence. CACM ,
8:627–633.
Roller, S., E. Dinan, N. Goyal, D. Ju,
M. Williamson, Y . Liu, J. Xu,
M. Ott, E. M. Smith, Y .-L. Boureau,
and J. Weston. 2021. Recipes for
building an open-domain chatbot.
EACL .
Rooth, M., S. Riezler, D. Prescher,
G. Carroll, and F. Beil. 1999. Induc-
ing a semantically annotated lexicon
via EM-based clustering. ACL.
Rosenblatt, F. 1958. The percep-
tron: A probabilistic model for in-
formation storage and organization
in the brain. Psychological review ,
65(6):386–408.
Rosenfeld, R. 1992. Adaptive Statis-
tical Language Modeling: A Maxi-
mum Entropy Approach . Ph.D. the-
sis, Carnegie Mellon University.
Rosenfeld, R. 1996. A maximum en-
tropy approach to adaptive statisti-
cal language modeling. Computer
Speech and Language , 10:187–228.
Rosenthal, S. and K. McKeown. 2017.
Detecting inﬂuencers in multiple on-
line genres. ACM Transactions on
Internet Technology (TOIT) , 17(2).
Rothe, S., S. Ebert, and H. Sch ¨utze.
2016. Ultradense Word Embed-
dings by Orthogonal Transforma-
tion. NAACL HLT .Roy, N., J. Pineau, and S. Thrun. 2000.
Spoken dialogue management using
probabilistic reasoning. ACL.
Rudinger, R., J. Naradowsky,
B. Leonard, and B. Van Durme.
2018. Gender bias in coreference
resolution. NAACL HLT .
Rumelhart, D. E., G. E. Hinton, and
R. J. Williams. 1986. Learning in-
ternal representations by error prop-
agation. In D. E. Rumelhart and
J. L. McClelland, eds, Parallel Dis-
tributed Processing , volume 2, 318–
362. MIT Press.
Rumelhart, D. E. and J. L. McClelland.
1986a. On learning the past tense of
English verbs. In D. E. Rumelhart
and J. L. McClelland, eds, Parallel
Distributed Processing , volume 2,
216–271. MIT Press.
Rumelhart, D. E. and J. L. McClelland,
eds. 1986b. Parallel Distributed
Processing . MIT Press.
Rumelhart, D. E. and A. A. Abraham-
son. 1973. A model for analogi-
cal reasoning. Cognitive Psychol-
ogy, 5(1):1–28.
Rumelhart, D. E. and J. L. McClelland,
eds. 1986c. Parallel Distributed
Processing: Explorations in the Mi-
crostructure of Cognition , volume
1:Foundations . MIT Press.
Ruppenhofer, J., M. Ellsworth, M. R. L.
Petruck, C. R. Johnson, C. F. Baker,
and J. Scheffczyk. 2016. FrameNet
II: Extended theory and practice.
Ruppenhofer, J., C. Sporleder,
R. Morante, C. F. Baker, and
M. Palmer. 2010. Semeval-2010
task 10: Linking events and their
participants in discourse. 5th In-
ternational Workshop on Semantic
Evaluation .
Russell, J. A. 1980. A circum-
plex model of affect. Journal of
personality and social psychology ,
39(6):1161–1178.
Russell, S. and P. Norvig. 2002. Ar-
tiﬁcial Intelligence: A Modern Ap-
proach , 2nd edition. Prentice Hall.
Rutherford, A. and N. Xue. 2015. Im-
proving the inference of implicit dis-
course relations via classifying ex-
plicit discourse connectives. NAACL
HLT.
Sachan, D. S., M. Lewis, D. Yo-
gatama, L. Zettlemoyer, J. Pineau,
and M. Zaheer. 2023. Questions are
all you need to train a dense passage
retriever. TACL , 11:600–616.
Sacks, H., E. A. Schegloff, and G. Jef-
ferson. 1974. A simplest system-
atics for the organization of turn-
taking for conversation. Language ,
50(4):696–735.Sag, I. A. and M. Y . Liberman. 1975.
The intonational disambiguation of
indirect speech acts. In CLS-75 ,
487–498. University of Chicago.
Sagae, K. 2009. Analysis of dis-
course structure with syntactic de-
pendencies and data-driven shift-
reduce parsing. IWPT-09 .
Sagawa, S., P. W. Koh, T. B.
Hashimoto, and P. Liang. 2020. Dis-
tributionally robust neural networks
for group shifts: On the importance
of regularization for worst-case gen-
eralization. ICLR .
Sagisaka, Y . 1988. Speech synthe-
sis by rule using an optimal selec-
tion of non-uniform synthesis units.
ICASSP .
Sagisaka, Y ., N. Kaiki, N. Iwahashi,
and K. Mimura. 1992. Atr – n-talk
speech synthesis system. ICSLP .
Sahami, M., S. T. Dumais, D. Heck-
erman, and E. Horvitz. 1998. A
Bayesian approach to ﬁltering junk
e-mail. AAAI Workshop on Learning
for Text Categorization .
Sakoe, H. and S. Chiba. 1971. A
dynamic programming approach to
continuous speech recognition. Pro-
ceedings of the Seventh Interna-
tional Congress on Acoustics , vol-
ume 3. Akad ´emiai Kiad ´o.
Sakoe, H. and S. Chiba. 1984. Dy-
namic programming algorithm opti-
mization for spoken word recogni-
tion. IEEE Transactions on ASSP ,
ASSP-26(1):43–49.
Salomaa, A. 1969. Probabilistic and
weighted grammars. Information
and Control , 15:529–544.
Salton, G. 1971. The SMART Re-
trieval System: Experiments in Au-
tomatic Document Processing . Pren-
tice Hall.
Salvetti, F., J. B. Lowe, and J. H. Mar-
tin. 2016. A tangled web: The faint
signals of deception in text - boul-
der lies and truth corpus (BLT-C).
LREC .
Sampson, G. 1987. Alternative gram-
matical coding systems. In R. Gar-
side, G. Leech, and G. Sampson,
eds, The Computational Analysis of
English , 165–183. Longman.
Sankoff, D. and W. Labov. 1979. On the
uses of variable rules. Language in
society , 8(2-3):189–222.
Sap, M., D. Card, S. Gabriel, Y . Choi,
and N. A. Smith. 2019. The risk of
racial bias in hate speech detection.
ACL.
Sap, M., M. C. Prasettio, A. Holtzman,
H. Rashkin, and Y . Choi. 2017. Con-
notation frames of power and agency
in modern ﬁlms. EMNLP .

--- Page 577 ---
Bibliography 577
Saur´ı, R., J. Littman, B. Knippen,
R. Gaizauskas, A. Setzer, and
J. Pustejovsky. 2006. TimeML an-
notation guidelines version 1.2.1.
Manuscript.
Scha, R. and L. Polanyi. 1988. An
augmented context free grammar for
discourse. COLING .
Schank, R. C. and R. P. Abelson. 1975.
Scripts, plans, and knowledge. Pro-
ceedings of IJCAI-75 .
Schank, R. C. and R. P. Abelson. 1977.
Scripts, Plans, Goals and Under-
standing . Lawrence Erlbaum.
Schegloff, E. A. 1968. Sequencing in
conversational openings. American
Anthropologist , 70:1075–1095.
Scherer, K. R. 2000. Psychological
models of emotion. In J. C. Borod,
ed., The neuropsychology of emo-
tion, 137–162. Oxford.
Schiebinger, L. 2013. Machine
translation: Analyzing gender.
http://genderedinnovations.
stanford.edu/case-studies/
nlp.html#tabs-2 .
Schiebinger, L. 2014. Scientiﬁc re-
search must take gender into ac-
count. Nature , 507(7490):9.
Schluter, N. 2018. The word analogy
testing caveat. NAACL HLT .
Schone, P. and D. Jurafsky. 2000.
Knowlege-free induction of mor-
phology using latent semantic anal-
ysis. CoNLL .
Schone, P. and D. Jurafsky. 2001a. Is
knowledge-free induction of multi-
word unit dictionary headwords a
solved problem? EMNLP .
Schone, P. and D. Jurafsky. 2001b.
Knowledge-free induction of inﬂec-
tional morphologies. NAACL .
Schuster, M. and K. Nakajima. 2012.
Japanese and Korean voice search.
ICASSP .
Schuster, M. and K. K. Paliwal. 1997.
Bidirectional recurrent neural net-
works. IEEE Transactions on Signal
Processing , 45:2673–2681.
Sch¨utze, H. 1992a. Context space.
AAAI Fall Symposium on Proba-
bilistic Approaches to Natural Lan-
guage .
Sch¨utze, H. 1992b. Dimensions of
meaning. Proceedings of Supercom-
puting ’92 . IEEE Press.
Sch¨utze, H. 1997. Ambiguity Resolu-
tion in Language Learning – Com-
putational and Cognitive Models .
CSLI, Stanford, CA.
Sch¨utze, H., D. A. Hull, and J. Peder-
sen. 1995. A comparison of clas-
siﬁers and document representations
for the routing problem. SIGIR-95 .Sch¨utze, H. and J. Pedersen. 1993. A
vector model for syntagmatic and
paradigmatic relatedness. 9th An-
nual Conference of the UW Centre
for the New OED and Text Research .
Sch¨utze, H. and Y . Singer. 1994. Part-
of-speech tagging using a variable
memory Markov model. ACL.
Schwartz, H. A., J. C. Eichstaedt,
M. L. Kern, L. Dziurzynski, S. M.
Ramones, M. Agrawal, A. Shah,
M. Kosinski, D. Stillwell, M. E. P.
Seligman, and L. H. Ungar. 2013.
Personality, gender, and age in the
language of social media: The open-
vocabulary approach. PloS one ,
8(9):e73791.
Schwenk, H. 2007. Continuous space
language models. Computer Speech
& Language , 21(3):492–518.
Schwenk, H. 2018. Filtering and min-
ing parallel data in a joint multilin-
gual space. ACL.
Schwenk, H., D. Dechelotte, and J.-L.
Gauvain. 2006. Continuous space
language models for statistical ma-
chine translation. COLING/ACL .
Schwenk, H., G. Wenzek, S. Edunov,
E. Grave, A. Joulin, and A. Fan.
2021. CCMatrix: Mining billions
of high-quality parallel sentences on
the web. ACL.
S´eaghdha, D. O. 2010. Latent vari-
able models of selectional prefer-
ence. ACL.
Seddah, D., R. Tsarfaty, S. K ¨ubler,
M. Candito, J. D. Choi, R. Farkas,
J. Foster, I. Goenaga, K. Gojenola,
Y . Goldberg, S. Green, N. Habash,
M. Kuhlmann, W. Maier, J. Nivre,
A. Przepi ´orkowski, R. Roth,
W. Seeker, Y . Versley, V . Vincze,
M. Woli ´nski, A. Wr ´oblewska, and
E. Villemonte de la Cl ´ergerie.
2013. Overview of the SPMRL
2013 shared task: cross-framework
evaluation of parsing morpholog-
ically rich languages. 4th Work-
shop on Statistical Parsing of
Morphologically-Rich Languages .
See, A., S. Roller, D. Kiela, and
J. Weston. 2019. What makes a
good conversation? how control-
lable attributes affect human judg-
ments. NAACL HLT .
Sekine, S. and M. Collins. 1997.
The evalb software. http:
//cs.nyu.edu/cs/projects/
proteus/evalb .
Sellam, T., D. Das, and A. Parikh. 2020.
BLEURT: Learning robust metrics
for text generation. ACL.
Sennrich, R., B. Haddow, and A. Birch.
2016. Neural machine translation of
rare words with subword units. ACL.
Seo, M., A. Kembhavi, A. Farhadi, and
H. Hajishirzi. 2017. Bidirectionalattention ﬂow for machine compre-
hension. ICLR .
Shannon, C. E. 1948. A mathematical
theory of communication. Bell Sys-
tem Technical Journal , 27(3):379–
423. Continued in the following vol-
ume.
Shannon, C. E. 1951. Prediction and en-
tropy of printed English. Bell System
Technical Journal , 30:50–64.
Sheil, B. A. 1976. Observations on con-
text free parsing. SMIL: Statistical
Methods in Linguistics , 1:71–109.
Shen, J., R. Pang, R. J. Weiss,
M. Schuster, N. Jaitly, Z. Yang,
Z. Chen, Y . Zhang, Y . Wang,
R. Skerry-Ryan, R. A. Saurous,
Y . Agiomyrgiannakis, and Y . Wu.
2018. Natural TTS synthesis by con-
ditioning WaveNet on mel spectro-
gram predictions. ICASSP .
Sheng, E., K.-W. Chang, P. Natarajan,
and N. Peng. 2019. The woman
worked as a babysitter: On biases in
language generation. EMNLP .
Shi, P. and J. Lin. 2019. Simple BERT
models for relation extraction and
semantic role labeling. ArXiv.
Shi, W., S. Min, M. Yasunaga, M. Seo,
R. James, M. Lewis, L. Zettlemoyer,
and W.-t. Yih. 2023. REPLUG:
Retrieval-augmented black-box lan-
guage models. ArXiv preprint.
Shriberg, E., R. Bates, P. Taylor,
A. Stolcke, D. Jurafsky, K. Ries,
N. Coccaro, R. Martin, M. Meteer,
and C. Van Ess-Dykema. 1998. Can
prosody aid the automatic classiﬁca-
tion of dialog acts in conversational
speech? Language and Speech (Spe-
cial Issue on Prosody and Conversa-
tion) , 41(3-4):439–487.
Sidner, C. L. 1979. Towards a compu-
tational theory of deﬁnite anaphora
comprehension in English discourse.
Technical Report 537, MIT Artiﬁ-
cial Intelligence Laboratory, Cam-
bridge, MA.
Sidner, C. L. 1983. Focusing in the
comprehension of deﬁnite anaphora.
In M. Brady and R. C. Berwick,
eds, Computational Models of Dis-
course , 267–330. MIT Press.
Simmons, R. F. 1965. Answering En-
glish questions by computer: A sur-
vey. CACM , 8(1):53–70.
Simmons, R. F. 1973. Semantic net-
works: Their computation and use
for understanding English sentences.
In R. C. Schank and K. M. Colby,
eds, Computer Models of Thought
and Language , 61–113. W.H. Free-
man & Co.
Simmons, R. F., S. Klein, and K. Mc-
Conlogue. 1964. Indexing and de-
pendency logic for answering En-
glish questions. American Docu-
mentation , 15(3):196–204.

--- Page 578 ---
578 Bibliography
Simons, G. F. and C. D. Fennig.
2018. Ethnologue: Languages of
the world, 21st edition. SIL Inter-
national.
Singh, S. P., D. J. Litman, M. Kearns,
and M. A. Walker. 2002. Optimiz-
ing dialogue management with re-
inforcement learning: Experiments
with the NJFun system. JAIR ,
16:105–133.
Singh, S., F. Vargus, D. D’souza,
B. F. Karlsson, A. Mahendiran,
W.-Y . Ko, H. Shandilya, J. Pa-
tel, D. Mataciunas, L. O’Mahony,
M. Zhang, R. Hettiarachchi, J. Wil-
son, M. Machado, L. S. Moura,
D. Krzemi ´nski, H. Fadaei, I. Erg ¨un,
I. Okoh, A. Alaagib, O. Mudan-
nayake, Z. Alyafeai, V . M. Chien,
S. Ruder, S. Guthikonda, E. A.
Alghamdi, S. Gehrmann, N. Muen-
nighoff, M. Bartolo, J. Kreutzer,
A. ¨U¨Ust¨un, M. Fadaee, and
S. Hooker. 2024. Aya dataset: An
open-access collection for multi-
lingual instruction tuning. ArXiv
preprint.
Sleator, D. and D. Temperley. 1993.
Parsing English with a link gram-
mar. IWPT-93 .
Sloan, M. C. 2010. Aristotle’s Nico-
machean Ethics as the original lo-
cus for the Septem Circumstantiae.
Classical Philology , 105(3):236–
251.
Slobin, D. I. 1996. Two ways to
travel. In M. Shibatani and S. A.
Thompson, eds, Grammatical Con-
structions: Their Form and Mean-
ing, 195–220. Clarendon Press.
Smith, V . L. and H. H. Clark. 1993. On
the course of answering questions.
Journal of Memory and Language ,
32:25–38.
Smolensky, P. 1988. On the proper
treatment of connectionism. Behav-
ioral and brain sciences , 11(1):1–
23.
Smolensky, P. 1990. Tensor product
variable binding and the representa-
tion of symbolic structures in con-
nectionist systems. Artiﬁcial intel-
ligence , 46(1-2):159–216.
Snover, M., B. Dorr, R. Schwartz,
L. Micciulla, and J. Makhoul. 2006.
A study of translation edit rate with
targeted human annotation. AMTA-
2006 .
Snow, R., D. Jurafsky, and A. Y . Ng.
2005. Learning syntactic patterns
for automatic hypernym discovery.
NeurIPS .
Socher, R., J. Bauer, C. D. Man-
ning, and A. Y . Ng. 2013. Pars-
ing with compositional vector gram-
mars. ACL.Socher, R., C. C.-Y . Lin, A. Y . Ng, and
C. D. Manning. 2011. Parsing natu-
ral scenes and natural language with
recursive neural networks. ICML .
Soderland, S., D. Fisher, J. Aseltine,
and W. G. Lehnert. 1995. CRYS-
TAL: Inducing a conceptual dictio-
nary. IJCAI-95 .
Søgaard, A. 2010. Simple semi-
supervised training of part-of-
speech taggers. ACL.
Søgaard, A. and Y . Goldberg. 2016.
Deep multi-task learning with low
level tasks supervised at lower lay-
ers.ACL.
Søgaard, A., A. Johannsen, B. Plank,
D. Hovy, and H. M. Alonso. 2014.
What’s in a p-value in NLP? CoNLL .
Soldaini, L., R. Kinney, A. Bha-
gia, D. Schwenk, D. Atkinson,
R. Authur, B. Bogin, K. Chandu,
J. Dumas, Y . Elazar, V . Hofmann,
A. H. Jha, S. Kumar, L. Lucy,
X. Lyu, N. Lambert, I. Magnus-
son, J. Morrison, N. Muennighoff,
A. Naik, C. Nam, M. E. Pe-
ters, A. Ravichander, K. Richardson,
Z. Shen, E. Strubell, N. Subramani,
O. Tafjord, P. Walsh, L. Zettlemoyer,
N. A. Smith, H. Hajishirzi, I. Belt-
agy, D. Groeneveld, J. Dodge, and
K. Lo. 2024. Dolma: An open cor-
pus of three trillion tokens for lan-
guage model pretraining research.
ArXiv preprint.
Solorio, T., E. Blair, S. Maharjan,
S. Bethard, M. Diab, M. Ghoneim,
A. Hawwari, F. AlGhamdi,
J. Hirschberg, A. Chang, and
P. Fung. 2014. Overview for the
ﬁrst shared task on language iden-
tiﬁcation in code-switched data.
Workshop on Computational Ap-
proaches to Code Switching .
Somasundaran, S., J. Burstein, and
M. Chodorow. 2014. Lexical chain-
ing for measuring discourse coher-
ence quality in test-taker essays.
COLING .
Soon, W. M., H. T. Ng, and D. C. Y .
Lim. 2001. A machine learning ap-
proach to coreference resolution of
noun phrases. Computational Lin-
guistics , 27(4):521–544.
Soricut, R. and D. Marcu. 2003. Sen-
tence level discourse parsing using
syntactic and lexical information.
HLT-NAACL .
Soricut, R. and D. Marcu. 2006.
Discourse generation using utility-
trained coherence models. COL-
ING/ACL .
Sorokin, D. and I. Gurevych. 2018.
Mixing context granularities for im-
proved entity linking on question
answering data across entity cate-
gories. *SEM .Sparck Jones, K. 1972. A statistical in-
terpretation of term speciﬁcity and
its application in retrieval. Journal
of Documentation , 28(1):11–21.
Sparck Jones, K. 1986. Synonymy and
Semantic Classiﬁcation . Edinburgh
University Press, Edinburgh. Repub-
lication of 1964 PhD Thesis.
Sporleder, C. and A. Lascarides. 2005.
Exploiting linguistic cues to classify
rhetorical relations. RANLP-05 .
Sporleder, C. and M. Lapata. 2005. Dis-
course chunking and its application
to sentence compression. EMNLP .
Sproat, R., A. W. Black, S. F.
Chen, S. Kumar, M. Ostendorf, and
C. Richards. 2001. Normalization
of non-standard words. Computer
Speech & Language , 15(3):287–
333.
Sproat, R. and K. Gorman. 2018. A
brief summary of the Kaggle text
normalization challenge.
Srivastava, N., G. E. Hinton,
A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. 2014. Dropout:
a simple way to prevent neural net-
works from overﬁtting. JMLR ,
15(1):1929–1958.
Stab, C. and I. Gurevych. 2014a. Anno-
tating argument components and re-
lations in persuasive essays. COL-
ING.
Stab, C. and I. Gurevych. 2014b. Identi-
fying argumentative discourse struc-
tures in persuasive essays. EMNLP .
Stab, C. and I. Gurevych. 2017. Parsing
argumentation structures in persua-
sive essays. Computational Linguis-
tics, 43(3):619–659.
Stalnaker, R. C. 1978. Assertion. In
P. Cole, ed., Pragmatics: Syntax and
Semantics Volume 9 , 315–332. Aca-
demic Press.
Stamatatos, E. 2009. A survey of mod-
ern authorship attribution methods.
JASIST , 60(3):538–556.
Stanovsky, G., N. A. Smith, and
L. Zettlemoyer. 2019. Evaluating
gender bias in machine translation.
ACL.
Stede, M. 2011. Discourse processing .
Morgan & Claypool.
Stede, M. and J. Schneider. 2018. Argu-
mentation Mining . Morgan & Clay-
pool.
Stern, M., J. Andreas, and D. Klein.
2017. A minimal span-based neural
constituency parser. ACL.
Stevens, K. N., S. Kasowski, and G. M.
Fant. 1953. An electrical analog of
the vocal tract. JASA , 25(4):734–
742.
Stevens, S. S. and J. V olkmann. 1940.
The relation of pitch to frequency: A
revised scale. The American Journal
of Psychology , 53(3):329–353.

--- Page 579 ---
Bibliography 579
Stevens, S. S., J. V olkmann, and E. B.
Newman. 1937. A scale for the mea-
surement of the psychological mag-
nitude pitch. JASA , 8:185–190.
Stifelman, L. J., B. Arons,
C. Schmandt, and E. A. Hulteen.
1993. V oiceNotes: A speech inter-
face for a hand-held voice notetaker.
INTERCHI 1993 .
Stolcke, A. 1998. Entropy-based prun-
ing of backoff language models.
Proc. DARPA Broadcast News Tran-
scription and Understanding Work-
shop .
Stolcke, A. 2002. SRILM – an exten-
sible language modeling toolkit. IC-
SLP.
Stolcke, A., Y . Konig, and M. Wein-
traub. 1997. Explicit word error min-
imization in N-best list rescoring.
EUROSPEECH , volume 1.
Stolcke, A., K. Ries, N. Coccaro,
E. Shriberg, R. Bates, D. Jurafsky,
P. Taylor, R. Martin, M. Meteer,
and C. Van Ess-Dykema. 2000. Di-
alogue act modeling for automatic
tagging and recognition of conversa-
tional speech. Computational Lin-
guistics , 26(3):339–371.
Stolz, W. S., P. H. Tannenbaum, and
F. V . Carstensen. 1965. A stochastic
approach to the grammatical coding
of English. CACM , 8(6):399–405.
Stone, P., D. Dunphry, M. Smith, and
D. Ogilvie. 1966. The General In-
quirer: A Computer Approach to
Content Analysis . MIT Press.
Str¨otgen, J. and M. Gertz. 2013. Mul-
tilingual and cross-domain temporal
tagging. Language Resources and
Evaluation , 47(2):269–298.
Strube, M. and U. Hahn. 1996. Func-
tional centering. ACL.
Strubell, E., A. Ganesh, and A. McCal-
lum. 2019. Energy and policy con-
siderations for deep learning in NLP.
ACL.
Su, Y ., H. Sun, B. Sadler, M. Srivatsa,
I. G¨ur, Z. Yan, and X. Yan. 2016. On
generating characteristic-rich ques-
tion sets for QA evaluation. EMNLP .
Subba, R. and B. Di Eugenio. 2009. An
effective discourse parser that uses
rich linguistic information. NAACL
HLT.
Sukhbaatar, S., A. Szlam, J. Weston,
and R. Fergus. 2015. End-to-end
memory networks. NeurIPS .
Sundheim, B., ed. 1991. Proceedings of
MUC-3 .
Sundheim, B., ed. 1992. Proceedings of
MUC-4 .
Sundheim, B., ed. 1993. Proceedings of
MUC-5 . Baltimore, MD.
Sundheim, B., ed. 1995. Proceedings of
MUC-6 .Surdeanu, M. 2013. Overview of the
TAC2013 Knowledge Base Popula-
tion evaluation: English slot ﬁlling
and temporal slot ﬁlling. TAC-13 .
Surdeanu, M., S. Harabagiu,
J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures
for information extraction. ACL.
Surdeanu, M., T. Hicks, and M. A.
Valenzuela-Escarcega. 2015. Two
practical rhetorical structure theory
parsers. NAACL HLT .
Surdeanu, M., R. Johansson, A. Mey-
ers, L. M `arquez, and J. Nivre. 2008.
The CoNLL 2008 shared task on
joint parsing of syntactic and seman-
tic dependencies. CoNLL .
Sutskever, I., O. Vinyals, and Q. V . Le.
2014. Sequence to sequence learn-
ing with neural networks. NeurIPS .
Suzgun, M., L. Melas-Kyriazi, and
D. Jurafsky. 2023a. Follow the wis-
dom of the crowd: Effective text
generation via minimum Bayes risk
decoding. Findings of ACL 2023 .
Suzgun, M., N. Scales, N. Sch ¨arli,
S. Gehrmann, Y . Tay, H. W. Chung,
A. Chowdhery, Q. Le, E. Chi,
D. Zhou, and J. Wei. 2023b.
Challenging BIG-bench tasks and
whether chain-of-thought can solve
them. ACL Findings .
Swerts, M., D. J. Litman, and J. Hirsch-
berg. 2000. Corrections in spoken
dialogue systems. ICSLP .
Swier, R. and S. Stevenson. 2004. Un-
supervised semantic role labelling.
EMNLP .
Switzer, P. 1965. Vector images in doc-
ument retrieval. Statistical Associa-
tion Methods For Mechanized Docu-
mentation. Symposium Proceedings.
Washington, D.C., USA, March 17,
1964 .https://nvlpubs.nist.
gov/nistpubs/Legacy/MP/
nbsmiscellaneouspub269.pdf .
Syrdal, A. K., C. W. Wightman,
A. Conkie, Y . Stylianou, M. Beut-
nagel, J. Schroeter, V . Strom, and
K.-S. Lee. 2000. Corpus-based
techniques in the AT&T NEXTGEN
synthesis system. ICSLP .
Talmy, L. 1985. Lexicalization patterns:
Semantic structure in lexical forms.
In T. Shopen, ed., Language Typol-
ogy and Syntactic Description, Vol-
ume 3 . Cambridge University Press.
Originally appeared as UC Berkeley
Cognitive Science Program Report
No. 30, 1980.
Talmy, L. 1991. Path to realization: A
typology of event conﬂation. BLS-
91.
Tan, C., V . Niculae, C. Danescu-
Niculescu-Mizil, and L. Lee. 2016.
Winning arguments: Interaction dy-
namics and persuasion strategiesin good-faith online discussions.
WWW-16 .
Tannen, D. 1979. What’s in a frame?
Surface evidence for underlying ex-
pectations. In R. Freedle, ed., New
Directions in Discourse Processing ,
137–181. Ablex.
Taylor, P. 2009. Text-to-Speech Synthe-
sis. Cambridge University Press.
Taylor, W. L. 1953. Cloze procedure: A
new tool for measuring readability.
Journalism Quarterly , 30:415–433.
Teranishi, R. and N. Umeda. 1968. Use
of pronouncing dictionary in speech
synthesis experiments. 6th Interna-
tional Congress on Acoustics .
Tesni `ere, L. 1959. ´El´ements de Syntaxe
Structurale . Librairie C. Klinck-
sieck, Paris.
Tetreault, J. R. 2001. A corpus-based
evaluation of centering and pronoun
resolution. Computational Linguis-
tics, 27(4):507–520.
Teufel, S., J. Carletta, and M. Moens.
1999. An annotation scheme for
discourse-level argumentation in re-
search articles. EACL .
Teufel, S., A. Siddharthan, and
C. Batchelor. 2009. Towards
domain-independent argumenta-
tive zoning: Evidence from chem-
istry and computational linguistics.
EMNLP .
Thede, S. M. and M. P. Harper. 1999. A
second-order hidden Markov model
for part-of-speech tagging. ACL.
Thompson, B. and P. Koehn. 2019. Ve-
calign: Improved sentence align-
ment in linear time and space.
EMNLP .
Thompson, K. 1968. Regular ex-
pression search algorithm. CACM ,
11(6):419–422.
Tian, Y ., V . Kulkarni, B. Perozzi,
and S. Skiena. 2016. On the
convergent properties of word em-
bedding methods. ArXiv preprint
arXiv:1605.03956.
Tibshirani, R. J. 1996. Regression
shrinkage and selection via the lasso.
Journal of the Royal Statistical So-
ciety. Series B (Methodological) ,
58(1):267–288.
Timkey, W. and M. van Schijndel. 2021.
All bark and no bite: Rogue dimen-
sions in transformer language mod-
els obscure representational quality.
EMNLP .
Titov, I. and E. Khoddam. 2014. Unsu-
pervised induction of semantic roles
within a reconstruction-error mini-
mization framework. NAACL HLT .
Titov, I. and A. Klementiev. 2012. A
Bayesian approach to unsupervised
semantic role induction. EACL .

--- Page 580 ---
580 Bibliography
Tomkins, S. S. 1962. Affect, imagery,
consciousness: Vol. I. The positive
affects . Springer.
Toutanova, K., D. Klein, C. D. Man-
ning, and Y . Singer. 2003. Feature-
rich part-of-speech tagging with a
cyclic dependency network. HLT-
NAACL .
Trichelair, P., A. Emami, J. C. K.
Cheung, A. Trischler, K. Suleman,
and F. Diaz. 2018. On the eval-
uation of common-sense reasoning
in natural language understanding.
NeurIPS 2018 Workshop on Cri-
tiquing and Correcting Trends in
Machine Learning .
Trnka, K., D. Yarrington, J. McCaw,
K. F. McCoy, and C. Pennington.
2007. The effects of word pre-
diction on communication rate for
AAC. NAACL-HLT .
Turian, J. P., L. Shen, and I. D. Mela-
med. 2003. Evaluation of machine
translation and its evaluation. Pro-
ceedings of MT Summit IX .
Turian, J., L. Ratinov, and Y . Bengio.
2010. Word representations: a sim-
ple and general method for semi-
supervised learning. ACL.
Turney, P. D. 2002. Thumbs up or
thumbs down? Semantic orienta-
tion applied to unsupervised classi-
ﬁcation of reviews. ACL.
Turney, P. D. and M. Littman. 2003.
Measuring praise and criticism: In-
ference of semantic orientation from
association. ACM Transactions
on Information Systems (TOIS) ,
21:315–346.
Turney, P. D. and M. L. Littman. 2005.
Corpus-based learning of analogies
and semantic relations. Machine
Learning , 60(1-3):251–278.
Umeda, N. 1976. Linguistic rules for
text-to-speech synthesis. Proceed-
ings of the IEEE , 64(4):443–451.
Umeda, N., E. Matui, T. Suzuki, and
H. Omura. 1968. Synthesis of fairy
tale using an analog vocal tract. 6th
International Congress on Acous-
tics.
Ung, M., J. Xu, and Y .-L. Boureau.
2022. SaFeRDialogues: Taking
feedback gracefully after conversa-
tional safety failures. ACL.
Uryupina, O., R. Artstein, A. Bristot,
F. Cavicchio, F. Delogu, K. J. Ro-
driguez, and M. Poesio. 2020. An-
notating a broad range of anaphoric
phenomena, in a variety of genres:
The ARRAU corpus. Natural Lan-
guage Engineering , 26(1):1–34.
Uszkoreit, J. 2017. Transformer: A
novel neural network architecture
for language understanding. Google
Research blog post, Thursday Au-
gust 31, 2017.van Deemter, K. and R. Kibble.
2000. On coreferring: corefer-
ence in MUC and related annotation
schemes. Computational Linguis-
tics, 26(4):629–637.
van der Maaten, L. and G. E. Hinton.
2008. Visualizing high-dimensional
data using t-SNE. JMLR , 9:2579–
2605.
van Rijsbergen, C. J. 1975. Information
Retrieval . Butterworths.
Vaswani, A., N. Shazeer, N. Parmar,
J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin. 2017.
Attention is all you need. NeurIPS .
Vauquois, B. 1968. A survey of for-
mal grammars and algorithms for
recognition and transformation in
machine translation. IFIP Congress
1968 .
Velichko, V . M. and N. G. Zagoruyko.
1970. Automatic recognition of
200 words. International Journal of
Man-Machine Studies , 2:223–234.
Velikovich, L., S. Blair-Goldensohn,
K. Hannan, and R. McDonald. 2010.
The viability of web-derived polarity
lexicons. NAACL HLT .
Vendler, Z. 1967. Linguistics in Philos-
ophy . Cornell University Press.
Verhagen, M., R. Gaizauskas,
F. Schilder, M. Hepple, J. Moszkow-
icz, and J. Pustejovsky. 2009. The
TempEval challenge: Identifying
temporal relations in text. Lan-
guage Resources and Evaluation ,
43(2):161–179.
Verhagen, M., I. Mani, R. Sauri,
R. Knippen, S. B. Jang, J. Littman,
A. Rumshisky, J. Phillips, and
J. Pustejovsky. 2005. Automating
temporal annotation with TARSQI.
ACL.
Versley, Y . 2008. Vagueness and ref-
erential ambiguity in a large-scale
annotated corpus. Research on
Language and Computation , 6(3-
4):333–353.
Vieira, R. and M. Poesio. 2000. An em-
pirically based system for process-
ing deﬁnite descriptions. Computa-
tional Linguistics , 26(4):539–593.
Vilain, M., J. D. Burger, J. Aberdeen,
D. Connolly, and L. Hirschman.
1995. A model-theoretic coreference
scoring scheme. MUC-6 .
Vintsyuk, T. K. 1968. Speech discrim-
ination by dynamic programming.
Cybernetics , 4(1):52–57. Origi-
nal Russian: Kibernetika 4(1):81-
88. 1968.
Vinyals, O., Ł. Kaiser, T. Koo,
S. Petrov, I. Sutskever, and G. Hin-
ton. 2015. Grammar as a foreign lan-
guage. NeurIPS .V oorhees, E. M. 1999. TREC-8 ques-
tion answering track report. Pro-
ceedings of the 8th Text Retrieval
Conference .
V oorhees, E. M. and D. K. Harman.
2005. TREC: Experiment and
Evaluation in Information Retrieval .
MIT Press.
V outilainen, A. 1999. Handcrafted
rules. In H. van Halteren, ed., Syn-
tactic Wordclass Tagging , 217–246.
Kluwer.
Vrande ˇci´c, D. and M. Kr ¨otzsch. 2014.
Wikidata: a free collaborative
knowledge base. CACM , 57(10):78–
85.
Wade, E., E. Shriberg, and P. J. Price.
1992. User behaviors affecting
speech recognition. ICSLP .
Wagner, R. A. and M. J. Fischer. 1974.
The string-to-string correction prob-
lem. Journal of the ACM , 21:168–
173.
Waibel, A., T. Hanazawa, G. Hin-
ton, K. Shikano, and K. J. Lang.
1989. Phoneme recognition using
time-delay neural networks. IEEE
Transactions on ASSP , 37(3):328–
339.
Walker, M. A. 2000. An applica-
tion of reinforcement learning to di-
alogue strategy selection in a spo-
ken dialogue system for email. JAIR ,
12:387–416.
Walker, M. A., J. C. Fromer, and S. S.
Narayanan. 1998a. Learning optimal
dialogue strategies: A case study of
a spoken dialogue agent for email.
COLING/ACL .
Walker, M. A., M. Iida, and S. Cote.
1994. Japanese discourse and the
process of centering. Computational
Linguistics , 20(2):193–232.
Walker, M. A., A. K. Joshi, and
E. Prince, eds. 1998b. Centering in
Discourse . Oxford University Press.
Wang, A., A. Singh, J. Michael, F. Hill,
O. Levy, and S. R. Bowman. 2018a.
Glue: A multi-task benchmark and
analysis platform for natural lan-
guage understanding. ICLR .
Wang, S. and C. D. Manning. 2012.
Baselines and bigrams: Simple,
good sentiment and topic classiﬁca-
tion. ACL.
Wang, W. and B. Chang. 2016. Graph-
based dependency parsing with bidi-
rectional LSTM. ACL.
Wang, Y ., S. Li, and J. Yang. 2018b.
Toward fast and accurate neural dis-
course segmentation. EMNLP .
Wang, Y ., S. Mishra, P. Alipoormo-
labashi, Y . Kordi, A. Mirzaei,
A. Naik, A. Ashok, A. S.
Dhanasekaran, A. Arunkumar,
D. Stap, E. Pathak, G. Kara-
manolakis, H. Lai, I. Purohit,

--- Page 581 ---
Bibliography 581
I. Mondal, J. Anderson, K. Kuz-
nia, K. Doshi, K. K. Pal, M. Pa-
tel, M. Moradshahi, M. Par-
mar, M. Purohit, N. Varshney,
P. R. Kaza, P. Verma, R. S. Puri,
R. Karia, S. Doshi, S. K. Sampat,
S. Mishra, S. Reddy A, S. Patro,
T. Dixit, and X. Shen. 2022. Super-
NaturalInstructions: Generaliza-
tion via declarative instructions on
1600+ NLP tasks. EMNLP .
Wang, Y ., R. Skerry-Ryan, D. Stan-
ton, Y . Wu, R. J. Weiss, N. Jaitly,
Z. Yang, Y . Xiao, Z. Chen, S. Ben-
gio, Q. Le, Y . Agiomyrgiannakis,
R. Clark, and R. A. Saurous.
2017. Tacotron: Towards end-to-end
speech synthesis. INTERSPEECH .
Watanabe, S., T. Hori, S. Karita,
T. Hayashi, J. Nishitoba, Y . Unno,
N. E. Y . Soplin, J. Heymann,
M. Wiesner, N. Chen, A. Renduch-
intala, and T. Ochiai. 2018. ESP-
net: End-to-end speech processing
toolkit. INTERSPEECH .
Weaver, W. 1949/1955. Translation. In
W. N. Locke and A. D. Boothe, eds,
Machine Translation of Languages ,
15–23. MIT Press. Reprinted from a
memorandum written by Weaver in
1949.
Webber, B. L. 1978. A Formal
Approach to Discourse Anaphora .
Ph.D. thesis, Harvard University.
Webber, B. L. 1983. So what can we
talk about now? In M. Brady and
R. C. Berwick, eds, Computational
Models of Discourse , 331–371. The
MIT Press.
Webber, B. L. 1991. Structure and os-
tension in the interpretation of dis-
course deixis. Language and Cogni-
tive Processes , 6(2):107–135.
Webber, B. L. and B. Baldwin. 1992.
Accommodating context change.
ACL.
Webber, B. L., M. Egg, and V . Kor-
doni. 2012. Discourse structure and
language technology. Natural Lan-
guage Engineering , 18(4):437–490.
Webber, B. L. 1988. Discourse deixis:
Reference to discourse segments.
ACL.
Webson, A. and E. Pavlick. 2022. Do
prompt-based models really under-
stand the meaning of their prompts?
NAACL HLT .
Webster, K., M. Recasens, V . Axel-
rod, and J. Baldridge. 2018. Mind
the GAP: A balanced corpus of gen-
dered ambiguous pronouns. TACL ,
6:605–617.
Wei, J., X. Wang, D. Schuurmans,
M. Bosma, F. Xia, E. Chi, Q. V .
Le, D. Zhou, et al. 2022. Chain-of-
thought prompting elicits reasoning
in large language models. NeurIPS ,
volume 35.Weischedel, R., M. Meteer,
R. Schwartz, L. A. Ramshaw, and
J. Palmucci. 1993. Coping with am-
biguity and unknown words through
probabilistic models. Computational
Linguistics , 19(2):359–382.
Weizenbaum, J. 1966. ELIZA – A
computer program for the study of
natural language communication be-
tween man and machine. CACM ,
9(1):36–45.
Weizenbaum, J. 1976. Computer Power
and Human Reason: From Judge-
ment to Calculation . W.H. Freeman
& Co.
Werbos, P. 1974. Beyond regression:
new tools for prediction and analy-
sis in the behavioral sciences . Ph.D.
thesis, Harvard University.
Werbos, P. J. 1990. Backpropagation
through time: what it does and how
to do it. Proceedings of the IEEE ,
78(10):1550–1560.
Weston, J., S. Chopra, and A. Bordes.
2015. Memory networks. ICLR
2015 .
Widrow, B. and M. E. Hoff. 1960.
Adaptive switching circuits. IRE
WESCON Convention Record , vol-
ume 4.
Wiebe, J. 1994. Tracking point of view
in narrative. Computational Linguis-
tics, 20(2):233–287.
Wiebe, J. 2000. Learning subjective ad-
jectives from corpora. AAAI .
Wiebe, J., R. F. Bruce, and T. P. O’Hara.
1999. Development and use of a
gold-standard data set for subjectiv-
ity classiﬁcations. ACL.
Wierzbicka, A. 1992. Semantics, Cul-
ture, and Cognition: University Hu-
man Concepts in Culture-Speciﬁc
Conﬁgurations . Oxford University
Press.
Wierzbicka, A. 1996. Semantics:
Primes and Universals . Oxford Uni-
versity Press.
Wilensky, R. 1983. Planning and
Understanding: A Computational
Approach to Human Reasoning .
Addison-Wesley.
Wilks, Y . 1973. An artiﬁcial intelli-
gence approach to machine transla-
tion. In R. C. Schank and K. M.
Colby, eds, Computer Models of
Thought and Language , 114–151.
W.H. Freeman.
Wilks, Y . 1975a. Preference semantics.
In E. L. Keenan, ed., The Formal Se-
mantics of Natural Language , 329–
350. Cambridge Univ. Press.
Wilks, Y . 1975b. A preferential,
pattern-seeking, semantics for natu-
ral language inference. Artiﬁcial In-
telligence , 6(1):53–74.Williams, A., N. Nangia, and S. Bow-
man. 2018. A broad-coverage chal-
lenge corpus for sentence under-
standing through inference. NAACL
HLT.
Williams, J. D., K. Asadi, and
G. Zweig. 2017. Hybrid code
networks: practical and efﬁcient
end-to-end dialog control with su-
pervised and reinforcement learning.
ACL.
Williams, J. D., A. Raux, and M. Hen-
derson. 2016. The dialog state track-
ing challenge series: A review. Dia-
logue & Discourse , 7(3):4–33.
Williams, J. D. and S. J. Young. 2007.
Partially observable markov deci-
sion processes for spoken dialog sys-
tems. Computer Speech and Lan-
guage , 21(1):393–422.
Wilson, T., J. Wiebe, and P. Hoffmann.
2005. Recognizing contextual polar-
ity in phrase-level sentiment analy-
sis.EMNLP .
Winograd, T. 1972. Understanding Nat-
ural Language . Academic Press.
Winston, P. H. 1977. Artiﬁcial Intelli-
gence . Addison Wesley.
Wiseman, S., A. M. Rush, and S. M.
Shieber. 2016. Learning global
features for coreference resolution.
NAACL HLT .
Wiseman, S., A. M. Rush, S. M.
Shieber, and J. Weston. 2015. Learn-
ing anaphoricity and antecedent
ranking features for coreference res-
olution. ACL.
Witten, I. H. and T. C. Bell. 1991.
The zero-frequency problem: Es-
timating the probabilities of novel
events in adaptive text compression.
IEEE Transactions on Information
Theory , 37(4):1085–1094.
Witten, I. H. and E. Frank. 2005. Data
Mining: Practical Machine Learn-
ing Tools and Techniques , 2nd edi-
tion. Morgan Kaufmann.
Wittgenstein, L. 1953. Philosoph-
ical Investigations. (Translated by
Anscombe, G.E.M.) . Blackwell.
Wolf, F. and E. Gibson. 2005. Rep-
resenting discourse coherence: A
corpus-based analysis. Computa-
tional Linguistics , 31(2):249–287.
Wolf, M. J., K. W. Miller, and F. S.
Grodzinsky. 2017. Why we should
have seen that coming: Comments
on Microsoft’s Tay “experiment,”
and wider implications. The ORBIT
Journal , 1(2):1–12.
Woods, W. A. 1978. Semantics and
quantiﬁcation in natural language
question answering. In M. Yovits,
ed.,Advances in Computers , 2–64.
Academic.

--- Page 582 ---
582 Bibliography
Woods, W. A., R. M. Kaplan, and B. L.
Nash-Webber. 1972. The lunar sci-
ences natural language information
system: Final report. Technical Re-
port 2378, BBN.
Woodsend, K. and M. Lapata. 2015.
Distributed representations for un-
supervised semantic role labeling.
EMNLP .
Wu, D. 1996. A polynomial-time algo-
rithm for statistical machine transla-
tion. ACL.
Wu, F. and D. S. Weld. 2007. Au-
tonomously semantifying Wiki-
pedia. CIKM-07 .
Wu, F. and D. S. Weld. 2010. Open
information extraction using Wiki-
pedia. ACL.
Wu, L., F. Petroni, M. Josifoski,
S. Riedel, and L. Zettlemoyer. 2020.
Scalable zero-shot entity linking
with dense entity retrieval. EMNLP .
Wu, S. and M. Dredze. 2019. Beto,
Bentz, Becas: The surprising cross-
lingual effectiveness of BERT.
EMNLP .
Wu, Y ., M. Schuster, Z. Chen, Q. V .
Le, M. Norouzi, W. Macherey,
M. Krikun, Y . Cao, Q. Gao,
K. Macherey, J. Klingner, A. Shah,
M. Johnson, X. Liu, Ł. Kaiser,
S. Gouws, Y . Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian,
N. Patil, W. Wang, C. Young,
J. Smith, J. Riesa, A. Rud-
nick, O. Vinyals, G. S. Corrado,
M. Hughes, and J. Dean. 2016.
Google’s neural machine translation
system: Bridging the gap between
human and machine translation.
ArXiv preprint arXiv:1609.08144.
Wundt, W. 1900. V¨olkerpsychologie:
eine Untersuchung der Entwick-
lungsgesetze von Sprache, Mythus,
und Sitte . W. Engelmann, Leipzig.
Band II: Die Sprache, Zweiter Teil.
Xu, A., E. Pathak, E. Wallace, S. Gu-
rurangan, M. Sap, and D. Klein.
2021. Detoxifying language models
risks marginalizing minority voices.
NAACL HLT .
Xu, J., D. Ju, M. Li, Y .-L. Boureau,
J. Weston, and E. Dinan. 2020.
Recipes for safety in open-
domain chatbots. ArXiv preprint
arXiv:2010.07079.
Xu, P., H. Saghir, J. S. Kang, T. Long,
A. J. Bose, Y . Cao, and J. C. K. Che-
ung. 2019. A cross-domain transfer-
able neural coherence model. ACL.
Xue, N., H. T. Ng, S. Pradhan,
A. Rutherford, B. L. Webber,
C. Wang, and H. Wang. 2016.
CoNLL 2016 shared task on mul-
tilingual shallow discourse parsing.
CoNLL-16 shared task .Xue, N. and M. Palmer. 2004. Calibrat-
ing features for semantic role label-
ing. EMNLP .
Yamada, H. and Y . Matsumoto. 2003.
Statistical dependency analysis with
support vector machines. IWPT-03 .
Yang, D., J. Chen, Z. Yang, D. Jurafsky,
and E. H. Hovy. 2019. Let’s make
your request more persuasive: Mod-
eling persuasive strategies via semi-
supervised neural nets on crowd-
funding platforms. NAACL HLT .
Yang, X., G. Zhou, J. Su, and C. L. Tan.
2003. Coreference resolution us-
ing competition learning approach.
ACL.
Yang, Y . and J. Pedersen. 1997. A com-
parative study on feature selection in
text categorization. ICML .
Yankelovich, N., G.-A. Levow, and
M. Marx. 1995. Designing
SpeechActs: Issues in speech user
interfaces. CHI-95 .
Yih, W.-t., M. Richardson, C. Meek,
M.-W. Chang, and J. Suh. 2016. The
value of semantic parse labeling for
knowledge base question answering.
ACL.
Young, S. J., M. Ga ˇsi´c, S. Keizer,
F. Mairesse, J. Schatzmann,
B. Thomson, and K. Yu. 2010. The
Hidden Information State model:
A practical framework for POMDP-
based spoken dialogue management.
Computer Speech & Language ,
24(2):150–174.
Younger, D. H. 1967. Recognition and
parsing of context-free languages in
time n3.Information and Control ,
10:189–208.
Yu, N., M. Zhang, and G. Fu. 2018.
Transition-based neural RST parsing
with implicit syntax features. COL-
ING.
Yu, Y ., Y . Zhu, Y . Liu, Y . Liu,
S. Peng, M. Gong, and A. Zeldes.
2019. GumDrop at the DISRPT2019
shared task: A model stacking ap-
proach to discourse unit segmenta-
tion and connective detection. Work-
shop on Discourse Relation Parsing
and Treebanking 2019 .
Zapirain, B., E. Agirre, L. M `arquez,
and M. Surdeanu. 2013. Selectional
preferences for semantic role classi-
ﬁcation. Computational Linguistics ,
39(3):631–663.
Zelle, J. M. and R. J. Mooney. 1996.
Learning to parse database queries
using inductive logic programming.
AAAI .
Zeman, D. 2008. Reusable tagset con-
version using tagset drivers. LREC .
Zens, R. and H. Ney. 2007. Efﬁcient
phrase-table representation for ma-
chine translation with applications toonline MT and speech translation.
NAACL-HLT .
Zettlemoyer, L. and M. Collins. 2005.
Learning to map sentences to log-
ical form: Structured classiﬁcation
with probabilistic categorial gram-
mars. Uncertainty in Artiﬁcial Intel-
ligence, UAI’05 .
Zettlemoyer, L. and M. Collins. 2007.
Online learning of relaxed CCG
grammars for parsing to logical
form. EMNLP/CoNLL .
Zhang, H., R. Sproat, A. H. Ng,
F. Stahlberg, X. Peng, K. Gorman,
and B. Roark. 2019. Neural models
of text normalization for speech ap-
plications. Computational Linguis-
tics, 45(2):293–337.
Zhang, R., C. N. dos Santos, M. Ya-
sunaga, B. Xiang, and D. Radev.
2018. Neural coreference resolution
with deep biafﬁne attention by joint
mention detection and mention clus-
tering. ACL.
Zhang, T., V . Kishore, F. Wu, K. Q.
Weinberger, and Y . Artzi. 2020.
BERTscore: Evaluating text gener-
ation with BERT. ICLR 2020 .
Zhang, Y ., V . Zhong, D. Chen, G. An-
geli, and C. D. Manning. 2017.
Position-aware attention and su-
pervised data improve slot ﬁlling.
EMNLP .
Zhao, H., W. Chen, C. Kit, and G. Zhou.
2009. Multilingual dependency
learning: A huge feature engineer-
ing method to semantic dependency
parsing. CoNLL .
Zhao, J., T. Wang, M. Yatskar, R. Cot-
terell, V . Ordonez, and K.-W. Chang.
2019. Gender bias in contextualized
word embeddings. NAACL HLT .
Zhao, J., T. Wang, M. Yatskar, V . Or-
donez, and K.-W. Chang. 2017. Men
also like shopping: Reducing gender
bias ampliﬁcation using corpus-level
constraints. EMNLP .
Zhao, J., T. Wang, M. Yatskar, V . Or-
donez, and K.-W. Chang. 2018a.
Gender bias in coreference reso-
lution: Evaluation and debiasing
methods. NAACL HLT .
Zhao, J., Y . Zhou, Z. Li, W. Wang,
and K.-W. Chang. 2018b. Learn-
ing gender-neutral word embed-
dings. EMNLP .
Zheng, J., L. Vilnis, S. Singh, J. D.
Choi, and A. McCallum. 2013.
Dynamic knowledge-base alignment
for coreference resolution. CoNLL .
Zhou, D., O. Bousquet, T. N. Lal,
J. Weston, and B. Sch ¨olkopf. 2004a.
Learning with local and global con-
sistency. NeurIPS .
Zhou, G., J. Su, J. Zhang, and
M. Zhang. 2005. Exploring var-
ious knowledge in relation extrac-
tion. ACL.

--- Page 583 ---
Bibliography 583
Zhou, J. and W. Xu. 2015a. End-to-
end learning of semantic role label-
ing using recurrent neural networks.
ACL.
Zhou, J. and W. Xu. 2015b. End-to-
end learning of semantic role label-
ing using recurrent neural networks.
ACL.
Zhou, K., K. Ethayarajh, D. Card, and
D. Jurafsky. 2022. Problems with
cosine as a measure of embedding
similarity for high frequency words.
ACL.
Zhou, K., J. Hwang, X. Ren, and
M. Sap. 2024. Relying on the un-
reliable: The impact of language
models’ reluctance to express uncer-
tainty. ACL.
Zhou, L., M. Ticrea, and E. H. Hovy.
2004b. Multi-document biography
summarization. EMNLP .
Zhou, Y ., A. I. Muresanu, Z. Han,
K. Paster, S. Pitis, H. Chan, and
J. Ba. 2023. Large language models
are human-level prompt engineers.
The Eleventh International Confer-
ence on Learning Representations .
Zhou, Y . and N. Xue. 2015. The Chi-
nese Discourse TreeBank: a Chinese
corpus annotated with discourse re-
lations. Language Resources and
Evaluation , 49(2):397–431.
Zhu, X. and Z. Ghahramani. 2002.
Learning from labeled and unlabeled
data with label propagation. Techni-
cal Report CMU-CALD-02, CMU.
Zhu, X., Z. Ghahramani, and J. Laf-
ferty. 2003. Semi-supervised learn-
ing using gaussian ﬁelds and har-
monic functions. ICML .
Zhu, Y ., R. Kiros, R. Zemel,
R. Salakhutdinov, R. Urtasun,
A. Torralba, and S. Fidler. 2015.
Aligning books and movies: To-
wards story-like visual explanations
by watching movies and reading
books. IEEE International Confer-
ence on Computer Vision .
Ziemski, M., M. Junczys-Dowmunt,
and B. Pouliquen. 2016. The United
Nations parallel corpus v1.0. LREC .

--- Page 585 ---
Subject Index
*?,9
+?,9
.wav format, 336
10-fold cross-validation, 69
!(derives), 389
ˆ,58
* (RE Kleene *), 7
+ (RE Kleene +), 7
. (RE any character), 7
$(RE end-of-line), 8
((RE precedence symbol),
8
[(RE character
disjunction), 6
\B(RE non
word-boundary), 8
\b(RE word-boundary), 8
](RE character
disjunction), 6
^(RE start-of-line), 8
[^](single-char negation),
6
4-gram, 37
4-tuple, 392
5-gram, 37
A-D conversion, 335
AAC, 32
AAE, 16
AB test, 353
ablating, 248
absolute position, 197
absolute temporal
expression, 452
abstract word, 485
accessible, 506
accessing a referent, 501
accomplishment
expressions, 450
accuracy, 366
achievement expressions,
450
acknowledgment speech
act, 312
activation, 133
activity expressions, 450
acute-eval, 325
ad hoc retrieval, 291
add gate, 172
add-k, 47
add-one smoothing, 45
adequacy, 280
adjacency pairs, 313
Adjectives, 364
adverb, 364
degree, 364
directional, 364
locative, 364
manner, 364
temporal, 364
Adverbs, 364
AED, 339
affective, 481
afﬁx, 24agent, as thematic role, 462
agglutinative
language, 267
AIFF ﬁle, 336
AISHELL-1, 334
aktionsart, 450
ALGOL, 409
algorithm
byte-pair encoding, 23
CKY , 397
minimum edit distance,
28
naive Bayes classiﬁer, 57
pointwise mutual
information, 114
semantic role labeling,
469
TextTiling, 544
Viterbi, 373
aligned, 249
alignment, 25,342
in ASR, 346
minimum cost, 28
string, 25
via minimum edit
distance, 28
Allen relations, 448
allocational harm, 126
ambiguity
amount of part-of-speech
in Brown corpus,
366
attachment, 396
coordination, 396
of referring expressions,
503
part-of-speech, 365
resolution of tag, 366
American Structuralism,
408
anaphor, 502
anaphora, 502
anaphoricity detector, 511
anchor texts, 520
anchors in regular
expressions, 8, 30
anisotropy, 234
antecedent, 502
Apple AIFF, 336
approximate
randomization, 71
arc eager, 423
arc standard, 417
argmax, 58
argumentation mining, 547
argumentation schemes,
548
argumentative relations,
547
argumentative zoning, 549
Aristotle, 362, 450
ARPA, 355
article (part-of-speech), 364
articulatory synthesis, 357aspect, 450
ASR, 331
conﬁdence, 320
association, 103
ATIS
corpus, 390
ATN, 478
ATRANS, 477
attachment ambiguity, 396
attention
cross-attention, 272
encoder-decoder, 272
history in transformers,
201
attention head, 188
attention mechanism, 179
Attribution (as coherence
relation), 534
augmentative
communication, 32
authorship attribution, 56
autoregressive generation,
167,207
Auxiliary, 365
B3,524
Babbage, C., 332
backoff, 48
in smoothing, 48
backprop, 147
backpropagation through
time, 161
backtrace
in minimum edit
distance, 29
backtranslation, 279
Backus-Naur form, 388
backward-looking center,
541
bag of words, 58, 59
in IR, 291
bakeoff, 355
speech recognition
competition, 355
barged in, 326
base model, 249
basic emotions, 482
batch training, 94
Bayes’ rule, 58
dropping denominator,
59, 372
Bayesian inference, 58
BDI, 329
beam search, 275,424
beam width, 275,424
Berkeley Restaurant
Project, 36
Bernoulli naive Bayes, 75
BERT
for affect, 497
best-worst scaling, 486
bias ampliﬁcation, 126
bias term, 79,133
bidirectional RNN, 169bigram, 34
binary branching, 394
binary naive Bayes, 63
binary tree, 394
BIO, 238,368
BIO tagging, 238
for NER, 238, 368
BIOES, 238, 368
bitext, 270
bits for measuring entropy,
49
blank in CTC, 342
BM25, 291,293
BNF (Backus-Naur form),
388
bootstrap, 73
bootstrap algorithm, 73
bootstrap test, 71
bootstrapping, 71
in IE, 441
bound pronoun, 504
BPE, 21
BPE, 23
bracketed notation, 391
bridging inference, 506
broadcast news
speech recognition of,
355
Brown corpus, 13
original tagging of, 384
byte-pair encoding, 21
calibrated, 290
CALLHOME, 333
Candide , 287
Cantonese, 267
capture group, 12
cascade
regular expression in
ELIZA, 13
case
sensitivity in regular
expression search, 5
case folding, 23
case frame, 463, 478
CAT, 263
cataphora, 504
CD (conceptual
dependency), 477
Centering Theory, 532, 540
centroid, 117
cepstrum
history, 355
CFG, seecontext-free
grammar
chain rule, 98,148
chain-of-thought, 254
channels in stored
waveforms, 336
chart parsing, 397
Chatbots, 309,321
chatbots, 4
CHiME, 333
Chinese
585

--- Page 586 ---
586 Subject Index
as verb-framed language,
267
words for brother, 266
Chomsky normal form, 394
Chomsky-adjunction, 395
chrF, 281
CIRCUS, 459
citation form, 102
Citizen Kane , 531
CKY algorithm, 387
claims, 547
class-based n-gram, 53
classiﬁer head, 235
clefts, 507
clitic, 19
origin of term, 362
closed book, 304
closed class, 363
cloze task, 226
cluster, 502
CNF, seeChomsky normal
form
Cocke-Kasami-Younger
algorithm, seeCKY
code switching, 16
coherence, 531
entity-based, 540
relations, 533
cohesion
lexical, 532, 544
ColBERT, 300
cold languages, 268
collection in IR, 291
commissive speech act, 312
common crawl, 211
common ground, 312, 328
Common nouns, 363
complementizers, 364
componential analysis, 476
compression, 335
Computational Grammar
Coder (CGC), 384
concatenation, 5, 30
conceptual dependency, 477
concrete word, 485
conditional generation, 204
conditional random ﬁeld,
376
conﬁdence, 285
ASR, 320
in relation extraction, 442
conﬁdence values, 442
conﬁguration, 417
confusion matrix, 66
Conjunctions, 364
connectionist, 157
connotation frame, 497
connotation frames, 479
connotations, 104,482
constative speech act, 312
constituency, 388
constituent, 388
titles which are not, 387
Constraint Grammar, 433
content planning, 319
context embedding, 122
context-free grammar, 388,
392, 407Chomsky normal form,
394
invention of, 409
non-terminal symbol,
389
productions, 388
rules, 388
terminal symbol, 389
weak and strong
equivalence, 394
contextual embeddings,
186,230
continued pretraining, 214
conversation, 309
conversation analysis, 313,
328
conversational implicature,
314
conversational speech, 333
convex, 90
coordination ambiguity, 396
copula, 365
CORAAL, 333
corefer, 501
coreference chain, 502
coreference resolution, 502
gender agreement, 508
Hobbs tree search
algorithm, 528
number agreement, 507
person agreement, 508
recency preferences, 508
selectional restrictions,
509
syntactic (“binding”)
constraints, 508
verb semantics, 509
corpora, 13
corpus, 13
ATIS, 390
Broadcast news, 355
Brown, 13, 384
ﬁsher, 355
LOB, 384
regular expression
searching inside, 5
Switchboard, 14,333,
335
TimeBank, 451
Wall Street Journal, 355
correction act detection,
319
cosine
as a similarity metric,
110
cost function, 87
count nouns, 363
counters, 30
counts
treating low as zero, 379
CRF, 376
compared to HMM, 376
inference, 380
Viterbi inference, 380
CRFs
learning, 381
cross-attention, 272
cross-brackets, 406cross-entropy, 51
cross-entropy loss, 88,145
cross-validation, 69
10-fold, 69
crowdsourcing, 485
CTC, 341
datasheet, 16
dative alternation, 463
debiasing, 127
decision boundary, 80,136
decoder-only model, 200
decoding, 207,372
Viterbi, 372
deep
neural networks, 132
deep learning, 132
deﬁnite reference, 504
degree adverb, 364
delexicalize, 320
demonstrations, 246
denoising, 226
dependency
grammar, 411
dependency tree, 414
dependent, 412
derivation
direct (in a formal
language), 392
syntactic, 389, 389, 392,
392
Det, 388
determiner, 364, 388
Determiners, 364
development set, 38
development test set, 69
development test set
(dev-test), 39
devset, seedevelopment
test set (dev-test), 69
DFT, 338
dialogue, 309
dialogue act
correction, 319
Dialogue acts, 318
dialogue policy, 319
dialogue systems, 309
design, 325
diathesis alternation, 463
diff program, 30
digit recognition, 332
digital divide, 263
digitization, 335
dilated convolutions, 352
dimension, 107
diphthong
origin of term, 362
direct derivation (in a
formal language),
392
directional adverb, 364
directive speech act, 312
disambiguation
in parsing, 403
syntactic, 397
discount, 46, 47, 49
discounting, 45, 46
discourse, 531segment, 534
discourse connectives, 535
discourse deixis, 503
discourse model, 501
discourse parsing, 536
discourse-new, 505
discourse-old, 505
discovery procedure, 408
discrete Fourier transform,
338
discriminative model, 78
disﬂuency, 14
disjunction, 30
pipe in regular
expressions as, 8
square braces in regular
expression as, 6
dispreferred response, 330
distant supervision, 443
distributional hypothesis,
101
distributional similarity,
408
divergences between
languages in MT,
265
document
in IR, 291
document frequency, 112
document vector, 117
domination in syntax, 389
dot product, 79,110
dot-product attention, 180
Dragon Systems, 355
dropout, 151
duration
temporal expression, 452
dynamic programming, 26
and parsing, 397
Viterbi as, 373
dynamic time warping, 355
edge-factored, 426
edit distance
minimum algorithm, 27
EDU, 534
effect size, 70
efﬁciency costs, 317
Elaboration (as coherence
relation), 533
ELIZA, 4
implementation, 13
sample conversation, 12
Elman Networks, 158
ELMo
for affect, 497
EM
for deleted interpolation,
48
embedding layer, 154
embeddings, 105
cosine for similarity, 110
skip-gram, learning, 120
sparse, 110
tf-idf, 112
word2vec, 117
emission probabilities, 370
EmoLex, 484

--- Page 587 ---
Subject Index 587
emotion, 482
Encoder-decoder, 175
encoder-decoder attention,
272
end-to-end training, 167
endpointing, 312
English
lexical differences from
French, 267
simpliﬁed grammar
rules, 390
verb-framed, 267
entity dictionary, 379
entity grid, 542
Entity linking, 520
entity linking, 502
entity-based coherence, 540
entropy, 49
and perplexity, 49
cross-entropy, 51
per-word, 50
rate, 50
relative, 474
error backpropagation, 147
ESPnet, 356
ethos, 547
Euclidean distance
in L2 regularization, 95
Eugene Onegin , 52
Euler’s formula, 338
Europarl, 270
evalb, 406
evaluating parsers, 405
evaluation
10-fold cross-validation,
69
AB test, 353
comparing models, 41
cross-validation, 69
development test set, 39,
69
devset, 69
devset or development
test set, 39
extrinsic, 38
ﬂuency in MT, 280
Matched-Pair Sentence
Segment Word Error
(MAPSSWE), 347
mean opinion score, 353
most frequent class
baseline, 366
MT, 280
named entity recognition,
240, 381
of n-gram, 38
of n-grams via
perplexity, 40
pseudoword, 476
relation extraction, 446
test set, 39
training on the test set, 39
training set, 39
TTS, 353
event coreference, 503
event extraction, 435,446
events, 450Evidence (as coherence
relation), 533
evoking a referent, 501
execution accuracy, 256
expansion, 390, 391
expletive, 507
explicit conﬁrmation, 319
extraposition, 507
extrinsic evaluation, 38
F (for F-measure), 67
F-measure, 67
F-measure
in NER, 240, 381
factoid question, 289
Faiss, 301
false negatives, 10
false positives, 10
Farsi, verb-framed, 267
fast Fourier transform, 338,
355
fasttext, 123
FASTUS, 457
feature cutoff, 379
feature interactions, 82
feature selection
information gain, 76
feature template, 421
feature templates, 82
part-of-speech tagging,
378
feature vectors, 334
Federalist papers, 75
feedforward network, 138
fenceposts, 398
few-shot, 246
FFT, 338, 355
ﬁle format, .wav, 336
ﬁlled pause, 14
ﬁller, 14
ﬁnetuning, 213,234
ﬁnetuning;supervsed, 249
ﬁrst-order co-occurrence,
124
ﬂuency, 280
in MT, 280
fold (in cross-validation),
69
forget gate, 172
formal language, 391
formant synthesis, 357
forward inference, 153
forward-looking centers,
541
Fosler, E., see
Fosler-Lussier, E.
foundation model, 221
fragment of word, 14
frame, 336
semantic, 467
frame elements, 467
FrameNet, 466
frames, 314
free word order, 411
Freebase, 437
freeze, 155,214
French, 265
Frump, 459fully-connected, 138
function word, 363, 383
fusion language, 268
Gaussian
prior on weights, 96
gazetteer, 379
General Inquirer, 64,484
generalize, 95
generalized semantic role,
464
generation
of sentences to test a
CFG grammar, 390
generative AI, 204
generative grammar, 391
generative model, 78
generative models, 59
generator, 389
generics, 507
German, 265
given-new, 506
Godzilla, speaker as, 472
gold labels, 66
gradient, 90
Grammar
Constraint, 433
Head-Driven Phrase
Structure (HPSG),
406
Link, 433
grammar
binary branching, 394
checking, 387
equivalence, 394
generative, 391
inversion transduction,
287
grammatical function, 412
grammatical relation, 412
grammatical sentences, 391
greedy decoding, 206
greedy RE patterns, 9
grep, 5, 5, 30
Gricean maxims, 314
grounding, 312
GUS, 314
hallucinate, 290
hallucination, 219
Hamilton, Alexander, 75
Hamming, 337
Hansard, 287
hanzi, 20
harmonic mean, 67
head, 188,198,406,412
ﬁnding, 406
Head-Driven Phrase
Structure Grammar
(HPSG), 406
Heaps’ Law, 15
Hearst patterns, 438
held-out, 48
Herdan’s Law, 15
hidden, 370
hidden layer, 138
as representation of
input, 139hidden units, 138
Hindi, 265
Hindi, verb-framed, 267
HKUST, 334
HMM, 370
formal deﬁnition of, 370
history in speech
recognition, 355
initial distribution, 370
observation likelihood,
370
observations, 370
simplifying assumptions
for POS tagging,
372
states, 370
transition probabilities,
370
Hobbs algorithm, 528
Hobbs tree search algorithm
for pronoun
resolution, 528
homonymy, 231
hot languages, 268
Hungarian
part-of-speech tagging,
382
hybrid, 356
hyperarticulation, 319
hypernym, 437
lexico-syntactic patterns
for, 438
hyperparameter, 92
hyperparameters, 152
IBM Models, 287
IBM Thomas J. Watson
Research Center,
53, 355
idf,113
idf term weighting, 113,
292
immediately dominates,
389
implicature, 314
implicit argument, 479
implicit conﬁrmation, 320
in-context learning, 247
indeﬁnite reference, 504
induction heads, 247
inference-based learning,
429
infoboxes, 437
information
structure, 505
status, 505
information extraction (IE),
435
bootstrapping, 441
information gain, 76
for feature selection, 76
Information retrieval, 108,
290
information retrieval, 290
initiative, 313
inner product, 110
instance, word, 14

--- Page 588 ---
588 Subject Index
Institutional Review Board,
327
Instruction tuning, 249
intent determination, 316
intercept, 79
Interjections, 364
interpolated precision, 296
interpolation
in smoothing, 48
interpretable, 98
interval algebra, 448
intrinsic evaluation, 38
inversion transduction
grammar (ITG), 287
inverted index, 295
IO, 238, 368
IOB tagging
for temporal expressions,
453
IR,290
idf term weighting, 113,
292
term weighting, 291
vector space model, 107
IRB, 327
is-a, 437
ISO 8601, 454
isolating language, 267
iSRL, 479
ITG (inversion transduction
grammar), 287
Japanese, 265, 267
Jay, John, 75
joint intention, 328
Kaldi, 356
KBP, 459
KenLM, 38,53
key,188
KL divergence, 474
Klatt formant synthesizer,
357
Kleene *, 7
sneakiness of matching
zero things, 7
Kleene +, 7
knowledge claim, 549
knowledge graphs, 435
Kullback-Leibler
divergence, 474
KV cache, 217
L1 regularization, 95
L2 regularization, 95
labeled precision, 405
labeled recall, 405
language
identiﬁcation, 354
universal, 264
language id, 56
language model, 32
language model:coined by,
53
language modeling head,
198
Laplace smoothing, 45for PMI, 116
lasso regression, 96
latent semantic analysis,
130
layer norm, 191
LDC, 19
learning rate, 90
lemma, 15,102
versus wordform, 15
Lemmatization, 24
lemmatization, 5
Levenshtein distance, 26
lexical
category, 389
cohesion, 532, 544
gap, 267
semantics, 102
trigger, in IE, 452
lexico-syntactic pattern,
438
lexicon, 388
LibriSpeech, 333
light verbs, 447
likelihood, 59
linear chain CRF, 376, 377
linear classiﬁers, 60
linear interpolation for
n-grams, 48
linearly separable, 136
Linguistic Data
Consortium, 19
Linguistic Discourse
model, 550
Link Grammar, 433
List (as coherence relation),
534
listen attend and spell, 339
LIWC, 64,485
LM, 32
LOB corpus, 384
localization, 263
location-based attention,
351
locative, 364
locative adverb, 364
log
why used for
probabilities, 37
why used to compress
speech, 336
log likelihood ratio, 493
log odds ratio, 493
log probabilities, 37, 37
logistic function, 79
logistic regression, 77
conditional maximum
likelihood
estimation, 88
Gaussian priors, 96
learning in, 87
regularization, 96
relation to neural
networks, 140
logit, 80,199
logit lens, 200
logos, 547
long short-term memory,
171lookahead in regex, 13
LoRA, 218
loss, 87
low frame rate, 340
LPC (Linear Predictive
Coding), 355
LSI, seelatent semantic
analysis
LSTM, 385
LUNAR, 307
machine learning
for NER, 382
textbooks, 75, 100
machine translation, 263
macroaveraging, 68
Madison, James, 75
MAE, 16
Mandarin, 265
Manhattan distance
in L1 regularization, 95
manner adverb, 364
Markov, 34
assumption, 34
Markov assumption, 369
Markov chain, 52, 369
formal deﬁnition of, 370
initial distribution, 370
n-gram as, 369
states, 370
transition probabilities,
370
Markov model, 34
formal deﬁnition of, 370
history, 53
Marx, G., 387
Masked Language
Modeling, 226
mass nouns, 363
maxent, 100
maxim, Gricean, 314
maximum entropy, 100
maximum spanning tree,
426
Mayan, 267
MBR, 277
McNemar’s test, 348
mean
element-wise, 167
mean average precision,
297
mean opinion score, 353
mean reciprocal rank, 305
mechanical indexing, 129
Mechanical Turk, 332
mel, 338
memory networks, 201
mention detection, 510
mention-pair, 513
mentions, 501
MERT, for training in MT,
287
MeSH (Medical Subject
Headings), 57
Message Understanding
Conference, 457
METEOR, 288
metonymy, 530microaveraging, 68
Microsoft .wav format, 336
mini-batch, 94
Minimum Bayes risk, 277
minimum edit distance, 25,
25, 373
example of, 28
for speech recognition
evaluation, 346
MINIMUM EDIT DISTANCE ,
28
minimum edit distance
algorithm, 27
Minimum Error Rate
Training, 287
MLE
for n-grams, 35
for n-grams, intuition, 35
MLM, 226
MLP, 138
MMLU, 258,304
modal verb, 365
model alignment, 249
model card, 74
morpheme, 24
MOS (mean opinion score),
353
Moses, Michelangelo statue
of, 309
Moses, MT toolkit, 287
MRR, 305
MS MARCO, 303
MT, 263
divergences, 265
post-editing, 263
mu-law, 336
MUC, 457, 459
MUC F-measure, 524
multi-head attention, 189
multi-hop, 303
multi-layer perceptrons,
138
multinomial logistic
regression, 84
multinomial naive Bayes,
57
multinomial naive Bayes
classiﬁer, 57
multiword expressions, 130
MWE, 130
n-best list, 341
n-gram, 32,34
add-one smoothing, 45
as approximation, 34
as generators, 43
as Markov chain, 369
equation for, 34
example of, 36, 37
for Shakespeare, 43
history of, 53
interpolation, 48
KenLM, 38,53
logprobs in, 37
normalizing, 36
parameter estimation, 35
sensitivity to corpus, 43
smoothing, 45

--- Page 589 ---
Subject Index 589
SRILM, 53
test set, 38
training set, 38
naive Bayes
multinomial, 57
simplifying assumptions,
59
naive Bayes assumption, 59
naive Bayes classiﬁer
use in text categorization,
57
named entity, 237,362,367
list of types, 237, 367
named entity recognition,
237,367
natural language inference,
236
Natural Questions, 303
negative log likelihood loss,
88,97,146
NER, 237,367
neural networks
relation to logistic
regression, 140
newline character, 10
Next Sentence Prediction,
228
NIST for MT evaluation,
288
noisy-or, 442
NomBank, 466
Nominal, 388
non-capturing group, 12
non-greedy, 9
non-standard words, 349
non-stationary process, 336
non-terminal symbols, 389,
390
normal form, 394, 394
normalization
temporal, 453
word, 23
normalization of
probabilities, 35
normalize, 83
normalizing, 140
noun
abstract, 363
common, 363
count, 363
mass, 363
proper, 363
noun phrase, 388
constituents, 388
Nouns, 363
NP,388, 390
nucleus, 533
null hypothesis, 70
Nyquist frequency, 335
observation likelihood
role in Viterbi, 374
one-hot vector, 153,197
open book, 304
open class, 363
open information
extraction, 444
operation list, 25operator precedence, 8, 9
optionality
use of ? in regular
expressions for, 7
output gate, 172
overﬁtting, 95
p-value, 71
Paired, 71
parallel corpus, 270
parallel distributed
processing, 157
parallelogram model, 124
parameter-efﬁcient ﬁne
tuning, 217
parse tree, 389, 391
PARSEV AL, 405
parsing
ambiguity, 395
CKY , 397
CYK, seeCKY
evaluation, 405
relation to grammars,
392
syntactic, 387
well-formed substring
table, 409
part of speech
as used in CFG, 389
part-of-speech
adjective, 364
adverb, 364
closed class, 363
interjection, 364
noun, 363
open class, 363
particle, 364
subtle distinction
between verb and
noun, 364
verb, 364
part-of-speech tagger
PARTS ,384
TAGGIT, 384
Part-of-speech tagging, 365
part-of-speech tagging
ambiguity and, 365
amount of ambiguity in
Brown corpus, 366
and morphological
analysis, 382
feature templates, 378
history of, 384
Hungarian, 382
Turkish, 382
unknown words, 376
particle, 364
PARTS tagger, 384
parts of speech, 362
pathos, 547
pattern, regular expression,
5
PCM (Pulse Code
Modulation), 336
PDP, 157
PDTB, 535
PEFT, 217Penn Discourse TreeBank,
535
Penn Treebank, 393
tagset, 365, 365
Penn Treebank
tokenization, 19
per-word entropy, 50
perceptron, 135
period disambiguation, 82
perplexity, 40,52
as weighted average
branching factor, 41
deﬁned via
cross-entropy, 52
perplexity:coined by, 53
personal pronoun, 364
persuasion, 548
phrasal verb, 364
phrase-based translation,
287
phrase-structure grammar,
388
PII,212
pipe, 8
planning
and speech acts, 329
shared plans, 328
pleonastic, 507
Pointwise mutual
information, 114
polysynthetic language, 267
pooling, 143,167
max, 167
mean, 167
Porter stemmer, 24
POS, 362
positional embeddings, 197
possessive pronoun, 364
post-editing, 263
post-training, 249
postings, 295
postposition, 265
Potts diagram, 492
PP, 390
PP-attachment ambiguity,
396
PPMI, 115
precedence, 8
precedence, operator, 8
Precision, 67
precision
for MT evaluation, 288
in NER, 240, 381
precision-recall curve, 296
premises, 547
prepositional phrase
constituency, 390
prepositions, 364
presequences, 313
pretraining, 145,203
primitive decomposition,
476
principle of contrast, 103
prior probability, 59
pro-drop languages, 268
probabilistic context-free
grammars, 409
productions, 388projective, 414
prompt, 243
prompt engineering, 243
pronoun, 364
bound, 504
demonstrative, 505
non-binary, 508
personal, 364
possessive, 364
wh-, 364
PropBank, 465
proper noun, 363
PROTO -AGENT ,464
PROTO -PATIENT ,464
pseudoword, 476
PTRANS, 477
punctuation
for numbers
cross-linguistically,
19
for sentence
segmentation, 25
tokenization, 19
treated as words, 13
treated as words in LM,
44
quantization, 335
query, 188,291
in IR, 291
question
factoid, 289
question answering
factoid questions, 289
Radio Rex, 331
RAG, 290,302
random sampling, 208
range, regular expression, 6
ranking, 281
rarefaction, 335
RDF, 437
RDF triple, 437
Read speech, 333
reading comprehension,
304
Reason (as coherence
relation), 533
Recall, 67
recall
for MT evaluation, 288
in NER, 240, 381
rectangular, 336
reference
bound pronouns, 504
cataphora, 504
deﬁnite, 504
generics, 507
indeﬁnite, 504
reference point, 449
referent, 501
accessing of, 501
evoking of, 501
referential density, 268
reﬂexive, 508
regex
regular expression, 5
register in regex, 12

--- Page 590 ---
590 Subject Index
regression
lasso, 96
ridge, 96
regular expression, 5, 29, 30
substitutions, 11
regularization, 95
rejection
conversation act, 320
relatedness, 103
relation extraction, 435
relative
temporal expression, 452
relative entropy, 474
relative frequency, 35
relevance, 314
relexicalize, 321
ReLU, 134
reporting events, 447
representation learning, 101
representational harm, 127
representational harms, 73
rescore, 341
residual stream, 190
resolve, 366
Resource Management, 355
retrieval-augmented
generation, 302
ReVerb, 445
rewrite, 389
Rhetorical Structure
Theory, seeRST
Riau Indonesian, 364
ridge regression, 96
RLHF, 325
RNN-T, 345
role-ﬁller extraction, 457
Rosebud, sled named, 531
row vector, 108
RST, 533
TreeBank, 535, 550
rules
context-free, 388
context-free, expansion,
389
context-free, sample, 390
Russian
fusion language, 268
verb-framed, 267
Sas start symbol in CFG,
390
salience, in discourse
model, 506
Sampling, 42
sampling
of analog waveform, 335
rate, 335
satellite, 267,533
satellite-framed language,
267
saturated, 135
scaling laws, 216
SCISOR, 459
sclite, 347
sclite package, 30
script
Schankian, 467
scripts, 456SDRT (Segmented
Discourse
Representation
Theory), 550
search engine, 290
search tree, 274
second-order
co-occurrence, 124
seed pattern in IE, 441
seed tuples, 441
segmentation
sentence, 25
word, 18
selectional association, 475
selectional preference
strength, 474
selectional preferences
pseudowords for
evaluation, 476
selectional restriction, 472
representing with events,
473
violations in WSD, 474
self-supervision, 118,163,
210
self-training, 155
semantic drift in IE, 442
semantic feature, 130
semantic ﬁeld, 103
semantic frame, 104
semantic relations in IE,
436
table, 437
semantic role, 462, 462,
464
Semantic role labeling, 468
semantics
lexical, 102
sense
word, 231
sentence
error rate, 347
segmentation, 25
sentence realization, 320
sentence segmentation, 5
sentence separation, 176
SentencePiece, 270
sentiment, 104
origin of term, 500
sentiment analysis, 56
sentiment lexicons, 64
SentiWordNet, 490
sequence labeling, 362
SFT, 249
SGNS, 117
Shakespeare
n-gram approximations
to, 43
shallow discourse parsing,
539
shared plans, 328
side sequence, 313
sigmoid, 79,133
signiﬁcance test
MAPSSWE for ASR,
347
McNemar’s, 348
similarity, 103cosine, 110
singleton, 502
singular they, 508
skip-gram, 117
slot error rate, 317
slot ﬁlling, 316,459
slots, 314
smoothing, 45, 45
add-one, 45
discounting, 46
interpolation, 48
Laplace, 45
linear interpolation, 48
softmax, 85,140
SOV language, 265
spam detection, 56,64
span, 403
Speaker diarization, 353
speaker identiﬁcation, 354
speaker recognition, 354
speaker veriﬁcation, 354
speech
telephone bandwidth,
335
speech acts, 312
speech recognition
architecture, 332, 339
history of, 354
speech synthesis, 332
split-half reliability, 487
SRILM, 53
SRL, 468
Stacked RNNs, 168
standardize, 82
start symbol, 389
states, 450
static embeddings, 118
stationary process, 336
stationary stochastic
process, 51
statistical MT, 287
statistical signiﬁcance
MAPSSWE for ASR,
347
McNemar’s test, 348
statistically signiﬁcant, 71
stative expressions, 450
stem, 24
Stemming, 5
stemming, 24
stop list, 294
stop words, 61
streaming, 345
stride, 336
structural ambiguity, 395
stupid backoff, 49
subdialogue, 313
subjectivity, 481,500
substitutability, 408
substitution operator
(regular
expressions), 11
subword tokens, 19
subwords, 21
supervised ﬁnetuning, 249
supervised machine
learning, 57
SVD, 130SVO language, 265
Swedish, verb-framed, 267
Switchboard, 333
Switchboard Corpus, 14,
333, 335
synchronous grammar, 287
synonyms, 103
syntactic disambiguation,
397
syntax, 387
origin of term, 362
TAC KBP, 438
Tacotron2, 351
TACRED dataset, 437
TAGGIT, 384
tagset
Penn Treebank, 365, 365
table of Penn Treebank
tags, 365
Tamil, 267
tanh, 134
target embedding, 122
task error rate, 317
Tay, 326
teacher forcing, 164,178,
210, 274
technai, 362
telephone-bandwidth
speech, 335
telic, 450
temperature sampling, 209
template, 245
template ﬁlling, 435,456
template recognition, 456
template, in IE, 456
templates, 244
temporal adverb, 364
temporal anchor, 455
temporal expression
absolute, 452
metaphor for, 449
relative, 452
temporal logic, 447
temporal normalization,
453
term
in IR, 291
weight in IR, 291
term frequency, 112
term weight, 291
term-document matrix, 106
term-term matrix, 109
terminal symbol, 389
test set, 38
development, 39
how to choose, 39
text categorization, 56
bag-of-words
assumption, 58
naive Bayes approach, 57
unknown words, 61
text normalization, 4,17
text summarization, 205
text-to-speech, 332
TextTiling, 544
tf-idf, 113
The Pile, 211

--- Page 591 ---
Subject Index 591
thematic grid, 463
thematic role, 462
and diathesis alternation,
463
examples of, 462
problems, 464
theme, 462
theme, as thematic role, 462
TimeBank, 451
tokenization, 4
sentence, 25
word, 18
Top-k sampling, 208
top-p sampling, 209
topic models, 104
toxicity detection, 74
training oracle, 419
training set, 38
cross-validation, 69
how to choose, 39
transcription
of speech, 331
reference, 346
transduction grammars, 287
transfer learning, 223
Transformations and
Discourse Analysis
Project (TDAP),
384
transition probability
role in Viterbi, 374
transition-based, 416
translation
divergences, 265
TREC, 308
treebank, 392
trigram, 37
TTS, 332
Turk, Mechanical, 332
Turkishagglutinative, 267
part-of-speech tagging,
382
turns, 311
TyDi QA, 304
typed dependency structure,
411
types
word, 14
typology, 265
linguistic, 265
unembedding, 199
ungrammatical sentences,
391
unigram
name of tokenization
algorithm, 270
unit production, 397
unit vector, 111
Universal Dependencies,
413
universal, linguistic, 264
Unix, 5
unknown words
in part-of-speech
tagging, 376
in text categorization, 61
user-centered design, 325
utterance, 14
value, 188
value sensitive design, 326
vanishing gradient, 135
vanishing gradients, 171
Vauquois triangle, 286
vector, 107,133
vector length, 110
Vector semantics, 105vector semantics, 101
vector space, 107
vector space model, 107
verb
copula, 365
modal, 365
phrasal, 364
verb alternations, 463
verb phrase, 390
verb-framed language, 267
Verbs, 364
Vietnamese, 267
Viterbi
and beam search, 274
Viterbi algorithm, 26, 373
inference in CRF, 380
VITERBI ALGORITHM , 373
vocoder, 349
vocoding, 349
voice user interface, 325
VSO language, 265
wake word, 353
Wall Street Journal
Wall Street Journal
speech recognition of,
355
warping, 355
waveﬁle format, 336
WaveNet, 351
Wavenet, 351
weight tying, 165,199
well-formed substring
table, 409
WFST, 409
wh-pronoun, 364
wikiﬁcation, 520
wildcard, regular
expression, 7Winograd Schema, 525
Wizard-of-Oz system, 325
word
boundary, regular
expression notation,
8
closed class, 363
deﬁnition of, 13
error rate, 334, 346
fragment, 14
function, 363, 383
open class, 363
punctuation as, 13
tokens, 14
types, 14
word normalization, 23
word segmentation, 18,20
word sense, 231
word sense disambiguation,
232,seeWSD
word shape, 378
word tokenization, 18
word-word matrix, 109
word2vec, 117
wordform, 15
and lemma, 102
versus lemma, 15
WordNet, 231
wordpiece, 269
WSD, 232
Yonkers Racetrack, 49
Yupik, 267
z-score, 82
zero anaphor, 505
zero-shot, 246
zero-width, 13
zeros, 45

